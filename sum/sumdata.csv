video_url,time_range,transcript,note_url,markdown
OoUX-nOEjG0?si=BX-IsfxNktMEOnA3,,"Okay, so welcome to lecture two of CS231N. On Tuesday we, just recall,
we, sort of, gave you the big picture view of
what is computer vision, what is the history, and a little bit of the
overview of the class. And today, we're really going
to dive in, for the first time, into the details. And we'll start to see,
in much more depth, exactly how some of
these learning algorithms actually work in practice. So, the first lecture of the class is probably, sort of, the
largest big picture vision. And the majority of the
lectures in this class will be much more detail orientated, much more focused on
the specific mechanics, of these different algorithms. So, today we'll see our
first learning algorithm and that'll be really exciting, I think. But, before we get to that, I wanted to talk about a couple
of administrative issues. One, is Piazza. So, I saw it when I checked yesterday, it seemed like we had maybe 500 students signed up on Piazza. Which means that there
are several hundred of you who are not yet there. So, we really want Piazza
to be the main source of communication between the
students and the core staff. So, we've gotten a lot of
questions to the staff list about project ideas or questions
about midterm attendance or poster session attendance. And, any, sort of, questions like that should really go to Piazza. You'll probably get answers
to your questions faster on Piazza, because all the
TAs are knowing to check that. And it's, sort of, easy
for emails to get lost in the shuffle if you just
send to the course list. It's also come to my attention
that some SCPD students are having a bit of a hard
time signing up for Piazza. SCPD students are supposed to receive a @stanford.edu email address. So, once you get that email address, then you can use the Stanford
email to sign into Piazza. Probably that doesn't
affect those of you who are sitting in the room right now, but, for those students listening on SCPD. The next administrative issue
is about assignment one. Assignment one will be up later today, probably sometime this afternoon, but I promise, before
I go to sleep tonight, it'll be up. But, if you're getting a little bit antsy and really want to start
working on it right now, then you can look at last year's version of assignment one. It'll be pretty much the same content. We're just reshuffling it
a little bit to make it, like, for example, upgrading
to work with Python 3, rather than Python 2.7. And some of these minor cosmetic changes, but the content of the
assignment will still be the same as last year. So, in this assignment you'll
be implementing your own k-nearest neighbor classifier, which we're going to talk
about in this lecture. You'll also implement several
different linear classifiers, including the SVM and Softmax, as well as a simple
two-layer neural network. And we'll cover all this content over the next couple of lectures. So, all of our assignments
are using Python and NumPy. If you aren't familiar
with Python or NumPy, then we have written a
tutorial that you can find on the course website to
try and get you up to speed. But, this is, actually, pretty important. NumPy lets you write these
very efficient vectorized operations that let you do
quite a lot of computation in just a couple lines of code. So this is super important for pretty much all aspects of numerical
computing and machine learning and everything like that, is efficiently implementing
these vectorized operations. And you'll get a lot of practice with this on the first assignment. So, for those of you who
don't have a lot of experience with Matlab or NumPy or
other types of vectorized tensor computation, I recommend
that you start looking at this assignment pretty early and also, read carefully
through the tutorial. The other thing I wanted to talk about is that we're happy to announce that we're officially supported
through Google Cloud for this class. So, Google Cloud is somewhat
similar to Amazon AWS. You can go and start virtual
machines up in the cloud. These virtual machines can have GPUs. We're working on the tutorial
for exactly how to use Google Cloud and get it to
work for the assignments. But our intention is that
you'll be able to just download some image, and it'll be very seamless for you to work on the assignments on one of these instances on the cloud. And because Google has, very generously, supported this course, we'll be able to distribute to each of you coupons that let you use
Google Cloud credits for free for the class. So you can feel free to use
these for the assignments and also for the course projects when you want to start using
GPUs and larger machines and whatnot. So, we'll post more details about that, probably, on Piazza later today. But, I just wanted to mention, because I know there had
been a couple of questions about, can I use my laptop? Do I have to run on corn? Do I have to, whatever? And the answer is that,
you'll be able to run on Google Cloud and we'll provide
you some coupons for that. Yeah, so, those are, kind of, the
major administrative issues I wanted to talk about today. And then, let's dive into the content. So, the last lecture
we talked a little bit about this task of image classification, which is really a core
task in computer vision. And this is something
that we'll really focus on throughout the course of the class. Is, exactly, how do we work on this
image classification task? So, a little bit more concretely, when you're doing image classification, your system receives some input image, which is this cute cat in this example, and the system is aware
of some predetermined set of categories or labels. So, these might be, like,
a dog or a cat or a truck or a plane, and there's some
fixed set of category labels, and the job of the computer
is to look at the picture and assign it one of these
fixed category labels. This seems like a really easy problem, because so much of your own
visual system in your brain is hardwired to doing these, sort of, visual recognition tasks. But this is actually a
really, really hard problem for a machine. So, if you dig in and
think about, actually, what does a computer see
when it looks at this image, it definitely doesn't get
this holistic idea of a cat that you see when you look at it. And the computer really
is representing the image as this gigantic grid of numbers. So, the image might be something
like 800 by 600 pixels. And each pixel is
represented by three numbers, giving the red, green, and
blue values for that pixel. So, to the computer, this is just a gigantic grid of numbers. And it's very difficult
to distill the cat-ness out of this, like, giant array
of thousands, or whatever, very many different numbers. So, we refer to this
problem as the semantic gap. This idea of a cat, or
this label of a cat, is a semantic label that
we're assigning to this image, and there's this huge gap between
the semantic idea of a cat and these pixel values that the
computer is actually seeing. And this is a really hard problem because you can change the picture
in very small, subtle ways that will cause this pixel
grid to change entirely. So, for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never going to happen, but we moved the camera to the other side, then every single grid,
every single pixel, in this giant grid of numbers would be completely different. But, somehow, it's still
representing the same cat. And our algorithms need
to be robust to this. But, not only viewpoint is one problem, another is illumination. There can be different
lighting conditions going on in the scene. Whether the cat is appearing
in this very dark, moody scene, or like is this very bright,
sunlit scene, it's still a cat, and our algorithms need
to be robust to that. Objects can also deform. I think cats are, maybe,
among the more deformable of animals that you might see out there. And cats can really assume a
lot of different, varied poses and positions. And our algorithms should
be robust to these different kinds of transforms. There can also be problems of occlusion, where you might only see part
of a cat, like, just the face, or in this extreme example,
just a tail peeking out from under the couch cushion. But, in these cases, it's pretty
easy for you, as a person, to realize that this is probably a cat, and you still recognize
these images as cats. And this is something that our algorithms also must be robust to, which is quite difficult, I think. There can also be problems
of background clutter, where maybe the foreground
object of the cat, could actually look quite
similar in appearance to the background. And this is another thing
that we need to handle. There's also this problem
of intraclass variation, that this one notion of
cat-ness, actually spans a lot of different visual appearances. And cats can come in
different shapes and sizes and colors and ages. And our algorithm, again, needs to work and handle all these different variations. So, this is actually a really,
really challenging problem. And it's sort of easy to
forget how easy this is because so much of your
brain is specifically tuned for dealing with these things. But now if we want our computer programs to deal with all of these
problems, all simultaneously, and not just for cats, by the way, but for just about any object
category you can imagine, this is a fantastically
challenging problem. And it's, actually, somewhat miraculous that this works at all, in my opinion. But, actually, not only does it work, but these things work very
close to human accuracy in some limited situations. And take only hundreds
of milliseconds to do so. So, this is some pretty
amazing, incredible technology, in my opinion, and over the
course of the rest of the class we will really see what
kinds of advancements have made this possible. So now, if you, kind of, think about what is the API for writing
an image classifier, you might sit down and try
to write a method in Python like this. Where you want to take in an image and then do some crazy magic and then, eventually,
spit out this class label to say cat or dog or whatnot. And there's really no obvious
way to do this, right? If you're taking an algorithms class and your task is to sort numbers or compute a convex hull or, even, do something
like RSA encryption, you, sort of, can write down an algorithm and enumerate all the
steps that need to happen in order for this things to work. But, when we're trying
to recognize objects, or recognize cats or images, there's no really clear,
explicit algorithm that makes intuitive sense, for how you might go about
recognizing these objects. So, this is, again, quite challenging, if you think about, if it was your first day programming and you had to sit down
and write this function, I think most people would be in trouble. That being said, people have definitely
made explicit attempts to try to write, sort
of, high-end coded rules for recognizing different animals. So, we touched on this a
little bit in the last lecture, but maybe one idea for cats is that, we know that cats have ears
and eyes and mouths and noses. And we know that edges,
from Hubel and Wiesel, we know that edges are pretty important when it comes to visual recognition. So one thing we might try to do is compute the edges of this image and then go in and try to
categorize all the different corners and boundaries, and
say that, if we have maybe three lines meeting this way,
then it might be a corner, and an ear has one corner
here and one corner there and one corner there, and then, kind of, write down
this explicit set of rules for recognizing cats. But this turns out not to work very well. One, it's super brittle. And, two, say, if you want
to start over for another object category, and maybe
not worry about cats, but talk about trucks or dogs
or fishes or something else, then you need to start all over again. So, this is really not a
very scalable approach. We want to come up with some
algorithm, or some method, for these recognition tasks which scales much more
naturally to all the variety of objects in the world. So, the insight that, sort
of, makes this all work is this idea of the data-driven approach. Rather than sitting down and
writing these hand-specified rules to try to craft exactly
what is a cat or a fish or what have you, instead, we'll go out onto the internet and collect a large
dataset of many, many cats and many, many airplanes
and many, many deer and different things like this. And we can actually use tools
like Google Image Search, or something like that, to go out and collect a very
large number of examples of these different categories. By the way, this actually
takes quite a lot of effort to go out and actually
collect these datasets but, luckily, there's a lot
of really good, high quality datasets out there already for you to use. Then once we get this dataset, we train this machine learning classifier that is going to ingest all of the data, summarize it in some way, and then spit out a model that summarizes the
knowledge of how to recognize these different object categories. Then finally, we'll
use this training model and apply it on new images that will then be able to recognize cats and dogs and whatnot. So here our API has changed a little bit. Rather than a single function that just inputs an image
and recognizes a cat, we have these two functions. One, called, train, that's
going to input images and labels and then output a model, and then, separately, another
function called, predict, which will input the model
and than make predictions for images. And this is, kind of, the key insight that allowed all these things
to start working really well over the last 10, 20 years or so. So, this class is primarily
about neural networks and convolutional neural networks and deep learning and all that, but this idea of a data-driven
approach is much more general than just deep learning. And I think it's useful to, sort of, step through this process for a very simple classifier first, before we get to these big, complex ones. So, probably, the simplest
classifier you can imagine is something we call nearest neighbor. The algorithm is pretty dumb, honestly. So, during the training
step we won't do anything, we'll just memorize all
of the training data. So this is very simple. And now, during the prediction step, we're going to take some new image and go and try to find
the most similar image in the training data to that new image, and now predict the label
of that most similar image. A very simple algorithm. But it, sort of, has a lot
of these nice properties with respect to
data-drivenness and whatnot. So, to be a little bit more concrete, you might imagine working on
this dataset called CIFAR-10, which is very commonly
used in machine learning, as kind of a small test case. And you'll be working with
this dataset on your homework. So, the CIFAR-10 dataset gives
you 10 different classes, airplanes and automobiles and
birds and cats and different things like that. And for each of those 10 categories it provides 50,000 training images, roughly evenly distributed
across these 10 categories. And then 10,000 additional testing images that you're supposed to
test your algorithm on. So here's an example
of applying this simple nearest neighbor classifier
to some of these test images on CIFAR-10. So, on this grid on the right, for the left most column, gives a test image in
the CIFAR-10 dataset. And now on the right, we've
sorted the training images and show the most similar training images to each of these test examples. And you can see that they
look kind of visually similar to the training images, although they are not
always correct, right? So, maybe on the second row,
we see that the testing, this is kind of hard to see, because these images are 32 by 32 pixels, you need to really dive in there and try to make your best guess. But, this image is a dog and
it's nearest neighbor is also a dog, but this next one,
I think is actually a deer or a horse or something else. But, you can see that it
looks quite visually similar, because there's kind of a
white blob in the middle and whatnot. So, if we're applying the
nearest neighbor algorithm to this image, we'll find the closest
example in the training set. And now, the closest
example, we know it's label, because it comes from the training set. And now, we'll simply say that
this testing image is also a dog. You can see from these
examples that is probably not going to work very well, but it's still kind of a
nice example to work through. But then, one detail
that we need to know is, given a pair of images, how can we actually compare them? Because, if we're going to take
our test image and compare it to all the training images, we actually have many different choices for exactly what that comparison
function should look like. So, in the example in the previous slide, we've used what's called the L1 distance, also sometimes called
the Manhattan distance. So, this is a really
sort of simple, easy idea for comparing images. And that's that we're going to
just compare individual pixels in these images. So, supposing that our test
image is maybe just a tiny four by four image of pixel values, then we're take this upper-left hand pixel of the test image, subtract off the value
in the training image, take the absolute value, and get the difference in that
pixel between the two images. And then, sum all these
up across all the pixels in the image. So, this is kind of a stupid
way to compare images, but it does some reasonable
things sometimes. But, this gives us a very concrete way to measure the difference
between two images. And in this case, we have
this difference of 456 between these two images. So, here's some full Python code for implementing this
nearest neighbor classifier and you can see it's pretty
short and pretty concise because we've made use of
many of these vectorized operations offered by NumPy. So, here we can see that
this training function, that we talked about earlier, is, again, very simple, in
the case of nearest neighbor, you just memorize the training data, there's not really much to do here. And now, at test time, we're
going to take in our image and then go in and compare
using this L1 distance function, our test image to each of
these training examples and find the most similar
example in the training set. And you can see that, we're
actually able to do this in just one or two lines of Python code by utilizing these vectorized
operations in NumPy. So, this is something that
you'll get practice with on the first assignment. So now, a couple questions
about this simple classifier. First, if we have N examples
in our training set, then how fast can we expect
training and testing to be? Well, training is probably constant because we don't really
need to do anything, we just need to memorize the data. And if you're just copying a pointer, that's going to be constant time no matter how big your dataset is. But now, at test time we need
to do this comparison stop and compare our test image to each of the N training
examples in the dataset. And this is actually quite slow. So, this is actually somewhat backwards, if you think about it. Because, in practice, we want our classifiers to
be slow at training time and then fast at testing time. Because, you might imagine,
that a classifier might go and be trained in a data center somewhere and you can afford to
spend a lot of computation at training time to make
the classifier really good. But then, when you go and deploy the
classifier at test time, you want it to run on your mobile phone or in a browser or some
other low power device, and you really want the
testing time performance of your classifier to be quite fast. So, from this perspective, this
nearest neighbor algorithm, is, actually, a little bit backwards. And we'll see that once we move to convolutional neural networks, and other types of parametric models, they'll be the reverse of this. Where you'll spend a lot of
compute at training time, but then they'll be quite
fast at testing time. So then, the question is, what exactly does this
nearest neighbor algorithm look like when you apply it in practice? So, here we've drawn, what
we call the decision regions of a nearest neighbor classifier. So, here our training set
consists of these points in the two dimensional plane, where the color of the point
represents the category, or the class label, of that point. So, here we see we have five classes and some blue ones up in the corner here, some purple ones in the
upper-right hand corner. And now for each pixel
in this entire plane, we've gone and computed
what is the nearest example in these training data, and then colored the
point of the background corresponding to what is the class label. So, you can see that this
nearest neighbor classifier is just sort of carving up the space and coloring the space
according to the nearby points. But this classifier is maybe not so great. And by looking at this picture we can start to see some of the
problems that might come out with a nearest neighbor classifier. For one, this central
region actually contains mostly green points, but one little yellow point in the middle. But because we're just looking
at the nearest neighbor, this causes a little
yellow island to appear in this middle of this green cluster. And that's, maybe, not so great. Maybe those points actually
should have been green. And then, similarly we also
see these, sort of, fingers, like the green region
pushing into the blue region, again, due to the presence of one point, which may have been noisy or spurious. So, this kind of motivates
a slight generalization of this algorithm called
k-nearest neighbors. So rather than just looking for
the single nearest neighbor, instead we'll do something
a little bit fancier and find K of our nearest neighbors, according to our distance metric, and then take a vote among
each of our neighbors. And then predict the majority vote among our neighbors. You can imagine slightly more
complex ways of doing this. Maybe you'd vote weighted on the distance, or something like that, but the simplest thing that
tends to work pretty well is just taking a majority vote. So here we've shown the
exact same set of points using this K=1 nearest
neighbor classifier, as well as K=3 and K=5 in
the middle and on the right. And once we move to K=3, you
can see that that spurious yellow point in the middle
of the green cluster is no longer causing the
points near that region to be classified as yellow. Now this entire green
portion in the middle is all being classified as green. You can also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting. And then, once we move to the K=5 case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice. So, generally when you're
using nearest neighbors classifiers, you almost always want
to use some value of K, which is larger than one because this tends to
smooth out your decision boundaries and lead to better results. Question? [student asking a question] Yes, so the question is, what is the deal with these white regions? The white regions are
where there was no majority among the k-nearest neighbors. You could imagine maybe doing
something slightly fancier and maybe taking a guess
or randomly selecting among the majority winners, but for this simple example
we're just coloring it white to indicate there was no nearest neighbor in those points. Whenever we're thinking
about computer vision I think it's really useful to kind of flip back and forth between
several different viewpoints. One, is this idea of high
dimensional points in the plane, and then the other is actually
looking at concrete images. Because the pixels of the image actually allow us to think of these
images as high dimensional vectors. And it's sort of useful to
ping pong back and forth between these two different viewpoints. So then, sort of taking
this k-nearest neighbor and going back to the images you can see that it's
actually not very good. Here I've colored in red and green which images would actually
be classified correctly or incorrectly according
to their nearest neighbor. And you can see that it's
really not very good. But maybe if we used a larger value of K then this would involve
actually voting among maybe the top three or the top five or maybe even the whole row. And you could imagine that
that would end up being a lot more robust to some
of this noise that we see when retrieving neighbors in this way. So another choice we
have when we're working with the k-nearest neighbor algorithm is determining exactly
how we should be comparing our different points. For the examples so far we've just shown we've talked about this L1 distance which takes the sum of the absolute values between the pixels. But another common choice is
the L2 or Euclidean distance where you take the square
root of the sum of the squares and take this as your distance. Choosing different
distance metrics actually is a pretty interesting topic because different distance metrics make different assumptions
about the underlying geometry or topology that
you'd expect in the space. So, this L1 distance, underneath
this, this is actually a circle according to the L1 distance and it forms this square shape thing around the origin. Where each of the points
on this, on the square, is equidistant from the
origin according to L1, whereas with the L2 or Euclidean distance then this circle is a familiar circle, it looks like what you'd expect. So one interesting thing to
point out between these two metrics in particular, is that the L1 distance
depends on your choice of coordinates system. So if you were to rotate
the coordinate frame that would actually change the L1 distance between the points. Whereas changing the coordinate
frame in the L2 distance doesn't matter, it's the
same thing no matter what your coordinate frame is. Maybe if your input features,
if the individual entries in your vector have some important meaning for your task, then maybe somehow L1 might
be a more natural fit. But if it's just a generic
vector in some space and you don't know which
of the different elements, you don't know what they actually mean, then maybe L2 is slightly more natural. And another point here is that by using different distance metrics we can actually generalize
the k-nearest neighbor classifier to many, many
different types of data, not just vectors, not just images. So, for example, imagine you
wanted to classify pieces of text, then the only
thing you need to do to use k-nearest neighbors is to specify some distance function that can measure distances
between maybe two paragraphs or two sentences or something like that. So, simply by specifying
different distance metrics we can actually apply this
algorithm very generally to basically any type of data. Even though it's a kind
of simple algorithm, in general, it's a very
good thing to try first when you're looking at a new problem. So then, it's also kind of
interesting to think about what is actually happening geometrically if we choose different distance metrics. So here we see the same
set of points on the left using the L1, or Manhattan distance, and then, on the right,
using the familiar L2, or Euclidean distance. And you can see that the
shapes of these decision boundaries actually change quite a bit between the two metrics. So when you're looking at
L1 these decision boundaries tend to follow the coordinate axes. And this is again because
the L1 depends on our choice of coordinate system. Where the L2 sort of doesn't
really care about the coordinate axis, it
just puts the boundaries where they should fall naturally. My confession is that
each of these examples that I've shown you is
actually from this interactive web demo that I built, where you can go and play
with this k-nearest neighbor classifier on your own. And this is really hard to
work on a projector screen. So maybe we'll do that on your own time. So, let's just go back to here. Man, this is kind of embarrassing. Okay, that was way more
trouble than it was worth. So, let's skip this, but I encourage you to go play with this in your browser. It's actually pretty fun and kind of nice to build intuition about how the decision boundary changes as you change the K and change your distance metric and all those sorts of things. Okay, so then the question is once you're actually trying
to use this algorithm in practice, there's several choices you need to make. We talked about choosing
different values of K. We talked about choosing
different distance metrics. And the question becomes how do you actually make
these choices for your problem and for your data? So, these choices, of things
like K and the distance metric, we call hyperparameters, because they are not necessarily
learned from the training data, instead these are choices about
your algorithm that you make ahead of time and there's no way to learn
them directly from the data. So, the question is how
do you set these things in practice? And they turn out to be
very problem-dependent. And the simple thing that
most people do is simply try different values of
hyperparameters for your data and for your problem, and
figure out which one works best. There's a question? [student asking a question] So, the question is, where L1
distance might be preferable to using L2 distance? I think it's mainly problem-dependent, it's sort of difficult to say in which cases you think
one might be better than the other. but I think that because L1
has this sort of coordinate dependency, it actually depends
on the coordinate system of your data, if you know that you have a vector, and maybe the individual
elements of the vector have meaning. Like maybe you're classifying
employees for some reason and then the different elements
of that vector correspond to different features or
aspects of an employee. Like their salary or the
number of years they've been working at the company
or something like that. So I think when your
individual elements actually have some meaning, is where I think maybe using
L1 might make a little bit more sense. But in general, again,
this is a hyperparameter and it really depends on
your problem and your data so the best answer is
just to try them both and see what works better. Even this idea of trying
out different values of hyperparameters and
seeing what works best, there are many different choices here. What exactly does it mean
to try hyperparameters and see what works best? Well, the first idea you might think of is simply choosing the
hyperparameters that give you the best accuracy or best performance on your training data. This is actually a really terrible idea. You should never do this. In the concrete case
of the nearest neighbor classifier, for example, if we set K=1, we will always
classify the training data perfectly. So if we use this strategy
we'll always pick K=1, but, as we saw from the examples earlier, in practice it seems that
setting K equals to larger values might cause us to misclassify
some of the training data, but, in fact, lead to better performance on points that were not
in the training data. And ultimately in machine learning we don't care about
fitting the training data, we really care about how our classifier, or how our method, will perform on unseen
data after training. So, this is a terrible
idea, don't do this. So, another idea that you might think of, is maybe we'll take our full dataset and we'll split it into some training data and some test data. And now I'll try training
my algorithm with different choices of hyperparameters
on the training data and then I'll go and apply
that trained classifier on the test data and now I will pick the set of hyperparameters
that cause me to perform best on the test data. This seems like maybe a
more reasonable strategy, but, in fact, this is also a terrible idea and you should never do this. Because, again, the point
of machine learning systems is that we want to know how
our algorithm will perform. So, the point of the test set is to give us some estimate
of how our method will do on unseen data that's
coming out from the wild. And if we use this strategy
of training many different algorithms with different hyperparameters, and then, selecting the
one which does the best on the test data, then, it's possible, that
we may have just picked the right set of hyperparameters that caused our algorithm
to work quite well on this testing set, but now our performance on this test set will no longer be representative of our performance of new, unseen data. So, again, you should not
do this, this is a bad idea, you'll get in trouble if you do this. What is much more common, is
to actually split your data into three different sets. You'll partition most of
your data into a training set and then you'll create a validation set and a test set. And now what we typically do
is go and train our algorithm with many different
choices of hyperparameters on the training set, evaluate on the validation set, and now pick the set of hyperparameters which performs best on the validation set. And now, after you've
done all your development, you've done all your debugging, after you've dome everything, then you'd take that best
performing classifier on the validation set and run it once on the test set. And now that's the number
that goes into your paper, that's the number that
goes into your report, that's the number that
actually is telling you how your algorithm is doing on unseen data. And this is actually
really, really important that you keep a very
strict separation between the validation data and the test data. So, for example, when we're
working on research papers, we typically only touch the test set at the very last minute. So, when I'm writing papers, I tend to only touch the
test set for my problem in maybe the week before
the deadline or so to really insure that we're not being dishonest here and
we're not reporting a number which is unfair. So, this is actually super important and you want to make sure
to keep your test data quite under control. So another strategy for
setting hyperparameters is called cross validation. And this is used a
little bit more commonly for small data sets, not used
so much in deep learning. So here the idea is we're
going to take our test data, or we're going to take our dataset, as usual, hold out some test
set to use at the very end, and now, for the rest of the data, rather than splitting it
into a single training and validation partition, instead, we can split our training data into many different folds. And now, in this way, we've
cycled through choosing which fold is going to be the validation set. So now, in this example, we're using five fold cross validation, so you would train your
algorithm with one set of hyperparameters on the first four folds, evaluate the performance on fold four, and now go and retrain
your algorithm on folds one, two, three, and five, evaluate on fold four, and cycle through all the different folds. And, when you do it this way, you get much higher confidence about which hyperparameters are going to perform more robustly. So this is kind of the
gold standard to use, but, in practice in deep learning when we're training large models and training is very
computationally expensive, these doesn't get used
too much in practice. Question? [student asking a question] Yeah, so the question is, a little bit more concretely, what's the difference
between the training and the validation set? So, if you think about the
k-nearest neighbor classifier then the training set is this
set of images with labels where we memorize the labels. And now, to classify an image, we're going to take the image
and compare it to each element in the training data, and then transfer the label
from the nearest training point. So now our algorithm
will memorize everything in the training set, and now we'll take each
element of the validation set and compare it to each
element in the training data and then use this to
determine what is the accuracy of our classifier when it's
applied on the validation set. So this is the distinction
between training and validation. Where your algorithm is
able to see the labels of the training set, but for the validation set, your algorithm doesn't have
direct access to the labels. We only use the labels
of the validation set to check how well our algorithm is doing. A question? [student asking a question] The question is, whether the test set, is it possible that the
test set might not be representative of data
out there in the wild? This definitely can be
a problem in practice, the underlying statistical
assumption here is that your data are all independently
and identically distributed, so that all of your data points should be drawn from the same underlying
probability distribution. Of course, in practice, this
might not always be the case, and you definitely can run into cases where the test set might
not be super representative of what you see in the wild. So this is kind of a problem
that dataset creators and dataset curators need to think about. But when I'm creating
datasets, for example, one thing I do, is I'll go and collect a whole
bunch of data all at once, using the exact same methodology
for collecting the data, and then afterwards you go
and partition it randomly between train and test. One thing that can screw you up here is maybe if you're collecting data over time and you make the earlier
data, that you collect first, be the training data, and the later data that you
collect be the test data, then you actually might
run into this shift that could cause problems. But as long as this partition is random among your entire set of data points, then that's how we try
to alleviate this problem in practice. So then, once you've gone through this cross validation procedure, then you end up with graphs
that look something like this. So here, on the X axis, we
are showing the value of K for a k-nearest neighbor
classifier on some problem, and now on the Y axis, we are
showing what is the accuracy of our classifier on some dataset for different values of K. And you can see that, in this case, we've done five fold cross
validation over the data, so, for each value of K we
have five different examples of how well this algorithm is doing. And, actually, going back
to the question about having some test sets
that are better or worse for your algorithm, using K fold cross validation is maybe one way to help
quantify that a little bit. And, in that, we can see the
variance of how this algorithm performs on different
of the validation folds. And that gives you some sense of, not just what is the best, but, also, what is the
distribution of that performance. So, whenever you're training
machine learning models you end up making plots like this, where they show you what is your accuracy, or your performance as a
function of your hyperparameters, and then you want to
go and pick the model, or the set of hyperparameters, at the end of the day, that performs the best
on the validation set. So, here we see that maybe
about K=7 probably works about best for this problem. So, k-nearest neighbor
classifiers on images are actually almost
never used in practice. Because, with all of these
problems that we've talked about. So, one problem is that
it's very slow at test time, which is the reverse of what we want, which we talked about earlier. Another problem is that these things like Euclidean
distance, or L1 distance, are really not a very good way to measure distances between images. These, sort of, vectorial
distance functions do not correspond very well
to perceptual similarity between images. How you perceive
differences between images. So, in this example, we've constructed, there's this image on the left of a girl, and then three different
distorted images on the right where we've blocked out her mouth, we've actually shifted
down by a couple pixels, or tinted the entire image blue. And, actually, if you compute
the Euclidean distance between the original and the boxed, the original and the shuffled, and original in the tinted, they all have the same L2 distance. Which is, maybe, not so good because it sort of
gives you the sense that the L2 distance is really
not doing a very good job at capturing these perceptional
distances between images. Another, sort of, problem
with the k-nearest neighbor classifier has to do with
something we call the curse of dimensionality. So, if you recall back this
viewpoint we had of the k-nearest neighbor classifier, it's sort of dropping paint
around each of the training data points and using that to
sort of partition the space. So that means that if we
expect the k-nearest neighbor classifier to work well, we kind of need our training
examples to cover the space quite densely. Otherwise our nearest neighbors
could actually be quite far away and might not actually
be very similar to our testing points. And the problem is, that actually densely covering the space, means that we need a number
of training examples, which is exponential in the
dimension of the problem. So this is very bad, exponential
growth is always bad, basically, you're never
going to get enough images to densely cover this space of pixels in this high dimensional space. So that's maybe another
thing to keep in mind when you're thinking about
using k-nearest neighbor. So, kind of the summary
is that we're using k-nearest neighbor to introduce this idea of image classification. We have a training set
of images and labels and then we use that to predict these labels on the test set. Question? [student asking a question] Oh, sorry, the question is, what was going on with this picture? What are the green and the blue dots? So here, we have some training samples which are represented by points, and the color of the dot
maybe represents the category of the point, of this training sample. So, if we're in one dimension, then you maybe only need
four training samples to densely cover the space, but if we move to two dimensions, then, we now need, four times
four is 16 training examples to densely cover this space. And if we move to three, four,
five, many more dimensions, the number of training
examples that we need to densely cover the space, grows exponentially with the dimension. So, this is kind of giving you the sense, that maybe in two dimensions we might have this kind
of funny curved shape, or you might have sort of
arbitrary manifolds of labels in different dimensional spaces. Because the k-nearest neighbor algorithm doesn't really make any
assumptions about these underlying manifolds, the only way it can perform properly is if it has quite a dense
sample of training points to work with. So, this is kind of the
overview of k-nearest neighbors and you'll get a chance
to actually implement this and try it out on images
in the first assignment. So, if there's any last minute
questions about K and N, I'm going to move on to the next topic. Question? [student is asking a question] Sorry, say that again. [student is asking a question] Yeah, so the question is, why do these images have
the same L2 distance? And the answer is that, I
carefully constructed them to have the same L2 distance. [laughing] But it's just giving you the
sense that the L2 distance is not a very good measure
of similarity between images. And these images are
actually all different from each other in quite disparate ways. If you're using K and N, then the only thing you
have to measure distance between images, is this single distance metric. And this kind of gives
you an example where that distance metric is
actually not capturing the full description of
distance or difference between images. So, if this case, I just sort
of carefully constructed these translations and these
offsets to match exactly. Question? [student asking a question] So, the question is, maybe this is actually good, because all of these things are actually having the
same distance to the image. That's maybe true for this example, but I think you could also
construct examples where maybe we have two original images and then by putting the
boxes in the right places or tinting them, we could cause it to be
nearer to pretty much anything that you want, right? Because in this example, we
can kind of like do arbitrary shifting and tinting to kind of change these
distances nearly arbitrarily without changing the perceptional
nature of these images. So, I think that this
can actually screw you up if you have many
different original images. Question? [student is asking a question] The question is, whether or not it's
common in real-world cases to go back and retrain the entire dataset once you've found those
best hyperparameters? So, people do sometimes
do this in practice, but it's somewhat a matter of taste. If you're really rushing for that deadline and you've really got to
get this model out the door then, if it takes a long
time to retrain the model on the whole dataset, then maybe you won't do it. But if you have a little
bit more time to spare and a little bit more compute to spare, and you want to squeeze out
that maybe that extra 1% of performance, then that
is a trick you can use. So we kind of saw that
the k-nearest neighbor has a lot of the nice properties of machine learning algorithms, but in practice it's not so great, and really not used very much in images. So the next thing I'd
like to talk about is linear classification. And linear classification is,
again, quite a simple learning algorithm, but this will
become super important and help us build up to
whole neural networks and whole convolutional networks. So, one analogy people often talk about when working with neural networks is we think of them as being
kind of like Lego blocks. That you can have different
kinds of components of neural networks and you
can stick these components together to build these
large different towers of convolutional networks. One of the most basic
building blocks that we'll see in different types of
deep learning applications is this linear classifier. So, I think it's actually
really important to have a good understanding
of what's happening with linear classification. Because these will end up
generalizing quite nicely to whole neural networks. So another example of kind
of this modular nature of neural networks comes from some research in our
own lab on image captioning, just as a little bit of a preview. So here the setup is that
we want to input an image and then output a descriptive sentence describing the image. And the way this kind of works is that we have one convolutional
neural network that's looking at the image, and a recurrent neural network that knows about language. And we can kind of just stick
these two pieces together like Lego blocks and train
the whole thing together and end up with a pretty cool system that can do some non-trivial things. And we'll work through the
details of this model as we go forward in the class, but this just gives you the sense that, these deep neural networks
are kind of like Legos and this linear classifier is kind of like the most
basic building blocks of these giant networks. But that's a little bit too
exciting for lecture two, so we have to go back to
CIFAR-10 for the moment. [laughing] So, recall that CIFAR-10 has
these 50,000 training examples, each image is 32 by 32 pixels
and three color channels. In linear classification,
we're going to take a bit of a different approach
from k-nearest neighbor. So, the linear classifier is
one of the simplest examples of what we call a parametric model. So now, our parametric model
actually has two different components. It's going to take in this image,
maybe, of a cat on the left, and this, that we usually write
as X for our input data, and also a set of parameters, or weights, which is usually called
W, also sometimes theta, depending on the literature. And now we're going to
write down some function which takes in both the data,
X, and the parameters, W, and this'll spit out now
10 numbers describing what are the scores
corresponding to each of those 10 categories in CIFAR-10. With the interpretation that,
like the larger score for cat, indicates a larger probability
of that input X being cat. And now, a question? [student asking a question] Sorry, can you repeat that? [student asking a question] Oh, so the question is what is the three? The three, in this example,
corresponds to the three color channels, red, green, and blue. Because we typically work on color images, that's nice information that
you don't want to throw away. So, in the k-nearest neighbor setup there was no parameters, instead, we just kind of keep around
the whole training data, the whole training set, and use that at test time. But now, in a parametric approach, we're going to summarize our
knowledge of the training data and stick all that knowledge
into these parameters, W. And now, at test time, we
no longer need the actual training data, we can throw it away. We only need these
parameters, W, at test time. So this allows our models
to now be more efficient and actually run on maybe
small devices like phones. So, kind of, the whole
story in deep learning is coming up with the
right structure for this function, F. You can imagine writing down
different functional forms for how to combine weights
and data in different complex ways, and these
could correspond to different network architectures. But the simplest possible example of combining these two things is just, maybe, to multiply them. And this is a linear classifier. So here our F of X, W is
just equal to the W times X. Probably the simplest
equation you can imagine. So here, if you kind of unpack the
dimensions of these things, we recall that our image was
maybe 32 by 32 by 3 values. So then, we're going to take
those values and then stretch them out into a long column vector that has 3,072 by one entries. And now we want to end
up with 10 class scores. We want to end up with
10 numbers for this image giving us the scores for
each of the 10 categories. Which means that now our matrix, W, needs to be ten by 3072. So that once we multiply
these two things out then we'll end up with
a single column vector 10 by one, giving us our 10 class scores. Also sometimes, you'll typically see this, we'll often add a bias term which will be a constant
vector of 10 elements that does not interact
with the training data, and instead just gives us
some sort of data independent preferences for some classes over another. So you might imagine that
if you're dataset was unbalanced and had many
more cats than dogs, for example, then the bias
elements corresponding to cat would be higher
than the other ones. So if you kind of think about pictorially what this function is doing, in this figure we have
an example on the left of a simple image with
just a two by two image, so it has four pixels total. So the way that the
linear classifier works is that we take this two by two image, we stretch it out into a column vector with four elements, and now, in this example,
we are just restricting to three classes, cat, dog, and ship, because you can't fit 10 on a slide, and now our weight matrix is
going to be four by three, so we have four pixels and three classes. And now, again, we have a
three element bias vector that gives us data independent bias terms for each category. Now we see that the cat score
is going to be the enter product between the pixels of our image and this row in the weight matrix added together with this bias term. So, when you look at it this way you can kind of understand
linear classification as almost a template matching approach. Where each of the rows in this matrix correspond to some template of the image. And now the enter product or dot product between the row of the
matrix and the column giving the pixels of the image, computing this dot
product kind of gives us a similarity between this
template for the class and the pixels of our image. And then bias just,
again, gives you this data independence scaling offset
to each of the classes. If we think about linear classification from this viewpoint of template matching we can actually take the
rows of that weight matrix and unravel them back into images and actually visualize
those templates as images. And this gives us some
sense of what a linear classifier might actually be doing to try to understand our data. So, in this example, we've
gone ahead and trained a linear classifier on our images. And now on the bottom we're visualizing what are those rows in
that learned weight matrix corresponding to each of the 10 categories in CIFAR-10. And in this way we kind
of get a sense for what's going on in these images. So, for example, in the
left, on the bottom left, we see the template for the plane class, kind of consists of this like blue blob, this kind of blobby thing in the middle and maybe blue in the background, which gives you the sense
that this linear classifier for plane is maybe looking for blue stuff and blobby stuff, and those
features are going to cause the classifier to like planes more. Or if we look at this car example, we kind of see that
there's a red blobby thing through the middle and a
blue blobby thing at the top that maybe is kind of a blurry windshield. But this is a little bit weird, this doesn't really look like a car. No individual car
actually looks like this. So the problem is that
the linear classifier is only learning one
template for each class. So if there's sort of
variations in how that class might appear, it's trying to average out all
those different variations, all those different appearances, and use just one single template to recognize each of those categories. We can also see this pretty
explicitly in the horse classifier. So in the horse classifier we
see green stuff on the bottom because horses are usually on grass. And then, if you look
carefully, the horse actually seems to have maybe two
heads, one head on each side. And I've never seen a
horse with two heads. But the linear classifier
is just doing the best that it can, because it's
only allowed to learn one template per category. And as we move forward
into neural networks and more complex models, we'll be able to achieve
much better accuracy because they no longer
have this restriction of just learning a single
template per category. Another viewpoint of the linear classifier is to go back to this idea of images as points and high dimensional space. And you can imagine
that each of our images is something like a point in
this high dimensional space. And now the linear classifier
is putting in these linear decision boundaries
to try to draw linear separation between one category and the rest of the categories. So maybe up on the upper-left hand side we see these training
examples of airplanes and throughout the process of training the linear classier will
go and try to draw this blue line to separate
out with a single line the airplane class from all
the rest of the classes. And it's actually kind of
fun if you watch during the training process these
lines will start out randomly and then go and snap into
place to try to separate the data properly. But when you think about
linear classification in this way, from this high
dimensional point of view, you can start to see again
what are some of the problems that might come up with
linear classification. And it's not too hard
to construct examples of datasets where a linear
classifier will totally fail. So, one example, on the left here, is that, suppose we have a
dataset of two categories, and these are all maybe
somewhat artificial, but maybe our dataset has two categories, blue and red. And the blue categories
are the number of pixels in the image, which are
greater than zero, is odd. And anything where the
number of pixels greater than zero is even, we want to
classify as the red category. So if you actually go and
draw what these different decisions regions look like in the plane, you can see that our blue class
with an odd number of pixels is going to be these two
quadrants in the plane, and even will be the
opposite two quadrants. So now, there's no way that we
can draw a single linear line to separate the blue from the red. So this would be an example
where a linear classifier would really struggle. And this is maybe not such an
artificial thing after all. Instead of counting pixels, maybe we're actually trying
to count whether the number of animals or people in
an image is odd or even. So this kind of a parity problem of separating odds from evens is something that linear classification really struggles with traditionally. Other situations where a linear
classifier really struggles are multimodal situations. So here on the right, maybe our blue category has
these three different islands of where the blue category lives, and then everything else
is some other category. So, for something like horses, we saw on the previous example, is something where this
actually might be happening in practice. Where there's maybe one
island in the pixel space of horses looking to the left, and another island of
horses looking to the right. And now there's no good
way to draw a single linear boundary between these two
isolated islands of data. So anytime where you have multimodal data, like one class that can appear in
different regions of space, is another place where linear
classifiers might struggle. So there's kind of a lot of problems with linear classifiers, but it
is a super simple algorithm, super nice and easy to interpret
and easy to understand. So you'll actually be
implementing these things on your first homework assignment. At this point, we kind of talked about what is the functional
form corresponding to a linear classifier. And we've seen that this functional form of matrix vector multiply corresponds this idea of template matching and learning a single
template for each category in your data. And then once we have this trained matrix you can use it to actually
go and get your scores for any new training example. But what we have not told you is how do you actually go
about choosing the right W for your dataset. We've just talked about
what is the functional form and what is going on with this thing. So that's something we'll
really focus on next time. And next lecture we'll talk about what are the strategies and algorithms for choosing the right W. And this will lead us to questions of loss functions and optimization and eventually ConvNets. So, that's a bit of the
preview for next week. And that's all we have for today.",https://cs231n.github.io/classification/,"




CS231n Convolutional Neural Networks for Visual Recognition









 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1\*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-46895817-2', 'auto');
 ga('send', 'pageview');
 



addBackToTop({
 backgroundColor: '#fff',
 innerHTML: 'Back to Top',
 textColor: '#333'
 })

 #back-to-top {
 border: 1px solid #ccc;
 border-radius: 0;
 font-family: sans-serif;
 font-size: 14px;
 width: 100px;
 text-align: center;
 line-height: 30px;
 height: 30px;
 }
 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io)
[Course Website](http://cs231n.stanford.edu/)





# 




This is an introductory lecture designed to introduce people from outside of
Computer Vision to the Image Classification problem, and the data-driven
approach. The Table of Contents:


* [Image Classification](#image-classification)
	+ [Nearest Neighbor Classifier](#nearest-neighbor-classifier)
	+ [k - Nearest Neighbor Classifier](#k---nearest-neighbor-classifier)
	+ [Validation sets for Hyperparameter tuning](#validation-sets-for-hyperparameter-tuning)
	+ [Summary](#summary)
	+ [Summary: Applying kNN in practice](#summary-applying-knn-in-practice)
		- [Further Reading](#further-reading)



## Image Classification


**Motivation**. In this section we will introduce the Image Classification
problem, which is the task of assigning an input image one label from a fixed
set of categories. This is one of the core problems in Computer Vision that,
despite its simplicity, has a large variety of practical applications. Moreover,
as we will see later in the course, many other seemingly distinct Computer
Vision tasks (such as object detection, segmentation) can be reduced to image
classification.


**Example**. For example, in the image below an image classification model takes
a single image and assigns probabilities to 4 labels, *{cat, dog, hat, mug}*. As
shown in the image, keep in mind that to a computer an image is represented as
one large 3-dimensional array of numbers. In this example, the cat image is 248
pixels wide, 400 pixels tall, and has three color channels Red,Green,Blue (or
RGB for short). Therefore, the image consists of 248 x 400 x 3 numbers, or a
total of 297,600 numbers. Each number is an integer that ranges from 0 (black)
to 255 (white). Our task is to turn this quarter of a million numbers into a
single label, such as *“cat”*.



![](/assets/classify.png)
The task in Image Classification is to predict a single label (or a distribution over labels as shown here to indicate our confidence) for a given image. Images are 3-dimensional arrays of integers from 0 to 255, of size Width x Height x 3. The 3 represents the three color channels Red, Green, Blue.

**Challenges**. Since this task of recognizing a visual concept (e.g. cat) is
relatively trivial for a human to perform, it is worth considering the
challenges involved from the perspective of a Computer Vision algorithm. As we
present (an inexhaustive) list of challenges below, keep in mind the raw
representation of images as a 3-D array of brightness values:


* **Viewpoint variation**. A single instance of an object can be oriented in many ways with respect to the camera.
* **Scale variation**. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image).
* **Deformation**. Many objects of interest are not rigid bodies and can be deformed in extreme ways.
* **Occlusion**. The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.
* **Illumination conditions**. The effects of illumination are drastic on the pixel level.
* **Background clutter**. The objects of interest may *blend* into their environment, making them hard to identify.
* **Intra-class variation**. The classes of interest can often be relatively broad, such as *chair*. There are many different types of these objects, each with their own appearance.


A good image classification model must be invariant to the cross product of all
these variations, while simultaneously retaining sensitivity to the inter-class
variations.



![](/assets/challenges.jpeg)


**Data-driven approach**. How might we go about writing an algorithm that can
classify images into distinct categories? Unlike writing an algorithm for, for
example, sorting a list of numbers, it is not obvious how one might write an
algorithm for identifying cats in images. Therefore, instead of trying to
specify what every one of the categories of interest look like directly in code,
the approach that we will take is not unlike one you would take with a child:
we’re going to provide the computer with many examples of each class and then
develop learning algorithms that look at these examples and learn about the
visual appearance of each class. This approach is referred to as a *data-driven
approach*, since it relies on first accumulating a *training dataset* of labeled
images. Here is an example of what such a dataset might look like:



![](/assets/trainset.jpg)
An example training set for four visual categories. In practice we may have thousands of categories and hundreds of thousands of images for each category.

**The image classification pipeline**. We’ve seen that the task in Image
Classification is to take an array of pixels that represents a single image and
assign a label to it. Our complete pipeline can be formalized as follows:


* **Input:** Our input consists of a set of *N* images, each labeled with one of *K* different classes. We refer to this data as the *training set*.
* **Learning:** Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as *training a classifier*, or *learning a model*.
* **Evaluation:** In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers (which we call the *ground truth*).



### Nearest Neighbor Classifier


As our first approach, we will develop what we call a **Nearest Neighbor
Classifier**. This classifier has nothing to do with Convolutional Neural
Networks and it is very rarely used in practice, but it will allow us to get an
idea about the basic approach to an image classification problem.


**Example image classification dataset: CIFAR-10.** One popular toy image
classification dataset is the [CIFAR-10
dataset](https://www.cs.toronto.edu/~kriz/cifar.html). This dataset consists of
60,000 tiny images that are 32 pixels high and wide. Each image is labeled with
one of 10 classes (for example *“airplane, automobile, bird, etc”*). These
60,000 images are partitioned into a training set of 50,000 images and a test
set of 10,000 images. In the image below you can see 10 random example images
from each one of the 10 classes:



![](/assets/nn.jpg)
Left: Example images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). Right: first column shows a few test images and next to each we show the top 10 nearest neighbors in the training set according to pixel-wise difference.

Suppose now that we are given the CIFAR-10 training set of 50,000 images (5,000
images for every one of the labels), and we wish to label the remaining 10,000.
The nearest neighbor classifier will take a test image, compare it to every
single one of the training images, and predict the label of the closest training
image. In the image above and on the right you can see an example result of such
a procedure for 10 example test images. Notice that in only about 3 out of 10
examples an image of the same class is retrieved, while in the other 7 examples
this is not the case. For example, in the 8th row the nearest training image to
the horse head is a red car, presumably due to the strong black background. As a
result, this image of a horse would in this case be mislabeled as a car.


You may have noticed that we left unspecified the details of exactly how we
compare two images, which in this case are just two blocks of 32 x 32 x 3. One
of the simplest possibilities is to compare the images pixel by pixel and add up
all the differences. In other words, given two images and representing them as
vectors \( I\_1, I\_2 \) , a reasonable choice for comparing them might be the
**L1 distance**:



\[d\_1 (I\_1, I\_2) = \sum\_{p} \left| I^p\_1 - I^p\_2 \right|\]

Where the sum is taken over all pixels. Here is the procedure visualized:



![](/assets/nneg.jpeg)
An example of using pixel-wise differences to compare two images with L1 distance (for one color channel in this example). Two images are subtracted elementwise and then all differences are added up to a single number. If two images are identical the result will be zero. But if the images are very different the result will be large.

Let’s also look at how we might implement the classifier in code. First, let’s
load the CIFAR-10 data into memory as 4 arrays: the training data/labels and the
test data/labels. In the code below, `Xtr` (of size 50,000 x 32 x 32 x 3) holds
all the images in the training set, and a corresponding 1-dimensional array
`Ytr` (of length 50,000) holds the training labels (from 0 to 9):



```python
Xtr, Ytr, Xte, Yte = load\_CIFAR10('data/cifar10/') # a magic function we provide
# flatten out all images to be one-dimensional
Xtr\_rows = Xtr.reshape(Xtr.shape[0], 32 \* 32 \* 3) # Xtr\_rows becomes 50000 x 3072
Xte\_rows = Xte.reshape(Xte.shape[0], 32 \* 32 \* 3) # Xte\_rows becomes 10000 x 3072

```

Now that we have all images stretched out as rows, here is how we could train
and evaluate a classifier:



```python
nn = NearestNeighbor() # create a Nearest Neighbor classifier class
nn.train(Xtr\_rows, Ytr) # train the classifier on the training images and labels
Yte\_predict = nn.predict(Xte\_rows) # predict labels on the test images
# and now print the classification accuracy, which is the average number
# of examples that are correctly predicted (i.e. label matches)
print 'accuracy: %f' % ( np.mean(Yte\_predict == Yte) )

```

Notice that as an evaluation criterion, it is common to use the **accuracy**,
which measures the fraction of predictions that were correct. Notice that all
classifiers we will build satisfy this one common API: they have a `train(X,y)`
function that takes the data and the labels to learn from. Internally, the class
should build some kind of model of the labels and how they can be predicted from
the data. And then there is a `predict(X)` function, which takes new data and
predicts the labels. Of course, we’ve left out the meat of things - the actual
classifier itself. Here is an implementation of a simple Nearest Neighbor
classifier with the L1 distance that satisfies this template:



```python
import numpy as np

class NearestNeighbor(object):
  def \_\_init\_\_(self):
    pass

  def train(self, X, y):
    """""" X is N x D where each row is an example. Y is 1-dimension of size N """"""
    # the nearest neighbor classifier simply remembers all the training data
    self.Xtr = X
    self.ytr = y

  def predict(self, X):
    """""" X is N x D where each row is an example we wish to predict label for """"""
    num\_test = X.shape[0]
    # lets make sure that the output type matches the input type
    Ypred = np.zeros(num\_test, dtype = self.ytr.dtype)

    # loop over all test rows
    for i in range(num\_test):
      # find the nearest training image to the i'th test image
      # using the L1 distance (sum of absolute value differences)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
      min\_index = np.argmin(distances) # get the index with smallest distance
      Ypred[i] = self.ytr[min\_index] # predict the label of the nearest example

    return Ypred

```

If you ran this code, you would see that this classifier only achieves **38.6%**
on CIFAR-10. That’s more impressive than guessing at random (which would give
10% accuracy since there are 10 classes), but nowhere near human performance
(which is [estimated at about
94%](https://karpathy.github.io/2011/04/27/manually-classifying-cifar10/)) or
near state-of-the-art Convolutional Neural Networks that achieve about 95%,
matching human accuracy (see the
[leaderboard](https://www.kaggle.com/c/cifar-10/leaderboard) of a recent Kaggle
competition on CIFAR-10).


**The choice of distance.** There are many other ways of computing distances
between vectors. Another common choice could be to instead use the **L2
distance**, which has the geometric interpretation of computing the euclidean
distance between two vectors. The distance takes the form:



\[d\_2 (I\_1, I\_2) = \sqrt{\sum\_{p} \left( I^p\_1 - I^p\_2 \right)^2}\]

In other words we would be computing the pixelwise difference as before, but
this time we square all of them, add them up and finally take the square root.
In numpy, using the code from above we would need to only replace a single line
of code. The line that computes the distances:



```python
distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))

```

Note that I included the `np.sqrt` call above, but in a practical nearest
neighbor application we could leave out the square root operation because square
root is a *monotonic function*. That is, it scales the absolute sizes of the
distances but it preserves the ordering, so the nearest neighbors with or
without it are identical. If you ran the Nearest Neighbor classifier on CIFAR-10
with this distance, you would obtain **35.4%** accuracy (slightly lower than our
L1 distance result).


**L1 vs. L2.** It is interesting to consider differences between the two
metrics. In particular, the L2 distance is much more unforgiving than the L1
distance when it comes to differences between two vectors. That is, the L2
distance prefers many medium disagreements to one big one. L1 and L2 distances
(or equivalently the L1/L2 norms of the differences between a pair of images)
are the most commonly used special cases of a
[p-norm](https://planetmath.org/vectorpnorm).



### k - Nearest Neighbor Classifier


You may have noticed that it is strange to only use the label of the nearest
image when we wish to make a prediction. Indeed, it is almost always the case
that one can do better by using what’s called a **k-Nearest Neighbor
Classifier**. The idea is very simple: instead of finding the single closest
image in the training set, we will find the top **k** closest images, and have
them vote on the label of the test image. In particular, when *k = 1*, we
recover the Nearest Neighbor classifier. Intuitively, higher values of **k**
have a smoothing effect that makes the classifier more resistant to outliers:



![](/assets/knn.jpeg)
An example of the difference between Nearest Neighbor and a 5-Nearest Neighbor classifier, using 2-dimensional points and 3 classes (red, blue, green). The colored regions show the **decision boundaries** induced by the classifier with an L2 distance. The white regions show points that are ambiguously classified (i.e. class votes are tied for at least two classes). Notice that in the case of a NN classifier, outlier datapoints (e.g. green point in the middle of a cloud of blue points) create small islands of likely incorrect predictions, while the 5-NN classifier smooths over these irregularities, likely leading to better **generalization** on the test data (not shown). Also note that the gray regions in the 5-NN image are caused by ties in the votes among the nearest neighbors (e.g. 2 neighbors are red, next two neighbors are blue, last neighbor is green).

In practice, you will almost always want to use k-Nearest Neighbor. But what
value of *k* should you use? We turn to this problem next.



### Validation sets for Hyperparameter tuning


The k-nearest neighbor classifier requires a setting for *k*. But what number
works best? Additionally, we saw that there are many different distance
functions we could have used: L1 norm, L2 norm, there are many other choices we
didn’t even consider (e.g. dot products). These choices are called
**hyperparameters** and they come up very often in the design of many Machine
Learning algorithms that learn from data. It’s often not obvious what
values/settings one should choose.


You might be tempted to suggest that we should try out many different values and
see what works best. That is a fine idea and that’s indeed what we will do, but
this must be done very carefully. In particular, **we cannot use the test set
for the purpose of tweaking hyperparameters**. Whenever you’re designing Machine
Learning algorithms, you should think of the test set as a very precious
resource that should ideally never be touched until one time at the very end.
Otherwise, the very real danger is that you may tune your hyperparameters to
work well on the test set, but if you were to deploy your model you could see a
significantly reduced performance. In practice, we would say that you
**overfit** to the test set. Another way of looking at it is that if you tune
your hyperparameters on the test set, you are effectively using the test set as
the training set, and therefore the performance you achieve on it will be too
optimistic with respect to what you might actually observe when you deploy your
model. But if you only use the test set once at end, it remains a good proxy for
measuring the **generalization** of your classifier (we will see much more
discussion surrounding generalization later in the class).



> 
> Evaluate on the test set only a single time, at the very end.
> 
> 
> 


Luckily, there is a correct way of tuning the hyperparameters and it does not
touch the test set at all. The idea is to split our training set in two: a
slightly smaller training set, and what we call a **validation set**. Using
CIFAR-10 as an example, we could for example use 49,000 of the training images
for training, and leave 1,000 aside for validation. This validation set is
essentially used as a fake test set to tune the hyper-parameters.


Here is what this might look like in the case of CIFAR-10:



```python
# assume we have Xtr\_rows, Ytr, Xte\_rows, Yte as before
# recall Xtr\_rows is 50,000 x 3072 matrix
Xval\_rows = Xtr\_rows[:1000, :] # take first 1000 for validation
Yval = Ytr[:1000]
Xtr\_rows = Xtr\_rows[1000:, :] # keep last 49,000 for train
Ytr = Ytr[1000:]

# find hyperparameters that work best on the validation set
validation\_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:

  # use a particular value of k and evaluation on validation data
  nn = NearestNeighbor()
  nn.train(Xtr\_rows, Ytr)
  # here we assume a modified NearestNeighbor class that can take a k as input
  Yval\_predict = nn.predict(Xval\_rows, k = k)
  acc = np.mean(Yval\_predict == Yval)
  print 'accuracy: %f' % (acc,)

  # keep track of what works on the validation set
  validation\_accuracies.append((k, acc))

```

By the end of this procedure, we could plot a graph that shows which values of
*k* work best. We would then stick with this value and evaluate once on the
actual test set.



> 
> Split your training set into training set and a validation set. Use validation
> set to tune all hyperparameters. At the end run a single time on the test set
> and report performance.
> 
> 
> 


**Cross-validation**. In cases where the size of your training data (and
therefore also the validation data) might be small, people sometimes use a more
sophisticated technique for hyperparameter tuning called **cross-validation**.
Working with our previous example, the idea is that instead of arbitrarily
picking the first 1000 datapoints to be the validation set and rest training
set, you can get a better and less noisy estimate of how well a certain value of
*k* works by iterating over different validation sets and averaging the
performance across these. For example, in 5-fold cross-validation, we would
split the training data into 5 equal folds, use 4 of them for training, and 1
for validation. We would then iterate over which fold is the validation fold,
evaluate the performance, and finally average the performance across the
different folds.



![](/assets/cvplot.png)
Example of a 5-fold cross-validation run for the parameter **k**. For each value of **k** we train on 4 folds and evaluate on the 5th. Hence, for each **k** we receive 5 accuracies on the validation fold (accuracy is the y-axis, each result is a point). The trend line is drawn through the average of the results for each **k** and the error bars indicate the standard deviation. Note that in this particular case, the cross-validation suggests that a value of about **k** = 7 works best on this particular dataset (corresponding to the peak in the plot). If we used more than 5 folds, we might expect to see a smoother (i.e. less noisy) curve.


**In practice**. In practice, people prefer to avoid cross-validation in favor
of having a single validation split, since cross-validation can be
computationally expensive. The splits people tend to use is between 50%-90% of
the training data for training and rest for validation. However, this depends on
multiple factors: For example if the number of hyperparameters is large you may
prefer to use bigger validation splits. If the number of examples in the
validation set is small (perhaps only a few hundred or so), it is safer to use
cross-validation. Typical number of folds you can see in practice would be
3-fold, 5-fold or 10-fold cross-validation.



![](/assets/crossval.jpeg)
Common data splits. A training and test set is given. The training set is split into folds (for example 5 folds here). The folds 1-4 become the training set. One fold (e.g. fold 5 here in yellow) is denoted as the Validation fold and is used to tune the hyperparameters. Cross-validation goes a step further and iterates over the choice of which fold is the validation fold, separately from 1-5. This would be referred to as 5-fold cross-validation. In the very end once the model is trained and all the best hyperparameters were determined, the model is evaluated a single time on the test data (red).


**Pros and Cons of Nearest Neighbor classifier.**


It is worth considering some advantages and drawbacks of the Nearest Neighbor
classifier. Clearly, one advantage is that it is very simple to implement and
understand. Additionally, the classifier takes no time to train, since all that
is required is to store and possibly index the training data. However, we pay
that computational cost at test time, since classifying a test example requires
a comparison to every single training example. This is backwards, since in
practice we often care about the test time efficiency much more than the
efficiency at training time. In fact, the deep neural networks we will develop
later in this class shift this tradeoff to the other extreme: They are very
expensive to train, but once the training is finished it is very cheap to
classify a new test example. This mode of operation is much more desirable in
practice.


As an aside, the computational complexity of the Nearest Neighbor classifier is
an active area of research, and several **Approximate Nearest Neighbor** (ANN)
algorithms and libraries exist that can accelerate the nearest neighbor lookup
in a dataset (e.g. [FLANN](https://github.com/mariusmuja/flann)). These
algorithms allow one to trade off the correctness of the nearest neighbor
retrieval with its space/time complexity during retrieval, and usually rely on a
pre-processing/indexing stage that involves building a kdtree, or running the
k-means algorithm.


The Nearest Neighbor Classifier may sometimes be a good choice in some settings
(especially if the data is low-dimensional), but it is rarely appropriate for
use in practical image classification settings. One problem is that images are
high-dimensional objects (i.e. they often contain many pixels), and distances
over high-dimensional spaces can be very counter-intuitive. The image below
illustrates the point that the pixel-based L2 similarities we developed above
are very different from perceptual similarities:



![](/assets/samenorm.png)
Pixel-based distances on high-dimensional data (and images especially) can be very unintuitive. An original image (left) and three other images next to it that are all equally far away from it based on L2 pixel distance. Clearly, the pixel-wise distance does not correspond at all to perceptual or semantic similarity.

Here is one more visualization to convince you that using pixel differences to
compare images is inadequate. We can use a visualization technique called
[t-SNE](https://lvdmaaten.github.io/tsne/) to take the CIFAR-10 images and embed
them in two dimensions so that their (local) pairwise distances are best
preserved. In this visualization, images that are shown nearby are considered to
be very near according to the L2 pixelwise distance we developed above:



![](/assets/pixels_embed_cifar10.jpg)
CIFAR-10 images embedded in two dimensions with t-SNE. Images that are nearby on this image are considered to be close based on the L2 pixel distance. Notice the strong effect of background rather than semantic class differences. Click [here](/assets/pixels_embed_cifar10_big.jpg) for a bigger version of this visualization.

In particular, note that images that are nearby each other are much more a
function of the general color distribution of the images, or the type of
background rather than their semantic identity. For example, a dog can be seen
very near a frog since both happen to be on white background. Ideally we would
like images of all of the 10 classes to form their own clusters, so that images
of the same class are nearby to each other regardless of irrelevant
characteristics and variations (such as the background). However, to get this
property we will have to go beyond raw pixels.



### Summary


In summary:


* We introduced the problem of **Image Classification**, in which we are given a set of images that are all labeled with a single category. We are then asked to predict these categories for a novel set of test images and measure the accuracy of the predictions.
* We introduced a simple classifier called the **Nearest Neighbor classifier**. We saw that there are multiple hyper-parameters (such as value of k, or the type of distance used to compare examples) that are associated with this classifier and that there was no obvious way of choosing them.
* We saw that the correct way to set these hyperparameters is to split your training data into two: a training set and a fake test set, which we call **validation set**. We try different hyperparameter values and keep the values that lead to the best performance on the validation set.
* If the lack of training data is a concern, we discussed a procedure called **cross-validation**, which can help reduce noise in estimating which hyperparameters work best.
* Once the best hyperparameters are found, we fix them and perform a single **evaluation** on the actual test set.
* We saw that Nearest Neighbor can get us about 40% accuracy on CIFAR-10. It is simple to implement but requires us to store the entire training set and it is expensive to evaluate on a test image.
* Finally, we saw that the use of L1 or L2 distances on raw pixel values is not adequate since the distances correlate more strongly with backgrounds and color distributions of images than with their semantic content.


In next lectures we will embark on addressing these challenges and eventually
arrive at solutions that give 90% accuracies, allow us to completely discard the
training set once learning is complete, and they will allow us to evaluate a
test image in less than a millisecond.



### Summary: Applying kNN in practice


If you wish to apply kNN in practice (hopefully not on images, or perhaps as
only a baseline) proceed as follows:


1. Preprocess your data: Normalize the features in your data (e.g. one pixel in images) to have zero mean and unit variance. We will cover this in more detail in later sections, and chose not to cover data normalization in this section because pixels in images are usually homogeneous and do not exhibit widely different distributions, alleviating the need for data normalization.
2. If your data is very high-dimensional, consider using a dimensionality reduction technique such as PCA ([wiki ref](https://en.wikipedia.org/wiki/Principal_component_analysis), [CS229ref](http://cs229.stanford.edu/notes/cs229-notes10.pdf), [blog ref](https://web.archive.org/web/20150503165118/http://www.bigdataexaminer.com:80/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/)), NCA ([wiki ref](https://en.wikipedia.org/wiki/Neighbourhood_components_analysis), [blog ref](https://kevinzakka.github.io/2020/02/10/nca/)), or even [Random Projections](https://scikit-learn.org/stable/modules/random_projection.html).
3. Split your training data randomly into train/val splits. As a rule of thumb, between 70-90% of your data usually goes to the train split. This setting depends on how many hyperparameters you have and how much of an influence you expect them to have. If there are many hyperparameters to estimate, you should err on the side of having larger validation set to estimate them effectively. If you are concerned about the size of your validation data, it is best to split the training data into folds and perform cross-validation. If you can afford the computational budget it is always safer to go with cross-validation (the more folds the better, but more expensive).
4. Train and evaluate the kNN classifier on the validation data (for all folds, if doing cross-validation) for many choices of **k** (e.g. the more the better) and across different distance types (L1 and L2 are good candidates)
5. If your kNN classifier is running too long, consider using an Approximate Nearest Neighbor library (e.g. [FLANN](https://github.com/mariusmuja/flann)) to accelerate the retrieval (at cost of some accuracy).
6. Take note of the hyperparameters that gave the best results. There is a question of whether you should use the full training set with the best hyperparameters, since the optimal hyperparameters might change if you were to fold the validation data into your training set (since the size of the data would be larger). In practice it is cleaner to not use the validation data in the final classifier and consider it to be *burned* on estimating the hyperparameters. Evaluate the best model on the test set. Report the test set accuracy and declare the result to be the performance of the kNN classifier on your data.



#### Further Reading


Here are some (optional) links you may find interesting for further reading:


* [A Few Useful Things to Know about Machine
Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf), where
especially section 6 is related but the whole paper is a warmly recommended
reading.
* [Recognizing and Learning Object
Categories](https://people.csail.mit.edu/torralba/shortCourseRLOC/index.html), a
short course of object categorization at ICCV 2005.









* [cs231n](https://github.com/cs231n)
* [cs231n](https://twitter.com/cs231n)












 // Make responsive
 MathJax.Hub.Config({
 ""HTML-CSS"": { linebreaks: { automatic: true } },
 ""SVG"": { linebreaks: { automatic: true } },
 });
 


"
h7iBpEHGVNc?si=PoPmFt5y_k4cSeol,"0, 2940","- Okay so welcome to
CS 231N Lecture three. Today we're going to talk about
loss functions and optimization but as usual, before we
get to the main content of the lecture, there's a
couple administrative things to talk about. So the first thing is that
assignment one has been released. You can find the link up on the website. And since we were a little bit late in getting this assignment
out to you guys, we've decided to change
the due date to Thursday, April 20th at 11:59 p.m., this will give you a full
two weeks from the assignment release date to go and
actually finish and work on it, so we'll update the syllabus
for this new due date in a little bit later today. And as a reminder, when you
complete the assignment, you should go turn in the
final zip file on Canvas so we can grade it and get
your grades back as quickly as possible. So the next thing is always
check out Piazza for interesting administrative stuff. So this week I wanted to
highlight that we have several example project ideas as
a pinned post on Piazza. So we went out and solicited
example of project ideas from various people in the
Stanford community or affiliated to Stanford, and they came
up with some interesting suggestions for projects
that they might want students in the class to work on. So check out this pinned post
on Piazza and if you want to work on any of these projects,
then feel free to contact the project mentors
directly about these things. Aditionally we posted office
hours on the course website, this is a Google calendar, so
this is something that people have been asking about
and now it's up there. The final administrative
note is about Google Cloud, as a reminder, because we're
supported by Google Cloud in this class, we're able to
give each of you an additional $100 credit for Google Cloud
to work on your assignments and projects, and the exact
details of how to redeem that credit will go out later
today, most likely on Piazza. So if there's, I guess if
there's no questions about administrative stuff then we'll
move on to course content. Okay cool. So recall from last time in lecture two, we were really talking about
the challenges of recognition and trying to hone in on this idea of a data-driven approach. We talked about this idea
of image classification, talked about why it's hard,
there's this semantic gap between the giant grid of
numbers that the computer sees and the actual image that you see. We talked about various
challenges regarding this around illumination,
deformation, et cetera, and why this is actually a
really, really hard problem even though it's super
easy for people to do with their human eyes
and human visual system. Then also recall last time
we talked about the k-nearest neighbor classifier as kind
of a simple introduction to this whole data-driven mindset. We talked about the CIFAR-10
data set where you can see an example of these images
on the upper left here, where CIFAR-10 gives you
these 10 different categories, airplane, automobile, whatnot, and we talked about how the
k-nearest neighbor classifier can be used to learn decision boundaries to separate these data points into classes based on the training data. This also led us to a
discussion of the idea of cross validation and setting
hyper parameters by dividing your data into train,
validation and test sets. Then also recall last time
we talked about linear classification as the first
sort of building block as we move toward neural networks. Recall that the linear
classifier is an example of a parametric classifier
where all of our knowledge about the training data gets summarized into this parameter matrix W that is set during the process of training. And this linear classifier
recall is super simple, where we're going to take
the image and stretch it out into a long vector. So here the image is x and
then we take that image which might be 32 by 32 by
3 pixels, stretch it out into a long column vector of 32 times 32 times 3 entries, where the 32 and 32 are
the height and width, and the 3 give you
the three color channels, red, green, blue. Then there exists some parameter matrix, W which will take this long column vector representing the image
pixels, and convert this and give you 10 numbers giving scores for each of the 10 classes
in the case of CIFAR-10. Where we kind of had this interpretation where larger values of those scores, so a larger value for the cat
class means the classifier thinks that the cat is
more likely for that image, and lower values for
maybe the dog or car class indicate lower probabilities
of those classes being present in the image. Also, so I think this point
was a little bit unclear last time that linear classification
has this interpretation as learning templates per class, where if you look at the
diagram on the lower left, you think that, so for
every pixel in the image, and for every one of our 10 classes, there exists some entry in this matrix W, telling us how much does that
pixel influence that class. So that means that each of
these rows in the matrix W ends up corresponding to
a template for the class. And if we take those rows and unravel, so each of those rows again corresponds to a weighting between the values of, between the pixel values of
the image and that class, so if we take that row and
unravel it back into an image, then we can visualize the
learned template for each of these classes. We also had this interpretation
of linear classification as learning linear decision
boundaries between pixels in some high dimensional
space where the dimensions of the space correspond
to the values of the pixel intensity values of the image. So this is kind of where
we left off last time. And so where we kind of
stopped, where we ended up last time is we got this idea
of a linear classifier, and we didn't talk about how
to actually choose the W. How to actually use the training data to determine which value
of W should be best. So kind of where we stopped off at is that for some setting
of W, we can use this W to come up with 10 with our
class scores for any image. So and some of these class
scores might be better or worse. So here in this simple example, we've shown maybe just a
training data set of three images along with the 10 class scores
predicted for some value of W for those images. And you can see that some
of these scores are better or worse than others. So for example in the image
on the left, if you look up, it's actually a cat because you're a human and you can tell these things, but if we look at the
assigned probabilities, cat, well not probabilities but scores, then the classifier maybe
for this setting of W gave the cat class a score
of 2.9 for this image, whereas the frog class gave 3.78. So maybe the classifier
is not doing not so good on this image, that's bad,
we wanted the true class to be actually the highest class score, whereas for some of these
other examples, like the car for example, you see
that the automobile class has a score of six which is much higher than any of the others, so that's good. And the frog, the predicted
scores are maybe negative four, which is much lower
than all the other ones, so that's actually bad. So this is kind of a hand wavy approach, just kind of looking at
the scores and eyeballing which ones are good
and which ones are bad. But to actually write
algorithms about these things and to actually to determine
automatically which W will be best, we need some
way to quantify the badness of any particular W. And that's this function
that takes in a W, looks at the scores and then
tells us how bad quantitatively is that W, is something that
we'll call a loss function. And in this lecture we'll
see a couple examples of different loss functions
that you can use for this image classification problem. So then once we've got this
idea of a loss function, this allows us to quantify
for any given value of W, how good or bad is it? But then we actually need to find and come up with an efficient procedure for searching through the
space of all possible Ws and actually come up with
what is the correct value of W that is the least bad, and this process will be
an optimization procedure and we'll talk more about
that in this lecture. So I'm going to shrink
this example a little bit because 10 classes is
a little bit unwieldy. So we'll kind of work with
this tiny toy data set of three examples and
three classes going forward in this lecture. So again, in this example, the
cat is maybe not so correctly classified, the car is correctly
classified, and the frog, this setting of W got this
frog image totally wrong, because the frog score is
much lower than others. So to formalize this a little
bit, usually when we talk about a loss function, we imagine that we have some training
data set of xs and ys, usually N examples of these
where the xs are the inputs to the algorithm in the
image classification case, the xs would be the actually
pixel values of your images, and the ys will be the things
you want your algorithm to predict, we usually call
these the labels or the targets. So in the case of image classification, remember we're trying
to categorize each image for CIFAR-10 to one of 10 categories, so the label y here will be an integer between one and 10 or
maybe between zero and nine depending on what programming
language you're using, but it'll be an integer telling you what is the correct category
for each one of those images x. And now our loss function
will denote L_i to denote the, so then we have this prediction function x which takes in our example
x and our weight matrix W and makes some prediction for y, in the case of image classification these will be our 10 numbers. Then we'll define some loss function L_i which will take in the predicted scores coming out of the function f together with the true target or label Y and give us some quantitative
value for how bad those predictions are for
that training example. And now the final loss L will
be the average of these losses summed over the entire data
set over each of the N examples in our data set. So this is actually a
very general formulation, and actually extends even
beyond image classification. Kind of as we move forward
and see other tasks, other examples of tasks and deep learning, the kind of generic setup
is that for any task you have some xs and ys
and you want to write down some loss function that
quantifies exactly how happy you are with your particular
parameter settings W and then you'll eventually
search over the space of W to find the W that minimizes
the loss on your training data. So as a first example of
a concrete loss function that is a nice thing to work
with in image classification, we'll talk about the multi-class SVM loss. You may have seen the binary
SVM, our support vector machine in CS 229 and the multiclass SVM is a generalization of that
to handle multiple classes. In the binary SVM case as
you may have seen in 229, you only had two classes, each example x was going to be classified
as either positive or negative example, but now we have 10 categories,
so we need to generalize this notion to handle multiple classes. So this loss function has kind
of a funny functional form, so we'll walk through it in a bit more, in quite a bit of detail over
the next couple of slides. But what this is saying
is that the loss L_i for any individual example,
the way we'll compute it is we're going to perform a sum
over all of the categories, Y, except for the true category, Y_i, so we're going to sum over
all the incorrect categories, and then we're going to compare the score of the correct category, and the score of the incorrect category, and now if the score
for the correct category is greater than the score
of the incorrect category, greater than the incorrect
score by some safety margin that we set to one, if that's
the case that means that the true score is much, or the
score for the true category is if it's much larger than
any of the false categories, then we'll get a loss of zero. And we'll sum this up over all
of the incorrect categories for our image and this
will give us our final loss for this one example in the data set. And again we'll take
the average of this loss over the whole training data set. So this kind of like if then
statement, like if the true class score is much
larger than the others, this kind of if then
formulation we often compactify into this single max of zero
S_j minus S_Yi plus one thing, but I always find that notation
a little bit confusing, and it always helps me to write it out in this
sort of case based notation to figure out exactly
what the two cases are and what's going on. And by the way, this
style of loss function where we take max of zero
and some other quantity is often referred to as
some type of a hinge loss, and this name comes from
the shape of the graph when you go and plot it, so here the x axis corresponds to the S_Yi, that is the score of the
true class for some training example, and now the y axis is the loss, and you can see that as the
score for the true category for this example increases, then the loss will go down linearly until we get to above this safety margin, after which the loss will be zero because we've already correctly
classified this example. So let's, oh, question? - [Student] Sorry, in terms of notation what is S underscore Yi? Is that your right score? - Yeah, so the question is in terms of notation,
what is S and what is SYI in particular, so the Ss
are the predicted scores for the classes that are
coming out of the classifier. So if one is the cat class and
two is the dog class then S1 and S2 would be the cat and
dog scores respectively. And remember we said that Yi
was the category of the ground truth label for the example
which is some integer. So then S sub Y sub i, sorry
for the double subscript, that corresponds to the
score of the true class for the i-th example in the training set. Question? - [Student] So what
exactly is this computing? - Yeah the question is what
exactly is this computing here? It's a little bit funny, I
think it will become more clear when we walk through an explicit
example, but in some sense what this loss is saying is
that we are happy if the true score is much higher than
all the other scores. It needs to be higher
than all the other scores by some safety margin,
and if the true score is not high enough, greater
than any of the other scores, then we will incur some
loss and that would be bad. So this might make a little bit more sense if we walk through an explicit example for this tiny three example data set. So here remember I've sort
of removed the case space notation and just switching
back to the zero one notation, and now if we look at, if we think about computing
this multi-class SVM loss for just this first training example on the left, then remember
we're going to loop over all of the incorrect
classes, so for this example, cat is the correct class, so
we're going to loop over the car and frog classes, and now for
car, we're going to compare the, we're going to look at the car
score, 5.1, minus the cat score, 3.2 plus one, when we're
comparing cat and car we expect to incur some loss here because
the car score is greater than the cat score which is bad. So for this one class,
for this one example, we'll incur a loss of 2.9, and then when we go and
compare the cat score and the frog score we see that cat is 3.2, frog is minus 1.7, so cat is more than one greater than frog, which means that between these two classes we incur zero loss. So then the multiclass SVM
loss for this training example will be the sum of the losses
across each of these pairs of classes, which will be
2.9 plus zero which is 2.9. Which is sort of saying that
2.9 is a quantitative measure of how much our classifier screwed up on this one training example. And then if we repeat this procedure for this next car image, then
again the true class is car, so we're going to iterate
over all the other categories when we compare the car and the cat score, we see that car is more
than one greater than cat so we get no loss here. When we compare car and frog, we again see that the car score is more
than one greater than frog, so we get again no loss
here, and our total loss for this training example is zero. And now I think you hopefully
get the picture by now, but, if you go look at frog, now
frog, we again compare frog and cat, incur quite a lot of
loss because the frog score is very low, compare frog
and car, incur a lot of loss because the score is very low,
and then our loss for this example is 12.9. And then our final loss
for the entire data set is the average of these losses across the different examples, so when you sum those out
it comes to about 5.3. So then it's sort of, this
is our quantitative measure that our classifier is
5.3 bad on this data set. Is there a question? - [Student] How do you
choose the plus one? - Yeah, the question is how
do you choose the plus one? That's actually a really great question, it seems like kind of an
arbitrary choice here, it's the only constant that
appears in the loss function and that seems to offend
your aesthetic sensibilities a bit maybe. But it turns out that this is somewhat of an arbitrary choice,
because we don't actually care about the absolute values of the scores in this loss function, we only care about the relative differences
between the scores. We only care that the correct score is much greater than the incorrect scores. So in fact if you imagine
scaling up your whole W up or down, then it kind
of rescales all the scores correspondingly and if you kind
of work through the details and there's a detailed derivation
of this in the course notes online, you find this choice
of one actually doesn't matter. That this free parameter
of one kind of washes out and is canceled with this scale, like the overall setting
of the scale in W. And again, check the course
notes for a bit more detail on that. So then I think it's
kind of useful to think about a couple different
questions to try to understand intuitively what this loss is doing. So the first question is what's
going to happen to the loss if we change the scores of the
car image just a little bit? Any ideas? Everyone's too scared to ask a question? Answer? [student speaking faintly] - Yeah, so the answer is
that if we jiggle the scores for this car image a little
bit, the loss will not change. So the SVM loss, remember,
the only thing it cares about is getting the correct
score to be greater than one more than the incorrect
scores, but in this case, the car score is already quite
a bit larger than the others, so if the scores for this
class changed for this example changed just a little
bit, this margin of one will still be retained and
the loss will not change, we'll still get zero loss. The next question, what's
the min and max possible loss for SVM? [student speaking faintly] Oh I hear some murmurs. So the minimum loss is zero,
because if you can imagine that across all the classes, if
our correct score was much larger then we'll incur zero
loss across all the classes and it will be zero, and if you think back to this
hinge loss plot that we had, then you can see that if the correct score goes very, very negative,
then we could incur potentially infinite loss. So the min is zero and
the max is infinity. Another question, sort of when
you initialize these things and start training from scratch, usually you kind of initialize W with some small random values,
as a result your scores tend to be sort of small
uniform random values at the beginning of training. And then the question is
that if all of your Ss, if all of the scores
are approximately zero and approximately equal, then what kind of loss do you expect when you're using multiclass SVM? - [Student] Number of classes minus one. - Yeah, so the answer is
number of classes minus one, because remember that
if we're looping over all of the incorrect classes,
so we're looping over C minus one classes, within
each of those classes the two Ss will be about the same, so we'll get a loss of one because of the margin and
we'll get C minus one. So this is actually kind
of useful because when you, this is a useful debugging strategy when you're using these things, that when you start off training, you should think about what
you expect your loss to be, and if the loss you actually
see at the start of training at that first iteration is
not equal to C minus one in this case, that means you probably have
a bug and you should go check your code, so this is actually
kind of a useful thing to be checking in practice. Another question, what happens
if, so I said we're summing an SVM over the incorrect
classes, what happens if the sum is also over the correct class if we just go over everything? - [Student] The loss increases by one. - Yeah, so the answer is that
the loss increases by one. And I think the reason
that we do this in practice is because normally loss of
zero is kind of, has this nice interpretation that
you're not losing at all, so that's nice, so I think your answers wouldn't really change, you would end up finding
the same classifier if you actually looped
over all the categories, but if just by conventions
we omit the correct class so that our minimum loss is zero. So another question, what if we used mean instead of sum here? - [Student] Doesn't change. - Yeah, the answer is
that it doesn't change. So the number of classes is
going to be fixed ahead of time when we select our data set,
so that's just rescaling the whole loss function by a constant, so it doesn't really matter,
it'll sort of wash out with all the other scale things because we don't actually
care about the true values of the scores, or the
true value of the loss for that matter. So now here's another
example, what if we change this loss formulation and we
actually added a square term on top of this max? Would this end up being the same problem or would this be a different
classification algorithm? - [Student] Different. - Yes, this would be different. So here the idea is that
we're kind of changing the trade-offs between good and badness in kind of a nonlinear way, so this would end up actually computing a different loss function. This idea of a squared hinge
loss actually does get used sometimes in practice, so
that's kind of another trick to have in your bag when you're making up your own loss functions
for your own problems. So now you'll end up,
oh, was there a question? - [Student] Why would
you use a squared loss instead of a non-squared loss? - Yeah, so the question is
why would you ever consider using a squared loss instead
of a non-squared loss? And the whole point of a loss function is to kind of quantify how
bad are different mistakes. And if the classifier is making
different sorts of mistakes, how do we weigh off the
different trade-offs between different types
of mistakes the classifier might make? So if you're using a squared loss, that sort of says that things
that are very, very bad are now going to be squared bad so that's like really, really bad, like we don't want anything that's totally
catastrophically misclassified, whereas if you're using this hinge loss, we don't actually care between
being a little bit wrong and being a lot wrong, being
a lot wrong kind of like, if an example is a lot
wrong, and we increase it and make it a little bit less wrong, that's kind of the same
goodness as an example which was only a little bit
wrong and then increasing it to be a little bit more right. So that's a little bit hand wavy, but this idea of using
a linear versus a square is a way to quantify how much we care about different categories of errors. And this is definitely something
that you should think about when you're actually applying
these things in practice, because the loss function is the way that you tell your algorithm
what types of errors you care about and what types of errors it should trade off against. So that's actually super
important in practice depending on your application. So here's just a little snippet
of sort of vectorized code in numpy, and you'll end up implementing something like this for
the first assignment, but this kind of gives you
the sense that this sum is actually like pretty easy to implement in numpy, it
only takes a couple lines of vectorized code. And you can see in practice,
like one nice trick is that we can actually go in
here and zero out the margins corresponding to the correct class, and that makes it easy to then just, that's sort of one nice
vectorized trick to skip, iterate over all but one class. You just kind of zero out
the one you want to skip and then compute the sum
anyway, so that's a nice trick you might consider
using on the assignment. So now, another question
about this loss function. Suppose that you were
lucky enough to find a W that has loss of zero,
you're not losing at all, you're totally winning, this loss function is crushing it, but then there's a
question, is this W unique or were there other Ws that could also have achieved zero loss? - [Student] There are other Ws. - Answer, yeah, so there
are definitely other Ws. And in particular, because
we talked a little bit about this thing of scaling
the whole problem up or down depending on W, so you
could actually take W multiplied by two and this
doubled W (Is it quad U now? I don't know.) [laughing] This would also achieve zero loss. So as a concrete example of this, you can go back to your favorite example and maybe work through the numbers a little bit later, but if you're taking W and we double W, then the margins between the
correct and incorrect scores will also double. So that means that if all these margins were already greater than
one, and we doubled them, they're still going to
be greater than one, so you'll still have zero loss. And this is kind of interesting, because if our loss function is the way that we tell our
classifier which W we want and which W we care about, this is a little bit weird, now there's this inconsistency and how is the classifier to choose between these different versions of W that all achieve zero loss? And that's because what we've done here is written down only a
loss in terms of the data, and we've only told our classifier that it should try to find the W that fits the training data. But really in practice,
we don't actually care that much about fitting the training data, the whole point of machine learning is that we use the training
data to find some classifier and then we'll apply
that thing on test data. So we don't really care about
the training data performance, we really care about the performance of this classifier on test data. So as a result, if the only thing we're telling our classifier to do is fit the training data, then we can lead ourselves into some of these weird
situations sometimes, where the classifier might
have unintuitive behavior. So a concrete, canonical example
of this sort of thing, by the way, this is not
linear classification anymore, this is a little bit of a more general machine learning concept, is that suppose we have this
data set of blue points, and we're going to fit some
curve to the training data, the blue points, then if the only thing we've
told our classifier to do is to try and fit the training data, it might go in and have very wiggly curves to try to perfectly classify all of the training data points. But this is bad, because
we don't actually care about this performance, we care about the
performance on the test data. So now if we have some new data come in that sort of follows the same trend, then this very wiggly blue line is going to be totally wrong. And in fact, what we
probably would have preferred the classifier to do was maybe predict this straight green line, rather than this very complex wiggly line to perfectly fit all the training data. And this is a core fundamental problem in machine learning, and the way we usually solve it, is this concept of regularization. So here we're going to
add an additional term to the loss function. In addition to the data loss, which will tell our
classifier that it should fit the training data,
we'll also typically add another term to the loss function called a regularization term, which encourages the model
to somehow pick a simpler W, where the concept of simple kind of depends on the task and the model. There's this whole idea of Occam's Razor, which is this fundamental
idea in scientific discovery more broadly, which is that
if you have many different competing hypotheses, that could explain your observations, you should generally
prefer the simpler one, because that's the explanation
that is more likely to generalize to new
observations in the future. And the way we
operationalize this intuition in machine learning is
typically through some explicit regularization penalty that's often written down as R. So then your standard loss function usually has these two terms, a data loss and a regularization loss, and there's some
hyper-parameter here, lambda, that trades off between the two. And we talked about hyper-parameters and cross-validation in the last lecture, so this regularization
hyper-parameter lambda will be one of the more important ones that you'll need to tune when training these models in practice. Question? - [Student] What does that lambda R W term have to do with [speaking faintly]. - Yeah, so the question is, what's the connection
between this lambda R W term and actually forcing this wiggly line to become a straight green line? I didn't want to go through
the derivation on this because I thought it would
lead us too far astray, but you can imagine, maybe you're doing a regression problem, in terms of different
polynomial basis functions, and if you're adding
this regression penalty, maybe the model has access to polynomials of very high degree, but
through this regression term you could encourage the
model to prefer polynomials of lower degree, if they
fit the data properly, or if they fit the data relatively well. So you could imagine
there's two ways to do this, either you can constrain your model class to just not contain the more powerful, more complex models, or you
can add this soft penalty where the model still has
access to more complex models, maybe high degree
polynomials in this case, but you add this soft constraint saying that if you want to
use these more complex models, you need to overcome this penalty for using their complexity. So that's the connection here, that is not quite linear classification, this is the picture that
many people have in mind when they think about
regularization at least. So there's actually a
lot of different types of regularization that
get used in practice. The most common one is
probably L2 regularization, or weight decay. But there's a lot of other
ones that you might see. This L2 regularization is
just the euclidean norm of this weight vector W, or sometimes the squared norm. Or sometimes half the squared norm because it makes your derivatives work out a little bit nicer. But the idea of L2 regularization is you're just penalizing
the euclidean norm of this weight vector. You might also sometimes
see L1 regularization, where we're penalizing the
L1 norm of the weight vector, and the L1 regularization
has some nice properties like encouraging sparsity
in this matrix W. Some other things you might see would be this elastic net regularization, which is some combination of L1 and L2. You sometimes see max norm regularization, penalizing the max norm
rather than the L1 or L2 norm. But these sorts of regularizations are things that you see
not just in deep learning, but across many areas of machine learning and even optimization more broadly. In some later lectures, we'll also see some types of regularization
that are more specific to deep learning. For example dropout, we'll
see in a couple lectures, or batch normalization, stochastic depth, these things get kind of
crazy in recent years. But the whole idea of regularization is just any thing that
you do to your model, that sort of penalizes somehow
the complexity of the model, rather than explicitly trying
to fit the training data. Question? [student speaking faintly] Yeah, so the question is, how does the L2 regularization
measure the complexity of the model? Thankfully we have an
example of that right here, maybe we can walk through. So here we maybe have
some training example, x, and there's two different
Ws that we're considering. So x is just this vector of four ones, and we're considering these
two different possibilities for W. One is a one in the
first, one is a single one and three zeros, and the other has this 0.25 spread across the four different entries. And now, when we're doing
linear classification, we're really taking dot products between our x and our W. So in terms of linear classification, these two Ws are the same, because they give the same result when dot producted with x. But now the question is, if you look at these two examples, which one would L2 regression prefer? Yeah, so L2 regression would prefer W2, because it has a smaller norm. So the answer is that the L2 regression measures complexity of the classifier in this relatively coarse way, where the idea is that, remember the Ws in linear classification had this interpretation of how much does this value of the vector x correspond to this output class? So L2 regularization is saying that it prefers to spread that influence across all the different values in x. Maybe this might be more robust, in case you come up with xs that vary, then our decisions are spread out and depend on the entire x vector, rather than depending
only on certain elements of the x vector. And by the way, L1 regularization has this opposite interpretation. So actually if we were
using L1 regularization, then we would actually prefer W1 over W2, because L1 regularization
has this different notion of complexity, saying that
maybe the model is less complex, maybe we measure model
complexity by the number of zeros in the weight vector, so the question of how
do we measure complexity and how does L2 measure complexity? They're kind of problem dependent. And you have to think about
for your particular setup, for your particular model and data, how do you think that
complexity should be measured on this task? Question? - [Student] So why would L1 prefer W1? Don't they sum to the same one? - Oh yes, you're right. So in this case, L1 is actually the same between these two. But you could construct
a similar example to this where W1 would be preferred
by L1 regularization. I guess the general intuition behind L1 is that it generally
prefers sparse solutions, that it drives all your
entries of W to zero for most of the entries,
except for a couple where it's allowed to deviate from zero. The way of measuring complexity for L1 is maybe the number of non-zero entries, and then for L2, it thinks
that things that spread the W across all the values are less complex. So it depends on your data,
depends on your problem. Oh and by the way, if
you're a hardcore Bayesian, then using L2 regularization
has this nice interpretation of MAP inference under a Gaussian prior on the parameter vector. I think there was a
homework problem about that in 229, but we won't talk about that for the rest of the quarter. That's sort of my long, deep dive into the multi-class SVM loss. Question? - [Student] Yeah, so I'm still confused about what the kind of stuff I need to do when the linear versus polynomial thing, because the use of this loss function isn't going to change the
fact that you're just doing, you're looking at a
linear classifier, right? - Yeah, so the question is that, adding a regularization is not going to change
the hypothesis class. This is not going to change us
away from a linear classifier. The idea is that maybe this example of this polynomial regression is definitely not linear regression. That could be seen as linear regression on top of a polynomial
expansion of the input, and in which case, this
regression sort of says that you're not allowed
to use as many polynomial coefficients as maybe you should have. Right, so you can imagine this is like, when you're doing polynomial regression, you can write out a polynomial as f of x equals A zero plus A one
x plus A two x squared plus A three x whatever, in that case your parameters, your Ws, would be these As, in which case, penalizing the W could force it towards lower degree polynomials. Except in the case of
polynomial regression, you don't actually want to parameterize in terms of As, there's
some other paramterization that you want to use, but that's the general idea, that you're sort of penalizing
the parameters of the model to force it towards the simpler hypotheses within your hypothesis class. And maybe we can take this offline if that's still a bit confusing. So then we've sort of seen
this multi-class SVM loss, and just by the way as a side note, this is one extension or
generalization of the SVM loss to multiple classes, there's actually a couple
different formulations that you can see around in literature, but I mean, my intuition is
that they all tend to work similarly in practice, at least in the context of deep learning. So we'll stick with this
one particular formulation of the multi-class SVM loss in this class. But of course there's many
different loss functions you might imagine. And another really popular choice, in addition to the multi-class SVM loss, another really popular
choice in deep learning is this multinomial logistic regression, or a softmax loss. And this one is probably
actually a bit more common in the context of deep learning, but I decided to present
this second for some reason. So remember in the context
of the multi-class SVM loss, we didn't actually have an interpretation for those scores. Remember, when we're
doing some classification, our model F, spits our these 10 numbers, which are our scores for the classes, and for the multi-class SVM, we didn't actually give much
interpretation to those scores. We just said that we want the true score, the score of the correct class to be greater than the incorrect classes, and beyond that we don't really
say what those scores mean. But now, for the multinomial
logistic regression loss function, we actually
will endow those scores with some additional meaning. And in particular we're
going to use those scores to compute a probability distribution over our classes. So we use this so-called softmax function where we take all of our scores, we exponentiate them so that
now they become positive, then we re-normalize them by
the sum of those exponents so now after we send our scores through this softmax function, now we end up with this
probability distribution, where now we have
probabilities over our classes, where each probability
is between zero and one, and the sum of probabilities
across all classes sum to one. And now the interpretation
is that we want, there's this computed
probability distribution that's implied by our scores, and we want to compare
this with the target or true probability distribution. So if we know that the thing is a cat, then the target probability distribution would put all of the
probability mass on cat, so we would have probability
of cat equals one, and zero probability for
all the other classes. So now what we want to do is encourage our computed probability distribution that's coming out of this softmax function to match this target
probability distribution that has all the mass
on the correct class. And the way that we do this, I mean, you can do this
equation in many ways, you can do this as a KL divergence between the target and the computed probability distribution, you can do this as a
maximum likelihood estimate, but at the end of the day, what we really want is
that the probability of the true class is
high and as close to one. So then our loss will
now be the negative log of the probability of the true class. This is confusing 'cause
we're putting this through multiple different things, but remember we wanted the probability to be close to one, so now log is a monotonic
function, it goes like this, and it turns out mathematically, it's easier to maximize log than it is to maximize
the raw probability, so we stick with log. And now log is monotonic, so if we maximize log P of correct class, that means we want that to be high, but loss functions measure
badness not goodness so we need to put in the minus one to make it go the right way. So now our loss function for SVM is going to be the minus
log of the probability of the true class. Yeah, so that's the summary here, is that we take our scores,
we run through the softmax, and now our loss is this
minus log of the probability of the true class. Okay, so then if you look
at what this looks like on a concrete example, then we go back to our
favorite beautiful cat with our three examples and
we've got these three scores that are coming out of
our linear classifier, and these scores are exactly
the way that they were in the context of the SVM loss. But now, rather than taking these scores and putting them directly
into our loss function, we're going to take them
all and exponentiate them so that they're all positive, and then we'll normalize them to make sure that they all sum to one. And now our loss will be the minus log of the true class score. So that's the softmax loss, also called multinomial
logistic regression. So now we asked several questions to try to gain intuition about
the multi-class SVM loss, and it's useful to think about
some of the same questions to contrast with the softmax loss. So then the question is, what's the min and max
value of the softmax loss? Okay, maybe not so sure, there's too many logs and sums and stuff going on in here. So the answer is that the min loss is zero and the max loss is infinity. And the way that you can see this, the probability distribution that we want is one on the correct class,
zero on the incorrect classes, the way that we do that is, so if that were the case, then this thing inside the
log would end up being one, because it's the log
probability of the true class, then log of one is zero, minus
log of one is still zero. So that means that if we
got the thing totally right, then our loss would be zero. But by the way, in order to
get the thing totally right, what would our scores have to look like? Murmuring, murmuring. So the scores would actually
have to go quite extreme, like towards infinity. So because we actually
have this exponentiation, this normalization, the only way we can actually get a
probability distribution of one and zero, is actually
putting an infinite score for the correct class,
and minus infinity score for all the incorrect classes. And computers don't do
so well with infinities, so in practice, you'll
never get to zero loss on this thing with finite precision. But you still have this interpretation that zero is the theoretical
minimum loss here. And the maximum loss is unbounded. So suppose that if we
had zero probability mass on the correct class, then
you would have minus log of zero, log of zero is minus infinity, so minus log of zero
would be plus infinity, so that's really bad. But again, you'll never really get here because the only way you can
actually get this probability to be zero, is if e to the
correct class score is zero, and that can only happen
if that correct class score is minus infinity. So again, you'll never
actually get to these minimum, maximum values with finite precision. So then, remember we had this debugging, sanity check question in the
context of the multi-class SVM, and we can ask the same for the softmax. If all the Ss are small and about zero, then what is the loss here? Yeah, answer? - [Student] Minus log one over C. - So minus log of one over C? I think that's, yeah, so then it'd be minus log of one over C, because log can flip the thing so then it's just log of C. Yeah, so it's just log of C. And again, this is a nice debugging thing, if you're training a model
with this softmax loss, you should check at the first iteration. If it's not log C, then
something's gone wrong. So then we can compare and
contrast these two loss functions a bit. In terms of linear classification, this setup looks the same. We've got this W matrix
that gets multiplied against our input to produce
this specter of scores, and now the difference
between the two loss functions is how we choose to interpret those scores to quantitatively measure
the badness afterwards. So for SVM, we were going to
go in and look at the margins between the scores of the correct class and the scores of the incorrect class, whereas for this softmax
or cross-entropy loss, we're going to go and compute
a probability distribution and then look at the minus log probability of the correct class. So sometimes if you look at, in terms of, nevermind,
I'll skip that point. [laughing] So another question that's interesting when contrasting these two
loss functions is thinking, suppose that I've got this example point, and if you change its scores, so assume that we've got
three scores for this, ignore the part on the bottom. But remember if we go back to this example where in the multi-class SVM loss, when we had the car, and the
car score was much better than all the incorrect classes, then jiggling the scores
for that car image didn't change the
multi-class SVM loss at all, because the only thing that the SVM loss cared about was getting that correct score to be greater than a margin
above the incorrect scores. But now the softmax loss
is actually quite different in this respect. The softmax loss actually
always wants to drive that probability mass all the way to one. So even if you're giving very high score to the correct class, and very low score to all the incorrect classes, softmax will want you to pile
more and more probability mass on the correct class, and
continue to push the score of that correct class up towards infinity, and the score of the incorrect classes down towards minus infinity. So that's the interesting difference between these two loss
functions in practice. That SVM, it'll get this
data point over the bar to be correctly classified
and then just give up, it doesn't care about
that data point any more. Whereas softmax will just always
try to continually improve every single data point
to get better and better and better and better. So that's an interesting difference between these two functions. In practice, I think it tends
not to make a huge difference which one you choose, they tend to perform pretty similarly across, at least a lot of deep
learning applications. But it is very useful to keep
some of these differences in mind. Yeah, so to recap where
we've come to from here, is that we've got some
data set of xs and ys, we use our linear classifier
to get some score function, to compute our scores
S, from our inputs, x, and then we'll use a loss function, maybe softmax or SVM or
some other loss function to compute how quantitatively
bad were our predictions compared to this ground true targets, y. And then we'll often
augment this loss function with a regularization term, that tries to trade off between
fitting the training data and preferring simpler models.",https://cs231n.github.io/linear-classify/,"




CS231n Convolutional Neural Networks for Visual Recognition









 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1\*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-46895817-2', 'auto');
 ga('send', 'pageview');
 



addBackToTop({
 backgroundColor: '#fff',
 innerHTML: 'Back to Top',
 textColor: '#333'
 })

 #back-to-top {
 border: 1px solid #ccc;
 border-radius: 0;
 font-family: sans-serif;
 font-size: 14px;
 width: 100px;
 text-align: center;
 line-height: 30px;
 height: 30px;
 }
 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io)
[Course Website](http://cs231n.stanford.edu/)





# 




Table of Contents:


* [Linear Classification](#linear-classification)
	+ [Parameterized mapping from images to label scores](#parameterized-mapping-from-images-to-label-scores)
	+ [Interpreting a linear classifier](#interpreting-a-linear-classifier)
	+ [Loss function](#loss-function)
		- [Multiclass Support Vector Machine loss](#multiclass-support-vector-machine-loss)
	+ [Practical Considerations](#practical-considerations)
	+ [Softmax classifier](#softmax-classifier)
	+ [SVM vs. Softmax](#svm-vs-softmax)
	+ [Interactive web demo](#interactive-web-demo)
	+ [Summary](#summary)
	+ [Further Reading](#further-reading)



## Linear Classification


In the last section we introduced the problem of Image Classification, which is
the task of assigning a single label to an image from a fixed set of categories.
Moreover, we described the k-Nearest Neighbor (kNN) classifier which labels
images by comparing them to (annotated) images from the training set. As we saw,
kNN has a number of disadvantages:


* The classifier must *remember* all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.
* Classifying a test image is expensive since it requires a comparison to all training images.


**Overview**. We are now going to develop a more powerful approach to image
classification that we will eventually naturally extend to entire Neural
Networks and Convolutional Neural Networks. The approach will have two major
components: a **score function** that maps the raw data to class scores, and a
**loss function** that quantifies the agreement between the predicted scores and
the ground truth labels. We will then cast this as an optimization problem in
which we will minimize the loss function with respect to the parameters of the
score function.



### Parameterized mapping from images to label scores


The first component of this approach is to define the score function that maps
the pixel values of an image to confidence scores for each class. We will
develop the approach with a concrete example. As before, let’s assume a training
dataset of images \( x\_i \in R^D \), each associated with a label \( y\_i \).
Here \( i = 1 \dots N \) and \( y\_i \in { 1 \dots K } \). That is, we have
**N** examples (each with a dimensionality **D**) and **K** distinct categories.
For example, in CIFAR-10 we have a training set of **N** = 50,000 images, each
with **D** = 32 x 32 x 3 = 3072 pixels, and **K** = 10, since there are 10
distinct classes (dog, cat, car, etc). We will now define the score function
\(f: R^D \mapsto R^K\) that maps the raw image pixels to class scores.


**Linear classifier.** In this module we will start out with arguably the
simplest possible function, a linear mapping:



\[f(x\_i, W, b) = W x\_i + b\]

In the above equation, we are assuming that the image \(x\_i\) has all of its
pixels flattened out to a single column vector of shape [D x 1]. The matrix
**W** (of size [K x D]), and the vector **b** (of size [K x 1]) are the
**parameters** of the function. In CIFAR-10, \(x\_i\) contains all pixels in the
i-th image flattened into a single [3072 x 1] column, **W** is [10 x 3072] and
**b** is [10 x 1], so 3072 numbers come into the function (the raw pixel values)
and 10 numbers come out (the class scores). The parameters in **W** are often
called the **weights**, and **b** is called the **bias vector** because it
influences the output scores, but without interacting with the actual data
\(x\_i\). However, you will often hear people use the terms *weights* and
*parameters* interchangeably.


There are a few things to note:


* First, note that the single matrix multiplication \(W x\_i\) is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of **W**.
* Notice also that we think of the input data \( (x\_i, y\_i) \) as given and fixed, but we have control over the setting of the parameters **W,b**. Our goal will be to set these in such way that the computed scores match the ground truth labels across the whole training set. We will go into much more detail about how this is done, but intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes.
* An advantage of this approach is that the training data is used to learn the parameters **W,b**, but once the learning is complete we can discard the entire training set and only keep the learned parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores.
* Lastly, note that classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images.



> 
> Foreshadowing: Convolutional Neural Networks will map image pixels to scores
> exactly as shown above, but the mapping ( f ) will be more complex and will
> contain more parameters.
> 
> 
> 



### Interpreting a linear classifier


Notice that a linear classifier computes the score of a class as a weighted sum
of all of its pixel values across all 3 of its color channels. Depending on
precisely what values we set for these weights, the function has the capacity to
like or dislike (depending on the sign of each weight) certain colors at certain
positions in the image. For instance, you can imagine that the “ship” class
might be more likely if there is a lot of blue on the sides of an image (which
could likely correspond to water). You might expect that the “ship” classifier
would then have a lot of positive weights across its blue channel weights
(presence of blue increases score of ship), and negative weights in the
red/green channels (presence of red/green decreases the score of ship).



![](/assets/imagemap.jpg)
An example of mapping an image to class scores. For the sake of visualization, we assume the image only has 4 pixels (4 monochrome pixels, we are not considering color channels in this example for brevity), and that we have 3 classes (red (cat), green (dog), blue (ship) class). (Clarification: in particular, the colors here simply indicate 3 classes and are not related to the RGB channels.) We stretch the image pixels into a column and perform matrix multiplication to get the scores for each class. Note that this particular set of weights W is not good at all: the weights assign our cat image a very low cat score. In particular, this set of weights seems convinced that it's looking at a dog.

**Analogy of images as high-dimensional points.** Since the images are stretched
into high-dimensional column vectors, we can interpret each image as a single
point in this space (e.g. each image in CIFAR-10 is a point in 3072-dimensional
space of 32x32x3 pixels). Analogously, the entire dataset is a (labeled) set of
points.


Since we defined the score of each class as a weighted sum of all image pixels,
each class score is a linear function over this space. We cannot visualize
3072-dimensional spaces, but if we imagine squashing all those dimensions into
only two dimensions, then we can try to visualize what the classifier might be
doing:



![](/assets/pixelspace.jpeg)

 Cartoon representation of the image space, where each image is a single point, and three classifiers are visualized. Using the example of the car classifier (in red), the red line shows all points in the space that get a score of zero for the car class. The red arrow shows the direction of increase, so all points to the right of the red line have positive (and linearly increasing) scores, and all points to the left have a negative (and linearly decreasing) scores.
 

As we saw above, every row of \(W\) is a classifier for one of the classes. The
geometric interpretation of these numbers is that as we change one of the rows
of \(W\), the corresponding line in the pixel space will rotate in different
directions. The biases \(b\), on the other hand, allow our classifiers to
translate the lines. In particular, note that without the bias terms, plugging
in \( x\_i = 0 \) would always give score of zero regardless of the weights, so
all lines would be forced to cross the origin.


**Interpretation of linear classifiers as template matching.** Another
interpretation for the weights \(W\) is that each row of \(W\) corresponds to a
*template* (or sometimes also called a *prototype*) for one of the classes. The
score of each class for an image is then obtained by comparing each template
with the image using an *inner product* (or *dot product*) one by one to find
the one that “fits” best. With this terminology, the linear classifier is doing
template matching, where the templates are learned. Another way to think of it
is that we are still effectively doing Nearest Neighbor, but instead of having
thousands of training images we are only using a single image per class
(although we will learn it, and it does not necessarily have to be one of the
images in the training set), and we use the (negative) inner product as the
distance instead of the L1 or L2 distance.



![](/assets/templates.jpg)

 Skipping ahead a bit: Example learned weights at the end of learning for CIFAR-10. Note that, for example, the ship template contains a lot of blue pixels as expected. This template will therefore give a high score once it is matched against images of ships on the ocean with an inner product.
 

Additionally, note that the horse template seems to contain a two-headed horse,
which is due to both left and right facing horses in the dataset. The linear
classifier *merges* these two modes of horses in the data into a single
template. Similarly, the car classifier seems to have merged several modes into
a single template which has to identify cars from all sides, and of all colors.
In particular, this template ended up being red, which hints that there are more
red cars in the CIFAR-10 dataset than of any other color. The linear classifier
is too weak to properly account for different-colored cars, but as we will see
later neural networks will allow us to perform this task. Looking ahead a bit, a
neural network will be able to develop intermediate neurons in its hidden layers
that could detect specific car types (e.g. green car facing left, blue car
facing front, etc.), and neurons on the next layer could combine these into a
more accurate car score through a weighted sum of the individual car detectors.


**Bias trick.** Before moving on we want to mention a common simplifying trick
to representing the two parameters \(W,b\) as one. Recall that we defined the
score function as:



\[f(x\_i, W, b) = W x\_i + b\]

As we proceed through the material it is a little cumbersome to keep track of
two sets of parameters (the biases \(b\) and weights \(W\)) separately. A
commonly used trick is to combine the two sets of parameters into a single
matrix that holds both of them by extending the vector \(x\_i\) with one
additional dimension that always holds the constant \(1\) - a default *bias
dimension*. With the extra dimension, the new score function will simplify to a
single matrix multiply:



\[f(x\_i, W) = W x\_i\]

With our CIFAR-10 example, \(x\_i\) is now [3073 x 1] instead of [3072 x 1] -
(with the extra dimension holding the constant 1), and \(W\) is now [10 x 3073]
instead of [10 x 3072]. The extra column that \(W\) now corresponds to the bias
\(b\). An illustration might help clarify:



![](/assets/wb.jpeg)

 Illustration of the bias trick. Doing a matrix multiplication and then adding a bias vector (left) is equivalent to adding a bias dimension with a constant of 1 to all input vectors and extending the weight matrix by 1 column - a bias column (right). Thus, if we preprocess our data by appending ones to all vectors we only have to learn a single matrix of weights instead of two matrices that hold the weights and the biases.
 

**Image data preprocessing.** As a quick note, in the examples above we used the
raw pixel values (which range from [0…255]). In Machine Learning, it is a very
common practice to always perform normalization of your input features (in the
case of images, every pixel is thought of as a feature). In particular, it is
important to **center your data** by subtracting the mean from every feature. In
the case of images, this corresponds to computing a *mean image* across the
training images and subtracting it from every image to get images where the
pixels range from approximately [-127 … 127]. Further common preprocessing is to
scale each input feature so that its values range from [-1, 1]. Of these, zero
mean centering is arguably more important but we will have to wait for its
justification until we understand the dynamics of gradient descent.



### Loss function


In the previous section we defined a function from the pixel values to class
scores, which was parameterized by a set of weights \(W\). Moreover, we saw that
we don’t have control over the data \( (x\_i,y\_i) \) (it is fixed and given),
but we do have control over these weights and we want to set them so that the
predicted class scores are consistent with the ground truth labels in the
training data.


For example, going back to the example image of a cat and its scores for the
classes “cat”, “dog” and “ship”, we saw that the particular set of weights in
that example was not very good at all: We fed in the pixels that depict a cat
but the cat score came out very low (-96.8) compared to the other classes (dog
score 437.9 and ship score 61.95). We are going to measure our unhappiness with
outcomes such as this one with a **loss function** (or sometimes also referred
to as the **cost function** or the **objective**). Intuitively, the loss will be
high if we’re doing a poor job of classifying the training data, and it will be
low if we’re doing well.



#### Multiclass Support Vector Machine loss


There are several ways to define the details of the loss function. As a first
example we will first develop a commonly used loss called the **Multiclass
Support Vector Machine** (SVM) loss. The SVM loss is set up so that the SVM
“wants” the correct class for each image to a have a score higher than the
incorrect classes by some fixed margin \(\Delta\). Notice that it’s sometimes
helpful to anthropomorphise the loss functions as we did above: The SVM “wants”
a certain outcome in the sense that the outcome would yield a lower loss (which
is good).


Let’s now get more precise. Recall that for the i-th example we are given the
pixels of image \( x\_i \) and the label \( y\_i \) that specifies the index of
the correct class. The score function takes the pixels and computes the vector
\( f(x\_i, W) \) of class scores, which we will abbreviate to \(s\) (short for
scores). For example, the score for the j-th class is the j-th element: \( s\_j
= f(x\_i, W)\_j \). The Multiclass SVM loss for the i-th example is then
formalized as follows:



\[L\_i = \sum\_{j\neq y\_i} \max(0, s\_j - s\_{y\_i} + \Delta)\]

**Example.** Lets unpack this with an example to see how it works. Suppose that
we have three classes that receive the scores \( s = [13, -7, 11]\), and that
the first class is the true class (i.e. \(y\_i = 0\)). Also assume that
\(\Delta\) (a hyperparameter we will go into more detail about soon) is 10. The
expression above sums over all incorrect classes (\(j \neq y\_i\)), so we get
two terms:



\[L\_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)\]

You can see that the first term gives zero since [-7 - 13 + 10] gives a negative
number, which is then thresholded to zero with the \(max(0,-)\) function. We get
zero loss for this pair because the correct class score (13) was greater than
the incorrect class score (-7) by at least the margin 10. In fact the difference
was 20, which is much greater than 10 but the SVM only cares that the difference
is at least 10; Any additional difference above the margin is clamped at zero
with the max operation. The second term computes [11 - 13 + 10] which gives 8.
That is, even though the correct class had a higher score than the incorrect
class (13 > 11), it was not greater by the desired margin of 10. The difference
was only 2, which is why the loss comes out to 8 (i.e. how much higher the
difference would have to be to meet the margin). In summary, the SVM loss
function wants the score of the correct class \(y\_i\) to be larger than the
incorrect class scores by at least by \(\Delta\) (delta). If this is not the
case, we will accumulate loss.


Note that in this particular module we are working with linear score functions (
\( f(x\_i; W) = W x\_i \) ), so we can also rewrite the loss function in this
equivalent form:



\[L\_i = \sum\_{j\neq y\_i} \max(0, w\_j^T x\_i - w\_{y\_i}^T x\_i + \Delta)\]

where \(w\_j\) is the j-th row of \(W\) reshaped as a column. However, this will
not necessarily be the case once we start to consider more complex forms of the
score function \(f\).


A last piece of terminology we’ll mention before we finish with this section is
that the threshold at zero \(max(0,-)\) function is often called the **hinge
loss**. You’ll sometimes hear about people instead using the squared hinge loss
SVM (or L2-SVM), which uses the form \(max(0,-)^2\) that penalizes violated
margins more strongly (quadratically instead of linearly). The unsquared version
is more standard, but in some datasets the squared hinge loss can work better.
This can be determined during cross-validation.



> 
> The loss function quantifies our unhappiness with predictions on the training
> set
> 
> 
> 



![](/assets/margin.jpg)

 The Multiclass Support Vector Machine ""wants"" the score of the correct class to be higher than all other scores by at least a margin of delta. If any class has a score inside the red region (or higher), then there will be accumulated loss. Otherwise the loss will be zero. Our objective will be to find the weights that will simultaneously satisfy this constraint for all examples in the training data and give a total loss that is as low as possible.  




**Regularization**. There is one bug with the loss function we presented above.
Suppose that we have a dataset and a set of parameters **W** that correctly
classify every example (i.e. all scores are so that all the margins are met, and
\(L\_i = 0\) for all i). The issue is that this set of **W** is not necessarily
unique: there might be many similar **W** that correctly classify the examples.
One easy way to see this is that if some parameters **W** correctly classify all
examples (so loss is zero for each example), then any multiple of these
parameters \( \lambda W \) where \( \lambda > 1 \) will also give zero loss
because this transformation uniformly stretches all score magnitudes and hence
also their absolute differences. For example, if the difference in scores
between a correct class and a nearest incorrect class was 15, then multiplying
all elements of **W** by 2 would make the new difference 30.


In other words, we wish to encode some preference for a certain set of weights
**W** over others to remove this ambiguity. We can do so by extending the loss
function with a **regularization penalty** \(R(W)\). The most common
regularization penalty is the squared **L2** norm that discourages large weights
through an elementwise quadratic penalty over all parameters:



\[R(W) = \sum\_k\sum\_l W\_{k,l}^2\]

In the expression above, we are summing up all the squared elements of \(W\).
Notice that the regularization function is not a function of the data, it is
only based on the weights. Including the regularization penalty completes the
full Multiclass Support Vector Machine loss, which is made up of two components:
the **data loss** (which is the average loss \(L\_i\) over all examples) and the
**regularization loss**. That is, the full Multiclass SVM loss becomes:



\[L = \underbrace{ \frac{1}{N} \sum\_i L\_i }\_\text{data loss} + \underbrace{ \lambda R(W) }\_\text{regularization loss} \\\\\]

Or expanding this out in its full form:



\[L = \frac{1}{N} \sum\_i \sum\_{j\neq y\_i} \left[ \max(0, f(x\_i; W)\_j - f(x\_i; W)\_{y\_i} + \Delta) \right] + \lambda \sum\_k\sum\_l W\_{k,l}^2\]

Where \(N\) is the number of training examples. As you can see, we append the
regularization penalty to the loss objective, weighted by a hyperparameter
\(\lambda\). There is no simple way of setting this hyperparameter and it is
usually determined by cross-validation.


In addition to the motivation we provided above there are many desirable
properties to include the regularization penalty, many of which we will come
back to in later sections. For example, it turns out that including the L2
penalty leads to the appealing **max margin** property in SVMs (See
[CS229](http://cs229.stanford.edu/notes/cs229-notes3.pdf) lecture notes for full
details if you are interested).


The most appealing property is that penalizing large weights tends to improve
generalization, because it means that no input dimension can have a very large
influence on the scores all by itself. For example, suppose that we have some
input vector \(x = [1,1,1,1] \) and two weight vectors \(w\_1 = [1,0,0,0]\),
\(w\_2 = [0.25,0.25,0.25,0.25] \). Then \(w\_1^Tx = w\_2^Tx = 1\) so both weight
vectors lead to the same dot product, but the L2 penalty of \(w\_1\) is 1.0
while the L2 penalty of \(w\_2\) is only 0.5. Therefore, according to the L2
penalty the weight vector \(w\_2\) would be preferred since it achieves a lower
regularization loss. Intuitively, this is because the weights in \(w\_2\) are
smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse
weight vectors, the final classifier is encouraged to take into account all
input dimensions to small amounts rather than a few input dimensions and very
strongly. As we will see later in the class, this effect can improve the
generalization performance of the classifiers on test images and lead to less
*overfitting*.


Note that biases do not have the same effect since, unlike the weights, they do
not control the strength of influence of an input dimension. Therefore, it is
common to only regularize the weights \(W\) but not the biases \(b\). However,
in practice this often turns out to have a negligible effect. Lastly, note that
due to the regularization penalty we can never achieve loss of exactly 0.0 on
all examples, because this would only be possible in the pathological setting of
\(W = 0\).


**Code**. Here is the loss function (without regularization) implemented in
Python, in both unvectorized and half-vectorized form:



```python
def L\_i(x, y, W):
  """"""
 unvectorized version. Compute the multiclass svm loss for a single example (x,y)
 - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)
 with an appended bias dimension in the 3073-rd position (i.e. bias trick)
 - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)
 - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)
 """"""
  delta = 1.0 # see notes about delta later in this section
  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class
  correct\_class\_score = scores[y]
  D = W.shape[0] # number of classes, e.g. 10
  loss\_i = 0.0
  for j in range(D): # iterate over all wrong classes
    if j == y:
      # skip for the true class to only loop over incorrect classes
      continue
    # accumulate loss for the i-th example
    loss\_i += max(0, scores[j] - correct\_class\_score + delta)
  return loss\_i

def L\_i\_vectorized(x, y, W):
  """"""
 A faster half-vectorized implementation. half-vectorized
 refers to the fact that for a single example the implementation contains
 no for loops, but there is still one loop over the examples (outside this function)
 """"""
  delta = 1.0
  scores = W.dot(x)
  # compute the margins for all classes in one vector operation
  margins = np.maximum(0, scores - scores[y] + delta)
  # on y-th position scores[y] - scores[y] canceled and gave delta. We want
  # to ignore the y-th position and only consider margin on max wrong class
  margins[y] = 0
  loss\_i = np.sum(margins)
  return loss\_i

def L(X, y, W):
  """"""
 fully-vectorized implementation :
 - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)
 - y is array of integers specifying correct class (e.g. 50,000-D array)
 - W are weights (e.g. 10 x 3073)
 """"""
  # evaluate loss over all examples in X without using any for loops
  # left as exercise to reader in the assignment

```

The takeaway from this section is that the SVM loss takes one particular
approach to measuring how consistent the predictions on training data are with
the ground truth labels. Additionally, making good predictions on the training
set is equivalent to minimizing the loss.



> 
> All we have to do now is to come up with a way to find the weights that minimize
> the loss.
> 
> 
> 


### Practical Considerations


**Setting Delta.** Note that we brushed over the hyperparameter \(\Delta\) and
its setting. What value should it be set to, and do we have to cross-validate
it? It turns out that this hyperparameter can safely be set to \(\Delta = 1.0\)
in all cases. The hyperparameters \(\Delta\) and \(\lambda\) seem like two
different hyperparameters, but in fact they both control the same tradeoff: The
tradeoff between the data loss and the regularization loss in the objective. The
key to understanding this is that the magnitude of the weights \(W\) has direct
effect on the scores (and hence also their differences): As we shrink all values
inside \(W\) the score differences will become lower, and as we scale up the
weights the score differences will all become higher. Therefore, the exact value
of the margin between the scores (e.g. \(\Delta = 1\), or \(\Delta = 100\)) is
in some sense meaningless because the weights can shrink or stretch the
differences arbitrarily. Hence, the only real tradeoff is how large we allow the
weights to grow (through the regularization strength \(\lambda\)).


**Relation to Binary Support Vector Machine**. You may be coming to this class
with previous experience with Binary Support Vector Machines, where the loss for
the i-th example can be written as:



\[L\_i = C \max(0, 1 - y\_i w^Tx\_i) + R(W)\]

where \(C\) is a hyperparameter, and \(y\_i \in \{ -1,1 \} \). You can convince
yourself that the formulation we presented in this section contains the binary
SVM as a special case when there are only two classes. That is, if we only had
two classes then the loss reduces to the binary SVM shown above. Also, \(C\) in
this formulation and \(\lambda\) in our formulation control the same tradeoff
and are related through reciprocal relation \(C \propto \frac{1}{\lambda}\).


**Aside: Optimization in primal**. If you’re coming to this class with previous
knowledge of SVMs, you may have also heard of kernels, duals, the SMO algorithm,
etc. In this class (as is the case with Neural Networks in general) we will
always work with the optimization objectives in their unconstrained primal form.
Many of these objectives are technically not differentiable (e.g. the max(x,y)
function isn’t because it has a *kink* when x=y), but in practice this is not a
problem and it is common to use a subgradient.


**Aside: Other Multiclass SVM formulations.** It is worth noting that the
Multiclass SVM presented in this section is one of few ways of formulating the
SVM over multiple classes. Another commonly used form is the *One-Vs-All* (OVA)
SVM which trains an independent binary SVM for each class vs. all other classes.
Related, but less common to see in practice is also the *All-vs-All* (AVA)
strategy. Our formulation follows the [Weston and Watkins 1999
(pdf)](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf)
version, which is a more powerful version than OVA (in the sense that you can
construct multiclass datasets where this version can achieve zero data loss, but
OVA cannot. See details in the paper if interested). The last formulation you
may see is a *Structured SVM*, which maximizes the margin between the score of
the correct class and the score of the highest-scoring incorrect runner-up
class. Understanding the differences between these formulations is outside of
the scope of the class. The version presented in these notes is a safe bet to
use in practice, but the arguably simplest OVA strategy is likely to work just
as well (as also argued by Rikin et al. 2004 in [In Defense of One-Vs-All
Classification
(pdf)](http://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf)).



### Softmax classifier


It turns out that the SVM is one of two commonly seen classifiers. The other
popular choice is the **Softmax classifier**, which has a different loss
function. If you’ve heard of the binary Logistic Regression classifier before,
the Softmax classifier is its generalization to multiple classes. Unlike the SVM
which treats the outputs \(f(x\_i,W)\) as (uncalibrated and possibly difficult
to interpret) scores for each class, the Softmax classifier gives a slightly
more intuitive output (normalized class probabilities) and also has a
probabilistic interpretation that we will describe shortly. In the Softmax
classifier, the function mapping \(f(x\_i; W) = W x\_i\) stays unchanged, but we
now interpret these scores as the unnormalized log probabilities for each class
and replace the *hinge loss* with a **cross-entropy loss** that has the form:



\[L\_i = -\log\left(\frac{e^{f\_{y\_i}}}{ \sum\_j e^{f\_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L\_i = -f\_{y\_i} + \log\sum\_j e^{f\_j}\]

where we are using the notation \(f\_j\) to mean the j-th element of the vector
of class scores \(f\). As before, the full loss for the dataset is the mean of
\(L\_i\) over all training examples together with a regularization term
\(R(W)\). The function \(f\_j(z) = \frac{e^{z\_j}}{\sum\_k e^{z\_k}} \) is
called the **softmax function**: It takes a vector of arbitrary real-valued
scores (in \(z\)) and squashes it to a vector of values between zero and one
that sum to one. The full cross-entropy loss that involves the softmax function
might look scary if you’re seeing it for the first time but it is relatively
easy to motivate.


**Information theory view**. The *cross-entropy* between a “true” distribution
\(p\) and an estimated distribution \(q\) is defined as:



\[H(p,q) = - \sum\_x p(x) \log q(x)\]

The Softmax classifier is hence minimizing the cross-entropy between the
estimated class probabilities ( \(q = e^{f\_{y\_i}} / \sum\_j e^{f\_j} \) as
seen above) and the “true” distribution, which in this interpretation is the
distribution where all probability mass is on the correct class (i.e. \(p = [0,
\ldots 1, \ldots, 0]\) contains a single 1 at the \(y\_i\) -th position.).
Moreover, since the cross-entropy can be written in terms of entropy and the
Kullback-Leibler divergence as \(H(p,q) = H(p) + D\_{KL}(p||q)\), and the
entropy of the delta function \(p\) is zero, this is also equivalent to
minimizing the KL divergence between the two distributions (a measure of
distance). In other words, the cross-entropy objective *wants* the predicted
distribution to have all of its mass on the correct answer.


**Probabilistic interpretation**. Looking at the expression, we see that



\[P(y\_i \mid x\_i; W) = \frac{e^{f\_{y\_i}}}{\sum\_j e^{f\_j} }\]

can be interpreted as the (normalized) probability assigned to the correct label
\(y\_i\) given the image \(x\_i\) and parameterized by \(W\). To see this,
remember that the Softmax classifier interprets the scores inside the output
vector \(f\) as the unnormalized log probabilities. Exponentiating these
quantities therefore gives the (unnormalized) probabilities, and the division
performs the normalization so that the probabilities sum to one. In the
probabilistic interpretation, we are therefore minimizing the negative log
likelihood of the correct class, which can be interpreted as performing *Maximum
Likelihood Estimation* (MLE). A nice feature of this view is that we can now
also interpret the regularization term \(R(W)\) in the full loss function as
coming from a Gaussian prior over the weight matrix \(W\), where instead of MLE
we are performing the *Maximum a posteriori* (MAP) estimation. We mention these
interpretations to help your intuitions, but the full details of this derivation
are beyond the scope of this class.



**Practical issues: Numeric stability**. When you’re writing code for computing
the Softmax function in practice, the intermediate terms \(e^{f\_{y\_i}}\) and
\(\sum\_j e^{f\_j}\) may be very large due to the exponentials. Dividing large
numbers can be numerically unstable, so it is important to use a normalization
trick. Notice that if we multiply the top and bottom of the fraction by a
constant \(C\) and push it into the sum, we get the following (mathematically
equivalent) expression:



\[\frac{e^{f\_{y\_i}}}{\sum\_j e^{f\_j}}
= \frac{Ce^{f\_{y\_i}}}{C\sum\_j e^{f\_j}}
= \frac{e^{f\_{y\_i} + \log C}}{\sum\_j e^{f\_j + \log C}}\]

We are free to choose the value of \(C\). This will not change any of the
results, but we can use this value to improve the numerical stability of the
computation. A common choice for \(C\) is to set \(\log C = -\max\_j f\_j \).
This simply states that we should shift the values inside the vector \(f\) so
that the highest value is zero. In code:



```python
f = np.array([123, 456, 789]) # example with 3 classes and each having large scores
p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup

# instead: first shift the values of f so that the highest number is 0:
f -= np.max(f) # f becomes [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer


```

**Possibly confusing naming conventions**. To be precise, the *SVM classifier*
uses the *hinge loss*, or also sometimes called the *max-margin loss*. The
*Softmax classifier* uses the *cross-entropy loss*. The Softmax classifier gets
its name from the *softmax function*, which is used to squash the raw class
scores into normalized positive values that sum to one, so that the
cross-entropy loss can be applied. In particular, note that technically it
doesn’t make sense to talk about the “softmax loss”, since softmax is just the
squashing function, but it is a relatively commonly used shorthand.



### SVM vs. Softmax


A picture might help clarify the distinction between the Softmax and SVM
classifiers:



![](/assets/svmvssoftmax.png)
Example of the difference between the SVM and Softmax classifiers for one datapoint. In both cases we compute the same score vector **f** (e.g. by matrix multiplication in this section). The difference is in the interpretation of the scores in **f**: The SVM interprets these as class scores and its loss function encourages the correct class (class 2, in blue) to have a score higher by a margin than the other class scores. The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high (equivalently the negative of it to be low). The final loss for this example is 1.58 for the SVM and 1.04 (note this is 1.04 using the natural logarithm, not base 2 or base 10) for the Softmax classifier, but note that these numbers are not comparable; They are only meaningful in relation to loss computed within the same classifier and with the same data.

**Softmax classifier provides “probabilities” for each class.** Unlike the SVM
which computes uncalibrated and not easy to interpret scores for all classes,
the Softmax classifier allows us to compute “probabilities” for all labels. For
example, given an image the SVM classifier might give you scores [12.5, 0.6,
-23.0] for the classes “cat”, “dog” and “ship”. The softmax classifier can
instead compute the probabilities of the three labels as [0.9, 0.09, 0.01],
which allows you to interpret its confidence in each class. The reason we put
the word “probabilities” in quotes, however, is that how peaky or diffuse these
probabilities are depends directly on the regularization strength \(\lambda\) -
which you are in charge of as input to the system. For example, suppose that the
unnormalized log-probabilities for some three classes come out to be [1, -2, 0].
The softmax function would then compute:



\[[1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]\]

Where the steps taken are to exponentiate and normalize to sum to one. Now, if
the regularization strength \(\lambda\) was higher, the weights \(W\) would be
penalized more and this would lead to smaller weights. For example, suppose that
the weights became one half smaller ([0.5, -1, 0]). The softmax would now
compute:



\[[0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]\]

where the probabilites are now more diffuse. Moreover, in the limit where the
weights go towards tiny numbers due to very strong regularization strength
\(\lambda\), the output probabilities would be near uniform. Hence, the
probabilities computed by the Softmax classifier are better thought of as
confidences where, similar to the SVM, the ordering of the scores is
interpretable, but the absolute numbers (or their differences) technically are
not.


**In practice, SVM and Softmax are usually comparable.** The performance
difference between the SVM and Softmax are usually very small, and different
people will have different opinions on which classifier works better. Compared
to the Softmax classifier, the SVM is a more *local* objective, which could be
thought of either as a bug or a feature. Consider an example that achieves the
scores [10, -2, 3] and where the first class is correct. An SVM (e.g. with
desired margin of \(\Delta = 1\)) will see that the correct class already has a
score higher than the margin compared to the other classes and it will compute
loss of zero. The SVM does not care about the details of the individual scores:
if they were instead [10, -100, -100] or [10, 9, 9] the SVM would be indifferent
since the margin of 1 is satisfied and hence the loss is zero. However, these
scenarios are not equivalent to a Softmax classifier, which would accumulate a
much higher loss for the scores [10, 9, 9] than for [10, -100, -100]. In other
words, the Softmax classifier is never fully happy with the scores it produces:
the correct class could always have a higher probability and the incorrect
classes always a lower probability and the loss would always get better.
However, the SVM is happy once the margins are satisfied and it does not
micromanage the exact scores beyond this constraint. This can intuitively be
thought of as a feature: For example, a car classifier which is likely spending
most of its “effort” on the difficult problem of separating cars from trucks
should not be influenced by the frog examples, which it already assigns very low
scores to, and which likely cluster around a completely different side of the
data cloud.



### Interactive web demo



[![](/assets/classifydemo.jpeg)](http://vision.stanford.edu/teaching/cs231n/linear-classify-demo)
We have written an interactive web demo to help your intuitions with linear classifiers. The demo visualizes the loss functions discussed in this section using a toy 3-way classification on 2D data. The demo also jumps ahead a bit and performs the optimization, which we will discuss in full detail in the next section.
 


### Summary


In summary,


* We defined a **score function** from image pixels to class scores (in this section, a linear function that depends on weights **W** and biases **b**).
* Unlike kNN classifier, the advantage of this **parametric approach** is that once we learn the parameters we can discard the training data. Additionally, the prediction for a new test image is fast since it requires a single matrix multiplication with **W**, not an exhaustive comparison to every single training example.
* We introduced the **bias trick**, which allows us to fold the bias vector into the weight matrix for convenience of only having to keep track of one parameter matrix.
* We defined a **loss function** (we introduced two commonly used losses for linear classifiers: the **SVM** and the **Softmax**) that measures how compatible a given set of parameters is with respect to the ground truth labels in the training dataset. We also saw that the loss function was defined in such way that making good predictions on the training data is equivalent to having a small loss.


We now saw one way to take a dataset of images and map each one to class scores
based on a set of parameters, and we saw two examples of loss functions that we
can use to measure the quality of the predictions. But how do we efficiently
determine the parameters that give the best (lowest) loss? This process is
*optimization*, and it is the topic of the next section.



### Further Reading


These readings are optional and contain pointers of interest.


* [Deep Learning using Linear Support Vector Machines](https://arxiv.org/abs/1306.0239) from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.









* [cs231n](https://github.com/cs231n)
* [cs231n](https://twitter.com/cs231n)












 // Make responsive
 MathJax.Hub.Config({
 ""HTML-CSS"": { linebreaks: { automatic: true } },
 ""SVG"": { linebreaks: { automatic: true } },
 });
 


"
h7iBpEHGVNc?si=PoPmFt5y_k4cSeol,"2941, 4440","of a lot of what we call
supervised learning, and what we'll see in deep
learning as we move forward, is that generally you'll want
to specify some function, f, that could be very complex in structure, specify some loss function that determines how well your algorithm is doing, given any value of the parameters, some regularization term for how to penalize model complexity and then you combine these things together and try to find the W that minimizes this final loss function. But then the question is, how do we actually go about doing that? How do we actually find this
W that minimizes the loss? And that leads us to the
topic of optimization. So when we're doing optimization, I usually think of things
in terms of walking around some large valley. So the idea is that you're
walking around this large valley with different mountains
and valleys and streams and stuff, and every
point on this landscape corresponds to some setting
of the parameters W. And you're this little guy who's
walking around this valley, and you're trying to find, and the height of each of these points, sorry, is equal to the loss
incurred by that setting of W. And now your job as this little man walking around this landscape, you need to somehow find
the bottom of this valley. And this is kind of a
hard problem in general. You might think, maybe I'm really smart and I can think really hard
about the analytic properties of my loss function, my
regularization all that, maybe I can just write down the minimizer, and that would sort of correspond
to magically teleporting all the way to the bottom of this valley. But in practice, once your
prediction function, f, and your loss function
and your regularizer, once these things get big and complex and using neural networks, there's really not much
hope in trying to write down an explicit analytic solution that takes you directly to the minima. So in practice we tend to use various
types of iterative methods where we start with some solution and then gradually improve it over time. So the very first, stupidest thing that you might imagine is random search, that will just take a bunch of Ws, sampled randomly, and throw
them into our loss function and see how well they do. So spoiler alert, this is
a really bad algorithm, you probably shouldn't use this, but at least it's one thing
you might imagine trying. And we can actually do this, we can actually try to
train a linear classifier via random search, for CIFAR-10 and for this there's 10 classes, so random chance is 10%, and if we did some
number of random trials, we eventually found just
through sheer dumb luck, some setting of W that
got maybe 15% accuracy. So it's better than random, but state of the art is maybe 95% so we've got a little
bit of gap to close here. So again, probably don't
use this in practice, but you might imagine
that this is something you could potentially do. So in practice, maybe a better strategy is actually using some
of the local geometry of this landscape. So if you're this little guy who's walking around this landscape, maybe you can't see directly the path down to the bottom of the valley, but what you can do is feel with your foot and figure out what is the local geometry, if I'm standing right here, which way will take me
a little bit downhill? So you can feel with your feet and feel where is the slope of the ground taking me down a little
bit in this direction? And you can take a step in that direction, and then you'll go down a little bit, feel again with your feet to
figure out which way is down, and then repeat over and over again and hope that you'll end up at the bottom of the valley eventually. So this also seems like a
relatively simple algorithm, but actually this one
tends to work really well in practice if you get
all the details right. So this is generally the
strategy that we'll follow when training these large neural networks and linear classifiers and other things. So then, that was a little hand wavy, so what is slope? If you remember back
to your calculus class, then at least in one dimension, the slope is the derivative
of this function. So if we've got some
one-dimensional function, f, that takes in a scalar x,
and then outputs the height of some curve, then we
can compute the slope or derivative at any point by imagining, if we take a small step,
h, in any direction, take a small step, h, and
compare the difference in the function value over that step and then drag the step size to zero, that will give us the
slope of that function at that point. And this generalizes quite naturally to multi-variable functions as well. So in practice, our x
is maybe not a scalar but a whole vector, 'cause remember, x
might be a whole vector, so we need to generalize this notion to multi-variable things. And the generalization that
we use of the derivative in the multi-variable
setting is the gradient, so the gradient is a vector
of partial derivatives. So the gradient will
have the same shape as x, and each element of the
gradient will tell us what is the slope of the function f, if we move in that coordinate direction. And the gradient turns out to have these very nice properties, so the gradient is now a
vector of partial derivatives, but it points in the
direction of greatest increase of the function and correspondingly, if you look at the negative
gradient direction, that gives you the direction
of greatest decrease of the function. And more generally, if you want to know, what is the slope of my
landscape in any direction? Then that's equal to the
dot product of the gradient with the unit vector
describing that direction. So this gradient is super important, because it gives you this
linear, first-order approximation to your function at your current point. So in practice, a lot of deep learning is about computing
gradients of your functions and then using those gradients
to iteratively update your parameter vector. So one naive way that you might imagine actually evaluating this
gradient on a computer, is using the method of finite differences, going back to the limit
definition of gradient. So here on the left, we
imagine that our current W is this parameter vector that maybe gives us some
current loss of maybe 1.25 and our goal is to
compute the gradient, dW, which will be a vector
of the same shape as W, and each slot in that
gradient will tell us how much will the loss
change is we move a tiny, infinitesimal amount in
that coordinate direction. So one thing you might imagine is just computing these
finite differences, that we have our W, we
might try to increment the first element of
W by a small value, h, and then re-compute the
loss using our loss function and our classifier and all that. And maybe in this setting,
if we move a little bit in the first dimension,
then our loss will decrease a little bit from 1.2534 to 1.25322. And then we can use this limit definition to come up with this finite
differences approximation to the gradient in this first dimension. And now you can imagine
repeating this procedure in the second dimension, where now we take the first dimension, set it back to the original value, and now increment the second
direction by a small step. And again, we compute the loss and use this finite
differences approximation to compute an approximation
to the gradient in the second slot. And now repeat this again for the third, and on and on and on. So this is actually a terrible idea because it's super slow. So you might imagine that
computing this function, f, might actually be super
slow if it's a large, convolutional neural network. And this parameter vector, W, probably will not have 10
entries like it does here, it might have tens of millions or even hundreds of millions
for some of these large, complex deep learning models. So in practice, you'll
never want to compute your gradients for your
finite differences, 'cause you'd have to wait
for hundreds of millions potentially of function evaluations to get a single gradient,
and that would be super slow and super bad. But thankfully we don't have to do that. Hopefully you took a calculus course at some point in your lives, so you know that thanks to these guys, we can just write down the
expression for our loss and then use the magical
hammer of calculus to just write down an expression for what this gradient should be. And this'll be way more efficient than trying to compute it analytically via finite differences. One, it'll be exact, and two, it'll be much faster
since we just need to compute this single expression. So what this would look like is now, if we go back to this
picture of our current W, rather than iterating over
all the dimensions of W, we'll figure out ahead of time what is the analytic
expression for the gradient, and then just write it down
and go directly from the W and compute the dW or
the gradient in one step. And that will be much better in practice. So in summary, this numerical gradient is something that's
simple and makes sense, but you won't really use it in practice. In practice, you'll always
take an analytic gradient and use that when actually performing
these gradient computations. However, one interesting note is that these numeric gradients
are actually a very useful debugging tool. Say you've written some code, and you wrote some code
that computes the loss and the gradient of the loss, then how do you debug this thing? How do you make sure that
this analytic expression that you derived and wrote down in code is actually correct? So a really common debugging
strategy for these things is to use the numeric gradient as a way, as sort of a unit test to make sure that your analytic gradient was correct. Again, because this is
super slow and inexact, then when doing this
numeric gradient checking, as it's called, you'll tend
to scale down the parameter of the problem so that it actually runs in a reasonable amount of time. But this ends up being a super
useful debugging strategy when you're writing your
own gradient computations. So this is actually very
commonly used in practice, and you'll do this on
your assignments as well. So then once we know how
to compute the gradient, then it leads us to this
super simple algorithm that's like three lines, but
turns out to be at the heart of how we train even these very biggest, most complex deep learning algorithms, and that's gradient descent. So gradient descent is
first we initialize our W as some random thing, then while true, we'll compute our loss and our gradient and then we'll update our weights in the opposite of the gradient direction, 'cause remember that the gradient was pointing in the direction
of greatest increase of the function, so minus gradient points in the direction
of greatest decrease, so we'll take a small
step in the direction of minus gradient, and
just repeat this forever and eventually your network will converge and you'll be very happy, hopefully. But this step size is
actually a hyper-parameter, and this tells us that every
time we compute the gradient, how far do we step in that direction. And this step size, also
sometimes called a learning rate, is probably one of the
single most important hyper-parameters that you need to set when you're actually training
these things in practice. Actually for me when I'm
training these things, trying to figure out this step size or this learning rate, is
the first hyper-parameter that I always check. Things like model size or
regularization strength I leave until a little bit later, and getting the learning
rate or the step size correct is the first thing that I
try to set at the beginning. So pictorially what this looks like here's a simple example in two dimensions. So here we've got maybe this bowl that's showing our loss function where this red region in the center is this region of low
loss we want to get to and these blue and green
regions towards the edge are higher loss that we want to avoid. So now we're going to start of our W at some random point in the space, and then we'll compute the
negative gradient direction, which will hopefully
point us in the direction of the minima eventually. And if we repeat this over and over again, we'll hopefully eventually
get to the exact minima. And what this looks like in practice is, oh man, we've got this
mouse problem again. So what this looks like in practice is that if we repeat this
thing over and over again, then we will start off at some point and eventually, taking tiny
gradient steps each time, you'll see that the parameter
will arc in toward the center, this region of minima, and that's really what you want, because you want to get to low loss. And by the way, as a bit of a teaser, we saw in the previous slide, this example of very
simple gradient descent, where at every step, we're
just stepping in the direction of the gradient. But in practice, over the
next couple of lectures, we'll see that there are
slightly fancier step, what they call these update rules, where you can take slightly fancier things to incorporate gradients
across multiple time steps and stuff like that, that tend
to work a little bit better in practice and are
used much more commonly than this vanilla gradient descent when training these things in practice. And then, as a bit of a preview, we can look at some of these
slightly fancier methods on optimizing the same problem. So again, the black will be
this same gradient computation, and these, I forgot which color they are, but these two other curves are using slightly fancier update rules to decide exactly how to
use the gradient information to make our next step. So one of these is gradient
descent with momentum, the other is this Adam optimizer, and we'll see more details about those later in the course. But the idea is that we have
this very basic algorithm called gradient descent, where we use the gradient
at every time step to determine where to step next, and there exist different
update rules which tell us how exactly do we use
that gradient information. But it's all the same basic algorithm of trying to go downhill
at every time step. But there's actually
one more little wrinkle that we should talk about. So remember that we
defined our loss function, we defined a loss that computes how bad is our classifier doing at
any single training example, and then we said that our
full loss over the data set was going to be the average loss across the entire training set. But in practice, this N
could be very very large. If we're using the image
net data set for example, that we talked about in the first lecture, then N could be like 1.3 million, so actually computing this loss could be actually very expensive and require computing perhaps
millions of evaluations of this function. So that could be really slow. And actually, because the
gradient is a linear operator, when you actually try
to compute the gradient of this expression, you see
that the gradient of our loss is now the sum of the
gradient of the losses for each of the individual terms. So now if we want to
compute the gradient again, it sort of requires us to iterate over the entire training data set all N of these examples. So if our N was like a million, this would be super super slow, and we would have to wait
a very very long time before we make any individual update to W. So in practice, we tend to use what is called stochastic
gradient descent, where rather than computing
the loss and gradient over the entire training set, instead at every iteration,
we sample some small set of training examples, called a minibatch. Usually this is a power
of two by convention, like 32, 64, 128 are common numbers, and then we'll use this small minibatch to compute an estimate of the full sum, and an estimate of the true gradient. And now this is stochastic
because you can view this as maybe a Monte Carlo
estimate of some expectation of the true value. So now this makes our
algorithm slightly fancier, but it's still only four lines. So now it's well true, sample
some random minibatch of data, evaluate your loss and
gradient on the minibatch, and now make an update on your parameters based on this estimate of the loss, and this estimate of the gradient. And again, we'll see
slightly fancier update rules of exactly how to integrate
multiple gradients over time, but this is the
basic training algorithm that we use for pretty much
all deep neural networks in practice. So we have another interactive web demo actually playing around
with linear classifiers, and training these things via
stochastic gradient descent, but given how miserable
the web demo was last time, I'm not actually going to open the link. Instead, I'll just play this video. [laughing] But I encourage you to go check this out and play with it online, because it actually helps
to build some intuition about linear classifiers and training them via gradient descent. So here you can see on the left, we've got this problem
where we're categorizing three different classes, and we've got these
green, blue and red points that are our training samples
from these three classes. And now we've drawn
the decision boundaries for these classes, which are
the colored background regions, as well as these directions, giving you the direction of
increase for the class scores for each of these three classes. And now if you see, if
you actually go and play with this thing online, you can see that we can
go in and adjust the Ws and changing the values of the Ws will cause these decision
boundaries to rotate. If you change the biases,
then the decision boundaries will not rotate, but will
instead move side to side or up and down. Then we can actually make steps that are trying to update this loss, or you can change the step
size with this slider. You can hit this button
to actually run the thing. So now with a big step size, we're running gradient descent right now, and these decision boundaries
are flipping around and trying to fit the data. So it's doing okay now, but we can actually change
the loss function in real time between these different SVM formulations and the different softmax. And you can see that as you flip between these different
formulations of loss functions, it's generally doing the same thing. Our decision regions are
mostly in the same place, but exactly how they end
up relative to each other and exactly what the trade-offs are between categorizing
these different things changes a little bit. So I really encourage you to go online and play with this thing to
try to get some intuition for what it actually looks like to try to train these linear classifiers via gradient descent. Now as an aside, I'd like
to talk about another idea, which is that of image features. So so far we've talked
about linear classifiers, which is just maybe taking
our raw image pixels and then feeding the raw pixels themselves into our linear classifier. But as we talked about
in the last lecture, this is maybe not such
a great thing to do, because of things like
multi-modality and whatnot. So in practice, actually
feeding raw pixel values into linear classifiers
tends to not work so well. So it was actually common
before the dominance of deep neural networks, was instead to have
this two-stage approach, where first, you would take your image and then compute various
feature representations of that image, that are maybe computing different kinds of quantities
relating to the appearance of the image, and then concatenate these
different feature vectors to give you some feature
representation of the image, and now this feature
representation of the image would be fed into a linear classifier, rather than feeding the
raw pixels themselves into the classifier. And the motivation here is that, so imagine we have a
training data set on the left of these red points, and
red points in the middle and blue points around that. And for this kind of data set, there's no way that we can
draw a linear decision boundary to separate the red points
from the blue points. And we saw more examples of
this in the last lecture. But if we use a clever feature transform, in this case transforming
to polar coordinates, then now after we do
the feature transform, then this complex data
set actually might become linearly separable, and actually could be classified correctly by a linear classifier. And the whole trick here
now is to figure out what is the right feature transform that is computing the right quantities for the problem that you care about. So for images, maybe
converting your pixels to polar coordinates, doesn't make sense, but you actually can try to write down feature representations of images that might make sense, and actually might help you out and might do better than
putting in raw pixels into the classifier. So one example of this kind
of feature representation that's super simple, is this
idea of a color histogram. So you'll take maybe each pixel, you'll take this hue color spectrum and divide it into buckets
and then for every pixel, you'll map it into one
of those color buckets and then count up how many pixels fall into each of these different buckets. So this tells you globally
what colors are in the image. Maybe if this example of a frog, this feature vector would tell us there's a lot of green stuff, and maybe not a lot of
purple or red stuff. And this is kind of a simple feature
vector that you might see in practice. Another common feature vector that we saw before the rise of neural networks, or before the dominance of neural networks was this histogram of oriented gradients. So remember from the first lecture, that Hubel and Wiesel
found these oriented edges are really important in
the human visual system, and this histogram of oriented gradients feature representation tries to capture the same intuition and
measure the local orientation of edges on the image. So what this thing is going to do, is take our image and then divide it into these little eight
by eight pixel regions. And then within each of those
eight by eight pixel regions, we'll compute what is the
dominant edge direction of each pixel, quantize
those edge directions into several buckets and then
within each of those regions, compute a histogram over these
different edge orientations. And now your full-feature vector will be these different
bucketed histograms of edge orientations across all the different
eight by eight regions in the image. So this is in some sense dual to the color histogram
classifier that we saw before. So color histogram is
saying, globally, what colors exist in the image, and this is saying, overall,
what types of edge information exist in the image. And even localized to
different parts of the image, what types of edges exist
in different regions. So maybe for this frog on the left, you can see he's sitting on a leaf, and these leaves have these
dominant diagonal edges, and if you visualize the
histogram of oriented gradient features, then you can
see that in this region, we've got a lot of diagonal edges, that this histogram of oriented gradient feature representation's capturing. So this was a super common
feature representation and was used a lot for object recognition actually not too long ago. Another feature representation
that you might see out there is this idea of bag of words. So this is taking inspiration from natural language processing. So if you've got a paragraph, then a way that you might
represent a paragraph by a feature vector is
counting up the occurrences of different words in that paragraph. So we want to take that
intuition and apply it to images in some way. But the problem is that
there's no really simple, straightforward analogy
of words to images, so we need to define our own vocabulary of visual words. So we take this two-stage approach, where first we'll get a bunch of images, sample a whole bunch of tiny random crops from those images and then cluster them using something like K means to come up with these
different cluster centers that are maybe representing
different types of visual words in the images. So if you look at this
example on the right here, this is a real example of clustering actually different image
patches from images, and you can see that after
this clustering step, our visual words capture
these different colors, like red and blue and yellow, as well as these different
types of oriented edges in different directions, which is interesting that
now we're starting to see these oriented edges
come out from the data in a data-driven way. And now, once we've got
these set of visual words, also called a codebook, then we can encode our
image by trying to say, for each of these visual words, how much does this visual
word occur in the image? And now this gives us, again, some slightly different information about what is the visual
appearance of this image. And actually this is a type
of feature representation that Fei-Fei worked on when
she was a grad student, so this is something
that you saw in practice not too long ago. So then as a bit of teaser, tying this all back together, the way that this image
classification pipeline might have looked like, maybe about five to 10 years ago, would be that you would take your image, and then compute these different
feature representations of your image, things like bag of words, or histogram of orientated gradients, concatenate a whole bunch
of features together, and then feed these feature extractors down into some linear classifier. I'm simplifying a little bit, the pipelines were a little
bit more complex than that, but this is the general intuition. And then the idea here was
that after you extracted these features, this feature extractor would be a fixed block
that would not be updated during training. And during training, you would only update
the linear classifier if it's working on top of features.",https://cs231n.github.io/optimization-1/,"




CS231n Convolutional Neural Networks for Visual Recognition









 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1\*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-46895817-2', 'auto');
 ga('send', 'pageview');
 



addBackToTop({
 backgroundColor: '#fff',
 innerHTML: 'Back to Top',
 textColor: '#333'
 })

 #back-to-top {
 border: 1px solid #ccc;
 border-radius: 0;
 font-family: sans-serif;
 font-size: 14px;
 width: 100px;
 text-align: center;
 line-height: 30px;
 height: 30px;
 }
 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io)
[Course Website](http://cs231n.stanford.edu/)





# 




Table of Contents:


* [Introduction](#intro)
* [Visualizing the loss function](#vis)
* [Optimization](#optimization)
	+ [Strategy #1: Random Search](#opt1)
	+ [Strategy #2: Random Local Search](#opt2)
	+ [Strategy #3: Following the gradient](#opt3)
* [Computing the gradient](#gradcompute)
	+ [Numerically with finite differences](#numerical)
	+ [Analytically with calculus](#analytic)
* [Gradient descent](#gd)
* [Summary](#summary)



### Introduction


In the previous section we introduced two key components in context of the image
classification task:


1. A (parameterized) **score function** mapping the raw image pixels to class scores (e.g. a linear function)
2. A **loss function** that measured the quality of a particular set of parameters based on how well the induced scores agreed with the ground truth labels in the training data. We saw that there are many ways and versions of this (e.g. Softmax/SVM).


Concretely, recall that the linear function had the form \( f(x\_i, W) = W x\_i
\) and the SVM we developed was formulated as:



\[L = \frac{1}{N} \sum\_i \sum\_{j\neq y\_i} \left[ \max(0, f(x\_i; W)\_j - f(x\_i; W)\_{y\_i} + 1) \right] + \alpha R(W)\]

We saw that a setting of the parameters \(W\) that produced predictions for
examples \(x\_i\) consistent with their ground truth labels \(y\_i\) would also
have a very low loss \(L\). We are now going to introduce the third and last key
component: **optimization**. Optimization is the process of finding the set of
parameters \(W\) that minimize the loss function.


**Foreshadowing:** Once we understand how these three core components interact,
we will revisit the first component (the parameterized function mapping) and
extend it to functions much more complicated than a linear mapping: First entire
Neural Networks, and then Convolutional Neural Networks. The loss functions and
the optimization process will remain relatively unchanged.



### Visualizing the loss function


The loss functions we’ll look at in this class are usually defined over very
high-dimensional spaces (e.g. in CIFAR-10 a linear classifier weight matrix is
of size [10 x 3073] for a total of 30,730 parameters), making them difficult to
visualize. However, we can still gain some intuitions about one by slicing
through the high-dimensional space along rays (1 dimension), or along planes (2
dimensions). For example, we can generate a random weight matrix \(W\) (which
corresponds to a single point in the space), then march along a ray and record
the loss function value along the way. That is, we can generate a random
direction \(W\_1\) and compute the loss along this direction by evaluating \(L(W
+ a W\_1)\) for different values of \(a\). This process generates a simple plot
with the value of \(a\) as the x-axis and the value of the loss function as the
y-axis. We can also carry out the same procedure with two dimensions by
evaluating the loss \( L(W + a W\_1 + b W\_2) \) as we vary \(a, b\). In a plot,
\(a, b\) could then correspond to the x-axis and the y-axis, and the value of
the loss function can be visualized with a color:



![](/assets/svm1d.png)
![](/assets/svm_one.jpg)
![](/assets/svm_all.jpg)

 Loss function landscape for the Multiclass SVM (without regularization) for one single example (left,middle) and for a hundred examples (right) in CIFAR-10. Left: one-dimensional loss by only varying **a**. Middle, Right: two-dimensional loss slice, Blue = low loss, Red = high loss. Notice the piecewise-linear structure of the loss function. The losses for multiple examples are combined with average, so the bowl shape on the right is the average of many piece-wise linear bowls (such as the one in the middle).
 

We can explain the piecewise-linear structure of the loss function by examining
the math. For a single example we have:



\[L\_i = \sum\_{j\neq y\_i} \left[ \max(0, w\_j^Tx\_i - w\_{y\_i}^Tx\_i + 1) \right]\]

It is clear from the equation that the data loss for each example is a sum of
(zero-thresholded due to the \(\max(0,-)\) function) linear functions of \(W\).
Moreover, each row of \(W\) (i.e. \(w\_j\)) sometimes has a positive sign in
front of it (when it corresponds to a wrong class for an example), and sometimes
a negative sign (when it corresponds to the correct class for that example). To
make this more explicit, consider a simple dataset that contains three
1-dimensional points and three classes. The full SVM loss (without
regularization) becomes:



\[\begin{align}
L\_0 = & \max(0, w\_1^Tx\_0 - w\_0^Tx\_0 + 1) + \max(0, w\_2^Tx\_0 - w\_0^Tx\_0 + 1) \\\\
L\_1 = & \max(0, w\_0^Tx\_1 - w\_1^Tx\_1 + 1) + \max(0, w\_2^Tx\_1 - w\_1^Tx\_1 + 1) \\\\
L\_2 = & \max(0, w\_0^Tx\_2 - w\_2^Tx\_2 + 1) + \max(0, w\_1^Tx\_2 - w\_2^Tx\_2 + 1) \\\\
L = & (L\_0 + L\_1 + L\_2)/3
\end{align}\]

Since these examples are 1-dimensional, the data \(x\_i\) and weights \(w\_j\)
are numbers. Looking at, for instance, \(w\_0\), some terms above are linear
functions of \(w\_0\) and each is clamped at zero. We can visualize this as
follows:



![](/assets/svmbowl.png)

 1-dimensional illustration of the data loss. The x-axis is a single weight and the y-axis is the loss. The data loss is a sum of multiple terms, each of which is either independent of a particular weight, or a linear function of it that is thresholded at zero. The full SVM data loss is a 30,730-dimensional version of this shape.
 

As an aside, you may have guessed from its bowl-shaped appearance that the SVM
cost function is an example of a [convex
function](http://en.wikipedia.org/wiki/Convex_function) There is a large amount
of literature devoted to efficiently minimizing these types of functions, and
you can also take a Stanford class on the topic ( [convex
optimization](http://stanford.edu/~boyd/cvxbook/) ). Once we extend our score
functions \(f\) to Neural Networks our objective functions will become
non-convex, and the visualizations above will not feature bowls but complex,
bumpy terrains.


*Non-differentiable loss functions*. As a technical note, you can also see that
the *kinks* in the loss function (due to the max operation) technically make the
loss function non-differentiable because at these kinks the gradient is not
defined. However, the [subgradient](http://en.wikipedia.org/wiki/Subderivative)
still exists and is commonly used instead. In this class will use the terms
*subgradient* and *gradient* interchangeably.



### Optimization


To reiterate, the loss function lets us quantify the quality of any particular
set of weights **W**. The goal of optimization is to find **W** that minimizes
the loss function. We will now motivate and slowly develop an approach to
optimizing the loss function. For those of you coming to this class with
previous experience, this section might seem odd since the working example we’ll
use (the SVM loss) is a convex problem, but keep in mind that our goal is to
eventually optimize Neural Networks where we can’t easily use any of the tools
developed in the Convex Optimization literature.



#### Strategy #1: A first very bad idea solution: Random search


Since it is so simple to check how good a given set of parameters **W** is, the
first (very bad) idea that may come to mind is to simply try out many different
random weights and keep track of what works best. This procedure might look as
follows:



```python
# assume X\_train is the data where each column is an example (e.g. 3073 x 50,000)
# assume Y\_train are the labels (e.g. 1D array of 50,000)
# assume the function L evaluates the loss function

bestloss = float(""inf"") # Python assigns the highest possible float value
for num in range(1000):
  W = np.random.randn(10, 3073) \* 0.0001 # generate random parameters
  loss = L(X\_train, Y\_train, W) # get the loss over the entire training set
  if loss < bestloss: # keep track of the best solution
    bestloss = loss
    bestW = W
  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)

# prints:
# in attempt 0 the loss was 9.401632, best 9.401632
# in attempt 1 the loss was 8.959668, best 8.959668
# in attempt 2 the loss was 9.044034, best 8.959668
# in attempt 3 the loss was 9.278948, best 8.959668
# in attempt 4 the loss was 8.857370, best 8.857370
# in attempt 5 the loss was 8.943151, best 8.857370
# in attempt 6 the loss was 8.605604, best 8.605604
# ... (trunctated: continues for 1000 lines)

```

In the code above, we see that we tried out several random weight vectors **W**,
and some of them work better than others. We can take the best weights **W**
found by this search and try it out on the test set:



```python
# Assume X\_test is [3073 x 10000], Y\_test [10000 x 1]
scores = Wbest.dot(Xte\_cols) # 10 x 10000, the class scores for all test examples
# find the index with max score in each column (the predicted class)
Yte\_predict = np.argmax(scores, axis = 0)
# and calculate accuracy (fraction of predictions that are correct)
np.mean(Yte\_predict == Yte)
# returns 0.1555

```

With the best **W** this gives an accuracy of about **15.5%**. Given that
guessing classes completely at random achieves only 10%, that’s not a very bad
outcome for a such a brain-dead random search solution!


**Core idea: iterative refinement**. Of course, it turns out that we can do much
better. The core idea is that finding the best set of weights **W** is a very
difficult or even impossible problem (especially once **W** contains weights for
entire complex neural networks), but the problem of refining a specific set of
weights **W** to be slightly better is significantly less difficult. In other
words, our approach will be to start with a random **W** and then iteratively
refine it, making it slightly better each time.



> 
> Our strategy will be to start with random weights and iteratively refine them
> over time to get lower loss
> 
> 
> 


**Blindfolded hiker analogy.** One analogy that you may find helpful going
forward is to think of yourself as hiking on a hilly terrain with a blindfold
on, and trying to reach the bottom. In the example of CIFAR-10, the hills are
30,730-dimensional, since the dimensions of **W** are 10 x 3073. At every point
on the hill we achieve a particular loss (the height of the terrain).



#### Strategy #2: Random Local Search


The first strategy you may think of is to try to extend one foot in a random
direction and then take a step only if it leads downhill. Concretely, we will
start out with a random \(W\), generate random perturbations \( \delta W \) to
it and if the loss at the perturbed \(W + \delta W\) is lower, we will perform
an update. The code for this procedure is as follows:



```python
W = np.random.randn(10, 3073) \* 0.001 # generate random starting W
bestloss = float(""inf"")
for i in range(1000):
  step\_size = 0.0001
  Wtry = W + np.random.randn(10, 3073) \* step\_size
  loss = L(Xtr\_cols, Ytr, Wtry)
  if loss < bestloss:
    W = Wtry
    bestloss = loss
  print 'iter %d loss is %f' % (i, bestloss)

```

Using the same number of loss function evaluations as before (1000), this
approach achieves test set classification accuracy of **21.4%**. This is better,
but still wasteful and computationally expensive.



#### Strategy #3: Following the Gradient


In the previous section we tried to find a direction in the weight-space that
would improve our weight vector (and give us a lower loss). It turns out that
there is no need to randomly search for a good direction: we can compute the
*best* direction along which we should change our weight vector that is
mathematically guaranteed to be the direction of the steepest descent (at least
in the limit as the step size goes towards zero). This direction will be related
to the **gradient** of the loss function. In our hiking analogy, this approach
roughly corresponds to feeling the slope of the hill below our feet and stepping
down the direction that feels steepest.


In one-dimensional functions, the slope is the instantaneous rate of change of
the function at any point you might be interested in. The gradient is a
generalization of slope for functions that don’t take a single number but a
vector of numbers. Additionally, the gradient is just a vector of slopes (more
commonly referred to as **derivatives**) for each dimension in the input space.
The mathematical expression for the derivative of a 1-D function with respect
its input is:



\[\frac{df(x)}{dx} = \lim\_{h\ \to 0} \frac{f(x + h) - f(x)}{h}\]

When the functions of interest take a vector of numbers instead of a single
number, we call the derivatives **partial derivatives**, and the gradient is
simply the vector of partial derivatives in each dimension.



### Computing the gradient


There are two ways to compute the gradient: A slow, approximate but easy way
(**numerical gradient**), and a fast, exact but more error-prone way that
requires calculus (**analytic gradient**). We will now present both.



#### Computing the gradient numerically with finite differences


The formula given above allows us to compute the gradient numerically. Here is a
generic function that takes a function `f`, a vector `x` to evaluate the
gradient on, and returns the gradient of `f` at `x`:



```python
def eval\_numerical\_gradient(f, x):
  """"""
 a naive implementation of numerical gradient of f at x
 - f should be a function that takes a single argument
 - x is the point (numpy array) to evaluate the gradient at
 """"""

  fx = f(x) # evaluate function value at original point
  grad = np.zeros(x.shape)
  h = 0.00001

  # iterate over all indexes in x
  it = np.nditer(x, flags=['multi\_index'], op\_flags=['readwrite'])
  while not it.finished:

    # evaluate function at x+h
    ix = it.multi\_index
    old\_value = x[ix]
    x[ix] = old\_value + h # increment by h
    fxh = f(x) # evalute f(x + h)
    x[ix] = old\_value # restore to previous value (very important!)

    # compute the partial derivative
    grad[ix] = (fxh - fx) / h # the slope
    it.iternext() # step to next dimension

  return grad

```

Following the gradient formula we gave above, the code above iterates over all
dimensions one by one, makes a small change `h` along that dimension and
calculates the partial derivative of the loss function along that dimension by
seeing how much the function changed. The variable `grad` holds the full
gradient in the end.


**Practical considerations**. Note that in the mathematical formulation the
gradient is defined in the limit as **h** goes towards zero, but in practice it
is often sufficient to use a very small value (such as 1e-5 as seen in the
example). Ideally, you want to use the smallest step size that does not lead to
numerical issues. Additionally, in practice it often works better to compute the
numeric gradient using the **centered difference formula**: \( [f(x+h) - f(x-h)]
/ 2 h \) . See [wiki](http://en.wikipedia.org/wiki/Numerical_differentiation)
for details.


We can use the function given above to compute the gradient at any point and for
any function. Lets compute the gradient for the CIFAR-10 loss function at some
random point in the weight space:



```python

# to use the generic code above we want a function that takes a single argument
# (the weights in our case) so we close over X\_train and Y\_train
def CIFAR10\_loss\_fun(W):
  return L(X\_train, Y\_train, W)

W = np.random.rand(10, 3073) \* 0.001 # random weight vector
df = eval\_numerical\_gradient(CIFAR10\_loss\_fun, W) # get the gradient

```

The gradient tells us the slope of the loss function along every dimension,
which we can use to make an update:



```python
loss\_original = CIFAR10\_loss\_fun(W) # the original loss
print 'original loss: %f' % (loss\_original, )

# lets see the effect of multiple step sizes
for step\_size\_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:
  step\_size = 10 \*\* step\_size\_log
  W\_new = W - step\_size \* df # new position in the weight space
  loss\_new = CIFAR10\_loss\_fun(W\_new)
  print 'for step size %f new loss: %f' % (step\_size, loss\_new)

# prints:
# original loss: 2.200718
# for step size 1.000000e-10 new loss: 2.200652
# for step size 1.000000e-09 new loss: 2.200057
# for step size 1.000000e-08 new loss: 2.194116
# for step size 1.000000e-07 new loss: 2.135493
# for step size 1.000000e-06 new loss: 1.647802
# for step size 1.000000e-05 new loss: 2.844355
# for step size 1.000000e-04 new loss: 25.558142
# for step size 1.000000e-03 new loss: 254.086573
# for step size 1.000000e-02 new loss: 2539.370888
# for step size 1.000000e-01 new loss: 25392.214036

```

**Update in negative gradient direction**. In the code above, notice that to
compute `W_new` we are making an update in the negative direction of the
gradient `df` since we wish our loss function to decrease, not increase.


**Effect of step size**. The gradient tells us the direction in which the
function has the steepest rate of increase, but it does not tell us how far
along this direction we should step. As we will see later in the course,
choosing the step size (also called the *learning rate*) will become one of the
most important (and most headache-inducing) hyperparameter settings in training
a neural network. In our blindfolded hill-descent analogy, we feel the hill
below our feet sloping in some direction, but the step length we should take is
uncertain. If we shuffle our feet carefully we can expect to make consistent but
very small progress (this corresponds to having a small step size). Conversely,
we can choose to make a large, confident step in an attempt to descend faster,
but this may not pay off. As you can see in the code example above, at some
point taking a bigger step gives a higher loss as we “overstep”.



![](/assets/stepsize.jpg)

 Visualizing the effect of step size. We start at some particular spot W and evaluate the gradient (or rather its negative - the white arrow) which tells us the direction of the steepest decrease in the loss function. Small steps are likely to lead to consistent but slow progress. Large steps can lead to better progress but are more risky. Note that eventually, for a large step size we will overshoot and make the loss worse. The step size (or as we will later call it - the **learning rate**) will become one of the most important hyperparameters that we will have to carefully tune.
 


**A problem of efficiency**. You may have noticed that evaluating the numerical
gradient has complexity linear in the number of parameters. In our example we
had 30730 parameters in total and therefore had to perform 30,731 evaluations of
the loss function to evaluate the gradient and to perform only a single
parameter update. This problem only gets worse, since modern Neural Networks can
easily have tens of millions of parameters. Clearly, this strategy is not
scalable and we need something better.



#### Computing the gradient analytically with Calculus


The numerical gradient is very simple to compute using the finite difference
approximation, but the downside is that it is approximate (since we have to pick
a small value of *h*, while the true gradient is defined as the limit as *h*
goes to zero), and that it is very computationally expensive to compute. The
second way to compute the gradient is analytically using Calculus, which allows
us to derive a direct formula for the gradient (no approximations) that is also
very fast to compute. However, unlike the numerical gradient it can be more
error prone to implement, which is why in practice it is very common to compute
the analytic gradient and compare it to the numerical gradient to check the
correctness of your implementation. This is called a **gradient check**.


Lets use the example of the SVM loss function for a single datapoint:



\[L\_i = \sum\_{j\neq y\_i} \left[ \max(0, w\_j^Tx\_i - w\_{y\_i}^Tx\_i + \Delta) \right]\]

We can differentiate the function with respect to the weights. For example,
taking the gradient with respect to \(w\_{y\_i}\) we obtain:



\[\nabla\_{w\_{y\_i}} L\_i = - \left( \sum\_{j\neq y\_i} \mathbb{1}(w\_j^Tx\_i - w\_{y\_i}^Tx\_i + \Delta > 0) \right) x\_i\]

where \(\mathbb{1}\) is the indicator function that is one if the condition
inside is true or zero otherwise. While the expression may look scary when it is
written out, when you’re implementing this in code you’d simply count the number
of classes that didn’t meet the desired margin (and hence contributed to the
loss function) and then the data vector \(x\_i\) scaled by this number is the
gradient. Notice that this is the gradient only with respect to the row of \(W\)
that corresponds to the correct class. For the other rows where \(j \neq y\_i \)
the gradient is:



\[\nabla\_{w\_j} L\_i = \mathbb{1}(w\_j^Tx\_i - w\_{y\_i}^Tx\_i + \Delta > 0) x\_i\]

Once you derive the expression for the gradient it is straight-forward to
implement the expressions and use them to perform the gradient update.



### Gradient Descent


Now that we can compute the gradient of the loss function, the procedure of
repeatedly evaluating the gradient and then performing a parameter update is
called *Gradient Descent*. Its **vanilla** version looks as follows:



```python
# Vanilla Gradient Descent

while True:
  weights\_grad = evaluate\_gradient(loss\_fun, data, weights)
  weights += - step\_size \* weights\_grad # perform parameter update

```

This simple loop is at the core of all Neural Network libraries. There are other
ways of performing the optimization (e.g. LBFGS), but Gradient Descent is
currently by far the most common and established way of optimizing Neural
Network loss functions. Throughout the class we will put some bells and whistles
on the details of this loop (e.g. the exact details of the update equation), but
the core idea of following the gradient until we’re happy with the results will
remain the same.


**Mini-batch gradient descent.** In large-scale applications (such as the ILSVRC
challenge), the training data can have on order of millions of examples. Hence,
it seems wasteful to compute the full loss function over the entire training set
in order to perform only a single parameter update. A very common approach to
addressing this challenge is to compute the gradient over **batches** of the
training data. For example, in current state of the art ConvNets, a typical
batch contains 256 examples from the entire training set of 1.2 million. This
batch is then used to perform a parameter update:



```python
# Vanilla Minibatch Gradient Descent

while True:
  data\_batch = sample\_training\_data(data, 256) # sample 256 examples
  weights\_grad = evaluate\_gradient(loss\_fun, data\_batch, weights)
  weights += - step\_size \* weights\_grad # perform parameter update

```

The reason this works well is that the examples in the training data are
correlated. To see this, consider the extreme case where all 1.2 million images
in ILSVRC are in fact made up of exact duplicates of only 1000 unique images
(one for each class, or in other words 1200 identical copies of each image).
Then it is clear that the gradients we would compute for all 1200 identical
copies would all be the same, and when we average the data loss over all 1.2
million images we would get the exact same loss as if we only evaluated on a
small subset of 1000. In practice of course, the dataset would not contain
duplicate images, the gradient from a mini-batch is a good approximation of the
gradient of the full objective. Therefore, much faster convergence can be
achieved in practice by evaluating the mini-batch gradients to perform more
frequent parameter updates.


The extreme case of this is a setting where the mini-batch contains only a
single example. This process is called **Stochastic Gradient Descent (SGD)** (or
also sometimes **on-line** gradient descent). This is relatively less common to
see because in practice due to vectorized code optimizations it can be
computationally much more efficient to evaluate the gradient for 100 examples,
than the gradient for one example 100 times. Even though SGD technically refers
to using a single example at a time to evaluate the gradient, you will hear
people use the term SGD even when referring to mini-batch gradient descent (i.e.
mentions of MGD for “Minibatch Gradient Descent”, or BGD for “Batch gradient
descent” are rare to see), where it is usually assumed that mini-batches are
used. The size of the mini-batch is a hyperparameter but it is not very common
to cross-validate it. It is usually based on memory constraints (if any), or set
to some value, e.g. 32, 64 or 128. We use powers of 2 in practice because many
vectorized operation implementations work faster when their inputs are sized in
powers of 2.



### Summary



![](/assets/dataflow.jpeg)

 Summary of the information flow. The dataset of pairs of **(x,y)** is given and fixed. The weights start out as random numbers and can change. During the forward pass the score function computes class scores, stored in vector **f**. The loss function contains two components: The data loss computes the compatibility between the scores **f** and the labels **y**. The regularization loss is only a function of the weights. During Gradient Descent, we compute the gradient on the weights (and optionally on data if we wish) and use them to perform a parameter update during Gradient Descent.
 

In this section,


* We developed the intuition of the loss function as a **high-dimensional optimization landscape** in which we are trying to reach the bottom. The working analogy we developed was that of a blindfolded hiker who wishes to reach the bottom. In particular, we saw that the SVM cost function is piece-wise linear and bowl-shaped.
* We motivated the idea of optimizing the loss function with
**iterative refinement**, where we start with a random set of weights and refine them step by step until the loss is minimized.
* We saw that the **gradient** of a function gives the steepest ascent direction and we discussed a simple but inefficient way of computing it numerically using the finite difference approximation (the finite difference being the value of *h* used in computing the numerical gradient).
* We saw that the parameter update requires a tricky setting of the **step size** (or the **learning rate**) that must be set just right: if it is too low the progress is steady but slow. If it is too high the progress can be faster, but more risky. We will explore this tradeoff in much more detail in future sections.
* We discussed the tradeoffs between computing the **numerical** and **analytic** gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a **gradient check**, in which its implementation is compared to the numerical gradient.
* We introduced the **Gradient Descent** algorithm which iteratively computes the gradient and performs a parameter update in loop.


**Coming up:** The core takeaway from this section is that the ability to
compute the gradient of a loss function with respect to its weights (and have
some intuitive understanding of it) is the most important skill needed to
design, train and understand neural networks. In the next section we will
develop proficiency in computing the gradient analytically using the chain rule,
otherwise also referred to as **backpropagation**. This will allow us to
efficiently optimize relatively arbitrary loss functions that express all kinds
of Neural Networks, including Convolutional Neural Networks.









* [cs231n](https://github.com/cs231n)
* [cs231n](https://twitter.com/cs231n)












 // Make responsive
 MathJax.Hub.Config({
 ""HTML-CSS"": { linebreaks: { automatic: true } },
 ""SVG"": { linebreaks: { automatic: true } },
 });
 


"
d14TUNcbn1k?si=dEOOalrwtuIxXbvk,"0, 3350","[students murmuring] - Okay, so good afternoon
everyone, let's get started. So hi, so for those of
you who I haven't met yet, my name is Serena Yeung and I'm the third and final instructor for this class, and I'm also a PhD student
in Fei-Fei's group. Okay, so today we're going
to talk about backpropagation and neural networks, and so
now we're really starting to get to some of the core
material in this class. Before we begin, let's see, oh. So a few administrative details, so assignment one is due
Thursday, April 20th, so a reminder, we shifted
the date back by a little bit and it's going to be due
11:59 p.m. on Canvas. So you should start thinking
about your projects, there are TA specialties
listed on the Piazza website so if you have questions
about a specific project topic you're thinking about, you
can go and try and find the TAs that might be most relevant. And then also for Google Cloud,
so all students are going to get $100 in credits
to use for Google Cloud for their assignments and project, so you should be receiving an email for that this week, I think. A lot of you may have already, and then for those of you who haven't,
they're going to come, should be by the end of this week. Okay so where we are, so
far we've talked about how to define a classifier
using a function f, parameterized by weights
W, and this function f is going to take data x as input,
and output a vector of scores for each of the classes
that you want to classify. And so from here we can also define a loss function, so for
example, the SVM loss function that we've talked about
which basically quantifies how happy or unhappy we are with the scores that we've produced, right, and then we can use that to
define a total loss term. So L here, which is a
combination of this data term, combined with a regularization
term that expresses how simple our model is,
and we have a preference for simpler models, for
better generalization. And so now we want to
find the parameters W that correspond to our lowest loss, right? We want to minimize the loss function, and so to do that we want to find the gradient of L with respect to W. So last lecture we talked
about how we can do this using optimization, and we're going to iteratively take steps in the direction of steepest descent, which is
the negative of the gradient, in order to walk down this loss landscape and get to the point
of lowest loss, right? And we saw how this gradient
descent can basically take this trajectory, looking
like this image on the right, getting to the bottom
of your loss landscape. Oh! Okay, and so we also
talked about different ways for computing a gradient, right? We can compute this numerically using finite difference approximation which is slow and approximate,
but at the same time it's really easy to write out, you know you can always
get the gradient this way. We also talked about how to
use the analytic gradient and computing this is, it's fast and exact once you've
gotten the expression for the analytic gradient, but
at the same time you have to do all the math and the
calculus to derive this, so it's also, you know, easy
to make mistakes, right? So in practice what we want
to do is we want to derive the analytic gradient and use this, but at the same time check
our implementation using the numerical gradient to make sure that we've gotten all of our math right. So today we're going to
talk about how to compute the analytic gradient for
arbitrarily complex functions, using a framework that I'm going
to call computational graphs. And so basically what a
computational graph is, is that we can use this
kind of graph in order to represent any function,
where the nodes of the graph are steps of computation
that we go through. So for example, in this example, the linear classifier
that we've talked about, the inputs here are x and W, right, and then this multiplication
node represents the matrix multiplier,
the multiplication of the parameters W with
our data x that we have, outputting our vector of scores. And then we have another
computational node which represents our hinge loss, right, computing our data loss term, Li. And we also have this
regularization term at the bottom right, so this node which computes our regularization term, and then our total loss
here at the end, L, is the sum of the regularization
term and the data term. And the advantage is
that once we can express a function using a computational graph, then we can use a technique
that we call backpropagation which is going to recursively
use the chain rule in order to compute the gradient with respect to every variable
in the computational graph, and so we're going to
see how this is done. And this becomes very
useful when we start working with really complex functions, so for example,
convolutional neural networks that we're going to talk
about later in this class. We have here the input image at the top, we have our loss at the bottom, and the input has to
go through many layers of transformations in order to get all the way down to the loss function. And this can get even
crazier with things like, the, you know, like a
neural turing machine, which is another kind
of deep learning model, and in this case you can see
that the computational graph for this is really insane, and especially, we end up, you know,
unrolling this over time. It's basically completely impractical if you want to compute the gradients for any of these intermediate variables. Okay, so how does backpropagation work? So we're going to start
off with a simple example, where again, our goal is
that we have a function. So in this case, f of x, y, z equals x plus y times z, and we want to find the
gradients of the output of the function with respect
to any of the variables. So the first step, always, is we want to take our function f, and we want to represent it using
a computational graph. Right, so here our computational
graph is on the right, and you can see that we have our, first we have the plus node, so x plus y, and then we have this
multiplication node, right, for the second computation
that we're doing. And then, now we're going
to do a forward pass of this network, so given the values of the variables that we have, so here, x equals negative two, y equals five and z equals negative four,
I'm going to fill these all in in our computational graph,
and then here we can compute an intermediate value,
so x plus y gives three, and then finally we pass it through again, through the last node, the multiplication, to get our final node
of f equals negative 12. So here we want to give every
intermediate variable a name. So here I've called this
intermediate variable after the plus node q, and we
have q equals x plus y, and then f equals q times z,
using this intermediate node. And I've also written
out here, the gradients of q with respect to x
and y, which are just one because of the addition,
and then the gradients of f with respect to q and z,
which is z and q respectively because of the multiplication rule. And so what we want to
find, is we want to find the gradients of f with
respect to x, y and z. So what backprop is, it's
a recursive application of the chain rule, so we're
going to start at the back, the very end of the computational graph, and then we're going to
work our way backwards and compute all the
gradients along the way. So here if we start at
the very end, right, we want to compute the
gradient of the output with respect to the last
variable, which is just f. And so this gradient is
just one, it's trivial. So now, moving backwards,
we want the gradient with respect to z, right, and we know that df over dz is equal to q. So the value of q is just three, and so we have here, df
over dz equals three. And so next if we want to do df over dq, what is the value of that? What is df over dq? So we have here, df over
dq is equal to z, right, and the value of z is negative four. So here we have df over dq
is equal to negative four. Okay, so now continuing to
move backwards to the graph, we want to find df over dy, right, but here in this case, the
gradient with respect to y, y is not connected directly to f, right? It's connected through an
intermediate node of z, and so the way we're going to do this is we can leverage the
chain rule which says that df over dy can be
written as df over dq, times dq over dy, and
so the intuition of this is that in order to get to
find the effect of y on f, this is actually equivalent to if we take the effect of q times q on f,
which we already know, right? df over dq is equal to negative four, and we compound it with the
effect of y on q, dq over dy. So what's dq over dy
equal to in this case? - [Student] One. - One, right. Exactly. So dq over dy is equal to
one, which means, you know, if we change y by a little bit, q is going to change by approximately the same amount right, this is the effect, and so what this is
doing is this is saying, well if I change y by a little bit, the effect of y on q is going to be one, and then the effect of q on f
is going to be approximately a factor of negative four, right? So then we multiply these together and we get that the effect of y on f is going to be negative four. Okay, so now if we want
to do the same thing for the gradient with respect to x, right, we can do the, we can
follow the same procedure, and so what is this going to be? [students speaking away from microphone] - I heard the same. Yeah exactly, so in this
case we want to, again, apply the chain rule, right? We know the effect of q on
f is negative four, and here again, since we have
also the same addition node, dq over dx is equal to one, again, we have negative four times
one, right, and the gradient with respect to x is
going to be negative four. Okay, so what we're doing is, in backprop, is we basically have all of these nodes in our computational graph, but each node is only aware of its
immediate surroundings, right? So we have, at each node,
we have the local inputs that are connected to this node, the values that are flowing into the node, and then we also have the output that is directly outputted from this node. So here our local inputs are
x and y, and the output is z. And at this node we also know
the local gradient, right, we can compute the gradient
of z with respect to x, and the gradient of z with respect to y, and these are usually really
simple operations, right? Each node is going to be something like the addition or the multiplication that we had in that earlier example, which is something where
we can just write down the gradient, and we
don't have to, you know, go through very complex
calculus in order to find this. - [Student] Can you go
back and explain why more in the last slide was
different than planning the first part of it using
just normal calculus? - Yeah, so basically if we go back, hold on, let me... So if we go back here, we
could exactly write out, find all of these using just calculus, so we could say, you know,
we want df over dx, right, and we can probably
expand out this expression and see that it's just going to be z, but we can do this for, in this case, because it's simple, but
we'll see examples later on where once this becomes a
really complicated expression, you don't want to have to use calculus to derive, right, the
gradient for something, for a super-complicated expression, and instead, if you use this formalism and you break it down into
these computational nodes, then you can only ever work with gradients of very simple computations, right, at the level of, you know,
additions, multiplications, exponentials, things as
simple as you want them, and then you just use the chain rule to multiply all these together, and get your, the value of your gradient without having to ever
derive the entire expression. Does that make sense? [student murmuring] Okay, so we'll see an
example of this later. And so, was there another question, yeah? [student speaking away from microphone] - [Student] What's the negative four next to the z representing? - Negative, okay yeah,
so the negative four, these were the, the green values on top were all the values of
the function as we passed it forward through the
computational graph, right? So we said up here that x
is equal to negative two, y is equal to five, and
z equals negative four, so we filled in all of these
values, and then we just wanted to compute the value of this function. Right, so we said this value
of q is going to be x plus y, it's going to be negative
two plus five, it is going to be three, and we have z
is equal to negative four so we fill that in here,
and then we multiplied q and z together, negative four times three in order to get the
final value of f, right? And then the red values underneath were as we were filling in the gradients as we were working backwards. Okay. Okay, so right, so we said that, you know, we have these local, these nodes, and each node basically gets
its local inputs coming in and the output that it
sees directly passing on to the next node, and we also
have these local gradients that we computed, right, the gradient of the immediate output of the node with respect to the inputs coming in. And so what happens during
backprop is we have these, we'll start from the
back of the graph, right, and then we work our way from the end all the way back to the beginning, and when we reach each
node, at each node we have the upstream gradients coming back, right, with respect to the
immediate output of the node. So by the time we reach
this node in backprop, we've already computed the gradient of our final loss l,
with respect to z, right? And so now what we want to find next is we want to find the
gradients with respect to just before the node,
to the values of x and y. And so as we saw earlier, we
do this using the chain rule, right, we have from the chain rule, that the gradient of this loss function with respect to x is going to be the gradient with respect
to z times, compounded by this gradient, local gradient
of z with respect to x. Right, so in the chain rule we always take this upstream gradient coming down, and we multiply it by the local gradient in order to get the gradient
with respect to the input. - [Student] So, sorry, is it, it's different because
this would never work to get a general formula into the, or general symbolic
formula for the gradient. It only works with instantaneous values, where you like. [student coughing] Or passing a little constant
value as a symbolic. - So the question is
whether this only works because we're working
with the current values of the function, and so it works, right, given the current values of
the function that we plug in, but we can write an expression for this, still in terms of the variables, right? So we'll see that gradient
of L with respect to z is going to be some
expression, and gradient of z with respect to x is going to
be another expression, right? But we plug in these,
we plug in the values of these numbers at the
time in order to get the value of the gradient
with respect to x. So what you could do is you
could recursively plug in all of these expressions, right? Gradient with respect, z with respect to x is going to be a simple,
simple expression, right? So in this case, if we
have a multiplication node, gradient of z with
respect to x is just going to be y, right, we know that, but the gradient of L with respect to z, this is probably a complex part of the graph in itself, right, so
here's where we want to just, in this case, have this numerical, right? So as you said, basically
this is going to be just a number coming down, right, a value, and then we just multiply it with the expression that we have
for the local gradient. And I think this will be
more clear when we go through a more complicated
example in a few slides. Okay, so now the gradient
of L with respect to y, we have exactly the
same idea, where again, we use the chain rule,
we have gradient of L with respect to z, times the gradient of z with respect to y, right,
we use the chain rule, multiply these together
and get our gradient. And then once we have these,
we'll pass these on to the node directly before,
or connected to this node. And so the main thing
to take away from this is that at each node we just
want to have our local gradient that we compute, just keep track of this, and then during backprop as
we're receiving, you know, numerical values of gradients
coming from upstream, we just take what that is, multiply it by the local gradient, and then this is what we then send back
to the connected nodes, the next nodes going backwards,
without having to care about anything else besides
these immediate surroundings. So now we're going to go
through another example, this time a little bit more complex, so we can see more why
backprop is so useful. So in this case, our
function is f of w and x, which is equal to one over one plus e to the negative of w-zero times x-zero plus w-one x-one, plus w-two, right? So again, the first step always is we want to write this out as
a computational graph. So in this case we can see
that in this graph, right, first we multiply together the
w and x terms that we have, w-zero with x-zero, w-one with x-one, and w-two, then we add all
of these together, right? Then we do, scale it by negative one, we take the exponential, we add one, and then finally we do
one over this whole term. And then here I've also
filled in values of these, so let's say given values that we have for the ws and xs, right,
we can make a forward pass and basically compute what the value is at every stage of the computation. And here I've also written
down here at the bottom the values, the expressions
for some derivatives that are going to be helpful later on, so same as we did before
with the simple example. Okay, so now then we're going
to do backprop through here, right, so again, we're going to start at the very end of the
graph, and so here again the gradient of the output with
respect to the last variable is just one, it's just trivial, and so now moving
backwards one step, right? So what's the gradient with respect to the input just before one over x? Well, so in this case, we know
that the upstream gradient that we have coming down,
right, is this red one, right? This is the upstream gradient
that we have flowing down, and then now we need to find
the local gradient, right, and the local gradient of this node, this node is one over x, right, so we have f of x equals
one over x here in red, and the local gradient of this df over dx is equal to negative one
over x-squared, right? So here we're going to take
negative one over x-squared, and plug in the value
of x that we had during this forward pass, 1.37,
and so our final gradient with respect to this variable is going to be negative one over
1.37 squared times one equals negative 0.53. So moving back to the next node, we're going to go through the
exact same process, right? So here, the gradient
flowing from upstream is going to be negative 0.53, right, and here the local gradient,
the node here is a plus one, and so now looking at our
reference of derivatives at the bottom, we have that
for a constant plus x, the local gradient is just one, right? So what's the gradient with respect to this variable using the chain rule? So it's going to be the upstream gradient of negative 0.53 times
our local gradient of one, which is equal to negative 0.53. So let's keep moving
backwards one more step. So here we have the exponential, right? So what's the upstream
gradient coming down? [student speaking away from microphone] Right, so the upstream
gradient is negative 0.53, what's the local gradient here? It's going to be the local
gradient of e to the x, right? This is an exponential
node, and so our chain rule is going to tell us that our gradient is going to be negative 0.53
times e to the power of x, which in this case is negative one, from our forward pass, and
this is going to give us our final gradient of negative 0.2. Okay, so now one more node here, the next node is, that
we reach, is going to be a multiplication with negative one, right? So here, what's the upstream
gradient coming down? - [Student] Negative 0.2? - [Serena] Negative 0.2,
right, and what's going to be the local gradient, can
look at the reference sheet. It's going to be, what was it? I think I heard it. - [Student] That's minus one? - It's going to be minus
one, exactly, yeah, because our local gradient
says it's going to be, df over dx is a, right, and the value of a that we scaled x by is negative one here. So we have here that the gradient is negative one times negative 0.2, and so our gradient is 0.2. Okay, so now we've
reached an addition node, and so in this case we
have these two branches both connected to it, right? So what's the upstream gradient here? It's going to be 0.2, right,
just as everything else, and here now the gradient with respect to each of these branches,
it's an addition, right, and we saw from before
in our simple example that when we have an addition node, the gradient with respect
to each of the inputs to the addition is just
going to be one, right? So here, our local gradient
for looking at our top stream is going to be one times
the upstream gradient of 0.2, which is going to give
a total gradient of 0.2, right? And then we, for our bottom branch we'd do the same thing, right, our
upstream gradient is 0.2, our local gradient is one again, and the total gradient is 0.2. So is everything clear about this? Okay. So we have a few more
gradients to fill out, so moving back now we've
reached w-zero and x-zero, and so here we have a
multiplication node, right, so we saw the multiplication
node from before, it just, the gradient with respect to one of the inputs just is
the value of the other input. And so in this case, what's the gradient with respect to w-zero? - [Student] Minus 0.2. - Minus, I'm hearing minus 0.2, exactly. Yeah, so with respect to w-zero, we have our upstream gradient, 0.2, right, times our, this is the bottom one, times our value of x,
which is negative one, we get negative 0.2 and
we can do the same thing for our gradient with respect to x-zero. It's going to be 0.2
times the value of w-zero which is two, and we get 0.4. Okay, so here we've filled
out most of these gradients, and so there was the question earlier about why this is simpler
than just computing, deriving the analytic gradient,
the expression with respect to any of these variables, right? And so you can see here,
all we ever dealt with was expressions for local gradients that we had to write out, so
once we had these expressions for local gradients, all we did was plug in the values for
each of these that we have, and use the chain rule to
numerically multiply this all the way backwards and get the gradients with respect to all of the variables. And so, you know, we can also fill out the gradients with respect
to w-one and x-one here in exactly the same way, and so one thing that I want to note is that
right when we're creating these computational graphs, we can define the computational nodes at any
granularity that we want to. So in this case, we broke it down into the absolute simplest
that we could, right, we broke it down into
additions and multiplications, you know, it basically can't
get any simpler than that, but in practice, right,
we can group some of these nodes together into
more complex nodes if we want. As long as we're able to write down the local gradient for that node, right? And so as an example, if we
look at a sigmoid function, so I've defined the sigmoid function in the upper-right here, of a sigmoid of x is equal to one over one
plus e to the negative x, and this is something that's
a really common function that you'll see a lot in
the rest of this class, and we can compute the gradient for this, we can write it out, and if
we do actually go through the math of doing this analytically, we can get a nice expression at the end. So in this case it's equal
to one minus sigma of x, so the output of this function
times sigma of x, right? And so in cases where we
have something like this, we could just take all the computations that we had in our graph
that made up this sigmoid, and we could just replace it with one big node that's a sigmoid, right, because we do know the local
gradient for this gate, it's this expression, d of the
sigmoid of x over dx, right? So basically the important thing here is that you can, group any nodes that you want to make any sorts of a little
bit more complex nodes, as long as you can write down
the local gradient for this. And so all this is is
basically a trade-off between, you know, how much math
that you want to do in order to get a more, kind
of concise and simpler graph, right, versus how simple you want each of your gradients to be, right? And then you can write out as complex of a computational graph that you want. Yeah, question? - [Student] This is a
question on the graph itself, is there a reason that the
first two multiplication nodes and the weights are not connected
to a single addition node? - So they could also be connected into a single addition node,
so the question was, is there a reason why w-zero and x-zero are not connected with w-two? All of these additions
just connected together, and yeah, so the reason, the answer is that you can do that if you want, and in practice, maybe you
would actually want to do that because this is still a
very simple node, right? So in this case I just wrote
this out into as simple as possible, where each node
only had up to two inputs, but yeah, you could definitely do that. Any other questions about this? Okay, so the one thing that I really like about thinking about this
like a computational graph is that I feel very comforted, right, like anytime I have to take a gradient, find gradients of something,
even if the expression that I want to compute
gradients of is really hairy, and really scary, you know,
whether it's something like this sigmoid or something worse, I know that, you know, I could
derive this if I want to, but really, if I just
sit down and write it out in terms of a computational graph, I can go as simple as I need to to always be able to apply
backprop and the chain rule, and be able to compute all
the gradients that I need. And so this is something that
you guys should think about when you're doing your homeworks,
as basically, you know, anytime you're having trouble
finding gradients of something just think about it as
a computational graph, break it down into all of these parts, and then use the chain rule. Okay, and so, you know, so we talked about how we could group these
set of nodes together into a sigmoid gate, and
just to confirm, like, that this is actually exactly equivalent, we can plug this in, right? So we have that our input
here to the sigmoid gate is going to be one, in
green, and then we have that the output is going
to be here, 0.73, right, and this'll work out if you plug it in to the sigmoid function. And so now if we want to
do, if we want to take the gradient, and we want
to treat this entire sigmoid as one node, now what we should do is we need to use this local gradient that we've derived up here, right? One minus sigmoid of x
times the sigmoid of x. So if we plug this in, and here we know that the value of sigmoid of x was 0.73, so if we plug this value
in we'll see that this, the value of this gradient
is equal to 0.2, right, and so the value of this
local gradient is 0.2, we multiply it by the x
upstream gradient which is one, and we're going to get
out exactly the same value of the gradient with respect
to before the sigmoid gate, as if we broke it down into all
of the smaller computations. Okay, and so as we're looking
at what's happening, right, as we're taking these
gradients going backwards through our computational graph, there's some patterns that you'll notice where there's some
intuitive interpretation that we can give these, right? So we saw that the add gate is
a gradient distributor right, when we passed through
this addition gate here, which had two branches coming out of it, it took the gradient,
the upstream gradient and it just distributed it,
passed the exact same thing to both of the branches
that were connected. So here's a couple more
that we can think about. So what's a max gate look like? So we have a max gate
here at the bottom, right, where the input's coming in are z and w, z has a value of two, w has
a value of negative one, and then we took the max of
this, which is two, right, and so we pass this
down into the remainder of our computational graph. So now if we're taking the
gradients with respect to this, the upstream gradient is, let's
say two coming back, right, and what does this local
gradient look like? So anyone, yes? - [Student] It'll be zero for
one, and one for the other? - Right. [student speaking away from microphone] Exactly, so the answer that was given is that z will have a gradient of two, w will have a value, a gradient of zero, and so one of these is going to get the full value of the
gradient just passed back, and routed to that variable,
and then the other one will have a gradient of zero, and so, so we can think of this as kind
of a gradient router, right, so, whereas the addition node passed back the same gradient to
both branches coming in, the max gate will just take the gradient and route it to one of the branches, and this makes sense because
if we look at our forward pass, what's happening is that only the value that was the maximum got passed down to the rest of the
computational graph, right? So it's the only value
that actually affected our function computation at
the end, and so it makes sense that when we're passing
our gradients back, we just want to adjust what, you know, flow it through that
branch of the computation. Okay, and so another one,
what's a multiplication gate, which we saw earlier, is there
any interpretation of this? [student speaking away from microphone] Okay, so the answer that was given is that the local
gradient is basically just the value of the other variable. Yeah, so that's exactly right. So we can think of this as
a gradient switcher, right? A switcher, and I guess
a scaler, where we take the upstream gradient and we scale it by the value of the other branch. Okay, and so one other thing to note is that when we have a place where one node is connected to multiple nodes, the gradients add up at this node, right? So at these branches, using
the multivariate chain rule, we're just going to take the value of the upstream gradient coming
back from each of these nodes, and we'll add these
together to get the total upstream gradient that's
flowing back into this node, and you can see this from
the multivariate chain rule and also thinking about this,
you can think about this that if you're going to
change this node a little bit, it's going to affect both
of these connected nodes in the forward pass,
right, when you're making your forward pass through the graph. And so then when you're
doing backprop, right, then now the, both of
these gradients coming back are going to affect this node, right, and so that's how we're
going to sum these up to be the total upstream gradient
flowing back into this node. Okay, so any questions about backprop, going through these forward
and backward passes? - [Student] So we haven't did anything to actually update the weights. [speaking away from microphone] - Right, so the question is,
we haven't done anything yet to update the values of these weights, we've only found the
gradients with respect to the variables, that's exactly right. So what we've talked about
so far in this lecture is how to compute gradients with
respect to any variables in our function, right,
and then once we have these we can just apply everything we learned in the optimization lecture,
last lecture, right? So given the gradient,
we now take a step in the direction of the gradient in order to update our weight,
our parameters, right? So you can just take this entire framework that we learned about last
lecture for optimization, and what we've done here is
just learn how to compute the gradients we need for
arbitrarily complex functions, right, and so this is going
to be useful when we talk about complex functions like
neural networks later on. Yeah? - [Student] Do you mind writing out the, all the variate, so you could help explain this slide a little better? - Yeah, so I can write
this maybe on the board. Right, so basically if we're
going to have, let's see, if we're going to have the gradient of f with respect to some variable x, right, and let's say it's
connected through variables, let's see, i, we can basically... Right, so this is basically saying that if x is connected to
these multiple elements, right, which in this case, different q-is, then the chain rule is taking all, it's going to take the effect of each of these intermediate variables, right, on our final output f, and
then compound each one with the local effect of our variable x on that intermediate value, right? So yeah, it's basically just
summing all these up together. Okay, so now that we've, you
know, done all these examples in the scalar case, we're going to look at what happens when we have vectors, right? So now if our variables x, y and z, instead of just being numbers,
we have vectors for these. And so everything stays exactly
the same, the entire flow, the only difference is
that now our gradients are going to be Jacobian matrices, right, so these are now going
to be matrices containing the derivative of each
element of, for example z with respect to each element of x. Okay, and so to, you
know, so give an example of something where this is
happening, right, let's say that we have our input is
going to now be a vector, so let's say we have a
4096-dimensional input vector, and this is kind of a common
size that you might see in convolutional neural networks later on, and our node is going to be an
element-wise maximum, right? So we have f of x is equal to the maximum of x compared with zero
element-wise, and then our output is going to be also a
4096-dimensional vector. Okay, so in this case, what's the size of our Jacobian matrix? Remember I said earlier,
the Jacobian matrix is going to be, like each row is, it's going to be partial derivatives, a matrix of partial derivatives
of each dimension of the output with respect to
each dimension of the input. Okay, so the answer I
heard was 4,096 squared, and that's, yeah, that's correct. So this is pretty large,
right, 4,096 by 4,096 and in practice this is
going to be even larger because we're going to
work with many batches of, you know, of, for example, 100 inputs at the same time, right,
and we'll put all of these through our node at the same
time to be more efficient, and so this is going to scale this by 100, and in practice our Jacobian's
actually going to turn out to be something like
409,000 by 409,000 right, so this is really huge, and basically completely impractical to work with. So in practice though,
we don't actually need to compute this huge
Jacobian most of the time, and so why is that, like, what does this Jacobian matrix look like? If we think about what's happening here, where we're taking this
element-wise maximum, and we think about what are each of the partial derivatives, right, which dimension of the inputs affect which dimensions of the output? What sort of structure can we
see in our Jacobian matrix? [student speaking away from microphone] Okay, so I heard that it's
diagonal, right, exactly. So because this is element-wise,
right, each element of the input, say the first
dimension, only affects that corresponding element
in the output, right? And so because of that
our Jacobian matrix, which is just going to
be a diagonal matrix. And so in practice then,
we don't actually have to write out and formulate
this entire Jacobian, we can just know the effect
of x on the output, right, and then we can just
use these values, right, and fill it in as we're
computing the gradient. Okay, so now we're going to go through a more concrete vectorized
example of a computational graph. Right, so let's look at a case where we have the function f of x and W is equal to, basically the
L-two of W multiplied by x, and so in this case we're going to say x is n-dimensional and W is n by n. Right, so again our first step, writing out the
computational graph, right? We have W multiplied by
x, and then followed by, I'm just going to call this L-two. And so now let's also fill
out some values for this, so we can see that, you
know, let's say have W be this two by two matrix, and x is going to be this
two-dimensional vector, right? And so we can say, label
again our intermediate nodes. So our intermediate node
after the multiplication it's going to be q, we
have q equals W times x, which we can write out
element-wise this way, where the first element is
just W-one-one times x-one plus W-one-two times x-two and so on, and then we can now express
f in relation to q, right? So looking at the second
node we have f of q is equal to the L-two norm of q, which is equal to q-one
squared plus q-two squared. Okay, so we filled this in, right, we get q and then we get our final output. Okay, so now let's do
backprop through this, right? So again, this is always the first step, we have the gradient with respect
to our output is just one. Okay, so now let's move back one node, so now we want to find the
gradient with respect to q, right, our intermediate
variable before the L-two. And so q is a two-dimensional vector, and what we want to do is we want to find how each element of q
affects our final value of f, right, and so if we
look at this expression that we've written out
for f here at the bottom, we can see that the gradient of f with respect to a specific
q-i, let's say q-one, is just going to be two times q-i, right? This is just taking this derivative here, and so we have this expression for, with respect to each element of q-i, we could also, you know, write this out in vector form if we want to, it's just going to be two
times our vector of q, right, if we want to write
this out in vector form, and so what we get is
that our gradient is 0.44, and 0.52, this vector, right? And so you can see that it just took q and it scaled it by two, right? Each element is just multiplied by two. So the gradient of a vector
is always going to be the same size as the original vector, and each element of this
gradient is going to, it means how much of
this particular element affects our final output of the function. Okay, so now let's move
one step backwards, right, what's the gradient with respect to W? And so here again we want
to use the same concept of trying to apply the chain rule, right, so we want to compute our local gradient of q with respect to W, and so let's look at this again element-wise,
and if we do that, let's see what's the
effect of each q, right, each element of q with
respect to each element of W, and so this is going to be the Jacobian that we talked about earlier,
and if we look at this in this multiplication, q is equal to W times x, right,
what's the derivative, or the gradient of the first element of q, so our first element up top,
with respect to W-one-one? So q-one with respect to W-one-one? What's that value? X-one, exactly. Yeah, so we know that this is x-one, and we can write this
out more generally of the gradient of q-k with respect
to W-i,j is equal to X-j. And then now if we want
to find the gradient with respect to, of f,
with respect to each W-i,j. So looking at these derivatives now, we can use this chain rule
that we talked earlier where we basically compound df over dq-k for each element of q with dq-k over W-i,j for each element of W-i,j, right? So we find the effect of each element of W on each element of q, and
sum this across all q. And so if you write this
out, this is going to give this expression of two
times q-i times x-j. Okay, and so filling this out then we get this gradient with respect to W, and so again we can compute
this each element-wise, or we can also look at this
expression that we've derived and write it out in
vectorized form, right? So okay, and remember, the important thing is always to check the gradient
with respect to a variable should have the same shape as
the variable, and something, so this is something
really useful in practice to sanity check, right,
like once you've computed what your gradient should
be, check that this is the same shape as your variable, because again, the element,
each element of your gradient is quantifying how much that element is contributing to your, is
affecting your final output. Yeah? [student speaking away from microphone] The both sides, oh the both sides one is an indicator function,
so this is saying that it's just one if k equals i. Okay, so let's see, so we've done that, and so now just see, one more example. Now our last thing we need to find is the gradient with respect to q-I. So here if we compute the
partial derivatives we can see that dq-k over dx-i is
equal to W-k,i, right, using the same way as we did it for W, and then again we can
just use the chain rule and get the total
expression for that, right? And so this is going to be the gradient with respect to x, again,
of the same shape as x, and we can also write this out in vectorized form if we want. Okay, so any questions about this, yeah? [student speaking away from microphone] So we are computing the Jacobian, so let me go back here, right, so if we're doing, so right, so we have these partial derivatives of q-k with respect to x-i, right, and these are forming your, the entries
of your Jacobian, right? And so in practice what we're going to do is we basically take that,
and you're going to see it up there in the chain rule,
so the vectorized expression of gradient with respect to x, right, this is going to have the Jacobian here which is this transposed value here, so you can write it
out in vectorized form. [student speaking away from microphone] So well, so in this case the matrix is going to be the same size as W right, so it's not actually a large
matrix in this case, right? Okay, so the way that we've
been thinking about this is like a really modularized
implementation, right, where in our computational graph, right, we look at each node
locally and we compute the local gradients and chain them with upstream gradients coming down, and so you can think of this as basically a forward and a backwards API, right? In the forward pass we
implement the, you know, a function computing
the output of this node, and then in the backwards
pass we compute the gradient. And so when we actually
implement this in code, we're going to do this
in exactly the same way. So we can basically think
about, for each gate, right, if we implement a forward
function and a backward function, where the backward function
is computing the chain rule, then if we have our entire
graph, we can just make a forward pass through the
entire graph by iterating through all the nodes in the graph, all the gates. Here I'm going to use
the word gate and node, kind of interchangeably,
we can iterate through all of these gates and just call forward on each of the gates, right? And we just want to do this
in topologically sorted order, so we process all of
the inputs coming in to a node before we process that node. And then going backwards,
we're just going to then go through all of the gates
in this reverse sorted order, and then call backwards
on each of these gates. Okay, and so if we look at then the implementation for
our particular gates, so for example, this MultiplyGate here, we want to implement
the forward pass, right, so it gets x and y as inputs,
and returns the value of z, and then when we go backwards, right, we get as input dz, which
is our upstream gradient, and we want to output the gradients on the input's x and
y to pass down, right? So we're going to output dx and dy, and so in this case, in this example, everything is back to
the scalar case here, and so if we look at
this in the forward pass, one thing that's important
is that we need to, we should cache the values
of the forward pass, right, because we end up using this in the backward pass a lot of the time. So here in the forward pass,
we want to cache the values of x and y, right, and
in the backward pass, using the chain rule,
we're going to, remember, take the value of the upstream gradient and scale it by the value
of the other branch, right, and so we'll keep, for
dx we'll take our value of self.y that we kept, and multiply it by dz coming down, and same for dy. Okay, so if you look at a lot
of deep-learning frameworks and libraries you'll see
that they exactly follow this kind of modularization, right? So for example, Caffe is a
popular deep learning framework, and you'll see, if you go look
through the Caffe source code you'll get to some
directory that says layers, and in layers, which are
basically computational nodes, usually layers might be
slightly more, you know, some of these more complex
computational nodes like the sigmoid that
we talked about earlier, you'll see, basically just a whole list of all different kinds of
computational nodes, right? So you might have the sigmoid, and I know there might be here, there's
like a convolution is one, there's an Argmax is another layer, you'll have all of these
layers and if you dig in to each of them, they're
just exactly implementing a forward pass and a backward pass, and then all of these are called when we do forward and backward pass through the entire network that we formed, and so our network is just basically going to be stacking up all of these, the different layers that we
choose to use in the network. So for example, if we
look at a specific one, in this case a sigmoid layer, you'll see that in the sigmoid layer, right, we've talked about the sigmoid function, you'll see that there's a forward pass which basically computes
exactly the sigmoid expression, and then a backward pass, right, where it is taking as input
something, basically a top_diff, which is our upstream
gradient in this case, and multiplying it by a local
gradient that we compute. So in assignment one you'll get practice with this kind of, this
computational graph way of thinking where, you know, you're
going to be writing your SVM and Softmax classes, and taking the gradients of these. And so again, remember always
you want to first step, represent it as a
computational graph, right? Figure out what are all the computations that you did leading up to the output, and then when you, when it's time to do your backward pass,
just take the gradient with respect to each of
these intermediate variables that you've defined in
your computational graph, and use the chain rule to
link them all together. Okay, so summary of what
we've talked about so far. When we get down to, you know,
working with neural networks, these are going to be
really large and complex, so it's going to be
impractical to write down the gradient formula by hand
for all your parameters. So in order to get these gradients, right, we talked about how, what we
should use is backpropagation, right, and this is kind of
one of the core techniques of, you know, neural
networks, is basically using backpropagation to
get your gradients, right? And so this is a recursive application of the chain rule where we have
this computational graph, and we start at the back and
we go backwards through it to compute the gradients with respect to all of the intermediate variables, which are your inputs, your parameters, and everything else in the middle. And we've also talked about how really this implementation and
this graph structure, each of these nodes is
really, you can see this as implementing a forward
and backwards API, right? And so in the forward
pass we want to compute the results of the operation, and we want to save any intermediate values that we might want to use later
in our gradient computation, and then in the backwards
pass we apply this chain rule and we take this upstream gradient, we chain it, multiply it
with our local gradient to compute the gradient with respect to the inputs of the node,
and we pass this down to the nodes that are connected next. Okay, so now finally we're going to talk about neural networks. All right, so really, you
know, neural networks, people draw a lot of analogies
between neural networks and the brain, and different types of biological inspirations,
and we'll get to that in a little bit, but first let's
talk about it, you know, just looking at it as a function, as a class of functions
without all of the brain stuff. So, so far we've talked about, you know, we've worked a lot with this
linear score function, right? f equals W times x, and
so we've been using this",https://cs231n.github.io/optimization-2/,"




CS231n Convolutional Neural Networks for Visual Recognition









 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1\*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-46895817-2', 'auto');
 ga('send', 'pageview');
 



addBackToTop({
 backgroundColor: '#fff',
 innerHTML: 'Back to Top',
 textColor: '#333'
 })

 #back-to-top {
 border: 1px solid #ccc;
 border-radius: 0;
 font-family: sans-serif;
 font-size: 14px;
 width: 100px;
 text-align: center;
 line-height: 30px;
 height: 30px;
 }
 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io)
[Course Website](http://cs231n.stanford.edu/)





# 




Table of Contents:


* [Introduction](#intro)
* [Simple expressions, interpreting the gradient](#grad)
* [Compound expressions, chain rule, backpropagation](#backprop)
* [Intuitive understanding of backpropagation](#intuitive)
* [Modularity: Sigmoid example](#sigmoid)
* [Backprop in practice: Staged computation](#staged)
* [Patterns in backward flow](#patterns)
* [Gradients for vectorized operations](#mat)
* [Summary](#summary)



### Introduction


**Motivation**. In this section we will develop expertise with an intuitive
understanding of **backpropagation**, which is a way of computing gradients of
expressions through recursive application of **chain rule**. Understanding of
this process and its subtleties is critical for you to understand, and
effectively develop, design and debug neural networks.


**Problem statement**. The core problem studied in this section is as follows:
We are given some function \(f(x)\) where \(x\) is a vector of inputs and we are
interested in computing the gradient of \(f\) at \(x\) (i.e. \(\nabla f(x)\) ).


**Motivation**. Recall that the primary reason we are interested in this problem
is that in the specific case of neural networks, \(f\) will correspond to the
loss function ( \(L\) ) and the inputs \(x\) will consist of the training data
and the neural network weights. For example, the loss could be the SVM loss
function and the inputs are both the training data \((x\_i,y\_i), i=1 \ldots N\)
and the weights and biases \(W,b\). Note that (as is usually the case in Machine
Learning) we think of the training data as given and fixed, and of the weights
as variables we have control over. Hence, even though we can easily use
backpropagation to compute the gradient on the input examples \(x\_i\), in
practice we usually only compute the gradient for the parameters (e.g. \(W,b\))
so that we can use it to perform a parameter update. However, as we will see
later in the class the gradient on \(x\_i\) can still be useful sometimes, for
example for purposes of visualization and interpreting what the Neural Network
might be doing.


If you are coming to this class and you’re comfortable with deriving gradients
with chain rule, we would still like to encourage you to at least skim this
section, since it presents a rarely developed view of backpropagation as
backward flow in real-valued circuits and any insights you’ll gain may help you
throughout the class.



### Simple expressions and interpretation of the gradient


Lets start simple so that we can develop the notation and conventions for more
complex expressions. Consider a simple multiplication function of two numbers
\(f(x,y) = x y\). It is a matter of simple calculus to derive the partial
derivative for either input:



\[f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x\]

**Interpretation**. Keep in mind what the derivatives tell you: They indicate
the rate of change of a function with respect to that variable surrounding an
infinitesimally small region near a particular point:



\[\frac{df(x)}{dx} = \lim\_{h\ \to 0} \frac{f(x + h) - f(x)}{h}\]

A technical note is that the division sign on the left-hand side is, unlike the
division sign on the right-hand side, not a division. Instead, this notation
indicates that the operator \( \frac{d}{dx} \) is being applied to the function
\(f\), and returns a different function (the derivative). A nice way to think
about the expression above is that when \(h\) is very small, then the function
is well-approximated by a straight line, and the derivative is its slope. In
other words, the derivative on each variable tells you the sensitivity of the
whole expression on its value. For example, if \(x = 4, y = -3\) then \(f(x,y) =
-12\) and the derivative on \(x\) \(\frac{\partial f}{\partial x} = -3\). This
tells us that if we were to increase the value of this variable by a tiny
amount, the effect on the whole expression would be to decrease it (due to the
negative sign), and by three times that amount. This can be seen by rearranging
the above equation ( \( f(x + h) = f(x) + h \frac{df(x)}{dx} \) ). Analogously,
since \(\frac{\partial f}{\partial y} = 4\), we expect that increasing the value
of \(y\) by some very small amount \(h\) would also increase the output of the
function (due to the positive sign), and by \(4h\).



> 
> The derivative on each variable tells you the sensitivity of the whole
> expression on its value.
> 
> 
> 


As mentioned, the gradient \(\nabla f\) is the vector of partial derivatives, so
we have that \(\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial
f}{\partial y}] = [y, x]\). Even though the gradient is technically a vector, we
will often use terms such as *“the gradient on x”* instead of the technically
correct phrase *“the partial derivative on x”* for simplicity.


We can also derive the derivatives for the addition operation:



\[f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1\]

that is, the derivative on both \(x,y\) is one regardless of what the values of
\(x,y\) are. This makes sense, since increasing either \(x,y\) would increase
the output of \(f\), and the rate of that increase would be independent of what
the actual values of \(x,y\) are (unlike the case of multiplication above). The
last function we’ll use quite a bit in the class is the *max* operation:



\[f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)\]

That is, the (sub)gradient is 1 on the input that was larger and 0 on the other
input. Intuitively, if the inputs are \(x = 4,y = 2\), then the max is 4, and
the function is not sensitive to the setting of \(y\). That is, if we were to
increase it by a tiny amount \(h\), the function would keep outputting 4, and
therefore the gradient is zero: there is no effect. Of course, if we were to
change \(y\) by a large amount (e.g. larger than 2), then the value of \(f\)
would change, but the derivatives tell us nothing about the effect of such large
changes on the inputs of a function; They are only informative for tiny,
infinitesimally small changes on the inputs, as indicated by the \(\lim\_{h
\rightarrow 0}\) in its definition.



### Compound expressions with chain rule


Lets now start to consider more complicated expressions that involve multiple
composed functions, such as \(f(x,y,z) = (x + y) z\). This expression is still
simple enough to differentiate directly, but we’ll take a particular approach to
it that will be helpful with understanding the intuition behind backpropagation.
In particular, note that this expression can be broken down into two
expressions: \(q = x + y\) and \(f = q z\). Moreover, we know how to compute the
derivatives of both expressions separately, as seen in the previous section.
\(f\) is just multiplication of \(q\) and \(z\), so \(\frac{\partial f}{\partial
q} = z, \frac{\partial f}{\partial z} = q\), and \(q\) is addition of \(x\) and
\(y\) so \( \frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1
\). However, we don’t necessarily care about the gradient on the intermediate
value \(q\) - the value of \(\frac{\partial f}{\partial q}\) is not useful.
Instead, we are ultimately interested in the gradient of \(f\) with respect to
its inputs \(x,y,z\). The **chain rule** tells us that the correct way to
“chain” these gradient expressions together is through multiplication. For
example, \(\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q}
\frac{\partial q}{\partial x} \). In practice this is simply a multiplication of
the two numbers that hold the two gradients. Lets see this with an example:



```python
# set some inputs
x = -2; y = 5; z = -4

# perform the forward pass
q = x + y # q becomes 3
f = q \* z # f becomes -12

# perform the backward pass (backpropagation) in reverse order:
# first backprop through f = q \* z
dfdz = q # df/dz = q, so gradient on z becomes 3
dfdq = z # df/dq = z, so gradient on q becomes -4
dqdx = 1.0
dqdy = 1.0
# now backprop through q = x + y
dfdx = dfdq \* dqdx  # The multiplication here is the chain rule!
dfdy = dfdq \* dqdy  

```

We are left with the gradient in the variables `[dfdx,dfdy,dfdz]`, which tell us
the sensitivity of the variables `x,y,z` on `f`!. This is the simplest example
of backpropagation. Going forward, we will use a more concise notation that
omits the `df` prefix. For example, we will simply write `dq` instead of `dfdq`,
and always assume that the gradient is computed on the final output.


This computation can also be nicely visualized with a circuit diagram:



-2-4x5-4y-43z3-4q+-121f\*

 The real-valued *""circuit""* on left shows the visual representation of the computation. The **forward pass** computes values from inputs to output (shown in green). The **backward pass** then performs backpropagation which starts at the end and recursively applies the chain rule to compute the gradients (shown in red) all the way to the inputs of the circuit. The gradients can be thought of as flowing backwards through the circuit.




### Intuitive understanding of backpropagation


Notice that backpropagation is a beautifully local process. Every gate in a
circuit diagram gets some inputs and can right away compute two things: 1. its
output value and 2. the *local* gradient of its output with respect to its
inputs. Notice that the gates can do this completely independently without being
aware of any of the details of the full circuit that they are embedded in.
However, once the forward pass is over, during backpropagation the gate will
eventually learn about the gradient of its output value on the final output of
the entire circuit. Chain rule says that the gate should take that gradient and
multiply it into every gradient it normally computes for all of its inputs.



> 
> This extra multiplication (for each input) due to the chain rule can turn a
> single and relatively useless gate into a cog in a complex circuit such as an
> entire neural network.
> 
> 
> 


Lets get an intuition for how this works by referring again to the example. The
add gate received inputs [-2, 5] and computed output 3. Since the gate is
computing the addition operation, its local gradient for both of its inputs is
+1. The rest of the circuit computed the final value, which is -12. During the
backward pass in which the chain rule is applied recursively backwards through
the circuit, the add gate (which is an input to the multiply gate) learns that
the gradient for its output was -4. If we anthropomorphize the circuit as
wanting to output a higher value (which can help with intuition), then we can
think of the circuit as “wanting” the output of the add gate to be lower (due to
negative sign), and with a *force* of 4. To continue the recurrence and to chain
the gradient, the add gate takes that gradient and multiplies it to all of the
local gradients for its inputs (making the gradient on both **x** and **y** 1 \*
-4 = -4). Notice that this has the desired effect: If **x,y** were to decrease
(responding to their negative gradient) then the add gate’s output would
decrease, which in turn makes the multiply gate’s output increase.


Backpropagation can thus be thought of as gates communicating to each other
(through the gradient signal) whether they want their outputs to increase or
decrease (and how strongly), so as to make the final output value higher.



### Modularity: Sigmoid example


The gates we introduced above are relatively arbitrary. Any kind of
differentiable function can act as a gate, and we can group multiple gates into
a single gate, or decompose a function into multiple gates whenever it is
convenient. Lets look at another expression that illustrates this point:



\[f(w,x) = \frac{1}{1+e^{-(w\_0x\_0 + w\_1x\_1 + w\_2)}}\]

as we will see later in the class, this expression describes a 2-dimensional
neuron (with inputs **x** and weights **w**) that uses the *sigmoid activation*
function. But for now lets think of this very simply as just a function from
inputs *w,x* to a single number. The function is made up of multiple gates. In
addition to the ones described already above (add, mul, max), there are four
more:



\[f(x) = \frac{1}{x} 
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = -1/x^2 
\\\\
f\_c(x) = c + x
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = 1 
\\\\
f(x) = e^x
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = e^x
\\\\
f\_a(x) = ax
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = a\]

Where the functions \(f\_c, f\_a\) translate the input by a constant of \(c\)
and scale the input by a constant of \(a\), respectively. These are technically
special cases of addition and multiplication, but we introduce them as (new)
unary gates here since we do not need the gradients for the constants \(c,a\).
The full circuit then looks as follows:



2.00-0.20w0-1.000.39x0-3.00-0.39w1-2.00-0.59x1-3.000.20w2-2.000.20\*6.000.20\*4.000.20+1.000.20+-1.00-0.20\*-10.37-0.53exp1.37-0.53+10.731.001/x

 Example circuit for a 2D neuron with a sigmoid activation function. The inputs are [x0,x1] and the (learnable) weights of the neuron are [w0,w1,w2]. As we will see later, the neuron computes a dot product with the input and then its activation is softly squashed by the sigmoid function to be in range from 0 to 1.



In the example above, we see a long chain of function applications that operates
on the result of the dot product between **w,x**. The function that these
operations implement is called the *sigmoid function* \(\sigma(x)\). It turns
out that the derivative of the sigmoid function with respect to its input
simplifies if you perform the derivation (after a fun tricky part where we add
and subtract a 1 in the numerator):



\[\sigma(x) = \frac{1}{1+e^{-x}} \\\\
\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) 
= \left( 1 - \sigma(x) \right) \sigma(x)\]

As we see, the gradient turns out to simplify and becomes surprisingly simple.
For example, the sigmoid expression receives the input 1.0 and computes the
output 0.73 during the forward pass. The derivation above shows that the *local*
gradient would simply be (1 - 0.73) \* 0.73 ~= 0.2, as the circuit computed
before (see the image above), except this way it would be done with a single,
simple and efficient expression (and with less numerical issues). Therefore, in
any real practical application it would be very useful to group these operations
into a single gate. Lets see the backprop for this neuron in code:



```python
w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]\*x[0] + w[1]\*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (backpropagation)
ddot = (1 - f) \* f # gradient on dot variable, using the sigmoid gradient derivation
dx = [w[0] \* ddot, w[1] \* ddot] # backprop into x
dw = [x[0] \* ddot, x[1] \* ddot, 1.0 \* ddot] # backprop into w
# we're done! we have the gradients on the inputs to the circuit

```

**Implementation protip: staged backpropagation**. As shown in the code above,
in practice it is always helpful to break down the forward pass into stages that
are easily backpropped through. For example here we created an intermediate
variable `dot` which holds the output of the dot product between `w` and `x`.
During backward pass we then successively compute (in reverse order) the
corresponding variables (e.g. `ddot`, and ultimately `dw, dx`) that hold the
gradients of those variables.


The point of this section is that the details of how the backpropagation is
performed, and which parts of the forward function we think of as gates, is a
matter of convenience. It helps to be aware of which parts of the expression
have easy local gradients, so that they can be chained together with the least
amount of code and effort.



### Backprop in practice: Staged computation


Lets see this with another example. Suppose that we have a function of the form:



\[f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}\]

To be clear, this function is completely useless and it’s not clear why you
would ever want to compute its gradient, except for the fact that it is a good
example of backpropagation in practice. It is very important to stress that if
you were to launch into performing the differentiation with respect to either
\(x\) or \(y\), you would end up with very large and complex expressions.
However, it turns out that doing so is completely unnecessary because we don’t
need to have an explicit function written down that evaluates the gradient. We
only have to know how to compute it. Here is how we would structure the forward
pass of such expression:



```python
x = 3 # example values
y = -4

# forward pass
sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator #(1)
num = x + sigy # numerator #(2)
sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)
xpy = x + y                                              #(4)
xpysqr = xpy\*\*2                                          #(5)
den = sigx + xpysqr # denominator #(6)
invden = 1.0 / den                                       #(7)
f = num \* invden # done! #(8)

```

Phew, by the end of the expression we have computed the forward pass. Notice
that we have structured the code in such way that it contains multiple
intermediate variables, each of which are only simple expressions for which we
already know the local gradients. Therefore, computing the backprop pass is
easy: We’ll go backwards and for every variable along the way in the forward
pass (`sigy, num, sigx, xpy, xpysqr, den, invden`) we will have the same
variable, but one that begins with a `d`, which will hold the gradient of the
output of the circuit with respect to that variable. Additionally, note that
every single piece in our backprop will involve computing the local gradient of
that expression, and chaining it with the gradient on that expression with a
multiplication. For each row, we also highlight which part of the forward pass
it refers to:



```python
# backprop f = num \* invden
dnum = invden # gradient on numerator #(8)
dinvden = num                                                     #(8)
# backprop invden = 1.0 / den 
dden = (-1.0 / (den\*\*2)) \* dinvden                                #(7)
# backprop den = sigx + xpysqr
dsigx = (1) \* dden                                                #(6)
dxpysqr = (1) \* dden                                              #(6)
# backprop xpysqr = xpy\*\*2
dxpy = (2 \* xpy) \* dxpysqr                                        #(5)
# backprop xpy = x + y
dx = (1) \* dxpy                                                   #(4)
dy = (1) \* dxpy                                                   #(4)
# backprop sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) \* sigx) \* dsigx # Notice += !! See notes below #(3)
# backprop num = x + sigy
dx += (1) \* dnum                                                  #(2)
dsigy = (1) \* dnum                                                #(2)
# backprop sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) \* sigy) \* dsigy                                 #(1)
# done! phew

```

Notice a few things:


**Cache forward pass variables**. To compute the backward pass it is very
helpful to have some of the variables that were used in the forward pass. In
practice you want to structure your code so that you cache these variables, and
so that they are available during backpropagation. If this is too difficult, it
is possible (but wasteful) to recompute them.


**Gradients add up at forks**. The forward expression involves the variables
**x,y** multiple times, so when we perform backpropagation we must be careful to
use `+=` instead of `=` to accumulate the gradient on these variables (otherwise
we would overwrite it). This follows the *multivariable chain rule* in Calculus,
which states that if a variable branches out to different parts of the circuit,
then the gradients that flow back to it will add.



### Patterns in backward flow


It is interesting to note that in many cases the backward-flowing gradient can
be interpreted on an intuitive level. For example, the three most commonly used
gates in neural networks (*add,mul,max*), all have very simple interpretations
in terms of how they act during backpropagation. Consider this example circuit:



3.00-8.00x-4.006.00y2.002.00z-1.000.00w-12.002.00\*2.002.00max-10.002.00+-20.001.00\*2

 An example circuit demonstrating the intuition behind the operations that backpropagation performs during the backward pass in order to compute the gradients on the inputs. Sum operation distributes gradients equally to all its inputs. Max operation routes the gradient to the higher input. Multiply gate takes the input activations, swaps them and multiplies by its gradient.



Looking at the diagram above as an example, we can see that:


The **add gate** always takes the gradient on its output and distributes it
equally to all of its inputs, regardless of what their values were during the
forward pass. This follows from the fact that the local gradient for the add
operation is simply +1.0, so the gradients on all inputs will exactly equal the
gradients on the output because it will be multiplied by x1.0 (and remain
unchanged). In the example circuit above, note that the + gate routed the
gradient of 2.00 to both of its inputs, equally and unchanged.


The **max gate** routes the gradient. Unlike the add gate which distributed the
gradient unchanged to all its inputs, the max gate distributes the gradient
(unchanged) to exactly one of its inputs (the input that had the highest value
during the forward pass). This is because the local gradient for a max gate is
1.0 for the highest value, and 0.0 for all other values. In the example circuit
above, the max operation routed the gradient of 2.00 to the **z** variable,
which had a higher value than **w**, and the gradient on **w** remains zero.


The **multiply gate** is a little less easy to interpret. Its local gradients
are the input values (except switched), and this is multiplied by the gradient
on its output during the chain rule. In the example above, the gradient on **x**
is -8.00, which is -4.00 x 2.00.


*Unintuitive effects and their consequences*. Notice that if one of the inputs
to the multiply gate is very small and the other is very big, then the multiply
gate will do something slightly unintuitive: it will assign a relatively huge
gradient to the small input and a tiny gradient to the large input. Note that in
linear classifiers where the weights are dot producted \(w^Tx\_i\) (multiplied)
with the inputs, this implies that the scale of the data has an effect on the
magnitude of the gradient for the weights. For example, if you multiplied all
input data examples \(x\_i\) by 1000 during preprocessing, then the gradient on
the weights will be 1000 times larger, and you’d have to lower the learning rate
by that factor to compensate. This is why preprocessing matters a lot, sometimes
in subtle ways! And having intuitive understanding for how the gradients flow
can help you debug some of these cases.



### Gradients for vectorized operations


The above sections were concerned with single variables, but all concepts extend
in a straight-forward manner to matrix and vector operations. However, one must
pay closer attention to dimensions and transpose operations.


**Matrix-Matrix multiply gradient**. Possibly the most tricky operation is the
matrix-matrix multiplication (which generalizes all matrix-vector and
vector-vector) multiply operations:



```python
# forward pass
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)

# now suppose we had the gradient on D from above in the circuit
dD = np.random.randn(\*D.shape) # same shape as D
dW = dD.dot(X.T) #.T gives the transpose of the matrix
dX = W.T.dot(dD)

```

*Tip: use dimension analysis!* Note that you do not need to remember the
expressions for `dW` and `dX` because they are easy to re-derive based on
dimensions. For instance, we know that the gradient on the weights `dW` must be
of the same size as `W` after it is computed, and that it must depend on matrix
multiplication of `X` and `dD` (as is the case when both `X,W` are single
numbers and not matrices). There is always exactly one way of achieving this so
that the dimensions work out. For example, `X` is of size [10 x 3] and `dD` of
size [5 x 3], so if we want `dW` and `W` has shape [5 x 10], then the only way
of achieving this is with `dD.dot(X.T)`, as shown above.


**Work with small, explicit examples**. Some people may find it difficult at
first to derive the gradient updates for some vectorized expressions. Our
recommendation is to explicitly write out a minimal vectorized example, derive
the gradient on paper and then generalize the pattern to its efficient,
vectorized form.


Erik Learned-Miller has also written up a longer related document on taking
matrix/vector derivatives which you might find helpful. [Find it
here](http://cs231n.stanford.edu/vecDerivs.pdf).



### Summary


* We developed intuition for what the gradients mean, how they flow backwards in the circuit, and how they communicate which part of the circuit should increase or decrease and with what force to make the final output higher.
* We discussed the importance of **staged computation** for practical implementations of backpropagation. You always want to break up your function into modules for which you can easily derive local gradients, and then chain them with chain rule. Crucially, you almost never want to write out these expressions on paper and differentiate them symbolically in full, because you never need an explicit mathematical equation for the gradient of the input variables. Hence, decompose your expressions into stages such that you can differentiate every stage independently (the stages will be matrix vector multiplies, or max operations, or sum operations, etc.) and then backprop through the variables one step at a time.


In the next section we will start to define neural networks, and backpropagation
will allow us to efficiently compute the gradient of a loss function with
respect to its parameters. In other words, we’re now ready to train neural nets,
and the most conceptually difficult part of this class is behind us! ConvNets
will then be a small step away.


### References


* [Automatic differentiation in machine learning: a survey](http://arxiv.org/abs/1502.05767)









* [cs231n](https://github.com/cs231n)
* [cs231n](https://twitter.com/cs231n)












 // Make responsive
 MathJax.Hub.Config({
 ""HTML-CSS"": { linebreaks: { automatic: true } },
 ""SVG"": { linebreaks: { automatic: true } },
 });
 


"
wEoyxE0GP2M?si=E2S7nfUQjGnQM8F-,"0,1620","- Okay, let's get started. Okay, so today we're going to
get into some of the details about how we train neural networks. So, some administrative details first. Assignment 1 is due today, Thursday, so 11:59 p.m. tonight on Canvas. We're also going to be
releasing Assignment 2 today, and then your project proposals are due Tuesday, April 25th. So you should be really starting to think about your projects now
if you haven't already. How many people have decided what they want to do for
their project so far? Okay, so some, some people, so yeah, everyone else, you
can go to TA office hours if you want suggestions and bounce ideas off of TAs. We also have a list of projects that other people have proposed. Some people usually
affiliated with Stanford, so on Piazza, so you
can take a look at those for additional ideas. And we also have some notes on backprop for a linear layer and a
vector and tensor derivatives that Justin's written up, so that should help with understanding how exactly backprop works and for vectors and matrices. So these are linked to
lecture four on the syllabus and you can go and take a look at those. Okay, so where we are now. We've talked about how
to express a function in terms of a computational graph, that we can represent any function in terms of a computational graph. And we've talked more explicitly
about neural networks, which is a type of graph where we have these linear layers that we stack on top of each other with nonlinearities in between. And we've also talked last lecture about convolutional neural networks, which are a particular type of network that uses convolutional layers to preserve the spatial structure throughout all the the
hierarchy of the network. And so we saw exactly how
a convolution layer looked, where each activation map in the convolutional layer output is produced by sliding a filter of weights over all of the spatial
locations in the input. And we also saw that usually we can have many filters per layer, each of which produces a
separate activation map. And so what we can get
is from an input right, with a certain depth, we'll
get an activation map output, which has some spatial
dimension that's preserved, as well as the depth is
the total number of filters that we have in that layer. And so what we want to do is we want to learn the values of all of these weights or parameters, and we saw that we can learn our network parameters
through optimization, which we talked about little bit earlier in the course, right? And so we want to get to a
point in the loss landscape that produces a low loss, and we can do this by taking steps in the direction of the negative gradient. And so the whole process we actually call a Mini-batch Stochastic Gradient Descent where the steps are that we continuously, we sample a batch of data. We forward prop it through our computational graph
or our neural network. We get the loss at the end. We backprop through our network to calculate the gradients. And then we update the parameters or the weights in our
network using this gradient. Okay, so now for the
next couple of lectures we're going to talk
about some of the details involved in training neural networks. And so this involves things like how do we set up our neural
network at the beginning, which activation functions that we choose, how do we preprocess the data, weight initialization,
regularization, gradient checking. We'll also talk about training dynamics. So, how do we babysit
the learning process? How do we choose how we
do parameter updates, specific perimeter update rules, and how do we do
hyperparameter optimization to choose the best hyperparameters? And then we'll also talk about evaluation and model ensembles. So today in the first part, I will talk about activation
functions, data preprocessing, weight initialization,
batch normalization, babysitting the learning process, and hyperparameter optimization. Okay, so first activation functions. So, we saw earlier how out
of any particular layer, we have the data coming in. We multiply by our weight in you know, fully connected or a convolutional layer. And then we'll pass this through an activation function or nonlinearity. And we saw some examples of this. We used sigmoid previously
in some of our examples. We also saw the ReLU nonlinearity. And so today we'll talk
more about different choices for these different nonlinearities and trade-offs between them. So first, the sigmoid,
which we've seen before, and probably the one we're
most comfortable with, right? So the sigmoid function
is as we have up here, one over one plus e to the negative x. And what this does is it takes each number that's input into the sigmoid
nonlinearity, so each element, and the elementwise squashes these into this range [0,1] right,
using this function here. And so, if you get very
high values as input, then output is going to
be something near one. If you get very low values, or, I'm sorry, very negative values, it's going to be near zero. And then we have this regime near zero that it's in a linear regime. It looks a bit like a linear function. And so this is been historically popular, because sigmoids, in a sense, you can interpret them as a kind of a saturating firing
rate of a neuron, right? So if it's something between zero and one, you could think of it as a firing rate. And we'll talk later about
other nonlinearities, like ReLUs that, in practice,
actually turned out to be more biologically plausible, but this does have a
kind of interpretation that you could make. So if we look at this
nonlinearity more carefully, there's several problems that
there actually are with this. So the first is that saturated neurons can kill off the gradient. And so what exactly does this mean? So if we look at a sigmoid gate right, a node in our computational graph, and we have our data X as input into it, and then we have the output of the sigmoid gate coming out of it, what does the gradient flow look like as we're coming back? We have dL over d sigma right? The upstream gradient coming down, and then we're going to
multiply this by dSigma over dX. This will be the gradient
of a local sigmoid function. And we're going to chain these together for our downstream
gradient that we pass back. So who can tell me what happens when X is equal to -10? It's very negative. What does is gradient look like? Zero, yeah, so that's right. So the gradient become zero and that's because in this negative, very negative region of the sigmoid, it's essentially flat,
so the gradient is zero, and we chain any upstream
gradient coming down. We multiply by basically
something near zero, and we're going to get
a very small gradient that's flowing back downwards, right? So, in a sense, after the chain rule, this kills the gradient flow
and you're going to have a zero gradient passed
down to downstream nodes. And so what happens
when X is equal to zero? So there it's, yeah,
it's fine in this regime. So, in this regime near zero, you're going to get a
reasonable gradient here, and then it'll be fine for backprop. And then what about X equals 10? Zero, right. So again, so when X is
equal to a very negative or X is equal to large positive numbers, then these are all regions where the sigmoid function is flat, and it's going to kill off the gradient and you're not going to get
a gradient flow coming back. Okay, so a second problem is that the sigmoid outputs are not zero centered. And so let's take a look
at why this is a problem. So, consider what happens when the input to a neuron is always positive. So in this case, all of our Xs
we're going to say is positive. It's going to be multiplied
by some weight, W, and then we're going to run it through our activation function. So what can we say about
the gradients on W? So think about what the local
gradient is going to be, right, for this linear layer. We have DL over whatever
the activation function, the loss coming down, and then we have our local gradient, which is going to be basically X, right? And so what does this mean,
if all of X is positive? Okay, so I heard it's
always going to be positive. So that's almost right. It's always going to be either positive, or all positive or all negative, right? So, our upstream gradient coming down is DL over our loss. L is going to be DL over DF. and this is going to be
either positive or negative. It's some arbitrary gradient coming down. And then our local gradient
that we multiply this by is, if we're going to find the gradients on W, is going to be DF over DW,
which is going to be X. And if X is always positive
then the gradients on W, which is multiplying these two together, are going to always be the sign of the upstream
gradient coming down. And so what this means is
that all the gradients of W, since they're always either
positive or negative, they're always going to
move in the same direction. You're either going to
increase all of the, when you do a parameter update, you're going to either
increase all of the values of W by a positive amount, or
differing positive amounts, or you will decrease them all. And so the problem with this is that, this gives very inefficient
gradient updates. So, if you look at on the right here, we have an example of a case where, let's say W is two-dimensional, so we have our two axes for W, and if we say that we can only have all positive or all negative updates, then we have these two quadrants, and, are the two places where the axis are either all positive or negative, and these are the only directions in which we're allowed to make a gradient update. And so in the case where, let's say our hypothetical optimal W is actually this blue vector here, right, and we're starting off
at you know some point, or at the top of the the the
beginning of the red arrows, we can't just directly take a gradient update in this direction, because this is not in one of those two allowed gradient directions. And so what we're going to have to do, is we'll have to take a
sequence of gradient updates. For example, in these red arrow directions that are each in allowed directions, in order to finally get to this optimal W. And so this is why also, in general, we want a zero mean data. So, we want our input X to be zero meaned, so that we actually have
positive and negative values and we don't get into this problem of the gradient updates. They'll be all moving
in the same direction. So is this clear? Any questions on this point? Okay. Okay, so we've talked about these two main problems of the sigmoid. The saturated neurons
can kill the gradients if we're too positive or
too negative of an input. They're also not zero-centered and so we get these, this inefficient kind of gradient update. And then a third problem, we have an exponential function in here, so this is a little bit
computationally expensive. In the grand scheme of your network, this is usually not the main problem, because we have all these convolutions and dot products that
are a lot more expensive, but this is just a minor
point also to observe. So now we can look at a second activation function here at tanh. And so this looks very
similar to the sigmoid, but the difference is that now it's squashing to the range [-1, 1]. So here, the main difference is that it's now zero-centered, so we've gotten rid of the
second problem that we had. It still kills the gradients,
however, when it's saturated. So, you still have these regimes where the gradient is essentially flat and you're going to
kill the gradient flow. So this is a bit better than the sigmoid, but it still has some problems. Okay, so now let's look at
the ReLU activation function. And this is one that we saw
in our examples last lecture when we were talking about the convolutional neural network. And we saw that we interspersed
ReLU nonlinearities between many of the convolutional layers. And so, this function is f of
x equals max of zero and x. So it takes an elementwise
operation on your input and basically if your input is negative, it's going to put it to zero. And then if it's positive, it's going to be just passed through. It's the identity. And so this is one that's
pretty commonly used, and if we look at this one and look at and think about the problems that we saw earlier with
the sigmoid and the tanh, we can see that it doesn't saturate in the positive region. So there's whole half of our input space where it's not going to saturate, so this is a big advantage. So this is also
computationally very efficient. We saw earlier that the sigmoid has this E exponential in it. And so the ReLU is just this simple max and there's, it's extremely fast. And in practice, using this ReLU, it converges much faster than
the sigmoid and the tanh, so about six times faster. And it's also turned out to be more biologically plausible than the sigmoid. So if you look at a neuron and you look at what the inputs look like, and you look at what
the outputs look like, and you try to measure this
in neuroscience experiments, you'll see that this one is actually a closer approximation to what's happening than sigmoids. And so ReLUs were starting to be used a lot around 2012 when we had AlexNet, the first major
convolutional neural network that was able to do well on ImageNet and large-scale data. They used the ReLU in their experiments. So a problem however, with the ReLU, is that it's still, it's not
not zero-centered anymore. So we saw that the sigmoid
was not zero-centered. Tanh fixed this and now
ReLU has this problem again. And so that's one of
the issues of the ReLU. And then we also have
this further annoyance of, again we saw that in the
positive half of the inputs, we don't have saturation, but this is not the case
of the negative half. Right, so just thinking about this a little bit more precisely. So what's happening here
when X equals negative 10? So zero gradient, that's right. What happens when X is
equal to positive 10? It's good, right. So, we're in the linear regime. And then what happens
when X is equal to zero? Yes, it undefined here, but in practice, we'll
say, you know, zero, right. And so basically, it's
killing the gradient in half of the regime. And so we can get this phenomenon of basically dead ReLUs, when we're in this bad part of the regime. And so there's, you can look at this in, as coming from several potential reasons. And so if we look at our data cloud here, this is all of our training data, then if we look at where
the ReLUs can fall, so the ReLUs can be,
each of these is basically the half of the plane where
it's going to activate. And so each of these is the plane that defines each of these ReLUs, and we can see that you
can have these dead ReLUs that are basically off of the data cloud. And in this case, it will never
activate and never update, as compared to an active ReLU where some of the data is going to be positive and passed through and some won't be. And so there's several reasons for this. The first is that it can happen when you have bad initialization. So if you have weights
that happen to be unlucky and they happen to be off the data cloud, so they happen to specify
this bad ReLU over here. Then they're never going to get a data input that causes it to activate, and so they're never going to get good gradient flow coming back. And so it'll just never
update and never activate. What's the more common case is when your learning rate is too high. And so this case you started
off with an okay ReLU, but because you're making
these huge updates, the weights jump around and then your ReLU unit in a sense, gets knocked off of the data manifold. And so this happens through training. So it was fine at the beginning and then at some point,
it became bad and it died. And so if in practice, if you freeze a network
that you've trained and you pass the data through, you can see it actually
is much as 10 to 20% of the network is these dead ReLUs. And so you know that's a problem, but also most networks do have this type of problem when you use ReLUs. Some of them will be dead, and in practice, people look into this, and it's a research problem, but it's still doing okay
for training networks. Yeah, is there a question? [student speaking off mic] Right. So the question is, yeah, so the data cloud is
just your training data. [student speaking off mic] Okay, so the question is when, how do you tell when the ReLU
is going to be dead or not, with respect to the data cloud? And so if you look at, this is an example of like a
simple two-dimensional case. And so our ReLU, we're going
to get our input to the ReLU, which is going to be a basically you know, W1 X1 plus W2 X2, and it we apply this, so that that defines this this
separating hyperplane here, and then we're going to take half of it that's going to be positive, and half of it's going to be killed off, and so yes, so you, you know you just, it's whatever the weights happened to be, and where the data happens
to be is where these, where these hyperplanes fall, and so, so yeah so just throughout
the course of training, some of your ReLUs will
be in different places, with respect to the data cloud. Oh, question. [student speaking off mic] Yeah. So okay, so the question is for the sigmoid we talked
about two drawbacks, and one of them was that the
neurons can get saturated, so let's go back to the sigmoid here, and the question was this is not the case, when all of your inputs are positive. So when all of your inputs are positive, they're all going to be coming in in this zero plus region here, and so you can still
get a saturating neuron, because you see up in
this positive region, it also plateaus at one, and so when it's when you
have large positive values as input you're also going
to get the zero gradient, because you have you
have a flat slope here. Okay. Okay, so in practice people
also like to initialize ReLUs with slightly positive biases, in order to increase the
likelihood of it being active at initialization
and to get some updates. Right and so this basically
just biases towards more ReLUs firing at the beginning, and in practice some say that it helps. Some say that it doesn't. Generally people don't always use this. It's yeah, a lot of times
people just initialize it with zero biases still. Okay, so now we can look
at some modifications on the ReLU that have come out since then, and so one example is this leaky ReLU. And so this looks very
similar to the original ReLU, and the only difference is
that now instead of being flat in the negative regime, we're going to give a
slight negative slope here And so this solves a lot of the problems that we mentioned earlier. Right here we don't have
any saturating regime, even in the negative space. It's still very computationally efficient. It still converges faster
than sigmoid and tanh, very similar to a ReLU. And it doesn't have this dying problem. And there's also another example is the parametric rectifier, so PReLU. And so in this case it's
just like a leaky ReLU where we again have this sloped region in the negative space, but now this slope in the negative regime is determined through
this alpha parameter, so we don't specify,
we don't hard-code it. but we treat it as now a parameter that we can backprop into and learn. And so this gives it a
little bit more flexibility. And we also have something called an Exponential Linear Unit, an ELU, so we have all these
different LUs, basically. and this one again, you know, it has all the benefits of the ReLu, but now you're, it is also
closer to zero mean outputs. So, that's actually an
advantage that the leaky ReLU, parametric ReLU, a lot
of these they allow you to have your mean closer to zero, but compared with the leaky ReLU, instead of it being sloped
in the negative regime, here you actually are building back in a negative saturation regime, and there's arguments that
basically this allows you to have some more robustness to noise, and you basically get
these deactivation states that can be more robust. And you can look at this paper for, there's a lot of kind
of more justification for why this is the case. And in a sense this is kind of something in between the ReLUs and the leaky ReLUs, where has some of this shape, which the Leaky ReLU does, which gives it closer to zero mean output, but then it also still has some of this more saturating behavior that ReLUs have. A question? [student speaking off mic] So, whether this parameter alpha is going to be specific for each neuron. So, I believe it is often specified, but I actually can't remember exactly, so you can look in the paper for exactly, yeah, how this is defined, but yeah, so I believe
this function is basically very carefully designed in order to have nice desirable properties. Okay, so there's basically all of these kinds of variants on the ReLU. And so you can see that,
all of these it's kind of, you can argue that each one
may have certain benefits, certain drawbacks in practice. People just want to run
experiments all of them, and see empirically what works better, try and justify it, and
come up with new ones, but they're all different things that are being experimented with. And so let's just mention one more. This is Maxout Neuron. So, this one looks a little bit different in that it doesn't have the
same form as the others did of taking your basic dot product, and then putting this element-wise nonlinearity in front of it. Instead, it looks like this, this max of W dot product of X plus B, and a second set of weights, W2 dot product with X plus B2. And so what does this,
is this is taking the max of these two functions in a sense. And so what it does is
it generalizes the ReLU and the leaky ReLu, because you're just you're
taking the max over these two, two linear functions. And so what this give us, it's again you're operating
in a linear regime. It doesn't saturate and it doesn't die. The problem is that here, you are doubling the number
of parameters per neuron. So, each neuron now has this
original set of weights, W, but it now has W1 and W2,
so you have twice these. So in practice, when we look at all of
these activation functions, kind of a good general
rule of thumb is use ReLU. This is the most standard one that generally just works well. And you know you do want
to be careful in general with your learning rates
to adjust them based, see how things do. We'll talk more about
adjusting learning rates later in this lecture, but you can also try out some of these fancier activation functions, the leaky ReLU, Maxout, ELU, but these are generally, they're still kind of more experimental. So, you can see how they
work for your problem. You can also try out tanh,",https://cs231n.github.io/neural-networks-1/,"




CS231n Convolutional Neural Networks for Visual Recognition









 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1\*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-46895817-2', 'auto');
 ga('send', 'pageview');
 



addBackToTop({
 backgroundColor: '#fff',
 innerHTML: 'Back to Top',
 textColor: '#333'
 })

 #back-to-top {
 border: 1px solid #ccc;
 border-radius: 0;
 font-family: sans-serif;
 font-size: 14px;
 width: 100px;
 text-align: center;
 line-height: 30px;
 height: 30px;
 }
 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io)
[Course Website](http://cs231n.stanford.edu/)





# 




Table of Contents:


* [Quick intro without brain analogies](#quick)
* [Modeling one neuron](#intro)
	+ [Biological motivation and connections](#bio)
	+ [Single neuron as a linear classifier](#classifier)
	+ [Commonly used activation functions](#actfun)
* [Neural Network architectures](#nn)
	+ [Layer-wise organization](#layers)
	+ [Example feed-forward computation](#feedforward)
	+ [Representational power](#power)
	+ [Setting number of layers and their sizes](#arch)
* [Summary](#summary)
* [Additional references](#add)



## Quick intro


It is possible to introduce neural networks without appealing to brain
analogies. In the section on linear classification we computed scores for
different visual categories given the image using the formula \( s = W x \),
where \(W\) was a matrix and \(x\) was an input column vector containing all
pixel data of the image. In the case of CIFAR-10, \(x\) is a [3072x1] column
vector, and \(W\) is a [10x3072] matrix, so that the output scores is a vector
of 10 class scores.


An example neural network would instead compute \( s = W\_2 \max(0, W\_1 x) \).
Here, \(W\_1\) could be, for example, a [100x3072] matrix transforming the image
into a 100-dimensional intermediate vector. The function \(max(0,-) \) is a
non-linearity that is applied elementwise. There are several choices we could
make for the non-linearity (which we’ll study below), but this one is a common
choice and simply thresholds all activations that are below zero to zero.
Finally, the matrix \(W\_2\) would then be of size [10x100], so that we again
get 10 numbers out that we interpret as the class scores. Notice that the
non-linearity is critical computationally - if we left it out, the two matrices
could be collapsed to a single matrix, and therefore the predicted class scores
would again be a linear function of the input. The non-linearity is where we get
the *wiggle*. The parameters \(W\_2, W\_1\) are learned with stochastic gradient
descent, and their gradients are derived with chain rule (and computed with
backpropagation).


A three-layer neural network could analogously look like \( s = W\_3 \max(0,
W\_2 \max(0, W\_1 x)) \), where all of \(W\_3, W\_2, W\_1\) are parameters to be
learned. The sizes of the intermediate hidden vectors are hyperparameters of the
network and we’ll see how we can set them later. Lets now look into how we can
interpret these computations from the neuron/network perspective.



## Modeling one neuron


The area of Neural Networks has originally been primarily inspired by the goal
of modeling biological neural systems, but has since diverged and become a
matter of engineering and achieving good results in Machine Learning tasks.
Nonetheless, we begin our discussion with a very brief and high-level
description of the biological system that a large portion of this area has been
inspired by.



### Biological motivation and connections


The basic computational unit of the brain is a **neuron**. Approximately 86
billion neurons can be found in the human nervous system and they are connected
with approximately 10^14 - 10^15 **synapses**. The diagram below shows a cartoon
drawing of a biological neuron (left) and a common mathematical model (right).
Each neuron receives input signals from its **dendrites** and produces output
signals along its (single) **axon**. The axon eventually branches out and
connects via synapses to dendrites of other neurons. In the computational model
of a neuron, the signals that travel along the axons (e.g. \(x\_0\)) interact
multiplicatively (e.g. \(w\_0 x\_0\)) with the dendrites of the other neuron
based on the synaptic strength at that synapse (e.g. \(w\_0\)). The idea is that
the synaptic strengths (the weights \(w\)) are learnable and control the
strength of influence (and its direction: excitory (positive weight) or
inhibitory (negative weight)) of one neuron on another. In the basic model, the
dendrites carry the signal to the cell body where they all get summed. If the
final sum is above a certain threshold, the neuron can *fire*, sending a spike
along its axon. In the computational model, we assume that the precise timings
of the spikes do not matter, and that only the frequency of the firing
communicates information. Based on this *rate code* interpretation, we model the
*firing rate* of the neuron with an **activation function** \(f\), which
represents the frequency of the spikes along the axon. Historically, a common
choice of activation function is the **sigmoid function** \(\sigma\), since it
takes a real-valued input (the signal strength after the sum) and squashes it to
range between 0 and 1. We will see details of these activation functions later
in this section.



![](/assets/nn1/neuron.png)
![](/assets/nn1/neuron_model.jpeg)
A cartoon drawing of a biological neuron (left) and its mathematical model (right).

An example code for forward-propagating a single neuron might look as follows:



```python
class Neuron(object):
  # ... 
  def forward(self, inputs):
    """""" assume inputs and weights are 1-D numpy arrays and bias is a number """"""
    cell\_body\_sum = np.sum(inputs \* self.weights) + self.bias
    firing\_rate = 1.0 / (1.0 + math.exp(-cell\_body\_sum)) # sigmoid activation function
    return firing\_rate

```

In other words, each neuron performs a dot product with the input and its
weights, adds the bias and applies the non-linearity (or activation function),
in this case the sigmoid \(\sigma(x) = 1/(1+e^{-x})\). We will go into more
details about different activation functions at the end of this section.


**Coarse model.** It’s important to stress that this model of a biological
neuron is very coarse: For example, there are many different types of neurons,
each with different properties. The dendrites in biological neurons perform
complex nonlinear computations. The synapses are not just a single weight,
they’re a complex non-linear dynamical system. The exact timing of the output
spikes in many systems is known to be important, suggesting that the rate code
approximation may not hold. Due to all these and many other simplifications, be
prepared to hear groaning sounds from anyone with some neuroscience background
if you draw analogies between Neural Networks and real brains. See this
[review](https://physics.ucsd.edu/neurophysics/courses/physics_171/annurev.neuro.28.061604.135703.pdf)
(pdf), or more recently this
[review](http://www.sciencedirect.com/science/article/pii/S0959438814000130) if
you are interested.



### Single neuron as a linear classifier


The mathematical form of the model Neuron’s forward computation might look
familiar to you. As we saw with linear classifiers, a neuron has the capacity to
“like” (activation near one) or “dislike” (activation near zero) certain linear
regions of its input space. Hence, with an appropriate loss function on the
neuron’s output, we can turn a single neuron into a linear classifier:


**Binary Softmax classifier**. For example, we can interpret
\(\sigma(\sum\_iw\_ix\_i + b)\) to be the probability of one of the classes
\(P(y\_i = 1 \mid x\_i; w) \). The probability of the other class would be
\(P(y\_i = 0 \mid x\_i; w) = 1 - P(y\_i = 1 \mid x\_i; w) \), since they must
sum to one. With this interpretation, we can formulate the cross-entropy loss as
we have seen in the Linear Classification section, and optimizing it would lead
to a binary Softmax classifier (also known as *logistic regression*). Since the
sigmoid function is restricted to be between 0-1, the predictions of this
classifier are based on whether the output of the neuron is greater than 0.5.


**Binary SVM classifier**. Alternatively, we could attach a max-margin hinge
loss to the output of the neuron and train it to become a binary Support Vector
Machine.


**Regularization interpretation**. The regularization loss in both SVM/Softmax
cases could in this biological view be interpreted as *gradual forgetting*,
since it would have the effect of driving all synaptic weights \(w\) towards
zero after every parameter update.



> 
> A single neuron can be used to implement a binary classifier (e.g. binary
> Softmax or binary SVM classifiers)
> 
> 
> 



### Commonly used activation functions


Every activation function (or *non-linearity*) takes a single number and
performs a certain fixed mathematical operation on it. There are several
activation functions you may encounter in practice:



![](/assets/nn1/sigmoid.jpeg)
![](/assets/nn1/tanh.jpeg)
**Left:** Sigmoid non-linearity squashes real numbers to range between [0,1] **Right:** The tanh non-linearity squashes real numbers to range between [-1,1].

**Sigmoid.** The sigmoid non-linearity has the mathematical form \(\sigma(x) = 1
/ (1 + e^{-x})\) and is shown in the image above on the left. As alluded to in
the previous section, it takes a real-valued number and “squashes” it into range
between 0 and 1. In particular, large negative numbers become 0 and large
positive numbers become 1. The sigmoid function has seen frequent use
historically since it has a nice interpretation as the firing rate of a neuron:
from not firing at all (0) to fully-saturated firing at an assumed maximum
frequency (1). In practice, the sigmoid non-linearity has recently fallen out of
favor and it is rarely ever used. It has two major drawbacks:


* *Sigmoids saturate and kill gradients*. A very undesirable property of the sigmoid neuron is that when the neuron’s activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.
* *Sigmoid outputs are not zero-centered*. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. \(x > 0\) elementwise in \(f = w^Tx + b\))), then the gradient on the weights \(w\) will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression \(f\)). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.


**Tanh.** The tanh non-linearity is shown on the image above on the right. It
squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its
activations saturate, but unlike the sigmoid neuron its output is zero-centered.
Therefore, in practice the *tanh non-linearity is always preferred to the
sigmoid nonlinearity.* Also note that the tanh neuron is simply a scaled sigmoid
neuron, in particular the following holds: \( \tanh(x) = 2 \sigma(2x) -1 \).



![](/assets/nn1/relu.jpeg)
![](/assets/nn1/alexplot.jpeg)
**Left:** Rectified Linear Unit (ReLU) activation function, which is zero when x < 0 and then linear with slope 1 when x > 0. **Right:** A plot from [Krizhevsky et al.](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) (pdf) paper indicating the 6x improvement in convergence with the ReLU unit compared to the tanh unit.

**ReLU.** The Rectified Linear Unit has become very popular in the last few
years. It computes the function \(f(x) = \max(0, x)\). In other words, the
activation is simply thresholded at zero (see image above on the left). There
are several pros and cons to using the ReLUs:


* (+) It was found to greatly accelerate (e.g. a factor of 6 in [Krizhevsky et al.](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.
* (+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.
* (-) Unfortunately, ReLU units can be fragile during training and can “die”. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.


**Leaky ReLU.** Leaky ReLUs are one attempt to fix the “dying ReLU” problem.
Instead of the function being zero when x < 0, a leaky ReLU will instead have a
small positive slope (of 0.01, or so). That is, the function computes \(f(x) =
\mathbb{1}(x < 0) (\alpha x) + \mathbb{1}(x>=0) (x) \) where \(\alpha\) is a
small constant. Some people report success with this form of activation
function, but the results are not always consistent. The slope in the negative
region can also be made into a parameter of each neuron, as seen in PReLU
neurons, introduced in [Delving Deep into
Rectifiers](http://arxiv.org/abs/1502.01852), by Kaiming He et al., 2015.
However, the consistency of the benefit across tasks is presently unclear.


**Maxout**. Other types of units have been proposed that do not have the
functional form \(f(w^Tx + b)\) where a non-linearity is applied on the dot
product between the weights and the data. One relatively popular choice is the
Maxout neuron (introduced recently by [Goodfellow et
al.](https://arxiv.org/abs/1302.4389)) that generalizes the ReLU and its leaky
version. The Maxout neuron computes the function \(\max(w\_1^Tx+b\_1, w\_2^Tx +
b\_2)\). Notice that both ReLU and Leaky ReLU are a special case of this form
(for example, for ReLU we have \(w\_1, b\_1 = 0\)). The Maxout neuron therefore
enjoys all the benefits of a ReLU unit (linear regime of operation, no
saturation) and does not have its drawbacks (dying ReLU). However, unlike the
ReLU neurons it doubles the number of parameters for every single neuron,
leading to a high total number of parameters.


This concludes our discussion of the most common types of neurons and their
activation functions. As a last comment, it is very rare to mix and match
different types of neurons in the same network, even though there is no
fundamental problem with doing so.


**TLDR**: “*What neuron type should I use?*” Use the ReLU non-linearity, be
careful with your learning rates and possibly monitor the fraction of “dead”
units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never
use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.



## Neural Network architectures



### Layer-wise organization


**Neural Networks as neurons in graphs**. Neural Networks are modeled as
collections of neurons that are connected in an acyclic graph. In other words,
the outputs of some neurons can become inputs to other neurons. Cycles are not
allowed since that would imply an infinite loop in the forward pass of a
network. Instead of an amorphous blobs of connected neurons, Neural Network
models are often organized into distinct layers of neurons. For regular neural
networks, the most common layer type is the **fully-connected layer** in which
neurons between two adjacent layers are fully pairwise connected, but neurons
within a single layer share no connections. Below are two example Neural Network
topologies that use a stack of fully-connected layers:



![](/assets/nn1/neural_net.jpeg)
![](/assets/nn1/neural_net2.jpeg)
**Left:** A 2-layer Neural Network (one hidden layer of 4 neurons (or units) and one output layer with 2 neurons), and three inputs. **Right:** A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Notice that in both cases there are connections (synapses) between neurons across layers, but not within a layer.

**Naming conventions.** Notice that when we say N-layer neural network, we do
not count the input layer. Therefore, a single-layer neural network describes a
network with no hidden layers (input directly mapped to output). In that sense,
you can sometimes hear people say that logistic regression or SVMs are simply a
special case of single-layer Neural Networks. You may also hear these networks
interchangeably referred to as *“Artificial Neural Networks”* (ANN) or
*“Multi-Layer Perceptrons”* (MLP). Many people do not like the analogies between
Neural Networks and real brains and prefer to refer to neurons as *units*.


**Output layer.** Unlike all layers in a Neural Network, the output layer
neurons most commonly do not have an activation function (or you can think of
them as having a linear identity activation function). This is because the last
output layer is usually taken to represent the class scores (e.g. in
classification), which are arbitrary real-valued numbers, or some kind of
real-valued target (e.g. in regression).


**Sizing neural networks**. The two metrics that people commonly use to measure
the size of neural networks are the number of neurons, or more commonly the
number of parameters. Working with the two example networks in the above
picture:


* The first network (left) has 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters.
* The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters.


To give you some context, modern Convolutional Networks contain on orders of 100
million parameters and are usually made up of approximately 10-20 layers (hence
*deep learning*). However, as we will see the number of *effective* connections
is significantly greater due to parameter sharing. More on this in the
Convolutional Neural Networks module.



### Example feed-forward computation


*Repeated matrix multiplications interwoven with activation function*. One of
the primary reasons that Neural Networks are organized into layers is that this
structure makes it very simple and efficient to evaluate Neural Networks using
matrix vector operations. Working with the example three-layer neural network in
the diagram above, the input would be a [3x1] vector. All connection strengths
for a layer can be stored in a single matrix. For example, the first hidden
layer’s weights `W1` would be of size [4x3], and the biases for all units would
be in the vector `b1`, of size [4x1]. Here, every single neuron has its weights
in a row of `W1`, so the matrix vector multiplication `np.dot(W1,x)` evaluates
the activations of all neurons in that layer. Similarly, `W2` would be a [4x4]
matrix that stores the connections of the second hidden layer, and `W3` a [1x4]
matrix for the last (output) layer. The full forward pass of this 3-layer neural
network is then simply three matrix multiplications, interwoven with the
application of the activation function:



```python
# forward-pass of a 3-layer neural network:
f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
out = np.dot(W3, h2) + b3 # output neuron (1x1)

```

In the above code, `W1,W2,W3,b1,b2,b3` are the learnable parameters of the
network. Notice also that instead of having a single input column vector, the
variable `x` could hold an entire batch of training data (where each input
example would be a column of `x`) and then all examples would be efficiently
evaluated in parallel. Notice that the final Neural Network layer usually
doesn’t have an activation function (e.g. it represents a (real-valued) class
score in a classification setting).



> 
> The forward pass of a fully-connected layer corresponds to one matrix
> multiplication followed by a bias offset and an activation function.
> 
> 
> 



### Representational power


One way to look at Neural Networks with fully-connected layers is that they
define a family of functions that are parameterized by the weights of the
network. A natural question that arises is: What is the representational power
of this family of functions? In particular, are there functions that cannot be
modeled with a Neural Network?


It turns out that Neural Networks with at least one hidden layer are *universal
approximators*. That is, it can be shown (e.g. see [*Approximation by
Superpositions of Sigmoidal
Function*](http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf) from 1989 (pdf), or
this [intuitive
explanation](http://neuralnetworksanddeeplearning.com/chap4.html) from Michael
Nielsen) that given any continuous function \(f(x)\) and some \(\epsilon > 0\),
there exists a Neural Network \(g(x)\) with one hidden layer (with a reasonable
choice of non-linearity, e.g. sigmoid) such that \( \forall x, \mid f(x) - g(x)
\mid < \epsilon \). In other words, the neural network can approximate any
continuous function.


If one hidden layer suffices to approximate any function, why use more layers
and go deeper? The answer is that the fact that a two-layer Neural Network is a
universal approximator is, while mathematically cute, a relatively weak and
useless statement in practice. In one dimension, the “sum of indicator bumps”
function \(g(x) = \sum\_i c\_i \mathbb{1}(a\_i < x < b\_i)\) where \(a,b,c\) are
parameter vectors is also a universal approximator, but noone would suggest that
we use this functional form in Machine Learning. Neural Networks work well in
practice because they compactly express nice, smooth functions that fit well
with the statistical properties of data we encounter in practice, and are also
easy to learn using our optimization algorithms (e.g. gradient descent).
Similarly, the fact that deeper networks (with multiple hidden layers) can work
better than a single-hidden-layer networks is an empirical observation, despite
the fact that their representational power is equal.


As an aside, in practice it is often the case that 3-layer neural networks will
outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much
more. This is in stark contrast to Convolutional Networks, where depth has been
found to be an extremely important component for a good recognition system (e.g.
on order of 10 learnable layers). One argument for this observation is that
images contain hierarchical structure (e.g. faces are made up of eyes, which are
made up of edges, etc.), so several layers of processing make intuitive sense
for this data domain.


The full story is, of course, much more involved and a topic of much recent
research. If you are interested in these topics we recommend for further
reading:


* [Deep Learning](http://www.deeplearningbook.org/) book in press by Bengio, Goodfellow, Courville, in particular [Chapter 6.4](http://www.deeplearningbook.org/contents/mlp.html).
* [Do Deep Nets Really Need to be Deep?](http://arxiv.org/abs/1312.6184)
* [FitNets: Hints for Thin Deep Nets](http://arxiv.org/abs/1412.6550)



### Setting number of layers and their sizes


How do we decide on what architecture to use when faced with a practical
problem? Should we use no hidden layers? One hidden layer? Two hidden layers?
How large should each layer be? First, note that as we increase the size and
number of layers in a Neural Network, the **capacity** of the network increases.
That is, the space of representable functions grows since the neurons can
collaborate to express many different functions. For example, suppose we had a
binary classification problem in two dimensions. We could train three separate
neural networks, each with one hidden layer of some size and obtain the
following classifiers:



![](/assets/nn1/layer_sizes.jpeg)
Larger Neural Networks can represent more complicated functions. The data are shown as circles colored by their class, and the decision regions by a trained neural network are shown underneath. You can play with these examples in this [ConvNetsJS demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html).

In the diagram above, we can see that Neural Networks with more neurons can
express more complicated functions. However, this is both a blessing (since we
can learn to classify more complicated data) and a curse (since it is easier to
overfit the training data). **Overfitting** occurs when a model with high
capacity fits the noise in the data instead of the (assumed) underlying
relationship. For example, the model with 20 hidden neurons fits all the
training data but at the cost of segmenting the space into many disjoint red and
green decision regions. The model with 3 hidden neurons only has the
representational power to classify the data in broad strokes. It models the data
as two blobs and interprets the few red points inside the green cluster as
**outliers** (noise). In practice, this could lead to better **generalization**
on the test set.


Based on our discussion above, it seems that smaller neural networks can be
preferred if the data is not complex enough to prevent overfitting. However,
this is incorrect - there are many other preferred ways to prevent overfitting
in Neural Networks that we will discuss later (such as L2 regularization,
dropout, input noise). In practice, it is always better to use these methods to
control overfitting instead of the number of neurons.


The subtle reason behind this is that smaller networks are harder to train with
local methods such as Gradient Descent: It’s clear that their loss functions
have relatively few local minima, but it turns out that many of these minima are
easier to converge to, and that they are bad (i.e. with high loss). Conversely,
bigger neural networks contain significantly more local minima, but these minima
turn out to be much better in terms of their actual loss. Since Neural Networks
are non-convex, it is hard to study these properties mathematically, but some
attempts to understand these objective functions have been made, e.g. in a
recent paper [The Loss Surfaces of Multilayer
Networks](http://arxiv.org/abs/1412.0233). In practice, what you find is that if
you train a small network the final loss can display a good amount of variance -
in some cases you get lucky and converge to a good place but in some cases you
get trapped in one of the bad minima. On the other hand, if you train a large
network you’ll start to find many different solutions, but the variance in the
final achieved loss will be much smaller. In other words, all solutions are
about equally as good, and rely less on the luck of random initialization.


To reiterate, the regularization strength is the preferred way to control the
overfitting of a neural network. We can look at the results achieved by three
different settings:



![](/assets/nn1/reg_strengths.jpeg)

 The effects of regularization strength: Each neural network above has 20 hidden neurons, but changing the regularization strength makes its final decision regions smoother with a higher regularization. You can play with these examples in this [ConvNetsJS demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html).
 

The takeaway is that you should not be using smaller networks because you are
afraid of overfitting. Instead, you should use as big of a neural network as
your computational budget allows, and use other regularization techniques to
control overfitting.



## Summary


In summary,


* We introduced a very coarse model of a biological **neuron**.
* We discussed several types of **activation functions** that are used in practice, with ReLU being the most common choice.
* We introduced **Neural Networks** where neurons are connected with **Fully-Connected layers** where neurons in adjacent layers have full pair-wise connections, but neurons within a layer are not connected.
* We saw that this layered architecture enables very efficient evaluation of Neural Networks based on matrix multiplications interwoven with the application of the activation function.
* We saw that that Neural Networks are **universal function approximators**, but we also discussed the fact that this property has little to do with their ubiquitous use. They are used because they make certain “right” assumptions about the functional forms of functions that come up in practice.
* We discussed the fact that larger networks will always work better than smaller networks, but their higher model capacity must be appropriately addressed with stronger regularization (such as higher weight decay), or they might overfit. We will see more forms of regularization (especially dropout) in later sections.



## Additional References


* [deeplearning.net tutorial](http://www.deeplearning.net/tutorial/mlp.html) with Theano
* [ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/) demos for intuitions
* [Michael Nielsen’s](http://neuralnetworksanddeeplearning.com/chap1.html) tutorials









* [cs231n](https://github.com/cs231n)
* [cs231n](https://twitter.com/cs231n)












 // Make responsive
 MathJax.Hub.Config({
 ""HTML-CSS"": { linebreaks: { automatic: true } },
 ""SVG"": { linebreaks: { automatic: true } },
 });
 


"
bNb2fEVKeEo?si=KTby9-qALxqN-PyB,,"- Okay, let's get started. Alright, so welcome to lecture five. Today we're going to be getting
to the title of the class, Convolutional Neural Networks. Okay, so a couple of
administrative details before we get started. Assignment one is due Thursday, April 20, 11:59 p.m. on Canvas. We're also going to be releasing
assignment two on Thursday. Okay, so a quick review of last time. We talked about neural
networks, and how we had the running example of
the linear score function that we talked about through
the first few lectures. And then we turned this
into a neural network by stacking these linear
layers on top of each other with non-linearities in between. And we also saw that
this could help address the mode problem where
we are able to learn intermediate templates
that are looking for, for example, different
types of cars, right. A red car versus a yellow car and so on. And to combine these
together to come up with the final score function for a class. Okay, so today we're going to talk about convolutional neural networks, which is basically the same sort of idea, but now we're going to
learn convolutional layers that reason on top of basically explicitly trying to maintain spatial structure. So, let's first talk a little bit about the history of neural
networks, and then also how convolutional neural
networks were developed. So we can go all the way back
to 1957 with Frank Rosenblatt, who developed the Mark
I Perceptron machine, which was the first
implementation of an algorithm called the perceptron, which
had sort of the similar idea of getting score functions,
right, using some, you know, W times X plus a bias. But here the outputs are going
to be either one or a zero. And then in this case
we have an update rule, so an update rule for our weights, W, which also look kind of similar
to the type of update rule that we're also seeing in
backprop, but in this case there was no principled
backpropagation technique yet, we just sort of took the
weights and adjusted them in the direction towards
the target that we wanted. So in 1960, we had Widrow and Hoff, who developed Adaline and
Madaline, which was the first time that we were able to
get, to start to stack these linear layers into
multilayer perceptron networks. And so this is starting to now
look kind of like this idea of neural network layers, but
we still didn't have backprop or any sort of principled
way to train this. And so the first time
backprop was really introduced was in 1986 with Rumelhart. And so here we can start
seeing, you know, these kinds of equations with the chain
rule and the update rules that we're starting to
get familiar with, right, and so this is the first time we started to have a principled way to train these kinds of network architectures. And so after that, you know,
it still wasn't able to scale to very large neural networks,
and so there was sort of a period in which there wasn't a whole lot of new things happening
here, or a lot of popular use of these kinds of networks. And so this really started
being reinvigorated around the 2000s, so in
2006, there was this paper by Geoff Hinton and Ruslan Salakhutdinov, which basically showed that we could train a deep neural network, and show that we could
do this effectively. But it was still not quite the sort of modern iteration
of neural networks. It required really careful initialization in order to be able to do backprop, and so what they had
here was they would have this first pre-training
stage, where you model each hidden layer through this kind of, through a restricted Boltzmann machine, and so you're going to get
some initialized weights by training each of
these layers iteratively. And so once you get all
of these hidden layers you then use that to
initialize your, you know, your full neural network,
and then from there you do backprop and fine tuning of that. And so when we really started
to get the first really strong results using neural networks,
and what sort of really sparked the whole craze
of starting to use these kinds of networks really
widely was at around 2012, where we had first the strongest results using for speech recognition, and so this is work out
of Geoff Hinton's lab for acoustic modeling
and speech recognition. And then for image recognition,
2012 was the landmark paper from Alex Krizhevsky
in Geoff Hinton's lab, which introduced the first
convolutional neural network architecture that was able to do, get really strong results
on ImageNet classification. And so it took the ImageNet,
image classification benchmark, and was able to dramatically reduce the error on that benchmark. And so since then, you
know, ConvNets have gotten really widely used in all
kinds of applications. So now let's step back and
take a look at what gave rise to convolutional neural
networks specifically. And so we can go back to the 1950s, where Hubel and Wiesel did
a series of experiments trying to understand how neurons in the visual cortex worked, and they studied this
specifically for cats. And so we talked a little bit
about this in lecture one, but basically in these
experiments they put electrodes in the cat, into the cat brain, and they gave the cat
different visual stimulus. Right, and so, things like, you know, different kinds of edges, oriented edges, different sorts of
shapes, and they measured the response of the
neurons to these stimuli. And so there were a couple
of important conclusions that they were able to
make, and observations. And so the first thing
found that, you know, there's sort of this topographical
mapping in the cortex. So nearby cells in the
cortex also represent nearby regions in the visual field. And so you can see for
example, on the right here where if you take kind
of the spatial mapping and map this onto a visual cortex there's more peripheral
regions are these blue areas, you know, farther away from the center. And so they also discovered
that these neurons had a hierarchical organization. And so if you look at different
types of visual stimuli they were able to find
that at the earliest layers retinal ganglion cells
were responsive to things that looked kind of like
circular regions of spots. And then on top of that
there are simple cells, and these simple cells are
responsive to oriented edges, so different orientation
of the light stimulus. And then going further,
they discover that these were then connected to more complex cells, which were responsive to
both light orientation as well as movement, and so on. And you get, you know,
increasing complexity, for example, hypercomplex
cells are now responsive to movement with kind
of an endpoint, right, and so now you're starting
to get the idea of corners and then blobs and so on. And so then in 1980, the neocognitron
was the first example of a network architecture, a model, that had this idea of
simple and complex cells that Hubel and Wiesel had discovered. And in this case Fukushima put these into these alternating layers of
simple and complex cells, where you had these simple cells that had modifiable parameters,
and then complex cells on top of these that
performed a sort of pooling so that it was invariant to, you know, different minor modifications
from the simple cells. And so this is work that
was in the 1980s, right, and so by 1998 Yann LeCun basically showed the first example of applying backpropagation
and gradient-based learning to train convolutional neural networks that did really well on
document recognition. And specifically they
were able to do a good job of recognizing digits of zip codes. And so these were then used pretty widely for zip code recognition
in the postal service. But beyond that it
wasn't able to scale yet to more challenging and
complex data, right, digits are still fairly simple and a limited set to recognize. And so this is where
Alex Krizhevsky, in 2012, gave the modern incarnation of
convolutional neural networks and his network we sort of
colloquially call AlexNet. But this network really
didn't look so much different than the convolutional neural networks that Yann LeCun was dealing with. They're now, you know,
they were scaled now to be larger and deeper and able to, the most important parts
were that they were now able to take advantage of
the large amount of data that's now available, in web
images, in ImageNet data set. As well as take advantage of the parallel computing power in GPUs. And so we'll talk more about that later. But fast forwarding
today, so now, you know, ConvNets are used everywhere. And so we have the initial
classification results on ImageNet from Alex Krizhevsky. This is able to do a really
good job of image retrieval. You can see that when we're
trying to retrieve a flower for example, the features that are learned are really powerful for
doing similarity matching. We also have ConvNets that
are used for detection. So we're able to do a really
good job of localizing where in an image is, for
example, a bus, or a boat, and so on, and draw precise
bounding boxes around that. We're able to go even deeper
beyond that to do segmentation, right, and so these are now richer tasks where we're not looking
for just the bounding box but we're actually going
to label every pixel in the outline of, you know,
trees, and people, and so on. And these kind of algorithms are used in, for example, self-driving cars, and a lot of this is powered
by GPUs as I mentioned earlier, that's able to do parallel processing and able to efficiently
train and run these ConvNets. And so we have modern
powerful GPUs as well as ones that work in embedded
systems, for example, that you would use in a self-driving car. So we can also look at some
of the other applications that ConvNets are used for. So, face-recognition, right,
we can put an input image of a face and get out a
likelihood of who this person is. ConvNets are applied to video,
and so this is an example of a video network that
looks at both images as well as temporal information, and from there is able to classify videos. We're also able to do pose recognition. Being able to recognize, you know, shoulders, elbows, and different joints. And so here are some images
of our fabulous TA, Lane, in various kinds of pretty
non-standard human poses. But ConvNets are able
to do a pretty good job of pose recognition these days. They're also used in game playing. So some of the work in
reinforcement learning, deeper enforcement learning
that you may have seen, playing Atari games, and Go, and so on, and ConvNets are an important
part of all of these. Some other applications,
so they're being used for interpretation and
diagnosis of medical images, for classification of galaxies,
for street sign recognition. There's also whale recognition, this is from a recent Kaggle Challenge. We also have examples of
looking at aerial maps and being able to draw
out where are the streets on these maps, where are buildings, and being able to segment all of these. And then beyond recognition
of classification detection, these types of tasks, we also have tasks like image captioning,
where given an image, we want to write a sentence description about what's in the image. And so this is something
that we'll go into a little bit later in the class. And we also have, you know,
really, really fancy and cool kind of artwork that we can
do using neural networks. And so on the left is an
example of a deep dream, where we're able to take
images and kind of hallucinate different kinds of objects
and concepts in the image. There's also neural style type
work, where we take an image and we're able to re-render this image using a style of a particular
artist and artwork, right. And so here we can take, for
example, Van Gogh on the right, Starry Night, and use that to redraw our original image using that style. And Justin has done a lot of work in this and so if you guys are interested, these are images produced
by some of his code and you guys should talk
to him more about it. Okay, so basically, you know,
this is just a small sample of where ConvNets are being used today. But there's really a huge amount
that can be done with this, right, and so, you know,
for you guys' projects, sort of, you know, let
your imagination go wild and we're excited to see
what sorts of applications you can come up with. So today we're going to talk about how convolutional neural networks work. And again, same as with neural
networks, we're going to first talk about how they work
from a functional perspective without any of the brain analogies. And then we'll talk briefly
about some of these connections. Okay, so, last lecture, we talked about this idea of a fully connected layer. And how, you know, for
a fully connected layer what we're doing is we operate
on top of these vectors, right, and so let's say we
have, you know, an image, a 3D image, 32 by 32 by three, so some of the images that we
were looking at previously. We'll take that, we'll stretch
all of the pixels out, right, and then we have this
3072 dimensional vector, for example in this case. And then we have these weights, right, so we're going to multiply
this by a weight matrix. And so here for example our W
we're going to say is 10 by 3072. And then we're going
to get the activations, the output of this layer,
right, and so in this case, we take each of our 10 rows
and we do this dot product with 3072 dimensional input. And from there we get this one number that's kind of the value of that neuron. And so in this case we're going to have 10 of these neuron outputs. And so a convolutional
layer, so the main difference between this and the fully connected layer that we've been talking about is that here we want to
preserve spatial structure. And so taking this 32 by 32 by three image that we had earlier, instead
of stretching this all out into one long vector, we're
now going to keep the structure of this image, right, this
three dimensional input. And then what we're going to do is our weights are going to
be these small filters, so in this case for example, a
five by five by three filter, and we're going to take this filter and we're going to slide
it over the image spatially and compute dot products
at every spatial location. And so we're going to go into
detail of exactly how this works. So, our filters, first of all, always extend the full
depth of the input volume. And so they're going to be
just a smaller spatial area, so in this case five by five, right, instead of our full 32
by 32 spatial input, but they're always going to go
through the full depth, right, so here we're going to
take five by five by three. And then we're going to take this filter and at a given spatial location we're going to do a dot product between this filter and
then a chunk of a image. So we're just going to overlay this filter on top of a spatial location in the image, right, and then do the dot product, the multiplication of each
element of that filter with each corresponding element
in that spatial location that we've just plopped it on top of. And then this is going
to give us a dot product. So in this case, we have
five times five times three, this is the number of multiplications that we're going to do,
right, plus the bias term. And so this is basically
taking our filter W and basically doing W transpose
times X and plus bias. So is that clear how this works? Yeah, question. [faint speaking] Yeah, so the question is,
when we do the dot product do we turn the five by five
by three into one vector? Yeah, in essence that's what you're doing. You can, I mean, you
can think of it as just plopping it on and doing the
element-wise multiplication at each location, but this is
going to give you the same result as if you stretched out
the filter at that point, stretched out the input
volume that it's laid over, and then took the dot product, and that's what's written
here, yeah, question. [faint speaking] Oh, this is, so the question is, any intuition for why
this is a W transpose? And this was just, not really, this is just the notation
that we have here to make the math work
out as a dot product. So it just depends on whether,
how you're representing W and whether in this case
if we look at the W matrix this happens to be each column
and so we're just taking the transpose to get a row out of it. But there's no intuition here, we're just taking the filters of W and we're stretching it
out into a one D vector, and in order for it to be a dot product it has to be like a one
by, one by N vector. [faint speaking] Okay, so the question is, is W here not five by five
by three, it's one by 75. So that's the case, right, if we're going to do this dot product
of W transpose times X, we have to stretch it out first before we do the dot product. So we take the five by five by three, and we just take all these values and stretch it out into a long vector. And so again, similar
to the other question, the actual operation that we're doing here is plopping our filter on top of a spatial location in the image and multiplying all of the
corresponding values together, but in order just to make it
kind of an easy expression similar to what we've seen before we can also just stretch
each of these out, make sure that dimensions
are transposed correctly so that it works out as a dot product. Yeah, question. [faint speaking] Okay, the question is, how do we slide the filter over the image. We'll go into that next, yes. [faint speaking] Okay, so the question is,
should we rotate the kernel by 180 degrees to better
match the convolution, the definition of a convolution. And so the answer is that
we'll also show the equation for this later, but
we're using convolution as kind of a looser definition
of what's happening. So for people from signal processing, what we are actually technically doing, if you want to call this a convolution, is we're convolving with the
flipped version of the filter. But for the most part, we
just don't worry about this and we just, yeah, do this operation and it's like a convolution in spirit. Okay, so... Okay, so we had a question
earlier, how do we, you know, slide this over all the spatial locations. Right, so what we're going to do is we're going to take this
filter, we're going to start at the upper left-hand
corner and basically center our filter on top of every
pixel in this input volume. And at every position, we're
going to do this dot product and this will produce one value in our output activation map. And so then we're going
to just slide this around. The simplest version
is just at every pixel we're going to do this
operation and fill in the corresponding point
in our output activation. You can see here that the
dimensions are not exactly what would happen, right,
if you're going to do this. I had 32 by 32 in the input and I'm having 28 by 28 in the output, and so we'll go into
examples later of the math of exactly how this is going
to work out dimension-wise, but basically you have a choice of how you're going to slide this, whether you go at every
pixel or whether you slide, let's say, you know, two
input values over at a time, two pixels over at a time, and so you can get different size outputs depending on how you choose to slide. But you're basically doing this
operation in a grid fashion. Okay, so what we just saw earlier, this is taking one filter, sliding it over all of the spatial locations in the image and then we're going to get
this activation map out, right, which is the value of that
filter at every spatial location. And so when we're dealing
with a convolutional layer, we want to work with
multiple filters, right, because each filter is kind
of looking for a specific type of template or concept
in the input volume. And so we're going to have
a set of multiple filters, and so here I'm going
to take a second filter, this green filter, which is
again five by five by three, I'm going to slide this over
all of the spatial locations in my input volume, and
then I'm going to get out this second green activation
map also of the same size. And we can do this for as many filters as we want to have in this layer. So for example, if we have six filters, six of these five by five filters, then we're going to get in
total six activation maps out. All of, so we're going
to get this output volume that's going to be
basically six by 28 by 28. Right, and so a preview
of how we're going to use these convolutional layers
in our convolutional network is that our ConvNet is
basically going to be a sequence of these convolutional layers stacked on top of each other,
same way as what we had with the simple linear layers
in their neural network. And then we're going to intersperse these with activation functions, so for example, a ReLU
activation function. Right, and so you're going to
get something like Conv, ReLU, and usually also some pooling layers, and then you're just going
to get a sequence of these each creating an output
that's now going to be the input to the next convolutional layer. Okay, and so each of these
layers, as I said earlier, has multiple filters, right, many filters. And each of the filter is
producing an activation map. And so when you look at
multiple of these layers stacked together in a ConvNet,
what ends up happening is you end up learning this
hierarching of filters, where the filters at the
earlier layers usually represent low-level features that
you're looking for. So things kind of like edges, right. And then at the mid-level, you're going to get more
complex kinds of features, so maybe it's looking more for things like corners and blobs and so on. And then at higher-level features, you're going to get
things that are starting to more resemble concepts than blobs. And we'll go into more
detail later in the class in how you can actually
visualize all these features and try and interpret what your network, what kinds of features
your network is learning. But the important thing for
now is just to understand that what these features end up being when you have a whole stack of these, is these types of simple
to more complex features. [faint speaking] Yeah. Oh, okay. Oh, okay, so the question
is, what's the intuition for increasing the depth each time. So here I had three filters
in the original layer and then six filters in the next layer. Right, and so this is
mostly a design choice. You know, people in practice have found certain types of these
configurations to work better. And so later on we'll go into
case studies of different kinds of convolutional
neural network architectures and design choices for these and why certain ones
work better than others. But yeah, basically the choice of, you're going to have many design choices in a convolutional neural network, the size of your filter, the stride, how many filters you have, and so we'll talk about
this all more later. Question. [faint speaking] Yeah, so the question is,
as we're sliding this filter over the image spatially it
looks like we're sampling the edges and corners less
than the other locations. Yeah, that's a really good point, and we'll talk I think in a few slides about how we try and compensate for that. Okay, so each of these
convolutional layers that we have stacked together,
we saw how we're starting with more simpler features
and then aggregating these into more complex features later on. And so in practice this is compatible with what Hubel and Wiesel
noticed in their experiments, right, that we had these simple cells at the earlier stages of processing, followed by more complex cells later on. And so even though we didn't explicitly force our ConvNet to learn
these kinds of features, in practice when you give it this type of hierarchical structure and
train it using backpropagation, these are the kinds of filters
that end up being learned. [faint speaking] Okay, so yeah, so the question is, what are we seeing in
these visualizations. And so, alright so, in
these visualizations, like, if we look at this Conv1, the
first convolutional layer, each of these grid, each part
of this grid is a one neuron. And so what we've visualized here is what the input looks
like that maximizes the activation of that particular neuron. So what sort of image you would get that would give you the largest value, make that neuron fire and
have the largest value. And so the way we do this is basically by doing backpropagation from
a particular neuron activation and seeing what in the input will trigger, will give you the highest
values of this neuron. And this is something
that we'll talk about in much more depth in a later lecture about how we create all
of these visualizations. But basically each element of these grids is showing what in the
input would look like that basically maximizes the
activation of the neuron. So in a sense, what is
the neuron looking for? Okay, so here is an example
of some of the activation maps produced by each filter, right. So we can visualize up here on the top we have this whole row of
example five by five filters, and so this is basically a real
case from a trained ConvNet where each of these is
what a five by five filter looks like, and then as we
convolve this over an image, so in this case this I think
it's like a corner of a car, the car light, what the
activation looks like. Right, and so here for example, if we look at this first
one, this red filter, filter like with a red box around it, we'll see that it's looking for, the template looks like an
edge, right, an oriented edge. And so if you slide it over the image, it'll have a high value,
a more white value where there are edges in
this type of orientation. And so each of these activation
maps is kind of the output of sliding one of these filters over and where these filters
are causing, you know, where this sort of template
is more present in the image. And so the reason we call
these convolutional is because this is related to the
convolution of two signals, and so someone pointed out earlier that this is basically this
convolution equation over here, for people who have
seen convolutions before in signal processing, and in practice it's actually more like a correlation where we're convolving
with the flipped version of the filter, but this
is kind of a subtlety, it's not really important for
the purposes of this class. But basically if you're
writing out what you're doing, it has an expression that
looks something like this, which is the standard
definition of a convolution. But this is basically
just taking a filter, sliding it spatially over the image and computing the dot
product at every location. Okay, so you know, as I
had mentioned earlier, like what our total
convolutional neural network is going to look like is we're
going to have an input image, and then we're going to pass it through this sequence of layers, right, where we're going to have a
convolutional layer first. We usually have our
non-linear layer after that. So ReLU is something
that's very commonly used that we're going to talk about more later. And then we have these Conv,
ReLU, Conv, ReLU layers, and then once in a while
we'll use a pooling layer that we'll talk about later as well that basically downsamples the
size of our activation maps. And then finally at the end
of this we'll take our last convolutional layer output
and then we're going to use a fully connected layer
that we've seen before, connected to all of these
convolutional outputs, and use that to get a final score function basically like what we've
already been working with. Okay, so now let's work out some examples of how the spatial dimensions work out. So let's take our 32 by 32
by three image as before, right, and we have our five
by five by three filter that we're going to slide over this image. And we're going to see how
we're going to use that to produce exactly this
28 by 28 activation map. So let's assume that we actually
have a seven by seven input just to be simpler, and let's assume we have a three by three filter. So what we're going to do is we're going to take this filter, plop it down in our
upper left-hand corner, right, and we're going to
multiply, do the dot product, multiply all these values
together to get our first value, and this is going to go into
the upper left-hand value of our activation map. Right, and then what
we're going to do next is we're just going to take this filter, slide it one position to the right, and then we're going to get
another value out from here. And so we can continue with
this to have another value, another, and in the end
what we're going to get is a five by five output, right, because what fit was
basically sliding this filter a total of five spatial
locations horizontally and five spatial locations vertically. Okay, so as I said before there's different kinds of
design choices that we can make. Right, so previously I
slid it at every single spatial location and the
interval at which I slide I'm going to call the stride. And so previously we
used the stride of one. And so now let's see what happens if we have a stride of two. Right, so now we're going
to take our first location the same as before, and
then we're going to skip this time two pixels over
and we're going to get our next value centered at this location. Right, and so now if
we use a stride of two, we have in total three
of these that can fit, and so we're going to get
a three by three output. Okay, and so what happens when
we have a stride of three, what's the output size of this? And so in this case, right, we have three, we slide it over by three again, and the problem is that here
it actually doesn't fit. Right, so we slide it over by three and now it doesn't fit
nicely within the image. And so what we in practice we
just, it just doesn't work. We don't do convolutions like this because it's going to lead to
asymmetric outputs happening. Right, and so just kind
of looking at the way that we computed how many, what
the output size is going to be, this actually can work into a nice formula where we take our
dimension of our input N, we have our filter size
F, we have our stride at which we're sliding along,
and our final output size, the spatial dimension of each output size is going to be N minus F
divided by the stride plus one, right, and you can kind of
see this as a, you know, if I'm going to take my
filter, let's say I fill it in at the very last possible
position that it can be in and then take all the pixels before that, how many instances of moving
by this stride can I fit in. Right, and so that's how this
equation kind of works out. And so as we saw before,
right, if we have N equal seven and F equals three, if
we want a stride of one we plug it into this
formula, we get five by five as we had before, and the
same thing we had for two. And with a stride of three,
this doesn't really work out. And so in practice it's actually common to zero pad the borders in order to make the size work out to what we want it to. And so this is kind of
related to a question earlier, which is what do we do,
right, at the corners. And so what in practice happens is we're going to actually pad
our input image with zeros and so now you're going to
be able to place a filter centered at the upper
right-hand pixel location of your actual input image. Okay, so here's a question,
so who can tell me if I have my same input, seven by seven, three by three filter, stride one, but now I pad with a one pixel border, what's the size of my output going to be? [faint speaking] So, I heard some sixes, heard some sev, so remember we have this
formula that we had before. So if we plug in N is equal
to seven, F is equal to three, right, and then our
stride is equal to one. So what we actually get, so
actually this is giving us seven, four, so seven
minus three is four, divided by one plus one is five. And so this is what we had before. So we actually need to adjust
this formula a little bit, right, so this was actually,
this formula is the case where we don't have zero padded pixels. But if we do pad it, then if
you now take your new output and you slide it along, you'll see that actually
seven of the filters fit, so you get a seven by seven output. And plugging in our
original formula, right, so our N now is not seven, it's nine, so if we go back here
we have N equals nine minus a filter size of
three, which gives six. Right, divided by our
stride, which is one, and so still six, and then
plus one we get seven. Right, and so once you've padded it you want to incorporate this
padding into your formula. Yes, question. [faint speaking] Seven, okay, so the question is, what's the actual output of the size, is it seven by seven or
seven by seven by three? The output is going to be seven by seven by the number of filters that you have. So remember each filter is
going to do a dot product through the entire depth
of your input volume. But then that's going to
produce one number, right, so each filter is, let's
see if we can go back here. Each filter is producing
a one by seven by seven in this case activation map
output, and so the depth is going to be the number
of filters that we have. [faint speaking] Sorry, let me just, one second go back. Okay, can you repeat your question again? [muffled speaking] Okay, so the question is, how
does this connect to before when we had a 32 by 32
by three input, right. So our input had depth
and here in this example I'm showing a 2D example with no depth. And so yeah, I'm showing
this for simplicity but in practice you're going to take your, you're going to multiply
throughout the entire depth as we had before, so you're going to, your filter is going to be
in this case a three be three spatial filter by whatever
input depth that you had. So three by three by three in this case. Yeah, everything else stays the same. Yes, question. [muffled speaking] Yeah, so the question
is, does the zero padding add some sort of extraneous
features at the corners? And yeah, so I mean, we're
doing our best to still, get some value and do, like, process that region of the image, and so zero padding is
kind of one way to do this, where I guess we can, we are detecting part of this template in this region. There's also other ways
to do this that, you know, you can try and like,
mirror the values here or extend them, and so it
doesn't have to be zero padding, but in practice this is one
thing that works reasonably. And so, yeah, so there is a
little bit of kind of artifacts at the edge and we sort of just, you do your best to deal with it. And in practice this works reasonably. I think there was another question. Yeah, question. [faint speaking] So if we have non-square
images, do we ever use a stride that's different
horizontally and vertically? So, I mean, there's nothing
stopping you from doing that, you could, but in practice we just usually take the same stride, we
usually operate square regions and we just, yeah we usually just take the same stride everywhere
and it's sort of like, in a sense it's a little bit like, it's a little bit like the
resolution at which you're, you know, looking at this image, and so usually there's kind
of, you might want to match sort of your horizontal
and vertical resolutions. But, yeah, so in practice you could but really people don't do that. Okay, another question. [faint speaking] So the question is, why
do we do zero padding? So the way we do zero padding is to maintain the same
input size as we had before. Right, so we started with seven by seven, and if we looked at just
starting your filter from the upper left-hand
corner, filling everything in, right, then we get a smaller size output, but we would like to maintain
our full size output. Okay, so, yeah, so we saw how padding
can basically help you maintain the size of the
output that you want, as well as apply your filter at these, like, corner regions and edge regions. And so in general in terms of choosing, you know, your stride, your
filter, your filter size, your stride size, zero
padding, what's common to see is filters of size three
by three, five by five, seven by seven, these are
pretty common filter sizes. And so each of these, for three by three you will want to zero pad with one in order to maintain
the same spatial size. If you're going to do five by five, you can work out the math,
but it's going to come out to you want to zero pad by two. And then for seven you
want to zero pad by three. Okay, and so again you
know, the motivation for doing this type of zero padding and trying to maintain
the input size, right, so we kind of alluded to this before, but if you have multiple of
these layers stacked together... So if you have multiple of
these layers stacked together you'll see that, you know,
if we don't do this kind of zero padding, or any kind of padding, we're going to really
quickly shrink the size of the outputs that we have. Right, and so this is not
something that we want. Like, you can imagine if you
have a pretty deep network then very quickly your, the
size of your activation maps is going to shrink to
something very small. And this is bad both because
we're kind of losing out on some of this information, right, now you're using a much
smaller number of values in order to represent your original image, so you don't want that. And then at the same time also as we talked about this earlier, your also kind of losing sort of some of
this edge information, corner information that each time we're losing out and
shrinking that further. Okay, so let's go through
a couple more examples of computing some of these sizes. So let's say that we have an input volume which is 32 by 32 by three. And here we have 10 five by five filters. Let's use stride one and pad two. And so who can tell me what's the output volume size of this? So you can think about
the formula earlier. Sorry, what was it? [faint speaking] 32 by 32 by 10, yes that's correct. And so the way we can see this, right, is so we have our input size, F is 32. Then in this case we want to augment it by the padding that we added onto this. So we padded it two in
each dimension, right, so we're actually going to get,
total width and total height's going to be 32 plus four on each side. And then minus our filter size five, divided by one plus one and we get 32. So our output is going to
be 32 by 32 for each filter. And then we have 10 filters total, so we have 10 of these activation maps, and our total output volume
is going to be 32 by 32 by 10. Okay, next question, so what's the number of
parameters in this layer? So remember we have 10
five by five filters. [faint speaking] I kind of heard something,
but it was quiet. Can you guys speak up? 250, okay so I heard 250, which is close, but remember that we're
also, our input volume, each of these filters
goes through by depth. So maybe this wasn't clearly written here because each of the filters
is five by five spatially, but implicitly we also have
the depth in here, right. It's going to go through the whole volume. So I heard, yeah, 750 I heard. Almost there, this is
kind of a trick question 'cause also remember
we usually always have a bias term, right, so
in practice each filter has five by five by three
weights, plus our one bias term, we have 76 parameters per filter, and then we have 10 of these total, and so there's 760 total parameters. Okay, and so here's just a summary of the convolutional layer
that you guys can read a little bit more carefully later on. But we have our input volume
of a certain dimension, we have all of these choice,
we have our filters, right, where we have number of
filters, the filter size, the stride of the size,
the amount of zero padding, and you basically can use all of these, go through the computations
that we talked about earlier in order to find out what
your output volume is actually going to be and how many total
parameters that you have. And so some common settings of this. You know, we talked earlier
about common filter sizes of three by three, five by five. Stride is usually one
and two is pretty common. And then your padding P is
going to be whatever fits, like, whatever will
preserve your spatial extent is what's common. And then the total number of filters K, usually we use powers of two
just to be nice, so, you know, 32, 64, 128 and so on, 512, these are pretty common
numbers that you'll see. And just as an aside, we can also do a one by one convolution, this still makes perfect sense where given a one by one convolution we still slide it over
each spatial extent, but now, you know, the spatial region is not really five by five it's just kind of the
trivial case of one by one, but we are still having this filter go through the entire depth. Right, so this is going
to be a dot product through the entire depth
of your input volume. And so the output here, right,
if we have an input volume of 56 by 56 by 64 depth and
we're going to do one by one convolution with 32 filters,
then our output is going to be 56 by 56 by our number of filters, 32. Okay, and so here's an example
of a convolutional layer in TORCH, a deep learning framework. And so you'll see that,
you know, last lecture we talked about how you can go into these deep learning frameworks,
you can see these definitions of each layer, right,
where they have kind of the forward pass and the backward pass implemented for each layer. And so you'll see convolutions, spatial convolution is going
to be just one of these, and then the arguments
that it's going to take are going to be all of these
design choices of, you know, I mean, I guess your
input and output sizes, but also your choices of
like your kernel width, your kernel size, padding,
and these kinds of things. Right, and so if we look at
another framework, Caffe, you'll see something very similar, where again now when you're
defining your network you define networks in Caffe
using this kind of, you know, proto text file where you're specifying each of your design choices for your layer and you can see for a convolutional layer will say things like, you
know, the number of outputs that we have, this is going
to be the number of filters for Caffe, as well as the kernel
size and stride and so on. Okay, and so I guess before I go on, any questions about convolution, how the convolution operation works? Yes, question. [faint speaking] Yeah, so the question is, what's the intuition behind
how you choose your stride. And so at one sense it's
kind of the resolution at which you slide it on, and
usually the reason behind this is because when we have a larger stride what we end up getting as the output is a down sampled image, right, and so what this downsampled
image lets us have is both, it's a way, it's kind of
like pooling in a sense but it's just a different
and sometimes works better way of doing pooling is one
of the intuitions behind this, 'cause you get the same effect
of downsampling your image, and then also as you're doing
this you're reducing the size of the activation maps
that you're dealing with at each layer, right, and so
this also affects later on the total number of
parameters that you have because for example at the
end of all your Conv layers, now you might put on fully
connected layers on top, for example, and now the
fully connected layer's going to be connected to every value of your convolutional output, right, and so a smaller one will
give you smaller number of parameters, and so now
you can get into, like, basically thinking about
trade offs of, you know, number of parameters you
have, the size of your model, overfitting, things
like that, and so yeah, these are kind of some of the things that you want to think about
with choosing your stride. Okay, so now if we look a
little bit at kind of the, you know, brain neuron view
of a convolutional layer, similar to what we
looked at for the neurons in the last lecture. So what we have is that
at every spatial location, we take a dot product between a filter and a specific part of the image, right, and we get one number out from here. And so this is the same idea of doing these types
of dot products, right, taking your input, weighting
it by these Ws, right, values of your filter, these
weights that are the synapses, and getting a value out. But the main difference
here is just that now your neuron has local connectivity. So instead of being connected
to the entire input, it's just looking at a local
region spatially of your image. And so this looks at a local region and then now you're going
to get kind of, you know, this, how much this
neuron is being triggered at every spatial location in your image. Right, so now you preserve
the spatial structure and you can say, you
know, be able to reason on top of these kinds of
activation maps in later layers. And just a little bit of terminology, again for, you know, we have
this five by five filter, we can also call this a
five by five receptive field for the neuron, because this is, the receptive field is
basically the, you know, input field that this field of vision that this neuron is receiving, right, and so that's just another common term that you'll hear for this. And then again remember each
of these five by five filters we're sliding them over
the spatial locations but they're the same set of weights, they share the same parameters. Okay, and so, you know, as we talked about what we're going to get at this output is going to be this volume, right, where spatially we have,
you know, let's say 28 by 28 and then our number of
filters is the depth. And so for example with five filters, what we're going to
get out is this 3D grid that's 28 by 28 by five. And so if you look at the filters across in one spatial location
of the activation volume and going through depth
these five neurons, all of these neurons, basically the way you can interpret this is they're all looking at the same region in the input volume, but they're just looking
for different things, right. So they're different filters applied to the same spatial
location in the image. And so just a reminder
again kind of comparing with the fully connected layer
that we talked about earlier. In that case, right, if we
look at each of the neurons in our activation or
output, each of the neurons was connected to the
entire stretched out input, so it looked at the
entire full input volume, compared to now where each one just looks at this local spatial region. Question. [muffled talking] Okay, so the question
is, within a given layer, are the filters completely symmetric? So what do you mean by
symmetric exactly, I guess? Right, so okay, so the
filters, are the filters doing, they're doing the same dimension,
the same calculation, yes. Okay, so is there anything different other than they have the
same parameter values? No, so you're exactly right, we're just taking a filter
with a given set of, you know, five by five by three parameter values, and we just slide this
in exactly the same way over the entire input volume
to get an activation map. Okay, so you know, we've
gone into a lot of detail in what these convolutional
layers look like, and so now I'm just going to go briefly through the other layers that we have that form this entire
convolutional network. Right, so remember again,
we have convolutional layers interspersed with pooling
layers once in a while as well as these non-linearities. Okay, so what the pooling layers do is that they make the representations smaller and more manageable, right, so we talked about this earlier with someone asked a question of
why we would want to make the representation smaller. And so this is again for it to have fewer, it effects the number of
parameters that you have at the end as well as basically does some, you know, invariance over a given region. And so what the pooling layer does is it does exactly just downsamples, and it takes your input
volume, so for example, 224 by 224 by 64, and
spatially downsamples this. So in the end you'll get out 112 by 112. And it's important to note
this doesn't do anything in the depth, right, we're
only pooling spatially. So the number of, your input depth is going to be the same
as your output depth. And so, for example, a common
way to do this is max pooling. So in this case our pooling
layer also has a filter size and this filter size is
going to be the region at which we pool over,
right, so in this case if we have two by two filters,
we're going to slide this, and so, here, we also have
stride two in this case, so we're going to take this filter and we're going to slide
it along our input volume in exactly the same way
as we did for convolution. But here instead of
doing these dot products, we just take the maximum value of the input volume in that region. Right, so here if we
look at the red values, the value of that will
be six is the largest. If we look at the greens
it's going to give an eight, and then we have a three and a four. Yes, question. [muffled speaking] Yeah, so the question is, is
it typical to set up the stride so that there isn't an overlap? And yeah, so for the pooling layers it is, I think the more common thing to do is to have them not have any overlap, and I guess the way you
can think about this is basically we just want to downsample and so it makes sense to
kind of look at this region and just get one value
to represent this region and then just look at the
next region and so on. Yeah, question. [faint speaking] Okay, so the question
is, why is max pooling better than just taking the, doing something like average pooling? Yes, that's a good point,
like, average pooling is also something that you can do, and intuition behind why
max pooling is commonly used is that it can have
this interpretation of, you know, if this is, these
are activations of my neurons, right, and so each value is kind of how much this neuron
fired in this location, how much this filter
fired in this location. And so you can think of
max pooling as saying, you know, giving a signal of
how much did this filter fire at any location in this image. Right, and if we're
thinking about detecting, you know, doing recognition, this might make some intuitive
sense where you're saying, well, you know, whether a
light or whether some aspect of your image that you're looking for, whether it happens anywhere in this region we want to fire at with a high value. Question. [muffled speaking] Yeah, so the question is,
since pooling and stride both have the same effect of downsampling, can you just use stride
instead of pooling and so on? Yeah, and so in practice I think looking at more recent
neural network architectures people have begun to use stride more in order to do the downsampling
instead of just pooling. And I think this gets into
things like, you know, also like fractional strides
and things that you can do. But in practice this in a
sense maybe has a little bit better way to get better
results using that, so. Yeah, so I think using
stride is definitely, you can do it and people are doing it. Okay, so let's see, where were we. Okay, so yeah, so with
these pooling layers, so again, there's right, some
design choices that you make, you take this input volume of W by H by D, and then you're going to
set your hyperparameters for design choices of your filter size or the spatial extent over
which you are pooling, as well as your stride, and
then you can again compute your output volume using the
same equation that you used earlier for convolution, it
still applies here, right, so we still have our W total extent minus filter size divided
by stride plus one. Okay, and so just one other thing to note, it's also, typically people
don't really use zero padding for the pooling layers
because you're just trying to do a direct downsampling, right, so there isn't this problem of like, applying a filter at the corner and having some part of the
filter go off your input volume. And so for pooling we don't
usually have to worry about this and we just directly downsample. And so some common settings
for the pooling layer is a filter size of two by
two or three by three strides. Two by two, you know, you can have, also you can still have
pooling of two by two even with a filter size of three by three, I think someone asked that earlier, but in practice it's pretty
common just to have two by two. Okay, so now we've talked about
these convolutional layers, the ReLU layers were the
same as what we had before with the, you know, just
the base neural network that we talked about last lecture. So we intersperse these and
then we have a pooling layer every once in a while when we
feel like downsampling, right. And then the last thing is that at the end we want to have a fully connected layer. And so this will be just exactly the same as the fully connected layers
that you've seen before. So in this case now what we do is we take the convolutional
network output, at the last layer we have some volume, so we're going to have width
by height by some depth, and we just take all of these and we essentially just
stretch these out, right. And so now we're going
to get the same kind of, you know, basically 1D
input that we're used to for a vanilla neural network,
and then we're going to apply this fully connected layer on top, so now we're going to have connections to every one of these
convolutional map outputs. And so what you can think
of this is basically, now instead of preserving, you know, before we were preserving
spatial structure, right, and so but at the
last layer at the end, we want to aggregate all of this together and we want to reason basically on top of all of this as we had before. And so what you get from that is just our score outputs as we had earlier. Okay, so-- - [Student] This is
sort of a silly question about this visual. Like what are the 16 pixels
that are on the far right, like what should be interpreting those as? - Okay, so the question
is, what are the 16 pixels that are on the far
right, do you mean the-- - [Student] Like that column of-- - [Instructor] Oh, each column. - [Student] The column
on the far right, yeah. - [Instructor] The green
ones or the black ones? - [Student] The ones labeled pool. - The one with hold on, pool. Oh, okay, yeah, so the question is how do we interpret this column,
right, for example at pool. And so what we're showing
here is each of these columns is the output activation maps, right, the output from one of these layers. And so starting from the
beginning, we have our car, after the convolutional layer we now have these activation
maps of each of the filters slid spatially over the input image. Then we pass that through a ReLU, so you can see the values
coming out from there. And then going all the way over, and so what you get for the pooling layer is that it's really just taking the output of the ReLU layer that came just before it
and then it's pooling it. So it's going to downsample it, right, and then it's going to take the max value in each filter location. And so now if you look at
this pool layer output, like, for example, the last
one that you were mentioning, it looks the same as this ReLU output except that it's downsampled
and that it has this kind of max value at every spatial location and so that's the minor difference that you'll see between those two. [distant speaking] So the question is, now this looks like just a very small amount
of information, right, so how can it know to
classify it from here? And so the way that you
should think about this is that each of these values inside one of these pool
outputs is actually, it's the accumulation of all
the processing that you've done throughout this entire network, right. So it's at the very top of your hierarchy, and so each actually represents kind of a higher level concept. So we saw before, you know,
for example, Hubel and Wiesel and building up these
hierarchical filters, where at the bottom level
we're looking for edges, right, or things like very simple
structures, like edges. And so after your convolutional layer the outputs that you see
here in this first column is basically how much do
specific, for example, edges, fire at different locations in the image. But then as you go through
you're going to get more complex, it's looking for more
complex things, right, and so the next convolutional layer is going to fire at how much, you know, let's say certain kinds of
corners show up in the image, right, because it's reasoning. Its input is not the original image, its input is the output, it's
already the edge maps, right, so it's reasoning on top of edge maps, and so that allows it to get more complex, detect more complex things. And so by the time you get all the way up to this last pooling layer,
each value is representing how much a relatively complex
sort of template is firing. Right, and so because of
that now you can just have a fully connected layer,
you're just aggregating all of this information together to get, you know, a score for your class. So each of these values is how much a pretty complicated
complex concept is firing. Question. [faint speaking] So the question is, when
do you know you've done enough pooling to do the classification? And the answer is you just try and see. So in practice, you know,
these are all design choices and you can think about this
a little bit intuitively, right, like you want to pool
but if you pool too much you're going to have very few values representing your entire image and so on, so it's just kind of a trade off. Something reasonable
versus people have tried a lot of different configurations so you'll probably cross validate, right, and try over different pooling sizes, different filter sizes,
different number of layers, and see what works best for
your problem because yeah, like every problem with
different data is going to, you know, different set of these sorts of hyperparameters might work best. Okay, so last thing, just
wanted to point you guys to this demo of training a ConvNet, which was created by Andre Karpathy, the originator of this class. And so he wrote up this demo where you can basically
train a ConvNet on CIFAR-10, the dataset that we've seen
before, right, with 10 classes. And what's nice about
this demo is you can, it basically plots for you
what each of these filters look like, what the
activation maps look like. So some of the images I showed earlier were taken from this demo. And so you can go try it
out, play around with it, and you know, just go through
and try and get a sense for what these activation maps look like. And just one thing to note, usually the first layer
activation maps are, you can interpret them, right, because they're operating
directly on the input image so you can see what these templates mean. As you get to higher level layers it starts getting really hard, like how do you actually
interpret what do these mean. So for the most part it's
just hard to interpret so you shouldn't, you know, don't worry if you can't really make
sense of what's going on. But it's still nice just
to see the entire flow and what outputs are coming out. Okay, so in summary, so
today we talked about how convolutional neural networks work, how they're basically stacks of these convolutional and pooling layers followed by fully connected
layers at the end. There's been a trend towards
having smaller filters and deeper architectures,
so we'll talk more about case studies for
some of these later on. There's also been a trend
towards getting rid of these pooling and fully
connected layers entirely. So just keeping these, just
having, you know, Conv layers, very deep networks of Conv layers, so again we'll discuss
all of this later on. And then typical architectures
again look like this, you know, as we had earlier. Conv, ReLU for some N number of steps followed by a pool every once in a while, this whole thing repeated
some number of times, and then followed by fully
connected ReLU layers that we saw earlier, you know, one or two or just a few of these, and then a softmax at the
end for your class scores. And so, you know, some typical values you might have N up to five of these. You're going to have pretty deep layers of Conv, ReLU, pool
sequences, and then usually just a couple of these fully
connected layers at the end. But we'll also go into
some newer architectures like ResNet and GoogLeNet,
which challenge this and will give pretty different
types of architectures. Okay, thank you and
see you guys next time.",https://cs231n.github.io/convolutional-networks/,"




CS231n Convolutional Neural Networks for Visual Recognition









 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1\*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-46895817-2', 'auto');
 ga('send', 'pageview');
 



addBackToTop({
 backgroundColor: '#fff',
 innerHTML: 'Back to Top',
 textColor: '#333'
 })

 #back-to-top {
 border: 1px solid #ccc;
 border-radius: 0;
 font-family: sans-serif;
 font-size: 14px;
 width: 100px;
 text-align: center;
 line-height: 30px;
 height: 30px;
 }
 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io)
[Course Website](http://cs231n.stanford.edu/)





# 




Table of Contents:


* [Architecture Overview](#overview)
* [ConvNet Layers](#layers)
	+ [Convolutional Layer](#conv)
	+ [Pooling Layer](#pool)
	+ [Normalization Layer](#norm)
	+ [Fully-Connected Layer](#fc)
	+ [Converting Fully-Connected Layers to Convolutional Layers](#convert)
* [ConvNet Architectures](#architectures)
	+ [Layer Patterns](#layerpat)
	+ [Layer Sizing Patterns](#layersizepat)
	+ [Case Studies](#case) (LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet)
	+ [Computational Considerations](#comp)
* [Additional References](#add)


## Convolutional Neural Networks (CNNs / ConvNets)


Convolutional Neural Networks are very similar to ordinary Neural Networks from
the previous chapter: they are made up of neurons that have learnable weights
and biases. Each neuron receives some inputs, performs a dot product and
optionally follows it with a non-linearity. The whole network still expresses a
single differentiable score function: from the raw image pixels on one end to
class scores at the other. And they still have a loss function (e.g.
SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we
developed for learning regular Neural Networks still apply.


So what changes? ConvNet architectures make the explicit assumption that the
inputs are images, which allows us to encode certain properties into the
architecture. These then make the forward function more efficient to implement
and vastly reduce the amount of parameters in the network.



### Architecture Overview


*Recall: Regular Neural Nets.* As we saw in the previous chapter, Neural
Networks receive an input (a single vector), and transform it through a series
of *hidden layers*. Each hidden layer is made up of a set of neurons, where each
neuron is fully connected to all neurons in the previous layer, and where
neurons in a single layer function completely independently and do not share any
connections. The last fully-connected layer is called the “output layer” and in
classification settings it represents the class scores.


*Regular Neural Nets don’t scale well to full images*. In CIFAR-10, images are
only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single
fully-connected neuron in a first hidden layer of a regular Neural Network would
have 32\*32\*3 = 3072 weights. This amount still seems manageable, but clearly
this fully-connected structure does not scale to larger images. For example, an
image of more respectable size, e.g. 200x200x3, would lead to neurons that have
200\*200\*3 = 120,000 weights. Moreover, we would almost certainly want to have
several such neurons, so the parameters would add up quickly! Clearly, this full
connectivity is wasteful and the huge number of parameters would quickly lead to
overfitting.


*3D volumes of neurons*. Convolutional Neural Networks take advantage of the
fact that the input consists of images and they constrain the architecture in a
more sensible way. In particular, unlike a regular Neural Network, the layers of
a ConvNet have neurons arranged in 3 dimensions: **width, height, depth**. (Note
that the word *depth* here refers to the third dimension of an activation
volume, not to the depth of a full Neural Network, which can refer to the total
number of layers in a network.) For example, the input images in CIFAR-10 are an
input volume of activations, and the volume has dimensions 32x32x3 (width,
height, depth respectively). As we will soon see, the neurons in a layer will
only be connected to a small region of the layer before it, instead of all of
the neurons in a fully-connected manner. Moreover, the final output layer would
for CIFAR-10 have dimensions 1x1x10, because by the end of the ConvNet
architecture we will reduce the full image into a single vector of class scores,
arranged along the depth dimension. Here is a visualization:



![](/assets/nn1/neural_net2.jpeg)
![](/assets/cnn/cnn.jpeg)
Left: A regular 3-layer Neural Network. Right: A ConvNet arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels).


> 
> A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an
> input 3D volume to an output 3D volume with some differentiable function that
> may or may not have parameters.
> 
> 
> 



### Layers used to build ConvNets


As we described above, a simple ConvNet is a sequence of layers, and every layer
of a ConvNet transforms one volume of activations to another through a
differentiable function. We use three main types of layers to build ConvNet
architectures: **Convolutional Layer**, **Pooling Layer**, and **Fully-Connected
Layer** (exactly as seen in regular Neural Networks). We will stack these layers
to form a full ConvNet **architecture**.


*Example Architecture: Overview*. We will go into more details below, but a
simple ConvNet for CIFAR-10 classification could have the architecture [INPUT -
CONV - RELU - POOL - FC]. In more detail:


* INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.
* CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.
* RELU layer will apply an elementwise activation function, such as the \(max(0,x)\) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).
* POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].
* FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.


In this way, ConvNets transform the original image layer by layer from the
original pixel values to the final class scores. Note that some layers contain
parameters and other don’t. In particular, the CONV/FC layers perform
transformations that are a function of not only the activations in the input
volume, but also of the parameters (the weights and biases of the neurons). On
the other hand, the RELU/POOL layers will implement a fixed function. The
parameters in the CONV/FC layers will be trained with gradient descent so that
the class scores that the ConvNet computes are consistent with the labels in the
training set for each image.


In summary:


* A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)
* There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)
* Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function
* Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)
* Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)



![](/assets/cnn/convnet.jpeg)

 The activations of an example ConvNet architecture. The initial volume stores the raw image pixels (left) and the last volume stores the class scores (right). Each volume of activations along the processing path is shown as a column. Since it's difficult to visualize 3D volumes, we lay out each volume's slices in rows. The last layer volume holds the scores for each class, but here we only visualize the sorted top 5 scores, and print the labels of each one. The full [web-based demo](http://cs231n.stanford.edu/) is shown in the header of our website. The architecture shown here is a tiny VGG Net, which we will discuss later.
 

We now describe the individual layers and the details of their hyperparameters
and their connectivities.



#### Convolutional Layer


The Conv layer is the core building block of a Convolutional Network that does
most of the computational heavy lifting.


**Overview and intuition without brain stuff.** Let’s first discuss what the
CONV layer computes without brain/neuron analogies. The CONV layer’s parameters
consist of a set of learnable filters. Every filter is small spatially (along
width and height), but extends through the full depth of the input volume. For
example, a typical filter on a first layer of a ConvNet might have size 5x5x3
(i.e. 5 pixels width and height, and 3 because images have depth 3, the color
channels). During the forward pass, we slide (more precisely, convolve) each
filter across the width and height of the input volume and compute dot products
between the entries of the filter and the input at any position. As we slide the
filter over the width and height of the input volume we will produce a
2-dimensional activation map that gives the responses of that filter at every
spatial position. Intuitively, the network will learn filters that activate when
they see some type of visual feature such as an edge of some orientation or a
blotch of some color on the first layer, or eventually entire honeycomb or
wheel-like patterns on higher layers of the network. Now, we will have an entire
set of filters in each CONV layer (e.g. 12 filters), and each of them will
produce a separate 2-dimensional activation map. We will stack these activation
maps along the depth dimension and produce the output volume.


**The brain view**. If you’re a fan of the brain/neuron analogies, every entry
in the 3D output volume can also be interpreted as an output of a neuron that
looks at only a small region in the input and shares parameters with all neurons
to the left and right spatially (since these numbers all result from applying
the same filter).


We now discuss the details of the neuron connectivities, their arrangement in
space, and their parameter sharing scheme.


**Local Connectivity.** When dealing with high-dimensional inputs such as
images, as we saw above it is impractical to connect neurons to all neurons in
the previous volume. Instead, we will connect each neuron to only a local region
of the input volume. The spatial extent of this connectivity is a hyperparameter
called the **receptive field** of the neuron (equivalently this is the filter
size). The extent of the connectivity along the depth axis is always equal to
the depth of the input volume. It is important to emphasize again this asymmetry
in how we treat the spatial dimensions (width and height) and the depth
dimension: The connections are local in 2D space (along width and height), but
always full along the entire depth of the input volume.


*Example 1*. For example, suppose that the input volume has size [32x32x3],
(e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is
5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in
the input volume, for a total of 5\*5\*3 = 75 weights (and +1 bias parameter).
Notice that the extent of the connectivity along the depth axis must be 3, since
this is the depth of the input volume.


*Example 2*. Suppose an input volume had size [16x16x20]. Then using an example
receptive field size of 3x3, every neuron in the Conv Layer would now have a
total of 3\*3\*20 = 180 connections to the input volume. Notice that, again, the
connectivity is local in 2D space (e.g. 3x3), but full along the input depth
(20).



![](/assets/cnn/depthcol.jpeg)
![](/assets/nn1/neuron_model.jpeg)

**Left:** An example input volume in red (e.g. a 32x32x3 CIFAR-10 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example) along the depth, all looking at the same region in the input: the lines that connect this column of 5 neurons do not represent the weights (i.e. these 5 neurons do not share the same weights, but they are associated with 5 different filters), they just indicate that these neurons are connected to or looking at the same receptive field or region of the input volume, i.e. they share the same receptive field but not the same weights. **Right:** The neurons from the Neural Network chapter remain unchanged: They still compute a dot product of their weights with the input followed by a non-linearity, but their connectivity is now restricted to be local spatially.
 

**Spatial arrangement**. We have explained the connectivity of each neuron in
the Conv Layer to the input volume, but we haven’t yet discussed how many
neurons there are in the output volume or how they are arranged. Three
hyperparameters control the size of the output volume: the **depth, stride** and
**zero-padding**. We discuss these next:


1. First, the **depth** of the output volume is a hyperparameter: it corresponds to the number of filters we would like to use, each learning to look for something different in the input. For example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of color. We will refer to a set of neurons that are all looking at the same region of the input as a **depth column** (some people also prefer the term *fibre*).
2. Second, we must specify the **stride** with which we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.
3. As we will soon see, sometimes it will be convenient to pad the input volume with zeros around the border. The size of this **zero-padding** is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes (most commonly as we’ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).


We can compute the spatial size of the output volume as a function of the input
volume size (\(W\)), the receptive field size of the Conv Layer neurons (\(F\)),
the stride with which they are applied (\(S\)), and the amount of zero padding
used (\(P\)) on the border. You can convince yourself that the correct formula
for calculating how many neurons “fit” is given by \((W - F + 2P)/S + 1\). For
example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a
5x5 output. With stride 2 we would get a 3x3 output. Lets also see one more
graphical example:



![](/assets/cnn/stride.jpeg)

 Illustration of spatial arrangement. In this example there is only one spatial dimension (x-axis), one neuron with a receptive field size of F = 3, the input size is W = 5, and there is zero padding of P = 1. **Left:** The neuron strided across the input in stride of S = 1, giving output of size (5 - 3 + 2)/1+1 = 5. **Right:** The neuron uses stride of S = 2, giving output of size (5 - 3 + 2)/2+1 = 3. Notice that stride S = 3 could not be used since it wouldn't fit neatly across the volume. In terms of the equation, this can be determined since (5 - 3 + 2) = 4 is not divisible by 3. 
   
The neuron weights are in this example [1,0,-1] (shown on very right), and its bias is zero. These weights are shared across all yellow neurons (see parameter sharing below).
 

*Use of zero-padding*. In the example above on left, note that the input
dimension was 5 and the output dimension was equal: also 5. This worked out so
because our receptive fields were 3 and we used zero padding of 1. If there was
no zero-padding used, then the output volume would have had spatial dimension of
only 3, because that is how many neurons would have “fit” across the original
input. In general, setting zero padding to be \(P = (F - 1)/2\) when the stride
is \(S = 1\) ensures that the input volume and output volume will have the same
size spatially. It is very common to use zero-padding in this way and we will
discuss the full reasons when we talk more about ConvNet architectures.


*Constraints on strides*. Note again that the spatial arrangement
hyperparameters have mutual constraints. For example, when the input has size
\(W = 10\), no zero-padding is used \(P = 0\), and the filter size is \(F = 3\),
then it would be impossible to use stride \(S = 2\), since \((W - F + 2P)/S + 1
= (10 - 3 + 0) / 2 + 1 = 4.5\), i.e. not an integer, indicating that the neurons
don’t “fit” neatly and symmetrically across the input. Therefore, this setting
of the hyperparameters is considered to be invalid, and a ConvNet library could
throw an exception or zero pad the rest to make it fit, or crop the input to
make it fit, or something. As we will see in the ConvNet architectures section,
sizing the ConvNets appropriately so that all the dimensions “work out” can be a
real headache, which the use of zero-padding and some design guidelines will
significantly alleviate.


*Real-world example*. The [Krizhevsky et
al.](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)
architecture that won the ImageNet challenge in 2012 accepted images of size
[227x227x3]. On the first Convolutional Layer, it used neurons with receptive
field size \(F = 11\), stride \(S = 4\) and no zero padding \(P = 0\). Since
(227 - 11)/4 + 1 = 55, and since the Conv layer had a depth of \(K = 96\), the
Conv layer output volume had size [55x55x96]. Each of the 55\*55\*96 neurons in
this volume was connected to a region of size [11x11x3] in the input volume.
Moreover, all 96 neurons in each depth column are connected to the same
[11x11x3] region of the input, but of course with different weights. As a fun
aside, if you read the actual paper it claims that the input images were
224x224, which is surely incorrect because (224 - 11)/4 + 1 is quite clearly not
an integer. This has confused many people in the history of ConvNets and little
is known about what happened. My own best guess is that Alex used zero-padding
of 3 extra pixels that he does not mention in the paper.


**Parameter Sharing.** Parameter sharing scheme is used in Convolutional Layers
to control the number of parameters. Using the real-world example above, we see
that there are 55\*55\*96 = 290,400 neurons in the first Conv Layer, and each
has 11\*11\*3 = 363 weights and 1 bias. Together, this adds up to 290400 \* 364
= 105,705,600 parameters on the first layer of the ConvNet alone. Clearly, this
number is very high.


It turns out that we can dramatically reduce the number of parameters by making
one reasonable assumption: That if one feature is useful to compute at some
spatial position (x,y), then it should also be useful to compute at a different
position (x2,y2). In other words, denoting a single 2-dimensional slice of depth
as a **depth slice** (e.g. a volume of size [55x55x96] has 96 depth slices, each
of size [55x55]), we are going to constrain the neurons in each depth slice to
use the same weights and bias. With this parameter sharing scheme, the first
Conv Layer in our example would now have only 96 unique set of weights (one for
each depth slice), for a total of 96\*11\*11\*3 = 34,848 unique weights, or
34,944 parameters (+96 biases). Alternatively, all 55\*55 neurons in each depth
slice will now be using the same parameters. In practice during backpropagation,
every neuron in the volume will compute the gradient for its weights, but these
gradients will be added up across each depth slice and only update a single set
of weights per slice.


Notice that if all neurons in a single depth slice are using the same weight
vector, then the forward pass of the CONV layer can in each depth slice be
computed as a **convolution** of the neuron’s weights with the input volume
(Hence the name: Convolutional Layer). This is why it is common to refer to the
sets of weights as a **filter** (or a **kernel**), that is convolved with the
input.



![](/assets/cnn/weights.jpeg)

 Example filters learned by Krizhevsky et al. Each of the 96 filters shown here is of size [11x11x3], and each one is shared by the 55\*55 neurons in one depth slice. Notice that the parameter sharing assumption is relatively reasonable: If detecting a horizontal edge is important at some location in the image, it should intuitively be useful at some other location as well due to the translationally-invariant structure of images. There is therefore no need to relearn to detect a horizontal edge at every one of the 55\*55 distinct locations in the Conv layer output volume.
 

Note that sometimes the parameter sharing assumption may not make sense. This is
especially the case when the input images to a ConvNet have some specific
centered structure, where we should expect, for example, that completely
different features should be learned on one side of the image than another. One
practical example is when the input are faces that have been centered in the
image. You might expect that different eye-specific or hair-specific features
could (and should) be learned in different spatial locations. In that case it is
common to relax the parameter sharing scheme, and instead simply call the layer
a **Locally-Connected Layer**.


**Numpy examples.** To make the discussion above more concrete, lets express the
same ideas but in code and with a specific example. Suppose that the input
volume is a numpy array `X`. Then:


* A *depth column* (or a *fibre*) at position `(x,y)` would be the activations `X[x,y,:]`.
* A *depth slice*, or equivalently an *activation map* at depth `d` would be the activations `X[:,:,d]`.


*Conv Layer Example*. Suppose that the input volume `X` has shape `X.shape:
(11,11,4)`. Suppose further that we use no zero padding (\(P = 0\)), that the
filter size is \(F = 5\), and that the stride is \(S = 2\). The output volume
would therefore have spatial size (11-5)/2+1 = 4, giving a volume with width and
height of 4. The activation map in the output volume (call it `V`), would then
look as follows (only some of the elements are computed in this example):


* `V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0`
* `V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0`
* `V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0`
* `V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0`


Remember that in numpy, the operation `*` above denotes elementwise
multiplication between the arrays. Notice also that the weight vector `W0` is
the weight vector of that neuron and `b0` is the bias. Here, `W0` is assumed to
be of shape `W0.shape: (5,5,4)`, since the filter size is 5 and the depth of the
input volume is 4. Notice that at each point, we are computing the dot product
as seen before in ordinary neural networks. Also, we see that we are using the
same weight and bias (due to parameter sharing), and where the dimensions along
the width are increasing in steps of 2 (i.e. the stride). To construct a second
activation map in the output volume, we would have:


* `V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1`
* `V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1`
* `V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1`
* `V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1`
* `V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1` (example of going along y)
* `V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1` (or along both)


where we see that we are indexing into the second depth dimension in `V` (at
index 1) because we are computing the second activation map, and that a
different set of parameters (`W1`) is now used. In the example above, we are for
brevity leaving out some of the other operations the Conv Layer would perform to
fill the other parts of the output array `V`. Additionally, recall that these
activation maps are often followed elementwise through an activation function
such as ReLU, but this is not shown here.


**Summary**. To summarize, the Conv Layer:


* Accepts a volume of size \(W\_1 \times H\_1 \times D\_1\)
* Requires four hyperparameters:
	+ Number of filters \(K\),
	+ their spatial extent \(F\),
	+ the stride \(S\),
	+ the amount of zero padding \(P\).
* Produces a volume of size \(W\_2 \times H\_2 \times D\_2\) where:
	+ \(W\_2 = (W\_1 - F + 2P)/S + 1\)
	+ \(H\_2 = (H\_1 - F + 2P)/S + 1\) (i.e. width and height are computed equally by symmetry)
	+ \(D\_2 = K\)
* With parameter sharing, it introduces \(F \cdot F \cdot D\_1\) weights per filter, for a total of \((F \cdot F \cdot D\_1) \cdot K\) weights and \(K\) biases.
* In the output volume, the \(d\)-th depth slice (of size \(W\_2 \times H\_2\)) is the result of performing a valid convolution of the \(d\)-th filter over the input volume with a stride of \(S\), and then offset by \(d\)-th bias.


A common setting of the hyperparameters is \(F = 3, S = 1, P = 1\). However,
there are common conventions and rules of thumb that motivate these
hyperparameters. See the [ConvNet architectures](#architectures) section below.


**Convolution Demo**. Below is a running demo of a CONV layer. Since 3D volumes
are hard to visualize, all the volumes (the input volume (in blue), the weight
volumes (in red), the output volume (in green)) are visualized with each depth
slice stacked in rows. The input volume is of size \(W\_1 = 5, H\_1 = 5, D\_1 =
3\), and the CONV layer parameters are \(K = 2, F = 3, S = 2, P = 1\). That is,
we have two filters of size \(3 \times 3\), and they are applied with a stride
of 2. Therefore, the output volume size has spatial size (5 - 3 + 2)/2 + 1 = 3.
Moreover, notice that a padding of \(P = 1\) is applied to the input volume,
making the outer border of the input volume zero. The visualization below
iterates over the output activations (green), and shows that each element is
computed by elementwise multiplying the highlighted input (blue) with the filter
(red), summing it up, and then offsetting the result by the bias.






**Implementation as Matrix Multiplication**. Note that the convolution operation
essentially performs dot products between the filters and local regions of the
input. A common implementation pattern of the CONV layer is to take advantage of
this fact and formulate the forward pass of a convolutional layer as one big
matrix multiply as follows:


1. The local regions in the input image are stretched out into columns in an operation commonly called **im2col**. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take [11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11\*11\*3 = 363. Iterating this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an output matrix `X_col` of *im2col* of size [363 x 3025], where every column is a stretched out receptive field and there are 55\*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may be duplicated in multiple distinct columns.
2. The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size [11x11x3] this would give a matrix `W_row` of size [96 x 363].
3. The result of a convolution is now equivalent to performing one large matrix multiply `np.dot(W_row, X_col)`, which evaluates the dot product between every filter and every receptive field location. In our example, the output of this operation would be [96 x 3025], giving the output of the dot product of each filter at each location.
4. The result must finally be reshaped back to its proper output dimension [55x55x96].


This approach has the downside that it can use a lot of memory, since some
values in the input volume are replicated multiple times in `X_col`. However,
the benefit is that there are many very efficient implementations of Matrix
Multiplication that we can take advantage of (for example, in the commonly used
[BLAS](http://www.netlib.org/blas/) API). Moreover, the same *im2col* idea can
be reused to perform the pooling operation, which we discuss next.


**Backpropagation.** The backward pass for a convolution operation (for both the
data and the weights) is also a convolution (but with spatially-flipped
filters). This is easy to derive in the 1-dimensional case with a toy example
(not expanded on for now).


**1x1 convolution**. As an aside, several papers use 1x1 convolutions, as first
investigated by [Network in Network](http://arxiv.org/abs/1312.4400). Some
people are at first confused to see 1x1 convolutions especially when they come
from signal processing background. Normally signals are 2-dimensional so 1x1
convolutions do not make sense (it’s just pointwise scaling). However, in
ConvNets this is not the case because one must remember that we operate over
3-dimensional volumes, and that the filters always extend through the full depth
of the input volume. For example, if the input is [32x32x3] then doing 1x1
convolutions would effectively be doing 3-dimensional dot products (since the
input depth is 3 channels).


**Dilated convolutions.** A recent development (e.g. see [paper by Fisher Yu and
Vladlen Koltun](https://arxiv.org/abs/1511.07122)) is to introduce one more
hyperparameter to the CONV layer called the *dilation*. So far we’ve only
discussed CONV filters that are contiguous. However, it’s possible to have
filters that have spaces between each cell, called dilation. As an example, in
one dimension a filter `w` of size 3 would compute over input `x` the following:
`w[0]*x[0] + w[1]*x[1] + w[2]*x[2]`. This is dilation of 0. For dilation 1 the
filter would instead compute `w[0]*x[0] + w[1]*x[2] + w[2]*x[4]`; In other words
there is a gap of 1 between the applications. This can be very useful in some
settings to use in conjunction with 0-dilated filters because it allows you to
merge spatial information across the inputs much more agressively with fewer
layers. For example, if you stack two 3x3 CONV layers on top of each other then
you can convince yourself that the neurons on the 2nd layer are a function of a
5x5 patch of the input (we would say that the *effective receptive field* of
these neurons is 5x5). If we use dilated convolutions then this effective
receptive field would grow much quicker.



#### Pooling Layer


It is common to periodically insert a Pooling layer in-between successive Conv
layers in a ConvNet architecture. Its function is to progressively reduce the
spatial size of the representation to reduce the amount of parameters and
computation in the network, and hence to also control overfitting. The Pooling
Layer operates independently on every depth slice of the input and resizes it
spatially, using the MAX operation. The most common form is a pooling layer with
filters of size 2x2 applied with a stride of 2 downsamples every depth slice in
the input by 2 along both width and height, discarding 75% of the activations.
Every MAX operation would in this case be taking a max over 4 numbers (little
2x2 region in some depth slice). The depth dimension remains unchanged. More
generally, the pooling layer:


* Accepts a volume of size \(W\_1 \times H\_1 \times D\_1\)
* Requires two hyperparameters:
	+ their spatial extent \(F\),
	+ the stride \(S\),
* Produces a volume of size \(W\_2 \times H\_2 \times D\_2\) where:
	+ \(W\_2 = (W\_1 - F)/S + 1\)
	+ \(H\_2 = (H\_1 - F)/S + 1\)
	+ \(D\_2 = D\_1\)
* Introduces zero parameters since it computes a fixed function of the input
* For Pooling layers, it is not common to pad the input using zero-padding.


It is worth noting that there are only two commonly seen variations of the max
pooling layer found in practice: A pooling layer with \(F = 3, S = 2\) (also
called overlapping pooling), and more commonly \(F = 2, S = 2\). Pooling sizes
with larger receptive fields are too destructive.


**General pooling**. In addition to max pooling, the pooling units can also
perform other functions, such as *average pooling* or even *L2-norm pooling*.
Average pooling was often used historically but has recently fallen out of favor
compared to the max pooling operation, which has been shown to work better in
practice.



![](/assets/cnn/pool.jpeg)
![](/assets/cnn/maxpool.jpeg)

 Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. **Left:** In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. **Right:** The most common downsampling operation is max, giving rise to **max pooling**, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).
 

**Backpropagation**. Recall from the backpropagation chapter that the backward
pass for a max(x, y) operation has a simple interpretation as only routing the
gradient to the input that had the highest value in the forward pass. Hence,
during the forward pass of a pooling layer it is common to keep track of the
index of the max activation (sometimes also called *the switches*) so that
gradient routing is efficient during backpropagation.


**Getting rid of pooling**. Many people dislike the pooling operation and think
that we can get away without it. For example, [Striving for Simplicity: The All
Convolutional Net](http://arxiv.org/abs/1412.6806) proposes to discard the
pooling layer in favor of architecture that only consists of repeated CONV
layers. To reduce the size of the representation they suggest using larger
stride in CONV layer once in a while. Discarding pooling layers has also been
found to be important in training good generative models, such as variational
autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely
that future architectures will feature very few to no pooling layers.



#### Normalization Layer


Many types of normalization layers have been proposed for use in ConvNet
architectures, sometimes with the intentions of implementing inhibition schemes
observed in the biological brain. However, these layers have since fallen out of
favor because in practice their contribution has been shown to be minimal, if
any. For various types of normalizations, see the discussion in Alex
Krizhevsky’s [cuda-convnet library
API](http://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map)).



#### Fully-connected layer


Neurons in a fully connected layer have full connections to all activations in
the previous layer, as seen in regular Neural Networks. Their activations can
hence be computed with a matrix multiplication followed by a bias offset. See
the *Neural Network* section of the notes for more information.



#### Converting FC layers to CONV layers


It is worth noting that the only difference between FC and CONV layers is that
the neurons in the CONV layer are connected only to a local region in the input,
and that many of the neurons in a CONV volume share parameters. However, the
neurons in both layers still compute dot products, so their functional form is
identical. Therefore, it turns out that it’s possible to convert between FC and
CONV layers:


* For any CONV layer there is an FC layer that implements the same forward function. The weight matrix would be a large matrix that is mostly zero except for at certain blocks (due to local connectivity) where the weights in many of the blocks are equal (due to parameter sharing).
* Conversely, any FC layer can be converted to a CONV layer. For example, an FC layer with \(K = 4096\) that is looking at some input volume of size \(7 \times 7 \times 512\) can be equivalently expressed as a CONV layer with \(F = 7, P = 0, S = 1, K = 4096\). In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be \(1 \times 1 \times 4096\) since only a single depth column “fits” across the input volume, giving identical result as the initial FC layer.


**FC->CONV conversion**. Of these two conversions, the ability to convert an FC
layer to a CONV layer is particularly useful in practice. Consider a ConvNet
architecture that takes a 224x224x3 image, and then uses a series of CONV layers
and POOL layers to reduce the image to an activations volume of size 7x7x512 (in
an *AlexNet* architecture that we’ll see later, this is done by use of 5 pooling
layers that downsample the input spatially by a factor of two each time, making
the final spatial size 224/2/2/2/2/2 = 7). From there, an AlexNet uses two FC
layers of size 4096 and finally the last FC layers with 1000 neurons that
compute the class scores. We can convert each of these three FC layers to CONV
layers as described above:


* Replace the first FC layer that looks at [7x7x512] volume with a CONV layer that uses filter size \(F = 7\), giving output volume [1x1x4096].
* Replace the second FC layer with a CONV layer that uses filter size \(F = 1\), giving output volume [1x1x4096]
* Replace the last FC layer similarly, with \(F=1\), giving final output [1x1x1000]


Each of these conversions could in practice involve manipulating (e.g.
reshaping) the weight matrix \(W\) in each FC layer into CONV layer filters. It
turns out that this conversion allows us to “slide” the original ConvNet very
efficiently across many spatial positions in a larger image, in a single forward
pass.


For example, if 224x224 image gives a volume of size [7x7x512] - i.e. a
reduction by 32, then forwarding an image of size 384x384 through the converted
architecture would give the equivalent volume in size [12x12x512], since 384/32
= 12. Following through with the next 3 CONV layers that we just converted from
FC layers would now give the final volume of size [6x6x1000], since (12 - 7)/1 +
1 = 6. Note that instead of a single vector of class scores of size [1x1x1000],
we’re now getting an entire 6x6 array of class scores across the 384x384 image.



> 
> Evaluating the original ConvNet (with FC layers) independently across 224x224
> crops of the 384x384 image in strides of 32 pixels gives an identical result to
> forwarding the converted ConvNet one time.
> 
> 
> 


Naturally, forwarding the converted ConvNet a single time is much more efficient
than iterating the original ConvNet over all those 36 locations, since the 36
evaluations share computation. This trick is often used in practice to get
better performance, where for example, it is common to resize an image to make
it bigger, use a converted ConvNet to evaluate the class scores at many spatial
positions and then average the class scores.


Lastly, what if we wanted to efficiently apply the original ConvNet over the
image but at a stride smaller than 32 pixels? We could achieve this with
multiple forward passes. For example, note that if we wanted to use a stride of
16 pixels we could do so by combining the volumes received by forwarding the
converted ConvNet twice: First over the original image and second over the image
but with the image shifted spatially by 16 pixels along both width and height.


* An IPython Notebook on [Net Surgery](https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb) shows how to perform the conversion in practice, in code (using Caffe)



### ConvNet Architectures


We have seen that Convolutional Networks are commonly made up of only three
layer types: CONV, POOL (we assume Max pool unless stated otherwise) and FC
(short for fully-connected). We will also explicitly write the RELU activation
function as a layer, which applies elementwise non-linearity. In this section we
discuss how these are commonly stacked together to form entire ConvNets.



#### Layer Patterns


The most common form of a ConvNet architecture stacks a few CONV-RELU layers,
follows them with POOL layers, and repeats this pattern until the image has been
merged spatially to a small size. At some point, it is common to transition to
fully-connected layers. The last fully-connected layer holds the output, such as
the class scores. In other words, the most common ConvNet architecture follows
the pattern:


`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`


where the `*` indicates repetition, and the `POOL?` indicates an optional
pooling layer. Moreover, `N >= 0` (and usually `N <= 3`), `M >= 0`, `K >= 0`
(and usually `K < 3`). For example, here are some common ConvNet architectures
you may see that follow this pattern:


* `INPUT -> FC`, implements a linear classifier. Here `N = M = K = 0`.
* `INPUT -> CONV -> RELU -> FC`
* `INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC`. Here we see that there is a single CONV layer between every POOL layer.
* `INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC` Here we see two CONV layers stacked before every POOL layer. This is generally a good idea for larger and deeper networks, because multiple stacked CONV layers can develop more complex features of the input volume before the destructive pooling operation.


*Prefer a stack of small filter CONV to one large receptive field CONV layer*.
Suppose that you stack three 3x3 CONV layers on top of each other (with
non-linearities in between, of course). In this arrangement, each neuron on the
first CONV layer has a 3x3 view of the input volume. A neuron on the second CONV
layer has a 3x3 view of the first CONV layer, and hence by extension a 5x5 view
of the input volume. Similarly, a neuron on the third CONV layer has a 3x3 view
of the 2nd CONV layer, and hence a 7x7 view of the input volume. Suppose that
instead of these three layers of 3x3 CONV, we only wanted to use a single CONV
layer with 7x7 receptive fields. These neurons would have a receptive field size
of the input volume that is identical in spatial extent (7x7), but with several
disadvantages. First, the neurons would be computing a linear function over the
input, while the three stacks of CONV layers contain non-linearities that make
their features more expressive. Second, if we suppose that all the volumes have
\(C\) channels, then it can be seen that the single 7x7 CONV layer would contain
\(C \times (7 \times 7 \times C) = 49 C^2\) parameters, while the three 3x3 CONV
layers would only contain \(3 \times (C \times (3 \times 3 \times C)) = 27 C^2\)
parameters. Intuitively, stacking CONV layers with tiny filters as opposed to
having one CONV layer with big filters allows us to express more powerful
features of the input, and with fewer parameters. As a practical disadvantage,
we might need more memory to hold all the intermediate CONV layer results if we
plan to do backpropagation.


**Recent departures.** It should be noted that the conventional paradigm of a
linear list of layers has recently been challenged, in Google’s Inception
architectures and also in current (state of the art) Residual Networks from
Microsoft Research Asia. Both of these (see details below in case studies
section) feature more intricate and different connectivity structures.


**In practice: use whatever works best on ImageNet**. If you’re feeling a bit of
a fatigue in thinking about the architectural decisions, you’ll be pleased to
know that in 90% or more of applications you should not have to worry about
these. I like to summarize this point as “*don’t be a hero*”: Instead of rolling
your own architecture for a problem, you should look at whatever architecture
currently works best on ImageNet, download a pretrained model and finetune it on
your data. You should rarely ever have to train a ConvNet from scratch or design
one from scratch. I also made this point at the [Deep Learning
school](https://www.youtube.com/watch?v=u6aEYuemt0M).



#### Layer Sizing Patterns


Until now we’ve omitted mentions of common hyperparameters used in each of the
layers in a ConvNet. We will first state the common rules of thumb for sizing
the architectures and then follow the rules with a discussion of the notation:


The **input layer** (that contains the image) should be divisible by 2 many
times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224
(e.g. common ImageNet ConvNets), 384, and 512.


The **conv layers** should be using small filters (e.g. 3x3 or at most 5x5),
using a stride of \(S = 1\), and crucially, padding the input volume with zeros
in such way that the conv layer does not alter the spatial dimensions of the
input. That is, when \(F = 3\), then using \(P = 1\) will retain the original
size of the input. When \(F = 5\), \(P = 2\). For a general \(F\), it can be
seen that \(P = (F - 1) / 2\) preserves the input size. If you must use bigger
filter sizes (such as 7x7 or so), it is only common to see this on the very
first conv layer that is looking at the input image.


The **pool layers** are in charge of downsampling the spatial dimensions of the
input. The most common setting is to use max-pooling with 2x2 receptive fields
(i.e. \(F = 2\)), and with a stride of 2 (i.e. \(S = 2\)). Note that this
discards exactly 75% of the activations in an input volume (due to downsampling
by 2 in both width and height). Another slightly less common setting is to use
3x3 receptive fields with a stride of 2, but this makes “fitting” more
complicated (e.g., a 32x32x3 layer would require zero padding to be used with a
max-pooling layer with 3x3 receptive field and stride 2). It is very uncommon to
see receptive field sizes for max pooling that are larger than 3 because the
pooling is then too lossy and aggressive. This usually leads to worse
performance.


*Reducing sizing headaches.* The scheme presented above is pleasing because all
the CONV layers preserve the spatial size of their input, while the POOL layers
alone are in charge of down-sampling the volumes spatially. In an alternative
scheme where we use strides greater than 1 or don’t zero-pad the input in CONV
layers, we would have to very carefully keep track of the input volumes
throughout the CNN architecture and make sure that all strides and filters “work
out”, and that the ConvNet architecture is nicely and symmetrically wired.


*Why use stride of 1 in CONV?* Smaller strides work better in practice.
Additionally, as already mentioned stride 1 allows us to leave all spatial
down-sampling to the POOL layers, with the CONV layers only transforming the
input volume depth-wise.


*Why use padding?* In addition to the aforementioned benefit of keeping the
spatial sizes constant after CONV, doing this actually improves performance. If
the CONV layers were to not zero-pad the inputs and only perform valid
convolutions, then the size of the volumes would reduce by a small amount after
each CONV, and the information at the borders would be “washed away” too
quickly.


*Compromising based on memory constraints.* In some cases (especially early in
the ConvNet architectures), the amount of memory can build up very quickly with
the rules of thumb presented above. For example, filtering a 224x224x3 image
with three 3x3 CONV layers with 64 filters each and padding 1 would create three
activation volumes of size [224x224x64]. This amounts to a total of about 10
million activations, or 72MB of memory (per image, for both activations and
gradients). Since GPUs are often bottlenecked by memory, it may be necessary to
compromise. In practice, people prefer to make the compromise at only the first
CONV layer of the network. For example, one compromise might be to use a first
CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As
another example, an AlexNet uses filter sizes of 11x11 and stride of 4.



#### Case studies


There are several architectures in the field of Convolutional Networks that have
a name. The most common are:


* **LeNet**. The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990’s. Of these, the best known is the [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture that was used to read zip codes, digits, etc.
* **AlexNet**. The first work that popularized Convolutional Networks in Computer Vision was the [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks), developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was submitted to the [ImageNet ILSVRC challenge](http://www.image-net.org/challenges/LSVRC/2014/) in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error). The Network had a very similar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer).
* **ZF Net**. The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. It became known as the [ZFNet](http://arxiv.org/abs/1311.2901) (short for Zeiler & Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller.
* **GoogLeNet**. The ILSVRC 2014 winner was a Convolutional Network from [Szegedy et al.](http://arxiv.org/abs/1409.4842) from Google. Its main contribution was the development of an *Inception Module* that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. There are also several followup versions to the GoogLeNet, most recently [Inception-v4](http://arxiv.org/abs/1602.07261).
* **VGGNet**. The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the [VGGNet](http://www.robots.ox.ac.uk/~vgg/research/very_deep/). Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. Their [pretrained model](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.
* **ResNet**. [Residual Network](http://arxiv.org/abs/1512.03385) developed by Kaiming He et al. was the winner of ILSVRC 2015. It features special *skip connections* and a heavy use of [batch normalization](http://arxiv.org/abs/1502.03167). The architecture is also missing fully connected layers at the end of the network. The reader is also referred to Kaiming’s presentation ([video](https://www.youtube.com/watch?v=1PGLj-uKT1w), [slides](http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)), and some [recent experiments](https://github.com/gcr/torch-residual-networks) that reproduce these networks in Torch. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 10, 2016). In particular, also see more recent developments that tweak the original architecture from [Kaiming He et al. Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (published March 2016).


**VGGNet in detail**. Lets break down the
[VGGNet](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) in more detail as
a case study. The whole VGGNet is composed of CONV layers that perform 3x3
convolutions with stride 1 and pad 1, and of POOL layers that perform 2x2 max
pooling with stride 2 (and no padding). We can write out the size of the
representation at each step of the processing and keep track of both the
representation size and the total number of weights:



```python
INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864
POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456
POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0
FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448
FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216
FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000

TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
TOTAL params: 138M parameters

```

As is common with Convolutional Networks, notice that most of the memory (and
also compute time) is used in the early CONV layers, and that most of the
parameters are in the last FC layers. In this particular case, the first FC
layer contains 100M weights, out of a total of 140M.



#### Computational Considerations


The largest bottleneck to be aware of when constructing ConvNet architectures is
the memory bottleneck. Many modern GPUs have a limit of 3/4/6GB memory, with the
best GPUs having about 12GB of memory. There are three major sources of memory
to keep track of:


* From the intermediate volume sizes: These are the raw number of **activations** at every layer of the ConvNet, and also their gradients (of equal size). Usually, most of the activations are on the earlier layers of a ConvNet (i.e. first Conv Layers). These are kept around because they are needed for backpropagation, but a clever implementation that runs a ConvNet only at test time could in principle reduce this by a huge amount, by only storing the current activations at any layer and discarding the previous activations on layers below.
* From the parameter sizes: These are the numbers that hold the network **parameters**, their gradients during backpropagation, and commonly also a step cache if the optimization is using momentum, Adagrad, or RMSProp. Therefore, the memory to store the parameter vector alone must usually be multiplied by a factor of at least 3 or so.
* Every ConvNet implementation has to maintain **miscellaneous** memory, such as the image data batches, perhaps their augmented versions, etc.


Once you have a rough estimate of the total number of values (for activations,
gradients, and misc), the number should be converted to size in GB. Take the
number of values, multiply by 4 to get the raw number of bytes (since every
floating point is 4 bytes, or maybe by 8 for double precision), and then divide
by 1024 multiple times to get the amount of memory in KB, MB, and finally GB. If
your network doesn’t fit, a common heuristic to “make it fit” is to decrease the
batch size, since most of the memory is usually consumed by the activations.



### Additional Resources


Additional resources related to implementation:


* [Soumith benchmarks for CONV performance](https://github.com/soumith/convnet-benchmarks)
* [ConvNetJS CIFAR-10 demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html) allows you to play with ConvNet architectures and see the results and computations in real time, in the browser.
* [Caffe](http://caffe.berkeleyvision.org/), one of the popular ConvNet libraries.
* [State of the art ResNets in Torch7](http://torch.ch/blog/2016/02/04/resnets.html)









* [cs231n](https://github.com/cs231n)
* [cs231n](https://twitter.com/cs231n)












 // Make responsive
 MathJax.Hub.Config({
 ""HTML-CSS"": { linebreaks: { automatic: true } },
 ""SVG"": { linebreaks: { automatic: true } },
 });
 


"
wEoyxE0GP2M?si=Z_PG1BKJAxiMxIgI,"1621, 4800","and ReLU variants are going to be better. And in general don't use sigmoid. This is one of the earliest
original activation functions, and ReLU and these other variants have generally worked better since then. Okay, so now let's talk a little bit about data preprocessing. Right, so the activation function, we design this is part of our network. Now we want to train the network, and we have our input data that we want to start training from. So, generally we want to
always preprocess the data, and this is something that
you've probably seen before in machine learning
classes if you taken those. And some standard types
of preprocessing are, you take your original data and you want to zero mean them, and then you probably want
to also normalize that, so normalized by the standard deviation, And so why do we want to do this? For zero centering, you
can remember earlier that we talked about when all the inputs are positive, for example, then we get all of our gradients on the weights to be positive, and we get this basically
suboptimal optimization. And in general even if it's
not all zero or all negative, any sort of bias will still
cause this type of problem. And so then in terms of
normalizing the data, this is basically you
want to normalize data typically in the machine
learning problems, so that all features
are in the same range, and so that they contribute equally. In practice, since for
images, which is what we're dealing with in this
course here for the most part, we do do the zero centering, but in practice we
don't actually normalize the pixel value so much,
because generally for images right at each location you already have relatively comparable
scale and distribution, and so we don't really
need to normalize so much, compared to more general
machine learning problems, where you might have different features that are very different and
of very different scales. And in machine learning, you might also see a
more complicated things, like PCA or whitening,
but again with images, we typically just stick
with the zero mean, and we don't do the normalization, and we also don't do some of these more complicated pre-processing. And one reason for this
is generally with images we don't really want to
take all of our input, let's say pixel values and project this onto a lower dimensional space of new kinds of features
that we're dealing with. We typically just want to apply convolutional networks spatially and have our spatial structure
over the original image. Yeah, question. [student speaking off mic] So the question is we
do this pre-processing in a training phase, do we also do the same kind
of thing in the test phase, and the answer is yes. So, let me just move
to the next slide here. So, in general on the training phase is where we determine our let's say, mean, and then we apply this exact
same mean to the test data. So, we'll normalize by the same empirical mean from the training data. Okay, so to summarize
basically for images, we typically just do the
zero mean pre-processing and we can subtract either
the entire mean image. So, from the training data, you compute the mean image, which will be the same size
as your, as each image. So, for example 32 by 32 by three, you'll get this array of numbers, and then you subtract that from each image that you're about to
pass through the network, and you'll do the same thing at test time for this array that you
determined at training time. In practice, we can
also for some networks, we also do this by just of subtracting a per-channel mean, and so instead of having
an entire mean image that were going to zero-center by, we just take the mean by channel, and this is just because it turns out that it was similar enough
across the whole image, it didn't make such a big difference to subtract the mean image versus just a per-channel value. And this is easier to just
pass around and deal with. So, you'll see this as well
for example, in a VGG Network, which is a network that
came after AlexNet, and we'll talk about that later. Question. [student speaking off mic] Okay, so there are two questions. The first is what's a
channel, in this case, when we are subtracting
a per-channel mean? And this is RGB, so our array, our images are typically for
example, 32 by 32 by three. So, width, height, each are 32, and our depth, we have three channels RGB, and so we'll have one
mean for the red channel, one mean for a green, one for blue. And then the second, what
was your second question? [student speaking off mic] Oh. Okay, so the question is when we're subtracting the mean image, what is the mean taken over? And the mean is taking over
all of your training images. So, you'll take all of
your training images and just compute the mean of all of those. Does that make sense? [student speaking off mic] Yeah the question is, we do this for the entire training set, once before we start training. We don't do this per batch, and yeah, that's exactly correct. So we just want to have a good sample, an empirical mean that we have. And so if you take it per batch, if you're sampling reasonable batches, it should be basically, you should be getting the same values anyways for the mean, and so it's more efficient and easier just do this once at the beginning. You might not even have to really take it over the entire training data. You could also just sample
enough training images to get a good estimate of your mean. Okay, so any other questions
about data preprocessing? Yes. [student speaking off mic] So, the question is does
the data preprocessing solve the sigmoid problem? So the data preprocessing
is doing zero mean right? And we talked about how sigmoid, we want to have zero mean. And so it does solve
this for the first layer that we pass it through. So, now our inputs to the first layer of our network is going to be zero mean, but we'll see later on that
we're actually going to have this problem come up in
much worse and greater form, as we have deep networks. You're going to get a lot of nonzero mean problems later on. And so in this case, this is
not going to be sufficient. So this only helps at the
first layer of your network. Okay, so now let's talk
about how do we want to initialize the weights of our network? So, we have let's say our standard two layer neural network and we have all of these
weights that we want to learn, but we have to start them
with some value, right? And then we're going to update them using our gradient updates from there. So first question. What happens when we use an initialization of W equals zero? We just set all of the
parameters to be zero. What's the problem with this? [student speaking off mic] So sorry, say that again. So I heard all the neurons
are going to be dead. No updates ever. So not exactly. So, part of that is correct in that all the neurons will do the same thing. So, they might not all be dead. Depending on your input value, I mean, you could be in any
regime of your neurons, so they might not be dead, but the key thing is that they
will all do the same thing. So, since your weights are zero, given an input, every
neuron is going to be, have the same operation
basically on top of your inputs. And so, since they're all
going to output the same thing, they're also all going
to get the same gradient. And so, because of that, they're all going to
update in the same way. And now you're just going to get all neurons that are exactly the same, which is not what you want. You want the neurons to
learn different things. And so, that's the problem when you initialize everything equally and there's basically no
symmetry breaking here. So, what's the first, yeah question? [student speaking off mic] So the question is, because that, because the gradient
also depends on our loss, won't one backprop differently
compared to the other? So in the last layer, like yes, you do have basically some of this, the gradients will get the same, sorry, will get different
loss for each specific neuron based on which class it was connected to, but if you look at all the neurons generally throughout your
network, like you're going to, you basically have a lot of these neurons that are connected in
exactly the same way. They had the same updates and it's basically
going to be the problem. Okay, so the first idea that we can have to try and improve upon this is to set all of the weights
to be small random numbers that we can sample from a distribution. So, in this case, we're
going to sample from basically a standard gaussian, but we're going to scale it so that the standard deviation is actually one E negative two, 0.01. And so, just give this
many small random weights. And so, this does work
okay for small networks, now we've broken the symmetry, but there's going to be
problems with deeper networks. And so, let's take a look
at why this is the case. So, here this is basically
an experiment that we can do where let's take a deeper network. So in this case, let's initialize
a 10 layer neural network to have 500 neurons in
each of these 10 layers. Okay, we'll use tanh
nonlinearities in this case and we'll initialize it
with small random numbers as we described in the last slide. So here, we're going to basically just initialize this network. We have random data
that we're going to take, and now let's just pass it
through the entire network, and at each layer, look at the statistics of the activations that
come out of that layer. And so, what we'll see this is probably a little bit hard to read up top, but if we compute the mean and the standard deviations at each layer, well see that at the first layer this is, the means are always around zero. There's a funny sound in here. Interesting, okay well that was fixed. So, if we look at, if we look
at the outputs from here, the mean is always
going to be around zero, which makes sense. So, if we look here, let's see, if we take this, we looked at
the dot product of X with W, and then we took the tanh on linearity, and then we store these values and so, because it tanh is centered around zero, this will make sense, and then the standard
deviation however shrinks, and it quickly collapses to zero. So, if we're plotting this, here this second row of plots here is showing the mean
and standard deviations over time per layer
and then in the bottom, the sequence of plots is
showing for each of our layers. What's the distribution of
the activations that we have? And so, we can see that
at the first layer, we still have a reasonable
gaussian looking thing. It's a nice distribution. But the problem is that as we multiply by this W, these small numbers at each layer, this quickly shrinks and
collapses all of these values, as we multiply this over and over again. And so, by the end, we
get all of these zeros, which is not what we want. So we get all the activations become zero. And so now let's think
about the backwards pass. So, if we do a backward pass, now assuming this was our forward pass and now we want to compute our gradients. So first, what does the gradients look like on the weights? Does anyone have a guess? So, if we think about this, we have our input values are very small at each layer right, because they've all
collapsed at this near zero, and then now each layer, we have our upstream
gradient flowing down, and then in order to get
the gradient on the weights remember it's our upstream gradient
times our local gradient, which for this this dot
product were doing W times X. It's just basically going to
be X, which is our inputs. So, it's again a similar kind of problem that we saw earlier, where now since, so
here because X is small, our weights are getting
a very small gradient, and they're basically not updating. So, this is a way that you can
basically try and think about the effect of gradient
flows through your networks. You can always think about
what the forward pass is doing, and then think about what's happening as you have gradient flows coming down, and different types of inputs, what the effect of this
actually is on our weights and the gradients on them. And so also, if now if we think about what's the gradient that's
going to be flowing back from each layer as we're
chaining all these gradients. Alright, so this is going to be the flip thing where we have now the gradient flowing back
is our upstream gradient times in this case the local
gradient is W on our input X. And so again, because
this is the dot product, and so now, actually going
backwards at each layer, we're basically doing a multiplication of the upstream gradient by our weights in order to get the next
gradient flowing downwards. And so because here, we're multiplying by
W over and over again. You're getting basically
the same phenomenon as we had in the forward pass where everything is getting
smaller and smaller. And now the gradient, upstream gradients are collapsing to zero as well. Question? [student speaking off mic] Yes, I guess upstream and downstream is, can be interpreted differently, depending on if you're
going forward and backward, but in this case we're going, we're doing, we're going backwards, right? We're doing back propagation. And so upstream is the gradient flowing, you can think of a flow from your loss, all the way back to your input. And so upstream is what came from what you've already done, flowing you know, down
into your current node. Right, so we're for flowing downwards, and what we get coming into
the node through backprop is coming from upstream. Okay, so now let's think
about what happens when, you know we saw that this was a problem when our weights were pretty small, right? So, we can think about well, what if we just try and solve this by making our weights big? So, let's sample from
this standard gaussian, now with standard deviation one instead of 0.01. So what's the problem here? Does anyone have a guess? If our weights are now all
big, and we're passing them, and we're taking these
outputs of W times X, and passing them through
tanh nonlinearities, remember we were talking
about what happens at different values of inputs to tanh, so what's the problem? Okay, so yeah I heard that
it's going to be saturated, so that's right. Basically now, because our
weights are going to be big, we're going to always
be at saturated regimes of either very negative or
very positive of the tanh. And so in practice, what
you're going to get here is now if we look at the
distribution of the activations at each of the layers here on the bottom, they're going to be all basically
negative one or plus one. Right, and so this will have the problem that we talked about with the tanh earlier,
when they're saturated, that all the gradients will be zero, and our weights are not updating. So basically, it's really hard to get your weight initialization right. When it's too small they all collapse. When it's too large they saturate. So, there's been some work
in trying to figure out well, what's the proper way
to initialize these weights. And so, one kind of good rule
of thumb that you can use is the Xavier initialization. And so this is from this
paper by Glorot in 2010. And so what this formula is, is if we look at W up here, we can see that we want to
initialize them to these, we sample from our standard gaussian, and then we're going to scale by the number of inputs that we have. And you can go through the math, and you can see in the lecture notes as well as in this paper of exactly how this works out, but basically the way
we do it is we specify that we want the variance of the input to be the same as a
variance of the output, and then if you derive
what the weight should be you'll get this formula, and intuitively with this
kind of means is that if you have a small
number of inputs right, then we're going to divide
by the smaller number and get larger weights,
and we need larger weights, because with small inputs, and you're multiplying
each of these by weight, you need a larger weights to get the same larger variance at output, and kind of vice versa for
if we have many inputs, then we want smaller
weights in order to get the same spread at the output. So, you can look at the notes
for more details about this. And so basically now, if we
want to have a unit gaussian, right as input to each layer, we can use this kind of initialization to at training time, to be
able to initialize this, so that there is approximately a unit gaussian at each layer. Okay, and so one thing
is does assume though is that it is assumed that
there's linear activations. and so it assumes that
we are in the activation, in the active region of
the tanh, for example. And so again, you can look at the notes to really try and
understand its derivation, but the problem is that this breaks when now you use something like a ReLU. Right, and so with the
ReLU what happens is that, because it's killing half of your units, it's setting approximately half of them to zero at each time, it's actually halving the
variance that you get out of this. And so now, if you just
make the same assumptions as your derivation
earlier you won't actually get the right variance coming out, it's going to be too small. And so what you see is again
this kind of phenomenon, as the distributions starts collapsing. In this case you get more
and more peaked toward zero, and more units deactivated. And the way to address this with something that has been
pointed out in some papers, which is that you can you
can try to account for this with an extra, divided by two. So, now you're basically
adjusting for the fact that half the neurons get killed. And so you're kind of equivalent input has actually half this number of input, and so you just add this
divided by two factor in, this works much better, and you can see that the
distributions are pretty good throughout all layers of the network. And so in practice this is
been really important actually, for training these types of little things, to a really pay attention
to how your weights are, make a big difference. And so for example,
you'll see in some papers that this actually is the
difference between the network even training at all and performing well versus nothing happening. So, proper initialization is still an active area of research. And so if you're interested in this, you can look at a lot of
these papers and resources. A good general rule of thumb is basically use the Xavier
Initialization to start with, and then you can also think about some of these other kinds of methods. And so now we're going to talk
about a related idea to this, so this idea of wanting
to keep activations in a gaussian range that we want. Right, and so this idea behind what we're going to call
batch normalization is, okay we want unit gaussian activations. Let's just make them that way. Let's just force them to be that way. And so how does this work? So, let's consider a batch
of activations at some layer. And so now we have all of
our activations coming out. If we want to make this unit gaussian, we actually can just do
this empirically, right. We can take the mean of the
batch that we have so far of the current batch, and we
can just and the variance, and we can just normalize by this. Right, and so basically, instead of with weight initialization, we're setting this at
the start of training so that we try and get it into a good spot that we can have unit
gaussians at every layer, and hopefully during training
this will preserve this. Now we're going to
explicitly make that happen on every forward pass through the network. We're going to make this
happen functionally, and basically by normalizing by the mean and the variance of each neuron, we look at all of the
inputs coming into it and calculate the mean and
variance for that batch and normalize it by it. And the thing is that this is a, this is just a differentiable
function right? If we have our mean and
our variance as constants, this is just a sequence of
computational operations that we can differentiate and
do back prop through this. Okay, so just as I was
saying earlier right, if we look at our input
data, and we think of this as we have N training examples
in our current batch, and then each batch has dimension D, we're going to the
compute the empirical mean and variance independently
for each dimension, so each basically feature element, and we compute this across our batch, our current mini-batch that we have and we normalize by this. And so this is usually
inserted after fully connected or convolutional layers. We saw that would we were multiplying by W in these layers, which
we do over and over again, then we can have this bad
scaling effect with each one. And so this basically is
able to undo this effect. Right, and since we're basically
just scaling by the inputs connected to each neuron, each activation, we can apply this the same
way to fully connected convolutional layers, and
the only difference is that, with convolutional layers,
we want to normalize not just across all the training examples, and independently for each
each feature dimension, but we actually want to normalize jointly across both all the feature dimensions, all the spatial locations that we have in our activation map, as well as all of the training examples. And we do this, because we want to obey
the convolutional property, and we want nearby locations to be normalized the same way, right? And so with a convolutional layer, we're basically going to have a one mean and one standard deviation, per activation map that that we have, and we're going to normalize by this across all of the examples in the batch. And so this is something that you guys are going to implement
in your next homework. And so, all of these details
are explained very clearly in this paper from 2015. And so on this is a very useful, useful technique that you
want to use a lot in practice. You want to have these
batch normalization layers. And so you should read this paper. Go through all of the derivations, and then also go through the derivations of how to compute the
gradients with given these, this normalization operation. Okay, so one thing that I just
want to point out is that, it's not clear that, you know, we're doing this batch normalization after every fully connected layer, and it's not clear that
we necessarily want a unit gaussian input to
these tanh nonlinearities, because what this is doing
is this is constraining you to the linear regime of this nonlinearity, and we're not actually, you're
trying to basically say, let's not have any of this saturation, but maybe a little bit
of this is good, right? You you want to be able to control what's, how much saturation that you want to have. And so what, the way that we address this when we're doing batch normalization is that we have our normalization operation, but then after that we
have this additional squashing and scaling operation. So, we do our normalization. Then we're going to scale
by some constant gamma, and then shift by another factor of beta. Right, and so what this
actually does is that this allows you to be able to
recover the identity function if you wanted to. So, if the network wanted to, it could learn your scaling factor gamma to be just your variance. It could learn your beta to be your mean, and in this case you can
recover the identity mapping, as if you didn't have batch normalization. And so now you have the flexibility of doing kind of everything in between and making your the network learning how to make your tanh
more or less saturated, and how much to do so in order to have, to have good training. Okay, so just to sort of summarize the batch normalization idea. Right, so given our inputs, we're going to compute
our mini-batch mean. So, we do this for every
mini-batch that's coming in. We compute our variance. We normalize by the mean and variance, and we have this additional
scaling and shifting factor. And so this improves gradient
flow through the network. it's also more robust as a result. It works for more range of learning rates, and different kinds of initialization, so people have seen that once you put batch normalization in, and it's just easier to train, and so that's why you should do this. And then also when one thing
that I just want to point out is that you can also
think of this as in a way also doing some regularization. Right and so, because now
at the output of each layer, each of these activations,
each of these outputs, is an output of both your input X, as well as the other examples in the batch that it happens to be sampled with, right, because you're going to
normalize each input data by the empirical mean over that batch. So because of that,
it's no longer producing deterministic values for
a given training example, and it's tying all of these
inputs in a batch together. And so this basically, because
it's no longer deterministic, kind of jitters your
representation of X a little bit, and in a sense, gives some
sort of regularization effect. Yeah, question? [student speaking off camera] The question is gamma and
beta are learned parameters, and yes that's the case. [student speaking off mic] Yeah, so the question is
why do we want to learn this gamma and beta to be able to learn the identity function back, and the reason is because you want to give it the flexibility. Right, so what batch
normalization is doing, is it's forcing our data to
become this unit gaussian, our inputs to be unit gaussian, but even though in general
this is a good idea, it's not always that this is
exactly the best thing to do. And we saw in particular
for something like a tanh, you might want to control some degree of saturation that you have. And so what this does is it
gives you the flexibility of doing this exact like
unit gaussian normalization, if it wants to, but
also learning that maybe in this particular part of the network, maybe that's not the best thing to do. Maybe we want something
still in this general idea, but slightly different right,
slightly scaled or shifted. And so these parameters just
give it that extra flexibility to learn that if it wants to. And then yeah, if the the best thing to do is just batch normalization
then it'll learn the right parameters for that. Yeah? [student speaking off mic] Yeah, so basically each neuron output. So, we have output of a
fully connected layer. We have W times X. and so we have the values
of each of these outputs, and then we're going to apply batch normalization separately
to each of these neurons. Question? [student speaking off mic] Yeah, so the question is that for things like reinforcement learning, you might have a really small batch size. How do you deal with this? So in practice, I guess batch
normalization has been used a lot for like for standard
convolutional neural networks, and there's actually papers
on how do we want to do normalization for different
kinds of recurrent networks, or you know some of these networks that might also be in
reinforcement learning. And so there's different considerations that you might want to think of there. And this is still an
active area of research. There's papers on this and we might also talk about some of this more later, but for a typical
convolutional neural network this generally works fine. And then if you have a smaller batch size, maybe this becomes a
little bit less accurate, but you still get kind of the same effect. And you know it's possible also that you could design
your mean and variance to be computed maybe over
more examples, right, and I think in practice
usually it's just okay, so you don't see this too much, but this is something
that maybe could help if that was a problem. Yeah, question? [student speaking off mic] So the question, so the question is, if we force
the inputs to be gaussian, do we lose the structure? So, no in a sense that
you can think of like, if you had all your features distributed as a gaussian for example, even if you were just
doing data pre-processing, this gaussian is not
losing you any structure. All the, it's just shifting and scaling your data into a regime, that works well for the operations that you're going to perform on it. In convolutional layers,
you do have some structure, that you want to preserve
spatially, right. You want, like if you look
at your activation maps, you want them to relatively
all make sense to each other. So, in this case you do want to take that into consideration. And so now, we're going to normalize, find one mean for the
entire activation map, so we only find the
empirical mean and variance over training examples. And so that's something that you'll be doing in your homework, and also explained in the paper as well. So, you should refer to that. Yes. [student speaking off mic] So the question is, are
we normalizing the weight so that they become gaussian. So, if I understand
your question correctly, then the answer is, we're normalizing the
inputs to each layer, so we're not changing the
weights in this process. [student speaking off mic] Yeah, so the question is,
once we subtract by the mean and divide by the standard deviation, does this become gaussian,
and the answer is yes. So, if you think about the
operations that are happening, basically you're shifting
by the mean, right. And so this shift up to be zero-centered, and then you're scaling
by the standard deviation. This now transforms this
into a unit gaussian. And so if you want to look more into that, I think you can look at, there's a lot of machine
learning explanations that go into exactly what this, visualizing with this operation is doing, but yeah this basically takes your data and turns it into a gaussian distribution. Okay, so yeah question? [student speaking off mic] Uh-huh. So the question is, if we're going to be
doing the shift and scale, and learning these is the
batch normalization redundant, because you could recover
the identity mapping? So in the case that the network learns that identity mapping is always the best, and it learns these parameters, the yeah, there would be no
point for batch normalization, but in practice this doesn't happen. So in practice, we will
learn this gamma and beta. That's not the same as a identity mapping. So, it will shift and
scale by some amount, but not the amount that's going to give
you an identity mapping. And so what you get is you still get this batch
normalization effect. Right, so having this
identity mapping there, I'm only putting this here
to say that in the extreme, it could learn the identity mapping, but in practice it doesn't. Yeah, question. [student speaking off mic] Yeah. [student speaking off mic] Oh, right, right. Yeah, yeah sorry, I was
not clear about this, but yeah I think this is related to the other question earlier, that yeah when we're doing this we're actually getting zero
mean and unit gaussian, which put this into a nice shape, but it doesn't have to
actually be a gaussian. So yeah, I mean ideally, if we're looking at like
inputs coming in, as you know, sort of approximately gaussian, we would like it to have
this kind of effect, but yeah, in practice
it doesn't have to be. Okay, so ... Okay, so the last thing I just
want to mention about this is that, so at test time, the
batch normalization layer, we now take the empirical mean and variance from the training data. So, we don't re-compute this at test time. We just estimate this at training time, for example using running averages, and then we're going to
use this as at test time. So, we're just going to scale by that. Okay, so now I'm going to move on to babysitting the learning process. Right, so now we've defined
our network architecture, and we'll talk about how
do we monitor training, and how do we adjust
hyperparameters as we go, to get good learning results? So as always, so the
first step we want to do, is we want to pre-process the data. Right, so we want to zero mean the data as we talked about earlier. Then we want to choose the architecture, and so here we are starting
with one hidden layer of 50 neurons, for example, but we've basically we
can pick any architecture that we want to start with. And then the first
thing that we want to do is we initialize our network. We do a forward pass through it, and we want to make sure
that our loss is reasonable. So, we talked about this
several lectures ago, where we have a basically a, let's say we have a Softmax
classifier that we have here. We know what our loss should be, when our weights are small, and we have generally
a diffuse distribution. Then we want it to be, the
Softmax classifier loss is going to be your
negative log likelihood, which if we have 10 classes, it'll be something like
negative log of one over 10, which here is around 2.3,
and so we want to make sure that our loss is what we expect it to be. So, this is a good
sanity check that we want to always, always do. So, now once we've seen that
our original loss is good, now we want to, so first we want to do this
having zero regularization, right. So, when we disable the regularization, now our only loss term is this data loss, which is going to give 2.3 here. And so here, now we want to
crank up the regularization, and when we do that, we want
to see that our loss goes up, because we've added this
additional regularization term. So, this is a good next step that you can do for your sanity check. And then, now we can start training. So, now we start trying to train. What we do is, a good way to do this is to start up with a
very small amount of data, because if you have just
a very small training set, you should be able to
over fit this very well and get very good training loss on here. And so in this case we want to turn off our regularization again, and just see if we can make
the loss go down to zero. And so we can see how
our loss is changing, as we have all these epochs. We compute our loss at each epoch, and we want to see this go
all the way down to zero. Right, and here we can see that also our training accuracy is going all the way up to one,
and this makes sense right. If you have a very small number of data, you should be able to
over fit this perfectly. Okay, so now once you've done that, these are all sanity checks. Now you can start really trying to train. So, now you can take
your full training data, and now start with a small
amount of regularization, and let's first figure out
what's a good learning rate. So, learning rate is one of the most important hyperparameters, and it's something that
you want to adjust first. So, you want to try some
value of learning rate. and here I've tried one E negative six, and you can see that the
loss is barely changing. Right, and so the reason
this is barely changing is usually because your
learning rate is too small. So when it's too small, your gradient updates are not big enough, and your cost is basically about the same. Okay, so, one thing that I want to point out here, is that we can notice that even though our loss with barely changing, the training and the validation accuracy jumped up to 20% very quickly. And so does anyone have any idea for why this might be the case? Why, so remember we
have a Softmax function, and our loss didn't really change, but our accuracy improved a lot. Okay, so the reason for this is that here the probabilities
are still pretty diffuse, so our loss term is still pretty similar, but when we shift all
of these probabilities slightly in the right direction, because we're learning right? Our weights are changing
the right direction. Now the accuracy all of a sudden can jump, because we're taking the
maximum correct value, and so we're going to get
a big jump in accuracy, even though our loss is
still relatively diffuse. Okay, so now if we try
another learning rate, now here I'm jumping in the other extreme, picking a very big learning
rate, one E negative six. What's happening is that our
cost is now giving us NaNs. And, when you have NaNs,
what this usually means is that basically your cost exploded. And so, the reason for
that is typically that your learning rate was too high. So, then you can adjust your
learning rate down again. Here I can see that we're trying three E to the negative three. The cost is still exploding. So, usually this, the rough
range for learning rates that we want to look at is between one E negative
three, and one E negative five. And, this is the rough
range that we want to be cross-validating in between. So, you want to try out
values in this range, and depending on whether
your loss is too slow, or too small, or whether it's too large, adjust it based on this. And so how do we exactly
pick these hyperparameters? Do hyperparameter optimization, and pick the best values of
all of these hyperparameters? So, the strategy that we're going to use is for any hyperparameter
for example learning rate, is to do cross-validation. So, cross-validation is
training on your training set, and then evaluating on a validation set. How well do this hyperparameter do? Something that you guys have already done in your assignment. And so typically we want
to do this in stages. And so, we can do first of course stage, where we pick values
pretty spread out apart, and then we learn for only a few epochs. And with only a few epochs. you can already get a pretty good sense of which hyperparameters, which values are good or not, right. You can quickly see that it's a NaN, or you can see that nothing is happening, and you can adjust accordingly. So, typically once you do that, then you can see what's
sort of a pretty good range, and the range that you want to now do finer sampling of values in. And so, this is the second stage, where now you might want to
run this for a longer time, and do a finer search over that region. And one tip for detecting
explosions like NaNs, you can have in your training loop, right sample some hyperparameter, start training, and then look at your cost at every iteration or every epoch. And if you ever get a cost that's much larger than
your original cost, so for example, something like
three times original cost, then you know that this is not heading in the right direction. Right, it's getting
very big, very quickly, and you can just break out of your loop, stop this this hyperparameter choice and pick something else. Alright, so an example of this, let's say here we want to run now course search for five epochs. This is a similar network that we were talking about earlier, and what we can do is
we can see all of these validation accuracy that we're getting. And I've put in, highlighted in red the ones that gives better values. And so these are going to be regions that we're going to look
into in more detail. And one thing to note is that it's usually better to
optimize in log space. And so here instead of
sampling, I'd say uniformly between you know one E to
the negative 0.01 and 100, you're going to actually do
10 to the power of some range. Right, and this is because the learning rate is multiplying
your gradient update. And so it has these
multiplicative effects, and so it makes more sense to consider a range of learning
rates that are multiplied or divided by some value,
rather than uniformly sampled. So, you want to be dealing with orders of some magnitude here. Okay, so once you find that, you can then adjust your range. Right get in this case, we
have a range of you know, maybe of 10 to the negative four, right, to 10 to the zero power. This is a good range that
we want to narrow down into. And so we can do this again, and here we can see that we're getting a relatively good accuracy of 53%. And so this means we're
headed in the right direction. The one thing that I
want to point out is that here we actually have a problem. And so the problem is that we can see that our best accuracy here has a learning rate that's about, you know, all of our good learning rates are in this E to the negative four range. Right, and since the learning
rate that we specified was going from 10 to the
negative four to 10 to the zero, that means that all the
good learning rates, were at the edge of the
range that we were sampling. And so this is bad, because this means that
we might not have explored our space sufficiently, right. We might actually want to go
to 10 to the negative five, or 10 to the negative six. There might be still better ranges if we continue shifting down. So, you want to make sure that your range kind of has the good values
somewhere in the middle, or somewhere where you get
a sense that you've hit, you've explored your range fully. Okay, and so another thing is that we can sample all of our
different hyperparameters, using a kind of grid search, right. We can sample for a fixed
set of combinations, a fixed set of values
for each hyperparameter. Sample in a grid manner
over all of these values, but in practice it's
actually better to sample from a random layout,
so sampling random value of each hyperparameter in a range. And so what you'll get instead is we'll have these two
hyper parameters here that we want to sample from. You'll get samples that look
like this right side instead. And the reason for this is
that if a function is really sort of more of a function
of one variable than another, which is usually true. Usually we have little bit more, a lower effective dimensionality
than we actually have. Then you're going to get many more samples of the important variable that you have. You're going to be able to see this shape in this green function
that I've drawn on top, showing where the good values are, compared to if you just did a grid layout where we were only able to
sample three values here, and you've missed where
were the good regions. Right, and so basically
we'll get much more useful signal overall
since we have more samples of different values of
the important variable. And so, hyperparameters to play with, we've talked about learning rate, things like different
types of decay schedules, update types, regularization, also your network architecture, so the number of hidden units, the depth, all of these are hyperparameters
that you can optimize over. And we've talked about some of these, but we'll keep talking about more of these in the next lecture. And so you can think of
this as kind of, you know, if you're basically tuning
all the knobs right, of some turntable where you're, you're a neural networks practitioner. You can think of the music that's output is the loss function that you want, and you want to adjust
everything appropriately to get the kind of output that you want. Alright, so it's really kind
of an art that you're doing. And in practice, you're going to do a lot of hyperparameter optimization, a lot of cross validation. And so you know, in order to get numbers, people will run cross validation over tons of hyperparameters,
monitor all of them, see which ones are doing better, which ones are doing worse. Here we have all these loss curves. Pick the right ones, readjust, and keep going through this process. And so as I mentioned earlier, as you're monitoring each
of these loss curves, learning rate is an important one, but you'll get a sense for
how different learning rates, which learning rates are good and bad. So you'll see that if you have
a very high exploding one, right, this is your loss explodes, then your learning rate is too high. If it's too kind of linear and too flat, you'll see that it's too low,
it's not changing enough. And if you get something that looks like there's a steep
change, but then a plateau, this is also an indicator
of it being maybe too high, because in this case, you're
taking too large jumps, and you're not able to settle
well into your local optimum. And so a good learning rate usually ends up looking
something like this, where you have a relatively steep curve, but then it's continuing to go down, and then you might keep adjusting your learning rate from there. And so this is something that
you'll see through practice. Okay and just, I think
we're very close to the end, so just one last thing
that I want to point out is than in case you ever see
learning rate loss curves, where it's ... So if you ever see loss curves
where it's flat for a while, and then starts training all of a sudden, a potential reason could
be bad initialization. So in this case, your gradients
are not really flowing too well the beginning, so
nothing's really learning, and then at some point, it just happens to
adjust in the right way, such that it tips over and
things just start training right? And so there's a lot of
experience at looking at these and see what's wrong that
you'll get over time. And so you'll usually want to monitor and visualize your accuracy. If you have a big gap between
your training accuracy and your validation accuracy, it usually means that you
might have overfitting and you might want to increase your regularization strength. If you have no gap, you might want to increase
your model capacity, because you haven't overfit yet. You could potentially increase it more. And in general, we also
want to track the updates, the ratio of our weight updates
to our weight magnitudes. We can just take the norm of our parameters that we have to get a sense for how large they are, and when we have our update size, we can also take the norm of that, get a sense for how large that is, and we want this ratio to
be somewhere around 0.001. There's a lot of variance in this range, so you don't have to be exactly on this, but it's just this sense of you don't want your updates to be too
large compared to your value or too small, right? You don't want to dominate
or to have no effect. And so this is just
something that can help debug what might be a problem. Okay, so in summary, today we've looked at
activation functions, data preprocessing, weight initialization, batch norm, babysitting
the learning process,",https://cs231n.github.io/neural-networks-2/,"




CS231n Convolutional Neural Networks for Visual Recognition









 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1\*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-46895817-2', 'auto');
 ga('send', 'pageview');
 



addBackToTop({
 backgroundColor: '#fff',
 innerHTML: 'Back to Top',
 textColor: '#333'
 })

 #back-to-top {
 border: 1px solid #ccc;
 border-radius: 0;
 font-family: sans-serif;
 font-size: 14px;
 width: 100px;
 text-align: center;
 line-height: 30px;
 height: 30px;
 }
 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io)
[Course Website](http://cs231n.stanford.edu/)





# 




Table of Contents:


* [Setting up the data and the model](#intro)
	+ [Data Preprocessing](#datapre)
	+ [Weight Initialization](#init)
	+ [Batch Normalization](#batchnorm)
	+ [Regularization](#reg) (L2/L1/Maxnorm/Dropout)
* [Loss functions](#losses)
* [Summary](#summary)



## Setting up the data and the model


In the previous section we introduced a model of a Neuron, which computes a dot
product following a non-linearity, and Neural Networks that arrange neurons into
layers. Together, these choices define the new form of the **score function**,
which we have extended from the simple linear mapping that we have seen in the
Linear Classification section. In particular, a Neural Network performs a
sequence of linear mappings with interwoven non-linearities. In this section we
will discuss additional design choices regarding data preprocessing, weight
initialization, and loss functions.



### Data Preprocessing


There are three common forms of data preprocessing a data matrix `X`, where we
will assume that `X` is of size `[N x D]` (`N` is the number of data, `D` is
their dimensionality).


**Mean subtraction** is the most common form of preprocessing. It involves
subtracting the mean across every individual *feature* in the data, and has the
geometric interpretation of centering the cloud of data around the origin along
every dimension. In numpy, this operation would be implemented as: `X -=
np.mean(X, axis = 0)`. With images specifically, for convenience it can be
common to subtract a single value from all pixels (e.g. `X -= np.mean(X)`), or
to do so separately across the three color channels.


**Normalization** refers to normalizing the data dimensions so that they are of
approximately the same scale. There are two common ways of achieving this
normalization. One is to divide each dimension by its standard deviation, once
it has been zero-centered: (`X /= np.std(X, axis = 0)`). Another form of this
preprocessing normalizes each dimension so that the min and max along the
dimension is -1 and 1 respectively. It only makes sense to apply this
preprocessing if you have a reason to believe that different input features have
different scales (or units), but they should be of approximately equal
importance to the learning algorithm. In case of images, the relative scales of
pixels are already approximately equal (and in range from 0 to 255), so it is
not strictly necessary to perform this additional preprocessing step.



![](/assets/nn2/prepro1.jpeg)
Common data preprocessing pipeline. **Left**: Original toy, 2-dimensional input data. **Middle**: The data is zero-centered by subtracting the mean in each dimension. The data cloud is now centered around the origin. **Right**: Each dimension is additionally scaled by its standard deviation. The red lines indicate the extent of the data - they are of unequal length in the middle, but of equal length on the right.

**PCA and Whitening** is another form of preprocessing. In this process, the
data is first centered as described above. Then, we can compute the covariance
matrix that tells us about the correlation structure in the data:



```python
# Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix

```

The (i,j) element of the data covariance matrix contains the *covariance*
between i-th and j-th dimension of the data. In particular, the diagonal of this
matrix contains the variances. Furthermore, the covariance matrix is symmetric
and [positive
semi-definite](http://en.wikipedia.org/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices).
We can compute the SVD factorization of the data covariance matrix:



```python
U,S,V = np.linalg.svd(cov)

```

where the columns of `U` are the eigenvectors and `S` is a 1-D array of the
singular values. To decorrelate the data, we project the original (but
zero-centered) data into the eigenbasis:



```python
Xrot = np.dot(X, U) # decorrelate the data

```

Notice that the columns of `U` are a set of orthonormal vectors (norm of 1, and
orthogonal to each other), so they can be regarded as basis vectors. The
projection therefore corresponds to a rotation of the data in `X` so that the
new axes are the eigenvectors. If we were to compute the covariance matrix of
`Xrot`, we would see that it is now diagonal. A nice property of `np.linalg.svd`
is that in its returned value `U`, the eigenvector columns are sorted by their
eigenvalues. We can use this to reduce the dimensionality of the data by only
using the top few eigenvectors, and discarding the dimensions along which the
data has no variance. This is also sometimes referred to as [Principal Component
Analysis (PCA)](http://en.wikipedia.org/wiki/Principal_component_analysis)
dimensionality reduction:



```python
Xrot\_reduced = np.dot(X, U[:,:100]) # Xrot\_reduced becomes [N x 100]

```

After this operation, we would have reduced the original dataset of size [N x D]
to one of size [N x 100], keeping the 100 dimensions of the data that contain
the most variance. It is very often the case that you can get very good
performance by training linear classifiers or neural networks on the PCA-reduced
datasets, obtaining savings in both space and time.


The last transformation you may see in practice is **whitening**. The whitening
operation takes the data in the eigenbasis and divides every dimension by the
eigenvalue to normalize the scale. The geometric interpretation of this
transformation is that if the input data is a multivariable gaussian, then the
whitened data will be a gaussian with zero mean and identity covariance matrix.
This step would take the form:



```python
# whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)

```

*Warning: Exaggerating noise.* Note that we’re adding 1e-5 (or a small constant)
to prevent division by zero. One weakness of this transformation is that it can
greatly exaggerate the noise in the data, since it stretches all dimensions
(including the irrelevant dimensions of tiny variance that are mostly noise) to
be of equal size in the input. This can in practice be mitigated by stronger
smoothing (i.e. increasing 1e-5 to be a larger number).



![](/assets/nn2/prepro2.jpeg)
PCA / Whitening. **Left**: Original toy, 2-dimensional input data. **Middle**: After performing PCA. The data is centered at zero and then rotated into the eigenbasis of the data covariance matrix. This decorrelates the data (the covariance matrix becomes diagonal). **Right**: Each dimension is additionally scaled by the eigenvalues, transforming the data covariance matrix into the identity matrix. Geometrically, this corresponds to stretching and squeezing the data into an isotropic gaussian blob.

We can also try to visualize these transformations with CIFAR-10 images. The
training set of CIFAR-10 is of size 50,000 x 3072, where every image is
stretched out into a 3072-dimensional row vector. We can then compute the [3072
x 3072] covariance matrix and compute its SVD decomposition (which can be
relatively expensive). What do the computed eigenvectors look like visually? An
image might help:



![](/assets/nn2/cifar10pca.jpeg)
**Left:** An example set of 49 images. **2nd from Left:** The top 144 out of 3072 eigenvectors. The top eigenvectors account for most of the variance in the data, and we can see that they correspond to lower frequencies in the images. **2nd from Right:** The 49 images reduced with PCA, using the 144 eigenvectors shown here. That is, instead of expressing every image as a 3072-dimensional vector where each element is the brightness of a particular pixel at some location and channel, every image above is only represented with a 144-dimensional vector, where each element measures how much of each eigenvector adds up to make up the image. In order to visualize what image information has been retained in the 144 numbers, we must rotate back into the ""pixel"" basis of 3072 numbers. Since U is a rotation, this can be achieved by multiplying by U.transpose()[:144,:], and then visualizing the resulting 3072 numbers as the image. You can see that the images are slightly blurrier, reflecting the fact that the top eigenvectors capture lower frequencies. However, most of the information is still preserved. **Right:** Visualization of the ""white"" representation, where the variance along every one of the 144 dimensions is squashed to equal length. Here, the whitened 144 numbers are rotated back to image pixel basis by multiplying by U.transpose()[:144,:]. The lower frequencies (which accounted for most variance) are now negligible, while the higher frequencies (which account for relatively little variance originally) become exaggerated.

**In practice.** We mention PCA/Whitening in these notes for completeness, but
these transformations are not used with Convolutional Networks. However, it is
very important to zero-center the data, and it is common to see normalization of
every pixel as well.


**Common pitfall**. An important point to make about the preprocessing is that
any preprocessing statistics (e.g. the data mean) must only be computed on the
training data, and then applied to the validation / test data. E.g. computing
the mean and subtracting it from every image across the entire dataset and then
splitting the data into train/val/test splits would be a mistake. Instead, the
mean must be computed only over the training data and then subtracted equally
from all splits (train/val/test).



### Weight Initialization


We have seen how to construct a Neural Network architecture, and how to
preprocess the data. Before we can begin to train the network we have to
initialize its parameters.


**Pitfall: all zero initialization**. Lets start with what we should not do.
Note that we do not know what the final value of every weight should be in the
trained network, but with proper data normalization it is reasonable to assume
that approximately half of the weights will be positive and half of them will be
negative. A reasonable-sounding idea then might be to set all the initial
weights to zero, which we expect to be the “best guess” in expectation. This
turns out to be a mistake, because if every neuron in the network computes the
same output, then they will also all compute the same gradients during
backpropagation and undergo the exact same parameter updates. In other words,
there is no source of asymmetry between neurons if their weights are initialized
to be the same.


**Small random numbers**. Therefore, we still want the weights to be very close
to zero, but as we have argued above, not identically zero. As a solution, it is
common to initialize the weights of the neurons to small numbers and refer to
doing so as *symmetry breaking*. The idea is that the neurons are all random and
unique in the beginning, so they will compute distinct updates and integrate
themselves as diverse parts of the full network. The implementation for one
weight matrix might look like `W = 0.01* np.random.randn(D,H)`, where `randn`
samples from a zero mean, unit standard deviation gaussian. With this
formulation, every neuron’s weight vector is initialized as a random vector
sampled from a multi-dimensional gaussian, so the neurons point in random
direction in the input space. It is also possible to use small numbers drawn
from a uniform distribution, but this seems to have relatively little impact on
the final performance in practice.


*Warning*: It’s not necessarily the case that smaller numbers will work strictly
better. For example, a Neural Network layer that has very small weights will
during backpropagation compute very small gradients on its data (since this
gradient is proportional to the value of the weights). This could greatly
diminish the “gradient signal” flowing backward through a network, and could
become a concern for deep networks.


**Calibrating the variances with 1/sqrt(n)**. One problem with the above
suggestion is that the distribution of the outputs from a randomly initialized
neuron has a variance that grows with the number of inputs. It turns out that we
can normalize the variance of each neuron’s output to 1 by scaling its weight
vector by the square root of its *fan-in* (i.e. its number of inputs). That is,
the recommended heuristic is to initialize each neuron’s weight vector as: `w =
np.random.randn(n) / sqrt(n)`, where `n` is the number of its inputs. This
ensures that all neurons in the network initially have approximately the same
output distribution and empirically improves the rate of convergence.


The sketch of the derivation is as follows: Consider the inner product \(s =
\sum\_i^n w\_i x\_i\) between the weights \(w\) and input \(x\), which gives the
raw activation of a neuron before the non-linearity. We can examine the variance
of \(s\):



\[\begin{align}
\text{Var}(s) &= \text{Var}(\sum\_i^n w\_ix\_i) \\\\
&= \sum\_i^n \text{Var}(w\_ix\_i) \\\\
&= \sum\_i^n [E(w\_i)]^2\text{Var}(x\_i) + [E(x\_i)]^2\text{Var}(w\_i) + \text{Var}(x\_i)\text{Var}(w\_i) \\\\
&= \sum\_i^n \text{Var}(x\_i)\text{Var}(w\_i) \\\\
&= \left( n \text{Var}(w) \right) \text{Var}(x)
\end{align}\]

where in the first 2 steps we have used [properties of
variance](http://en.wikipedia.org/wiki/Variance). In third step we assumed zero
mean inputs and weights, so \(E[x\_i] = E[w\_i] = 0\). Note that this is not
generally the case: For example ReLU units will have a positive mean. In the
last step we assumed that all \(w\_i, x\_i\) are identically distributed. From
this derivation we can see that if we want \(s\) to have the same variance as
all of its inputs \(x\), then during initialization we should make sure that the
variance of every weight \(w\) is \(1/n\). And since \(\text{Var}(aX) =
a^2\text{Var}(X)\) for a random variable \(X\) and a scalar \(a\), this implies
that we should draw from unit gaussian and then scale it by \(a = \sqrt{1/n}\),
to make its variance \(1/n\). This gives the initialization `w =
np.random.randn(n) / sqrt(n)`.


A similar analysis is carried out in [Understanding the difficulty of training
deep feedforward neural
networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf) by
Glorot et al. In this paper, the authors end up recommending an initialization
of the form \( \text{Var}(w) = 2/(n\_{in} + n\_{out}) \) where \(n\_{in},
n\_{out}\) are the number of units in the previous layer and the next layer.
This is based on a compromise and an equivalent analysis of the backpropagated
gradients. A more recent paper on this topic, [Delving Deep into Rectifiers:
Surpassing Human-Level Performance on ImageNet
Classification](http://arxiv-web3.library.cornell.edu/abs/1502.01852) by He et
al., derives an initialization specifically for ReLU neurons, reaching the
conclusion that the variance of neurons in the network should be \(2.0/n\). This
gives the initialization `w = np.random.randn(n) * sqrt(2.0/n)`, and is the
current recommendation for use in practice in the specific case of neural
networks with ReLU neurons.


**Sparse initialization**. Another way to address the uncalibrated variances
problem is to set all weight matrices to zero, but to break symmetry every
neuron is randomly connected (with weights sampled from a small gaussian as
above) to a fixed number of neurons below it. A typical number of neurons to
connect to may be as small as 10.


**Initializing the biases**. It is possible and common to initialize the biases
to be zero, since the asymmetry breaking is provided by the small random numbers
in the weights. For ReLU non-linearities, some people like to use small constant
value such as 0.01 for all biases because this ensures that all ReLU units fire
in the beginning and therefore obtain and propagate some gradient. However, it
is not clear if this provides a consistent improvement (in fact some results
seem to indicate that this performs worse) and it is more common to simply use 0
bias initialization.


**In practice**, the current recommendation is to use ReLU units and use the `w
= np.random.randn(n) * sqrt(2.0/n)`, as discussed in [He et
al.](http://arxiv-web3.library.cornell.edu/abs/1502.01852).



**Batch Normalization**. A recently developed technique by Ioffe and Szegedy
called [Batch Normalization](http://arxiv.org/abs/1502.03167) alleviates a lot
of headaches with properly initializing neural networks by explicitly forcing
the activations throughout a network to take on a unit gaussian distribution at
the beginning of the training. The core observation is that this is possible
because normalization is a simple differentiable operation. In the
implementation, applying this technique usually amounts to insert the BatchNorm
layer immediately after fully connected layers (or convolutional layers, as
we’ll soon see), and before non-linearities. We do not expand on this technique
here because it is well described in the linked paper, but note that it has
become a very common practice to use Batch Normalization in neural networks. In
practice networks that use Batch Normalization are significantly more robust to
bad initialization. Additionally, batch normalization can be interpreted as
doing preprocessing at every layer of the network, but integrated into the
network itself in a differentiable manner. Neat!



### Regularization


There are several ways of controlling the capacity of Neural Networks to prevent
overfitting:


**L2 regularization** is perhaps the most common form of regularization. It can
be implemented by penalizing the squared magnitude of all parameters directly in
the objective. That is, for every weight \(w\) in the network, we add the term
\(\frac{1}{2} \lambda w^2\) to the objective, where \(\lambda\) is the
regularization strength. It is common to see the factor of \(\frac{1}{2}\) in
front because then the gradient of this term with respect to the parameter \(w\)
is simply \(\lambda w\) instead of \(2 \lambda w\). The L2 regularization has
the intuitive interpretation of heavily penalizing peaky weight vectors and
preferring diffuse weight vectors. As we discussed in the Linear Classification
section, due to multiplicative interactions between weights and inputs this has
the appealing property of encouraging the network to use all of its inputs a
little rather than some of its inputs a lot. Lastly, notice that during gradient
descent parameter update, using the L2 regularization ultimately means that
every weight is decayed linearly: `W += -lambda * W` towards zero.


**L1 regularization** is another relatively common form of regularization, where
for each weight \(w\) we add the term \(\lambda \mid w \mid\) to the objective.
It is possible to combine the L1 regularization with the L2 regularization:
\(\lambda\_1 \mid w \mid + \lambda\_2 w^2\) (this is called [Elastic net
regularization](http://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&%20Hastie.pdf)).
The L1 regularization has the intriguing property that it leads the weight
vectors to become sparse during optimization (i.e. very close to exactly zero).
In other words, neurons with L1 regularization end up using only a sparse subset
of their most important inputs and become nearly invariant to the “noisy”
inputs. In comparison, final weight vectors from L2 regularization are usually
diffuse, small numbers. In practice, if you are not concerned with explicit
feature selection, L2 regularization can be expected to give superior
performance over L1.


**Max norm constraints**. Another form of regularization is to enforce an
absolute upper bound on the magnitude of the weight vector for every neuron and
use projected gradient descent to enforce the constraint. In practice, this
corresponds to performing the parameter update as normal, and then enforcing the
constraint by clamping the weight vector \(\vec{w}\) of every neuron to satisfy
\(\Vert \vec{w} \Vert\_2 < c\). Typical values of \(c\) are on orders of 3 or 4.
Some people report improvements when using this form of regularization. One of
its appealing properties is that network cannot “explode” even when the learning
rates are set too high because the updates are always bounded.


**Dropout** is an extremely effective, simple and recently introduced
regularization technique by Srivastava et al. in [Dropout: A Simple Way to
Prevent Neural Networks from
Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) (pdf)
that complements the other methods (L1, L2, maxnorm). While training, dropout is
implemented by only keeping a neuron active with some probability \(p\) (a
hyperparameter), or setting it to zero otherwise.



![](/assets/nn2/dropout.jpeg)
Figure taken from the [Dropout paper](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) that illustrates the idea. During training, Dropout can be interpreted as sampling a Neural Network within the full Neural Network, and only updating the parameters of the sampled network based on the input data. (However, the exponential number of possible sampled networks are not independent because they share the parameters.) During testing there is no dropout applied, with the interpretation of evaluating an averaged prediction across the exponentially-sized ensemble of all sub-networks (more about ensembles in the next section).

Vanilla dropout in an example 3-layer Neural Network would be implemented as
follows:



```python
"""""" Vanilla Dropout: Not recommended implementation (see notes below) """"""

p = 0.5 # probability of keeping a unit active. higher = less dropout

def train\_step(X):
  """""" X contains the data """"""
  
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = np.random.rand(\*H1.shape) < p # first dropout mask
  H1 \*= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = np.random.rand(\*H2.shape) < p # second dropout mask
  H2 \*= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)
  
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) \* p # NOTE: scale the activations
  H2 = np.maximum(0, np.dot(W2, H1) + b2) \* p # NOTE: scale the activations
  out = np.dot(W3, H2) + b3

```

In the code above, inside the `train_step` function we have performed dropout
twice: on the first hidden layer and on the second hidden layer. It is also
possible to perform dropout right on the input layer, in which case we would
also create a binary mask for the input `X`. The backward pass remains
unchanged, but of course has to take into account the generated masks `U1,U2`.


Crucially, note that in the `predict` function we are not dropping anymore, but
we are performing a scaling of both hidden layer outputs by \(p\). This is
important because at test time all neurons see all their inputs, so we want the
outputs of neurons at test time to be identical to their expected outputs at
training time. For example, in case of \(p = 0.5\), the neurons must halve their
outputs at test time to have the same output as they had during training time
(in expectation). To see this, consider an output of a neuron \(x\) (before
dropout). With dropout, the expected output from this neuron will become \(px +
(1-p)0\), because the neuron’s output will be set to zero with probability
\(1-p\). At test time, when we keep the neuron always active, we must adjust \(x
\rightarrow px\) to keep the same expected output. It can also be shown that
performing this attenuation at test time can be related to the process of
iterating over all the possible binary masks (and therefore all the
exponentially many sub-networks) and computing their ensemble prediction.


The undesirable property of the scheme presented above is that we must scale the
activations by \(p\) at test time. Since test-time performance is so critical,
it is always preferable to use **inverted dropout**, which performs the scaling
at train time, leaving the forward pass at test time untouched. Additionally,
this has the appealing property that the prediction code can remain untouched
when you decide to tweak where you apply dropout, or if at all. Inverted dropout
looks as follows:



```python
"""""" 
Inverted Dropout: Recommended implementation example.
We drop and scale at train time and don't do anything at test time.
""""""

p = 0.5 # probability of keeping a unit active. higher = less dropout

def train\_step(X):
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(\*H1.shape) < p) / p # first dropout mask. Notice /p!
  H1 \*= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(\*H2.shape) < p) / p # second dropout mask. Notice /p!
  H2 \*= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)
  
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3

```

There has a been a large amount of research after the first introduction of
dropout that tries to understand the source of its power in practice, and its
relation to the other regularization techniques. Recommended further reading for
an interested reader includes:


* [Dropout paper](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) by Srivastava et al. 2014.
* [Dropout Training as Adaptive Regularization](http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf): “we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix”.


**Theme of noise in forward pass**. Dropout falls into a more general category
of methods that introduce stochastic behavior in the forward pass of the
network. During testing, the noise is marginalized over *analytically* (as is
the case with dropout when multiplying by \(p\)), or *numerically* (e.g. via
sampling, by performing several forward passes with different random decisions
and then averaging over them). An example of other research in this direction
includes [DropConnect](http://cs.nyu.edu/~wanli/dropc/), where a random set of
weights is instead set to zero during forward pass. As foreshadowing,
Convolutional Neural Networks also take advantage of this theme with methods
such as stochastic pooling, fractional pooling, and data augmentation. We will
go into details of these methods later.


**Bias regularization**. As we already mentioned in the Linear Classification
section, it is not common to regularize the bias parameters because they do not
interact with the data through multiplicative interactions, and therefore do not
have the interpretation of controlling the influence of a data dimension on the
final objective. However, in practical applications (and with proper data
preprocessing) regularizing the bias rarely leads to significantly worse
performance. This is likely because there are very few bias terms compared to
all the weights, so the classifier can “afford to” use the biases if it needs
them to obtain a better data loss.


**Per-layer regularization**. It is not very common to regularize different
layers to different amounts (except perhaps the output layer). Relatively few
results regarding this idea have been published in the literature.


**In practice**: It is most common to use a single, global L2 regularization
strength that is cross-validated. It is also common to combine this with dropout
applied after all layers. The value of \(p = 0.5\) is a reasonable default, but
this can be tuned on validation data.



### Loss functions


We have discussed the regularization loss part of the objective, which can be
seen as penalizing some measure of complexity of the model. The second part of
an objective is the *data loss*, which in a supervised learning problem measures
the compatibility between a prediction (e.g. the class scores in classification)
and the ground truth label. The data loss takes the form of an average over the
data losses for every individual example. That is, \(L = \frac{1}{N} \sum\_i
L\_i\) where \(N\) is the number of training data. Lets abbreviate \(f = f(x\_i;
W)\) to be the activations of the output layer in a Neural Network. There are
several types of problems you might want to solve in practice:


**Classification** is the case that we have so far discussed at length. Here, we
assume a dataset of examples and a single correct label (out of a fixed set) for
each example. One of two most commonly seen cost functions in this setting is
the SVM (e.g. the Weston Watkins formulation):



\[L\_i = \sum\_{j\neq y\_i} \max(0, f\_j - f\_{y\_i} + 1)\]

As we briefly alluded to, some people report better performance with the squared
hinge loss (i.e. instead using \(\max(0, f\_j - f\_{y\_i} + 1)^2\)). The second
common choice is the Softmax classifier that uses the cross-entropy loss:



\[L\_i = -\log\left(\frac{e^{f\_{y\_i}}}{ \sum\_j e^{f\_j} }\right)\]

**Problem: Large number of classes**. When the set of labels is very large (e.g.
words in English dictionary, or ImageNet which contains 22,000 categories),
computing the full softmax probabilities becomes expensive. For certain
applications, approximate versions are popular. For instance, it may be helpful
to use *Hierarchical Softmax* in natural language processing tasks (see one
explanation [here](http://arxiv.org/pdf/1310.4546.pdf) (pdf)). The hierarchical
softmax decomposes words as labels in a tree. Each label is then represented as
a path along the tree, and a Softmax classifier is trained at every node of the
tree to disambiguate between the left and right branch. The structure of the
tree strongly impacts the performance and is generally problem-dependent.


**Attribute classification**. Both losses above assume that there is a single
correct answer \(y\_i\). But what if \(y\_i\) is a binary vector where every
example may or may not have a certain attribute, and where the attributes are
not exclusive? For example, images on Instagram can be thought of as labeled
with a certain subset of hashtags from a large set of all hashtags, and an image
may contain multiple. A sensible approach in this case is to build a binary
classifier for every single attribute independently. For example, a binary
classifier for each category independently would take the form:



\[L\_i = \sum\_j \max(0, 1 - y\_{ij} f\_j)\]

where the sum is over all categories \(j\), and \(y\_{ij}\) is either +1 or -1
depending on whether the i-th example is labeled with the j-th attribute, and
the score vector \(f\_j\) will be positive when the class is predicted to be
present and negative otherwise. Notice that loss is accumulated if a positive
example has score less than +1, or when a negative example has score greater
than -1.


An alternative to this loss would be to train a logistic regression classifier
for every attribute independently. A binary logistic regression classifier has
only two classes (0,1), and calculates the probability of class 1 as:



\[P(y = 1 \mid x; w, b) = \frac{1}{1 + e^{-(w^Tx +b)}} = \sigma (w^Tx + b)\]

Since the probabilities of class 1 and 0 sum to one, the probability for class 0
is \(P(y = 0 \mid x; w, b) = 1 - P(y = 1 \mid x; w,b)\). Hence, an example is
classified as a positive example (y = 1) if \(\sigma (w^Tx + b) > 0.5\), or
equivalently if the score \(w^Tx +b > 0\). The loss function then maximizes this
probability. You can convince yourself that this simplifies to minimizing the
negative log-likelihood:



\[L\_i = -\sum\_j y\_{ij} \log(\sigma(f\_j)) + (1 - y\_{ij}) \log(1 - \sigma(f\_j))\]

where the labels \(y\_{ij}\) are assumed to be either 1 (positive) or 0
(negative), and \(\sigma(\cdot)\) is the sigmoid function. The expression above
can look scary but the gradient on \(f\) is in fact extremely simple and
intuitive: \(\partial{L\_i} / \partial{f\_j} = \sigma(f\_j) - y\_{ij}\) (as you
can double check yourself by taking the derivatives).


**Regression** is the task of predicting real-valued quantities, such as the
price of houses or the length of something in an image. For this task, it is
common to compute the loss between the predicted quantity and the true answer
and then measure the L2 squared norm, or L1 norm of the difference. The L2 norm
squared would compute the loss for a single example of the form:



\[L\_i = \Vert f - y\_i \Vert\_2^2\]

The reason the L2 norm is squared in the objective is that the gradient becomes
much simpler, without changing the optimal parameters since squaring is a
monotonic operation. The L1 norm would be formulated by summing the absolute
value along each dimension:



\[L\_i = \Vert f - y\_i \Vert\_1 = \sum\_j \mid f\_j - (y\_i)\_j \mid\]

where the sum \(\sum\_j\) is a sum over all dimensions of the desired
prediction, if there is more than one quantity being predicted. Looking at only
the j-th dimension of the i-th example and denoting the difference between the
true and the predicted value by \(\delta\_{ij}\), the gradient for this
dimension (i.e. \(\partial{L\_i} / \partial{f\_j}\)) is easily derived to be
either \(\delta\_{ij}\) with the L2 norm, or \(sign(\delta\_{ij})\). That is,
the gradient on the score will either be directly proportional to the difference
in the error, or it will be fixed and only inherit the sign of the difference.


*Word of caution*: It is important to note that the L2 loss is much harder to
optimize than a more stable loss such as Softmax. Intuitively, it requires a
very fragile and specific property from the network to output exactly one
correct value for each input (and its augmentations). Notice that this is not
the case with Softmax, where the precise value of each score is less important:
It only matters that their magnitudes are appropriate. Additionally, the L2 loss
is less robust because outliers can introduce huge gradients. When faced with a
regression problem, first consider if it is absolutely inadequate to quantize
the output into bins. For example, if you are predicting star rating for a
product, it might work much better to use 5 independent classifiers for ratings
of 1-5 stars instead of a regression loss. Classification has the additional
benefit that it can give you a distribution over the regression outputs, not
just a single output with no indication of its confidence. If you’re certain
that classification is not appropriate, use the L2 but be careful: For example,
the L2 is more fragile and applying dropout in the network (especially in the
layer right before the L2 loss) is not a great idea.



> 
> When faced with a regression task, first consider if it is absolutely necessary.
> Instead, have a strong preference to discretizing your outputs to bins and
> perform classification over them whenever possible.
> 
> 
> 


**Structured prediction**. The structured loss refers to a case where the labels
can be arbitrary structures such as graphs, trees, or other complex objects.
Usually it is also assumed that the space of structures is very large and not
easily enumerable. The basic idea behind the structured SVM loss is to demand a
margin between the correct structure \(y\_i\) and the highest-scoring incorrect
structure. It is not common to solve this problem as a simple unconstrained
optimization problem with gradient descent. Instead, special solvers are usually
devised so that the specific simplifying assumptions of the structure space can
be taken advantage of. We mention the problem briefly but consider the specifics
to be outside of the scope of the class.



## Summary


In summary:


* The recommended preprocessing is to center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature
* Initialize the weights by drawing them from a gaussian distribution with standard deviation of \(\sqrt{2/n}\), where \(n\) is the number of inputs to the neuron. E.g. in numpy: `w = np.random.randn(n) * sqrt(2.0/n)`.
* Use L2 regularization and dropout (the inverted version)
* Use batch normalization
* We discussed different tasks you might want to perform in practice, and the most common loss functions for each task


We’ve now preprocessed the data and set up and initialized the model. In the
next section we will look at the learning process and its dynamics.









* [cs231n](https://github.com/cs231n)
* [cs231n](https://twitter.com/cs231n)












 // Make responsive
 MathJax.Hub.Config({
 ""HTML-CSS"": { linebreaks: { automatic: true } },
 ""SVG"": { linebreaks: { automatic: true } },
 });
 


"
_JB0AO7QxSA?si=o0DA1DZflJGeU9KH,,"- Okay, it's after 12, so I think we should get started. Today we're going to kind of pick up where we left off last time. Last time we talked about a lot of sort of tips and tricks involved in the nitty gritty details of training neural networks. Today we'll pick up where we left off, and talk about a lot more of these sort of nitty gritty details about training these things. As usual, a couple administrative notes before we get into the material. As you all know, assignment
one is already due. Hopefully you all turned it in. Did it go okay? Was it not okay? Rough sentiment? Mostly okay. Okay, that's good. Awesome. [laughs] We're in the process of grading those, so stay turned. We're hoping to get grades back for those before A two is due. Another reminder, that
your project proposals are due tomorrow. Actually, no, today at 11:59. Make sure you send those in. Details are on the website and on Piazza. Also a reminder, assignment
two is already out. That'll be due a week from Thursday. Historically, assignment two has been the longest one in the class, so if you haven't started already on assignment two, I'd recommend you take a look at that pretty soon. Another reminder is
that for assignment two, I think of a lot of you will be using Google Cloud. Big reminder, make sure
to stop your instances when you're not using them because whenever your instance
is on, you get charged, and we only have so many coupons to distribute to you guys. Anytime your instance is on, even if you're not SSH to it, even if you're not running things immediately
in your Jupyter Notebook, any time that instance is on,
you're going to be charged. Just make sure that you explicitly stop your instances when you're not using them. In this example, I've
got a little screenshot of my dashboard on Google Cloud. I need to go in there and explicitly go to the dropdown and click stop. Just make sure that you do this when you're done working each day. Another thing to remember is it's kind of up to you guys to keep
track of your spending on Google Cloud. In particular, instances that use GPUs are a lot more expensive
than those with CPUs. Rough order of magnitude,
those GPU instances are around 90 cents to a dollar an hour. Those are actually quite pricey. The CPU instances are much cheaper. The general strategy is that you probably want to make two instances, one with a GPU and one without, and then only use that GPU instance when you really need the GPU. For example, on assignment two, most of the assignment,
you should only need the CPU, so you should only use your CPU instance for that. But then the final
question, about TensorFlow or PyTorch, that will need a GPU. This'll give you a little bit of practice with switching between multiple instances and only using that GPU
when it's really necessary. Again, just kind of watch your spending. Try not to go too crazy on these things. Any questions on the administrative stuff before we move on? Question. - [Student] How much RAM should we use? - Question is how much RAM should we use? I think eight or 16 gigs is probably good for everything that
you need in this class. As you scale up the number of CPUs and the number of RAM, you also end up spending more money. If you stick with two or four CPUs and eight or 16 gigs of RAM, that should be plenty for all the
homework-related stuff that you need to do. As a quick recap, last
time we talked about activation functions. We talked about this whole zoo of different activation functions and some of their different properties. We saw that the sigmoid, which used to be quite popular when
training neural networks maybe 10 years ago or so, has this problem with vanishing gradients near the two ends of the activation function. tanh has this similar sort of problem. Kind of the general recommendation is that you probably want to stick with ReLU for most cases as sort of a default choice 'cause it tends to work well for a lot of different architectures. We also talked about
weight initialization. Remember that up on the
top, we have this idea that when you initialize your weights at the start of training, if those weights are initialized to be
too small, then if you look at, then the activations will vanish as you go through the network because as you multiply by these small numbers over and over again, they'll all sort of decay to zero. Then everything will be
zero, learning won't happen, you'll be sad. On the other hand, if you initialize your weights too big,
then as you go through the network and multiply
by your weight matrix over and over again,
eventually they'll explode. You'll be unhappy,
there'll be no learning, it will be very bad. But if you get that
initialization just right, for example, using the
Xavier initialization or the MSRA initialization,
then you kind of keep a nice distribution of activations as you go through the network. Remember that this kind of gets more and more important and
more and more critical as your networks get deeper and deeper because as your network gets deeper, you're multiplying by
those weight matrices over and over again with these
more multiplicative terms. We also talked last time
about data preprocessing. We talked about how it's pretty typical in conv nets to zero center and normalize your data so it has zero
mean and unit variance. I wanted to provide a little
bit of extra intuition about why you might
actually want to do this. Imagine a simple setup where we have a binary classification problem where we want to draw a line to
separate these red points from these blue points. On the left, you have this idea where if those data points are
kind of not normalized and not centered and far
away from the origin, then we can still use a
line to separate them, but now if that line
wiggles just a little bit, then our classification is going to get totally destroyed. That kind of means that in the example on the left, the loss function is now extremely sensitive to small perturbations in that linear classifier
in our weight matrix. We can still represent the same functions, but that might make
learning quite difficult because, again, their
loss is very sensitive to our parameter vector,
whereas in the situation on the right, if you take that data cloud and you move it into the
origin and you make it unit variance, then
now, again, we can still classify that data quite well, but now as we wiggle that line a little bit, then our loss function is less sensitive to small perturbations
in the parameter values. That maybe makes optimization
a little bit easier, as we'll see a little bit going forward. By the way, this situation is not only in the linear classification case. Inside a neural network,
remember we kind of have these interleavings of these linear matrix multiplies, or
convolutions, followed by non-linear activation functions. If the input to some layer in your neural network is not centered or not zero mean, not unit variance, then again, small perturbations in the weight matrix of that layer of the network could cause large perturbations in
the output of that layer, which, again, might
make learning difficult. This is kind of a little bit of extra intuition about why normalization might be important. Because we have this
intuition that normalization is so important, we talked
about batch normalization, which is where we just
add this additional layer inside our networks to just force all of the intermediate
activations to be zero mean and unit variance. I've sort of resummarized the batch normalization equations here with the shapes a little
bit more explicitly. Hopefully this can help you out when you're implementing this thing on assignment two. But again, in batch normalization, we have this idea that in the forward pass, we use the statistics of the mini batch to compute a mean and
a standard deviation, and then use those estimates to normalize our data on the forward pass. Then we also reintroduce the scale and shift parameters to increase the expressivity of the layer. You might want to refer back to this when working on assignment two. We also talked last
time a little bit about babysitting the learning process, how you should probably be looking
at your loss curves during training. Here's an example of some networks I was actually training over the weekend. This is usually my setup when I'm working on these things. On the left, I have some plot showing the training loss over time. You can see it's kind of going down, which means my network
is reducing the loss. It's doing well. On the right, there's this plot where the X axis is, again, time,
or the iteration number, and the Y axis is my performance measure both on my training set
and on my validation set. You can see that as we go over time, then my training set performance goes up and up and up and up and
up as my loss function goes down, but at some
point, my validation set performance kind of plateaus. This kind of suggests that maybe I'm overfitting in this situation. Maybe I should have been trying to add additional regularization. We also talked a bit last time about hyperparameter search. All these networks have
sort of a large zoo of hyperparameters. It's pretty important
to set them correctly. We talked a little bit about grid search versus random search,
and how random search is maybe a little bit nicer in theory because in the situation
where your performance might be more sensitive, with respect to one hyperparameter than
other, and random search lets you cover that space
a little bit better. We also talked about the idea of coarse to fine search, where when you're doing this hyperparameter
optimization, probably you want to start with very wide ranges for your hyperparameters, only train for a couple iterations, and then based on those results, you kind of narrow in on the range of
hyperparameters that are good. Now, again, redo your
search in a smaller range for more iterations. You can kind of iterate this process to kind of hone in on the right region for hyperparameters. But again, it's really important to, at the start, have a very coarse range to start with, where
you want very, very wide ranges for all your hyperparameters. Ideally, those ranges should be so wide that your network is kind of blowing up at either end of the
range so that you know that you've searched a wide enough range for those things. Question? - [Student] How many
[speaks too low to hear] optimize at once? [speaks too low to hear] - The question is how many hyperparameters do we typically search at a time? Here is two, but there's
a lot more than two in these typical things. It kind of depends on the exact model and the exact architecture, but because the number of possibilities is exponential in the number of hyperparameters, you can't really test too many at a time. It also kind of depends
on how many machines you have available. It kind of varies from person to person and from experiment to experiment. But generally, I try
not to do this over more than maybe two or three or four at a time at most because, again,
this exponential search just gets out of control. Typically, learning rate
is the really important one that you need to nail first. Then other things, like regularization, like learning rate decay, model size, these other types of things tend to be a little bit less sensitive
than learning rate. Sometimes you might do kind of a block coordinate descent, where you go and find the good learning rate,
then you go back and try to look at different model sizes. This can help you cut down on the exponential search a little bit, but it's a little bit problem dependent on exactly which ones you
should be searching over in which order. More questions? - [Student] [speaks too low to hear] Another parameter, but then changing that other parameter, two or
three other parameters, makes it so that your
learning rate or the ideal learning rate is still
[speaks too low to hear]. - Question is how often
does it happen where when you change one hyperparameter, then the other, the
optimal values of the other hyperparameters change? That does happen sometimes,
although for learning rates, that's typically less of a problem. For learning rates,
typically you want to get in a good range, and
then set it maybe even a little bit lower than
optimal, and let it go for a long time. Then if you do that, combined with some of the fancier optimization
strategies that we'll talk about today,
then a lot of models tend to be a little bit less sensitive to learning rate once you
get them in a good range. Sorry, did you have a
question in front, as well? - [Student] [speaks too low to hear] - The question is what's wrong with having a small learning rate and increasing the number of epochs? The answer is that it might take a very long time. [laughs] - [Student] [speaks too low to hear] - Intuitively, if you
set the learning rate very low and let it go
for a very long time, then this should, in theory, always work. But in practice, those
factors of 10 or 100 actually matter a lot when you're training these things. Maybe if you got the right learning rate, you could train it in six hours, 12 hours or a day, but then if
you just were super safe and dropped it by a factor of 10 or by a factor of 100,
now that one-day training becomes 100 days of training. That's three months. That's not going to be good. When you're taking these intro computer science classes,
they always kind of sweep the constants under the rug, but when you're actually thinking
about training things, those constants end up mattering a lot. Another question? - [Student] If you have
a low learning rate, [speaks too low to hear]. - Question is for a low learning rate, are you more likely to
be stuck in local optima? I think that makes some intuitive sense, but in practice, that seems not to be much of a problem. I think we'll talk a bit
more about that later today. Today I wanted to talk about a couple other really interesting
and important topics when we're training neural networks. In particular, I wanted to talk, we've kind of alluded to this fact of fancier, more powerful
optimization algorithms a couple times. I wanted to spend some
time today and really dig into those and talk about what are the actual optimization
algorithms that most people are using these days. We also touched on regularization in earlier lectures. This concept of making your network do additional things to reduce the gap between train and test error. I wanted to talk about
some more strategies that people are using in practice of regularization, with
respect to neural networks. Finally, I also wanted to talk a bit about transfer learning, where you can sometimes get away with using less data than you think by transferring from one problem to another. If you recall from a few lectures ago, the kind of core strategy in training neural networks is an optimization problem where we write down some loss function, which defines, for each
value of the network weights, the loss function tells us how good or bad is that value of the weights
doing on our problem. Then we imagine that this loss function gives us some nice
landscape over the weights, where on the right, I've shown this maybe small, two-dimensional
problem, where the X and Y axes are two values of the weights. Then the color of the
plot kind of represents the value of the loss. In this kind of cartoon picture of a two-dimensional problem, we're only optimizing over these
two values, W one, W two. The goal is to find the most red region in this case, which
corresponds to the setting of the weights with the lowest loss. Remember, we've been working so far with this extremely simple
optimization algorithm, stochastic gradient descent, where it's super simple, it's three lines. While true, we first evaluate the loss in the gradient on some
mini batch of data. Then we step, updating
our parameter vector in the negative direction of the gradient because this gives, again, the direction of greatest decrease of the loss function. Then we repeat this over and over again, and hopefully we converge
to the red region and we get great errors
and we're very happy. But unfortunately, this relatively simple optimization algorithm has
quite a lot of problems that actually could come up in practice. One problem with stochastic
gradient descent, imagine what happens if
our objective function looks something like this, where, again, we're plotting two
values, W one and W two. As we change one of those values, the loss function changes very slowly. As we change the horizontal
value, then our loss changes slowly. As we go up and down in this landscape, now our loss is very sensitive to changes in the vertical direction. By the way, this is
referred to as the loss having a bad condition
number at this point, which is the ratio between
the largest and smallest singular values of the Hessian matrix at that point. But the intuitive idea is
that the loss landscape kind of looks like a taco shell. It's sort of very
sensitive in one direction, not sensitive in the other direction. The question is what might SGD, stochastic gradient
descent, do on a function that looks like this? If you run stochastic gradient descent on this type of function, you might get this characteristic zigzagging behavior, where because for this
type of objective function, the direction of the
gradient does not align with the direction towards the minima. When you compute the
gradient and take a step, you might step sort of over this line and sort of zigzag back and forth. In effect, you get very
slow progress along the horizontal dimension, which is the less sensitive dimension, and you get this zigzagging, nasty, nasty
zigzagging behavior across the fast-changing dimension. This is undesirable behavior. By the way, this problem actually becomes much more common in high dimensions. In this kind of cartoon
picture, we're only showing a two-dimensional
optimization landscape, but in practice, our
neural networks might have millions, tens of millions,
hundreds of millions of parameters. That's hundreds of millions of directions along which this thing can move. Now among those hundreds of millions of different directions to move, if the ratio between the largest one and the smallest one is bad, then SGD will not perform so nicely. You can imagine that if we
have 100 million parameters, probably the maximum
ratio between those two will be quite large. I think this is actually
quite a big problem in practice for many
high-dimensional problems. Another problem with SGD
has to do with this idea of local minima or saddle points. Here I've sort of swapped
the graph a little bit. Now the X axis is showing the value of one parameter, and then the Y axis is showing the value of the loss. In this top example, we have kind of this curvy objective function, where there's a valley in the middle. What happens to SGD in this situation? - [Student] [speaks too low to hear] - In this situation, SGD will get stuck because at this local minima, the gradient is zero because it's locally flat. Now remember with SGD,
we compute the gradient and step in the direction
of opposite gradient, so if at our current point,
the opposite gradient is zero, then we're not
going to make any progress, and we'll get stuck at this point. There's another problem with this idea of saddle points. Rather than being a local minima, you can imagine a point
where in one direction we go up, and in the other
direction we go down. Then at our current point,
the gradient is zero. Again, in this situation, the function will get stuck at the saddle point because the gradient is zero. Although one thing I'd like to point out is that in one dimension,
in a one-dimensional problem like this, local minima
seem like a big problem and saddle points seem like kind of not something to worry about, but in fact, it's the opposite once you move to very high-dimensional problems because, again, if you think about you're in this 100 million dimensional space, what does a saddle point mean? That means that at my current point, some directions the loss goes up, and some directions the loss goes down. If you have 100 million dimensions, that's probably going to
happen more frequently than, that's probably going to happen
almost everywhere, basically. Whereas a local minima
says that of all those 100 million directions that I can move, every one of them causes
the loss to go up. In fact, that seems pretty rare when you're thinking about, again, these very high-dimensional problems. Really, the idea that has come to light in the last few years is that when you're training these very
large neural networks, the problem is more about saddle points and less about local minima. By the way, this also is a problem not just exactly at the saddle point, but also near the saddle point. If you look at the example on the bottom, you see that in the regions around the saddle point, the gradient isn't zero, but the slope is very small. That means that if we're,
again, just stepping in the direction of the gradient, and that gradient is very
small, we're going to make very, very slow progress whenever our current parameter value
is near a saddle point in the objective landscape. This is actually a big problem. Another problem with SGD comes from the S. Remember that SGD is
stochastic gradient descent. Recall that our loss function is typically defined by computing the loss over many, many different examples. In this case, if N is
your whole training set, then that could be
something like a million. Each time computing
the loss would be very, very expensive. In practice, remember
that we often estimate the loss and estimate the gradient using a small mini batch of examples. What this means is that we're not actually getting the true information
about the gradient at every time step. Instead, we're just
getting some noisy estimate of the gradient at our current point. Here on the right, I've kind of faked this plot a little bit. I've just added random uniform noise to the gradient at every point, and then run SGD with these noisy,
messed up gradients. This is maybe not exactly what happens with the SGD process,
but it still give you the sense that if there's noise in your gradient estimates, then vanilla SGD kind of meanders around the space and might actually take a long time to get towards the minima. Now that we've talked about a lot of these problems. Sorry, was there a question? - [Student] [speaks too low to hear] - The question is do all of these just go away if we use
normal gradient descent? Let's see. I think that the taco shell problem of high condition numbers
is still a problem with full batch gradient descent. The noise. As we'll see, we might sometimes introduce additional noise into the network, not only due to sampling mini batches, but also due to explicit
stochasticity in the network, so we'll see that later. That can still be a problem. Saddle points, that's still a problem for full batch gradient descent because there can still be saddle points in the full objective landscape. Basically, even if we go to full batch gradient descent, it doesn't really solve these problems. We kind of need to think
about a slightly fancier optimization algorithm that can try to address these concerns. Thankfully, there's a really,
really simple strategy that works pretty well at addressing many of these problems. That's this idea of adding a momentum term to our stochastic gradient descent. Here on the left, we have our classic old friend, SGD, where we just always step in the direction of the gradient. But now on the right, we have this minor, minor variance called SGD plus momentum, which is now two equations
and five lines of code, so it's twice as complicated. But it's very simple. The idea is that we maintain a velocity over time, and we add
our gradient estimates to the velocity. Then we step in the
direction of the velocity, rather than stepping in the
direction of the gradient. This is very, very simple. We also have this hyperparameter rho now which corresponds to friction. Now at every time step, we take our current velocity, we
decay the current velocity by the friction constant,
rho, which is often something high, like
.9 is a common choice. We take our current velocity, we decay it by friction and we add in our gradient. Now we step in the direction
of our velocity vector, rather than the direction of our raw gradient vector. This super, super simple strategy actually helps for all of these problems that we just talked about. If you think about what
happens at local minima or saddle points, then if we're imagining velocity in this system,
then you kind of have this physical interpretation of this ball kind of rolling down the
hill, picking up speed as it comes down. Now once we have velocity,
then even when we pass that point of local minima, the point will still have velocity, even if it doesn't have gradient. Then we can hopefully get
over this local minima and continue downward. There's this similar
intuition near saddle points, where even though the gradient around the saddle point is very small, we have this velocity vector that we've built up as we roll downhill. That can hopefully carry us through the saddle point and
let us continue rolling all the way down. If you think about what happens in poor conditioning, now if we were to have these kind of zigzagging approximations to the gradient, then those zigzags will hopefully cancel
each other out pretty fast once we're using momentum. This will effectively reduce the amount by which we step in the
sensitive direction, whereas in the horizontal direction, our velocity will just keep building up, and will actually accelerate our descent across that less sensitive dimension. Adding momentum here can actually help us with this high condition
number problem, as well. Finally, on the right, we've repeated the same visualization of
gradient descent with noise. Here, the black is this vanilla SGD, which is sort of zigzagging
all over the place, where the blue line is showing now SGD with momentum. You can see that because we're adding it, we're building up this velocity over time, the noise kind of gets averaged out in our gradient estimates. Now SGD ends up taking
a much smoother path towards the minima, compared with the SGD, which is kind of meandering due to noise. Question? - [Student] [speaks too low to hear] - The question is how does SGD momentum help with the poorly
conditioned coordinate? The idea is that if you go back and look at this velocity estimate and look at the velocity
computation, we're adding in the gradient at every time step. It kind of depends on your setting of rho, that hyperparameter, but you can imagine that if the gradient is relatively small, and if rho is well
behaved in this situation, then our velocity could
actually monotonically increase up to a point where the velocity could now be larger than the actual gradient. Then we might actually
make faster progress along the poorly conditioned dimension. Kind of one picture that
you can have in mind when we're doing SGD plus momentum is that the red here is our current point. At our current point,
we have some red vector, which is the direction of the gradient, or rather our estimate of the gradient at the current point. Green is now the direction
of our velocity vector. Now when we do the momentum update, we're actually stepping according to a weighted average of these two. This helps overcome some noise in our gradient estimate. There's a slight variation of momentum that you sometimes see, called Nesterov accelerated gradient, also sometimes called Nesterov momentum. That switches up this order of things a little bit. In sort of normal SGD momentum, we imagine that we estimate the gradient
at our current point, and then take a mix of our velocity and our gradient. With Nesterov accelerated gradient, you do something a little bit different. Here, you start at the red point. You step in the direction
of where the velocity would take you. You evaluate the gradient at that point. Then you go back to your original point and kind of mix together those two. This is kind of a funny interpretation, but you can imagine that you're kind of mixing together information
a little bit more. If your velocity direction was actually a little bit wrong, it
lets you incorporate gradient information
from a little bit larger parts of the objective landscape. This also has some really
nice theoretical properties when it comes to convex optimization, but those guarantees go a little bit out the window once it
comes to non-convex problems like neural networks. Writing it down in
equations, Nesterov momentum looks something like this, where now to update our velocity, we take a step, according to our previous
velocity, and evaluate that gradient there. Now when we take our next step, we actually step in the
direction of our velocity that's incorporating information from these multiple points. Question? - [Student] [speaks too low to hear] - Oh, sorry. The question is what's
a good initialization for the velocity? This is almost always zero. It's not even a hyperparameter. Just set it to zero and don't worry. Another question? - [Student] [speaks too low to hear] - Intuitively, the velocity
is kind of a weighted sum of your gradients that
you've seen over time. - [Student] [speaks too low to hear] - With more recent gradients
being weighted heavier. At every time step, we
take our old velocity, we decay by friction and we add in our current gradient. You can kind of think of this as a smooth moving average
of your recent gradients with kind of a exponentially
decaying weight on your gradients going back in time. This Nesterov formulation
is a little bit annoying 'cause if you look at
this, normally when you have your loss function,
you want to evaluate your loss and your
gradient at the same point. Nesterov breaks this a little bit. It's a little bit annoying to work with. Thankfully, there's a
cute change of variables you can do. If you do the change of variables and reshuffle a little bit, then you can write Nesterov momentum in
a slightly different way that now, again, lets
you evaluate the loss and the gradient at the same point always. Once you make this change of variables, you get kind of a nice
interpretation of Nesterov, which is that here in the first step, this looks exactly like
updating the velocity in the vanilla SGD momentum case, where we have our current velocity,
we evaluate gradient at the current point and
mix these two together in a decaying way. Now in the second update,
now when we're actually updating our parameter vector, if you look at the second equation,
we have our current point plus our current velocity plus a weighted difference
between our current velocity and our previous velocity. Here, Nesterov momentum
is kind of incorporating some kind of error-correcting term between your current velocity and
your previous velocity. If we look at SGD, SGD momentum and Nesterov momentum
on this kind of simple problem, compared with SGD, we notice that SGD kind of takes this,
SGD is in the black, kind of taking this slow
progress toward the minima. The blue and the green show momentum and Nesterov. These have this behavior
of kind of overshooting the minimum 'cause they're
building up velocity going past the minimum, and then kind of correcting themselves and coming back towards the minima. Question? - [Student] [speaks too low to hear] - The question is this picture looks good, but what happens if your minima call lies in this very narrow basin? Will the velocity just cause you to skip right over that minima? That's actually a really
interesting point, and the subject of some
recent theoretical work, but the idea is that maybe those really sharp minima are actually bad minima. We don't want to even land in those 'cause the idea is that maybe if you have a very sharp minima, that actually could be a minima that overfits more. If you imagine that we
doubled our training set, the whole optimization
landscape would change, and maybe that very sensitive minima would actually disappear
if we were to collect more training data. We kind of have this intuition that we maybe want to land in very flat minima because those very flat
minima are probably more robust as we change
the training data. Those flat minima might
actually generalize better to testing data. This is again, sort of very recent theoretical work, but that's actually a really good point that you bring it up. In some sense, it's actually a feature and not a bug that SGD momentum actually skips over those very sharp minima. That's actually a good
thing, believe it or not. Another thing you can see is if you look at the difference between
momentum and Nesterov here, you can see that because of the correction factor in
Nesterov, maybe it's not overshooting quite as drastically, compared to vanilla momentum. There's another kind
of common optimization strategy is this algorithm called AdaGrad, which John Duchi, who's
now a professor here, worked on during his Ph.D. The idea with AdaGrad is that as you, during the course of the optimization, you're going to keep a running estimate or a running sum of all
the squared gradients that you see during training. Now rather than having a velocity term, instead we have this grad squared term. During training, we're
going to just keep adding the squared gradients to
this grad squared term. Now when we update our parameter vector, we'll divide by this grad squared term when we're making our update step. The question is what
does this kind of scaling do in this situation where we have a very high condition number? - [Student] [speaks too low to hear] - The idea is that if
we have two coordinates, one that always has a very high gradient and one that always has
a very small gradient, then as we add the sum of the squares of the small gradient,
we're going to be dividing by a small number, so
we'll accelerate movement along the slow dimension, along the one dimension. Then along the other
dimension, where the gradients tend to be very large,
then we'll be dividing by a large number, so
we'll kind of slow down our progress along the wiggling dimension. But there's kind of a problem here. That's the question of
what happens with AdaGrad over the course of training, as t gets larger and larger and larger? - [Student] [speaks too low to hear] - With AdaGrad, the steps
actually get smaller and smaller and smaller because we just continue updating this estimate of the squared gradients over
time, so this estimate just grows and grows
and grows monotonically over the course of training. Now this causes our
step size to get smaller and smaller and smaller over time. Again, in the convex case, there's some really nice theory showing
that this is actually really good 'cause in the convex case, as you approach a
minimum, you kind of want to slow down so you actually converge. That's actually kind of a feature in the convex case. But in the non-convex
case, that's a little bit problematic because as you come towards a saddle point, you might
get stuck with AdaGrad, and then you kind of no
longer make any progress. There's a slight variation of AdaGrad, called RMSProp, that actually addresses this concern a little bit. Now with RMSProp, we
still keep this estimate of the squared gradients, but instead of just letting that squared estimate continually accumulate over training, instead, we let that squared
estimate actually decay. This ends up looking kind
of like a momentum update, except we're having kind of momentum over the squared gradients,
rather than momentum over the actual gradients. Now with RMSProp, after
we compute our gradient, we take our current estimate
of the grad squared, we multiply it by this decay rate, which is commonly
something like .9 or .99. Then we add in this one
minus the decay rate of our current squared gradient. Now over time, you can imagine that. Then again, when we
make our step, the step looks exactly the same as AdaGrad, where we divide by the squared gradient in the step to again
have this nice property of accelerating movement
along the one dimension, and slowing down movement
along the other dimension. But now with RMSProp,
because these estimates are leaky, then it kind
of addresses the problem of maybe always slowing down where you might not want to. Here again, we're kind
of showing our favorite toy problem with SGD
in black, SGD momentum in blue and RMSProp in red. You can see that RMSProp and SGD momentum are both doing much better than SGD, but their qualitative behavior
is a little bit different. With SGD momentum, it kind of overshoots the minimum and comes back, whereas with RMSProp, it's kind of adjusting its trajectory such that we're making approximately equal progress among all the dimensions. By the way, you can't actually tell, but this plot is also
showing AdaGrad in green with the same learning rate, but it just gets stuck due to this
problem of continually decaying learning rates. In practice, AdaGrad
is maybe not so common for many of these things. That's a little bit of
an unfair comparison of AdaGrad. Probably you need to
increase the learning rate with AdaGrad, and then
it would end up looking kind of like RMSProp in this case. But in general, we tend not to use AdaGrad so much when training neural networks. Question? - [Student] [speaks too low to hear] - The answer is yes,
this problem is convex, but in this case, it's a little bit of an unfair comparison because the learning rates
are not so comparable among the methods. I've been a little bit unfair to AdaGrad in this visualization by showing the same learning rate between
the different algorithms, when probably you should have separately turned the learning rates per algorithm. We saw in momentum, we had this idea of velocity, where we're
building up velocity by adding in the gradients,
and then stepping in the direction of the velocity. We saw with AdaGrad and RMSProp that we had this other idea, of
building up an estimate of the squared gradients,
and then dividing by the squared gradients. Then these both seem like good ideas on their own. Why don't we just stick 'em together and use them both? Maybe that would be even better. That brings us to this
algorithm called Adam, or rather brings us very close to Adam. We'll see in a couple
slides that there's a slight correction we need to make here. Here with Adam, we maintain an estimate of the first moment and the second moment. Now in the red, we make this estimate of the first moment as a weighed sum of our gradients. We have this moving estimate
of the second moment, like AdaGrad and like RMSProp, which is a moving estimate of our squared gradients. Now when we make our update step, we step using both the first
moment, which is kind of our velocity, and also divide
by the second moment, or rather the square root
of the second moment, which is this squared gradient term. This idea of Adam ends
up looking a little bit like RMSProp plus momentum, or ends up looking like momentum plus
second squared gradients. It kind of incorporates the
nice properties of both. But there's a little
bit of a problem here. That's the question of what happens at the very first time step? At the very first time step, you can see that at the beginning, we've initialized our second moment with zero. Now after one update of the second moment, typically this beta two, second moment decay rate, is something like .9 or .99, something very close to one. After one update, our
second moment is still very, very close to zero. Now when we're making our update step here and we divide by our second moment, now we're dividing by a very small number. We're making a very, very large step at the beginning. This very, very large
step at the beginning is not really due to the
geometry of the problem. It's kind of an artifact
of the fact that we initialized our second
moment estimate was zero. Question? - [Student] [speaks too low to hear] - That's true. The comment is that if your first moment is also very small,
then you're multiplying by small and you're
dividing by square root of small squared, so
what's going to happen? They might cancel each other
out, you might be okay. That's true. Sometimes these cancel each other out and you're okay, but
sometimes this ends up in taking very large steps
right at the beginning. That can be quite bad. Maybe you initialize a little bit poorly. You take a very large step. Now your initialization
is completely messed up, and then you're in a very bad part of the objective landscape
and you just can't converge from there. Question? - [Student] [speaks too low to hear] - The idea is what is this 10 to the minus seven term
in the last equation? That's actually appeared in AdaGrad, RMSProp and Adam. The idea is that we're
dividing by something. We want to make sure we're
not dividing by zero, so we always add a small positive constant to the denominator, just to make sure we're not dividing by zero. That's technically a hyperparameter, but it tends not to matter too much, so just setting 10 to minus seven, 10 to minus eight, something like that, tends to work well. With Adam, remember we just talked about this idea of at the first couple steps, it gets very large, and we might take very large steps and mess ourselves up. Adam also adds this bias correction term to avoid this problem of
taking very large steps at the beginning. You can see that after we update our first and second moments, we
create an unbiased estimate of those first and second
moments by incorporating the current time step, t. Now we actually make our step using these unbiased estimates,
rather than the original first and second moment estimates. This gives us our full form of Adam. By the way, Adam is a
really, [laughs] really good optimization algorithm,
and it works really well for a lot of different
problems, so that's kind of my default optimization
algorithm for just about any new problem that I'm tackling. In particular, if you
set beta one equals .9, beta two equals .999, learning rate one e minus three or five e minus four, that's a great staring
point for just about all the architectures
I've ever worked with. Try that. That's a really good place
to start, in general. [laughs] If we actually plot these things out and look at SGD, SGD momentum, RMSProp and Adam on the same problem, you can see that Adam, in the purple here, kind of combines elements
of both SGD momentum and RMSProp. Adam kind of overshoots the minimum a little bit like SGD
momentum, but it doesn't overshoot quite as much as momentum. Adam also has this similar behavior of RMSProp of kind of trying to curve to make equal progress
along all dimensions. Maybe in this small
two-dimensional example, Adam converged about
similarly to other ones, but you can see qualitatively that it's kind of combining
the behaviors of both momentum and RMSProp. Any questions about
optimization algorithms? - [Student] [speaks too low to hear] They still take a very long time to train. [speaks too low to hear] - The question is what does Adam not fix? Would these neural
networks are still large, they still take a long time to train. There can still be a problem. In this picture where
we have this landscape of things looking like
ovals, if you imagine that we're kind of making estimates along each dimension independently to allow us to speed up or slow down along different coordinate axes, but one problem is that if that taco shell is kind of tilted and is not axis aligned, then we're still only making estimates
along the individual axes independently. That corresponds to taking your rotated taco shell and squishing it horizontally and vertically, but you
can't actually unrotate it. In cases where you have this kind of rotated picture of poor conditioning, then Adam or any of these other algorithms really can't address that, that concern. Another thing that we've seen in all these optimization
algorithms is learning rate as a hyperparameter. We've seen this picture
before a couple times, that as you use different learning rates, sometimes if it's too
high, it might explode in the yellow. If it's a very low
learning rate, in the blue, it might take a very
long time to converge. It's kind of tricky to pick the right learning rate. This is a little bit of a trick question because we don't actually have to stick with one learning rate
throughout the course of training. Sometimes you'll see people
decay the learning rates over time, where we can kind of combine the effects of these different curves on the left, and get the
nice properties of each. Sometimes you'll start
with a higher learning rate near the start of training, and then decay the learning rate and make it smaller and smaller throughout
the course of training. A couple strategies for
these would be a step decay, where at 100,000th
iteration, you just decay by some factor and you keep going. You might see an exponential decay, where you continually
decay during training. You might see different variations of continually decaying the learning rate during training. If you look at papers,
especially the resonate paper, you often see plots that
look kind of like this, where the loss is kind of going down, then dropping, then flattening again, then dropping again. What's going on in these plots is that they're using a step decay learning rate, where at these parts where it plateaus and then suddenly drops
again, those are the iterations where they
dropped the learning rate by some factor. This idea of dropping the learning rate, you might imagine that it got near some good region, but now
the gradients got smaller, it's kind of bouncing around too much. Then if we drop the learning rate, it lets it slow down and continue to make progress down the landscape. This tends to help in practice sometimes. Although one thing to point out is that learning rate decay is
a little bit more common with SGD momentum, and
a little bit less common with something like Adam. Another thing I'd like
to point out is that learning rate decay is kind of a second-order hyperparameter. You typically should not optimize over this thing from the start. Usually when you're
kind of getting networks to work at the beginning, you want to pick a good learning rate with
no learning rate decay from the start. Trying to cross-validate jointly over learning rate decay and
initial learning rate and other things, you'll
just get confused. What you do for setting
learning rate decay is try with no decay, see what happens. Then kind of eyeball
the loss curve and see where you think you might need decay. Another thing I wanted to mention briefly is this idea of all these algorithms that we've talked about are first-order optimization algorithms. In this picture, in this
one-dimensional picture, we have this kind of
curvy objective function at our current point in red. What we're basically doing is computing the gradient at that point. We're using the gradient
information to compute some linear approximation to our function, which is kind of a first-order
Taylor approximation to our function. Now we pretend that the
first-order approximation is our actual function, and we make a step to try to minimize the approximation. But this approximation doesn't hold for very large regions, so we can't step too far in that direction. But really, the idea
here is that we're only incorporating information about the first derivative of the function. You can actually go a little bit fancier. There's this idea of
second-order approximation, where we take into account
both first derivative and second derivative information. Now we make a second-order
Taylor approximation to our function and kind
of locally approximate our function with a quadratic. Now with a quadratic, you can step right to the minimum, and you're really happy. That's this idea of
second-order optimization. When you generalize this
to multiple dimensions, you get something called the Newton step, where you compute this Hessian matrix, which is a matrix of second derivatives, and you end up inverting
this Hessian matrix in order to step directly to the minimum of this quadratic
approximation to your function. Does anyone spot something
that's quite different about this update rule,
compared to the other ones that we've seen? - [Student] [speaks too low to hear] - This doesn't have a learning rate. That's kind of cool. We're making this quadratic approximation and we're stepping right to the minimum of the quadratic. At least in this vanilla
version of Newton's method, you don't actually need a learning rate. You just always step to the minimum at every time step. However, in practice, you might end up, have a learning rate
anyway because, again, that quadratic approximation
might not be perfect, so you might only want
to step in the direction towards the minimum, rather than actually stepping to the minimum, but at least in this vanilla version, it doesn't have a learning rate. But unfortunately, this is maybe a little bit impractical for deep learning because this Hessian matrix is N by N, where N is
the number of parameters in your network. If N is 100 million,
then 100 million squared is way too big. You definitely can't store that in memory, and you definitely can't invert it. In practice, people sometimes use these quasi-Newton methods
that, rather than working with the full Hessian and inverting the full Hessian, they
work with approximations. Low-rank approximations are common. You'll sometimes see
these for some problems. L-BFGS is one particular
second-order optimizer that has this approximate second, keeps this approximation of the Hessian that you'll sometimes
see, but in practice, it doesn't work too well for many deep learning problems
because these approximations, these second-order
approximations, don't really handle the stochastic case very much, very nicely. They also tend not to work so well with non-convex problems. I don't want to get into
that right now too much. In practice, what you should really do is probably Adam is a really good choice for many different neural network things, but if you're in a situation where you can afford to do full batch updates, and you know that your
problem doesn't have really any stochasticity, then L-BFGS is kind of a good choice. L-BFGS doesn't really
get used for training neural networks too much, but as we'll see in a couple of lectures, it does sometimes get used for things like style transfer, where you actually have less stochasticity and fewer parameters, but you still want to solve an optimization problem. All of these strategies we've talked about so far are about reducing training error. All these optimization
algorithms are really about driving down your training error and minimizing your objective function, but we don't really care about training error that much. Instead, we really care
about our performance on unseen data. We really care about reducing this gap between train and test error. The question is once we're already good at optimizing our objective function, what can we do to try to reduce this gap and make our model perform better on unseen data? One really quick and dirty, easy thing to try is this idea of model ensembles that sometimes works
across many different areas in machine learning. The idea is pretty simple. Rather than having just one model, we'll train 10 different
models independently from different initial random restarts. Now at test time, we'll run our data through all of the 10 models and average the predictions of those 10 models. Adding these multiple models together tends to reduce overfitting a little bit and tend to improve
performance a little bit, typically by a couple percent. This is generally not
a drastic improvement, but it is a consistent improvement. You'll see that in competitions, like ImageNet and other things like that, using model ensembles is very common to get maximal performance. You can actually get a little
bit creative with this. Sometimes rather than
training separate models independently, you can just keep multiple snapshots of your model during the course of training, and then use these as your ensembles. Then you still, at test
time, need to average the predictions of these
multiple snapshots, but you can collect the snapshots during the course of training. There's actually a very
nice paper being presented at ICLR this week that kind of has a fancy version of this idea, where we use a crazy learning rate schedule, where our learning rate goes very slow, then very fast, then very
slow, then very fast. The idea is that with this crazy learning rate schedule,
then over the course of training, the model
might be able to converge to different regions in
the objective landscape that all are reasonably good. If you do an ensemble over these different snapshots, then you can improve your performance quite nicely, even though you're only
training the model once. Questions? - [Student] [speaks too low to hear] - The question is, it's bad when there's a large gap between error 'cause that means you're overfitting, but if there's no gap, then
is that also maybe bad? Do we actually want
some small, optimal gap between the two? We don't really care about the gap. What we really care about is maximizing the performance on the validation set. What tends to happen is that if you don't see a gap, then
you could have improved your absolute performance, in many cases, by overfitting a little bit more. There's this weird correlation between the absolute performance
on the validation set and the size of that gap. We only care about absolute performance. Question in the back? - [Student] Are hyperparameters the same for the ensemble? - Are the hyperparameters the same for the ensembles? That's a good question. Sometimes they're not. You might want to try
different sizes of the model, different learning rates, different regularization strategies
and ensemble across these different things. That actually does happen sometimes. Another little trick you can do sometimes is that during training,
you might actually keep an exponentially decaying average of your parameter vector
itself to kind of have a smooth ensemble of your own network during training. Then use this smoothly decaying average of your parameter vector, rather than the actual checkpoints themselves. This is called Polyak averaging, and it sometimes helps a little bit. It's just another one
of these small tricks you can sometimes add, but it's not maybe too common in practice. Another question you might have is that how can we actually
improve the performance of single models? When we have ensembles,
we still need to run, like, 10 models at test time. That's not so great. We really want some strategies to improve the performance of our single models. That's really this idea of regularization, where we add something to our model to prevent it from
fitting the training data too well in the attempts
to make it perform better on unseen data. We've seen a couple
ideas, a couple methods for regularization already, where we add some explicit extra term to the loss. Where we have this one
term telling the model to fit the data, and another term that's a regularization term. You saw this in homework
one, where we used L2 regularization. As we talked about in lecture a couple lectures ago, this L2
regularization doesn't really make maybe a lot
of sense in the context of neural networks. Sometimes we use other
things for neural networks. One regularization strategy that's super, super common for neural networks is this idea of dropout. Dropout is super simple. Every time we do a forward pass through the network, at every
layer, we're going to randomly set some neurons to zero. Every time we do a forward pass, we'll set a different random subset of the neurons to zero. This kind of proceeds one layer at a time. We run through one layer, we compute the value of the layer, we randomly set some of them to zero,
and then we continue up through the network. Now if you look at this
fully connected network on the left versus a dropout version of the same network on
the right, you can see that after we do dropout, it kind of looks like a smaller version
of the same network, where we're only using
some subset of the neurons. This subset that we use
varies at each iteration, at each forward pass. Question? - [Student] [speaks too low to hear] - The question is what
are we setting to zero? It's the activations. Each layer is computing
previous activation times the weight matrix gives you our next activation. Then you just take that activation, set some of them to zero, and then your next layer will be
partially zeroed activations times another matrix give
you your next activations. Question? - [Student] [speaks too low to hear] - Question is which
layers do you do this on? It's more common in
fully connected layers, but you sometimes see this in
convolutional layers, as well. When you're working in
convolutional layers, sometimes instead of dropping each activation randomly,
instead you sometimes might drop entire feature maps randomly. In convolutions, you have
this channel dimension, and you might drop out entire channels, rather than random elements. Dropout is kind of super
simple in practice. It only requires adding two lines, one line per dropout call. Here we have a three-layer neural network, and we've added dropout. You can see that all we needed to do was add this extra line where we randomly set some things to zero. This is super easy to implement. But the question is why
is this even a good idea? We're seriously messing with the network at training time by setting a bunch of its values to zero. How can this possibly make sense? One sort of slightly hand wavy idea that people have is that
dropout helps prevent co-adaptation of features. Maybe if you imagine that we're trying to classify cats, maybe in some universe, the network might learn one neuron for having an ear, one
neuron for having a tail, one neuron for the input being furry. Then it kind of combines
these things together to decide whether or not it's a cat. But now if we have dropout, then in making the final decision about
catness, the network cannot depend too much on any of these one features. Instead, it kind of needs to distribute its idea of catness across
many different features. This might help prevent
overfitting somehow. Another interpretation of dropout that's come out a little bit more recently is that it's kind of like
doing model ensembling within a single model. If you look at the picture on the left, after you apply dropout to the network, we're kind of computing this subnetwork using some subset of the neurons. Now every different potential dropout mask leads to a different potential subnetwork. Now dropout is kind of
learning a whole ensemble of networks all at the same time that all share parameters. By the way, because of
the number of potential dropout masks grows
exponentially in the number of neurons, you're never going to sample all of these things. This is really a gigantic,
gigantic ensemble of networks that are all
being trained simultaneously. Then the question is what
happens at test time? Once we move to dropout,
we've kind of fundamentally changed the operation
of our neural network. Previously, we've had
our neural network, f, be a function of the weights, w, and the inputs, x, and then produce the output, y. But now, our network is also taking this additional input, z, which is some random dropout mask. That z is random. Having randomness at
test time is maybe bad. Imagine that you're working at Facebook, and you want to classify the images that people are uploading. Then today, your image
gets classified as a cat, and tomorrow it doesn't. That would be really weird and really bad. You'd probably want to eliminate this stochasticity at test
time once the network is already trained. Then we kind of want to average out this randomness. If you write this out, you can imagine actually marginalizing out this randomness with some integral, but in practice, this integral is totally intractable. We don't know how to evaluate this thing. You're in bad shape. One thing you might imagine doing is approximating this
integral via sampling, where you draw multiple samples of z and then average them out at test time, but this still would
introduce some randomness, which is little bit bad. Thankfully, in the case of dropout, we can actually approximate this integral in kind of a cheap way locally. If we consider a single
neuron, the output is a, the inputs are x and y, with two weights, w one, w two. Then at test time, our value a is just w one x plus w two y. Now imagine that we
trained to this network. During training, we used
dropout with probability 1/2 of dropping our neurons. Now the expected value
of a during training, we can kind of compute analytically for this small case. There's four possible dropout masks, and we're going to average out the values across these four masks. We can see that the expected value of a during training is 1/2
w one x plus w two y. There's this disconnect between this average value of w one x plus w two y at test time, and at training time, the average value is only 1/2 as much. One cheap thing we can do is that at test time, we don't
have any stochasticity. Instead, we just multiply this output by the dropout probability. Now these expected values are the same. This is kind of like a
local cheap approximation to this complex integral. This is what people really commonly do in practice with dropout. At dropout, we have this predict function, and we just multiply
our outputs of the layer by the dropout probability. The summary of dropout is
that it's really simple on the forward pass. You're just adding two
lines to your implementation to randomly zero out some nodes. Then at the test time prediction function, you just added one little multiplication by your probability. Dropout is super simple. It tends to work well sometimes for regularizing neural networks. By the way, one common
trick you see sometimes is this idea of inverted dropout. Maybe at test time, you
care more about efficiency, so you want to eliminate
that extra multiplication by p at test time. Then what you can do is, at test time, you use the entire weight matrix, but now at training time, instead you divide by p because training is
probably happening on a GPU. You don't really care if you do one extra multiply at training time, but then at test time, you kind of want this thing to be as efficient as possible. Question? - [Student] [speaks too low to hear] Now the gradient [speaks too low to hear]. - The question is what
happens to the gradient during training with dropout? You're right. We only end up propagating the gradients through the nodes that were not dropped. This has the consequence that when you're training with dropout, typically training takes longer because at each step, you're only updating some subparts of the network. When you're using dropout, it typically takes longer to train, but you might have a better generalization
after it's converged. Dropout, we kind of saw is like this one concrete instantiation. There's a little bit more general strategy for regularization where during training we add some kind of
randomness to the network to prevent it from fitting
the training data too well. To kind of mess it up and prevent it from fitting the training data perfectly. Now at test time, we want to average out all that randomness to hopefully improve our generalization. Dropout is probably
the most common example of this type of strategy, but actually batch normalization kind
of fits this idea, as well. Remember in batch
normalization, during training, one data point might appear
in different mini batches with different other data points. There's a bit of
stochasticity with respect to a single data point with how exactly that point gets normalized
during training. But now at test time,
we kind of average out this stochasticity by using some global estimates to normalize, rather than the per mini batch estimates. Actually batch normalization tends to have kind of a similar regularizing effect as dropout because they both introduce some kind of stochasticity or noise at training time, but then average it out at test time. Actually, when you train networks with batch normalization,
sometimes you don't use dropout at all, and just
the batch normalization adds enough of a regularizing effect to your network. Dropout is somewhat nice because you can actually tune the regularization strength by varying that parameter
p, and there's no such control in batch normalization. Another kind of strategy that fits in this paradigm is this
idea of data augmentation. During training, in a vanilla version for training, we have our
data, we have our label. We use it to update our
CNN at each time step. But instead, what we can do is randomly transform the image in
some way during training such that the label is preserved. Now we train on these
random transformations of the image rather than
the original images. Sometimes you might see
random horizontal flips 'cause if you take a cat and flip it horizontally, it's still a cat. You'll randomly sample
crops of different sizes from the image because the random crop of the cat is still a cat. Then during testing,
you kind of average out this stochasticity by evaluating with some fixed set of crops, often the four corners and the middle and their flips. What's very common is that when you read, for example, papers on
ImageNet, they'll report a single crop performance of their model, which is just like the whole image, and a 10 crop performance of their model, which are these five standard crops plus their flips. Also with data augmentation,
you'll sometimes use color jittering,
where you might randomly vary the contrast or
brightness of your image during training. You can get a little bit more complex with color jittering,
as well, where you try to make color jitters that are maybe in the PCA directions of your
data space or whatever, where you do some color jittering in some data-dependent way, but that's a little bit less common. In general, data
augmentation is this really general thing that you can apply to just about any problem. Whatever problem you're trying to solve, you kind of think about what are the ways that I can transform my data without changing the label? Now during training, you just apply these random transformations
to your input data. This sort of has a regularizing effect on the network because
you're, again, adding some kind of stochasticity
during training, and then marginalizing
it out at test time. Now we've seen three
examples of this pattern, dropout, batch normalization,
data augmentation, but there's many other examples, as well. Once you have this pattern in your mind, you'll kind of recognize this thing as you read other papers sometimes. There's another kind of
related idea to dropout called DropConnect. With DropConnect, it's the same idea, but rather than zeroing
out the activations at every forward pass, instead we randomly zero out some of the values
of the weight matrix instead. Again, it kind of has this similar flavor. Another kind of cool idea that I like, this one's not so commonly used, but I just think it's a really cool idea, is this idea of fractional max pooling. Normally when you do
two-by-two max pooling, you have these fixed two-by-two regions over which you pool over
in the forward pass, but now with fractional max pooling, every time we have our pooling layer, we're going to randomize exactly the pool that the regions over which we pool. Here in the example on the right, I've shown three different sets of random pooling regions
that you might see during training. Now during test time, you kind of average the stochasticity out by
trying many different, by either sticking to some
fixed set of pooling regions. or drawing many samples
and averaging over them. That's kind of a cool idea, even though it's not so commonly used. Another really kind of surprising paper in this paradigm that actually came out in the last year, so this is new since the last time we taught
the class, is this idea of stochastic depth. Here we have a network on the left. The idea is that we have
a very deep network. We're going to randomly
drop layers from the network during training. During training, we're going to eliminate some layers and only use some subset of the layers during training. Now during test time, we'll
use the whole network. This is kind of crazy. It's kind of amazing that this works, but this tends to have kind of a similar regularizing effect as dropout and these other strategies. But again, this is super,
super cutting-edge research. This is not super
commonly used in practice, but it is a cool idea. Any last minute questions
about regularization? No? Use it. It's a good idea. Yeah? - [Student] [speaks too low to hear] - The question is do you usually use more than one regularization method? You should generally be
using batch normalization as kind of a good thing to have in most networks nowadays because it helps you converge, especially
for very deep things. In many cases, batch normalization alone tends to be enough, but then sometimes if batch normalization
alone is not enough, then you can consider adding dropout or other thing once you see
your network overfitting. You generally don't do
a blind cross-validation over these things. Instead, you add them in in a targeted way once you see your network is overfitting. One quick thing, it's this
idea of transfer learning. We've kind of seen with regularization, we can help reduce the gap between train and test error by
adding these different regularization strategies. One problem with overfitting is sometimes you overfit 'cause you
don't have enough data. You want to use a big, powerful model, but that big, powerful
model just is going to overfit too much on your small dataset. Regularization is one way to combat that, but another way is through
using transfer learning. Transfer learning kind of busts this myth that you don't need a huge amount of data in order to train a CNN. The idea is really simple. You'll maybe first take some CNN. Here is kind of a VGG style architecture. You'll take your CNN, you'll train it in a very large dataset, like ImageNet, where you actually have enough data to train the whole network. Now the idea is that you want to apply the features from this dataset to some small dataset that you care about. Maybe instead of classifying the 1,000 ImageNet categories,
now you want to classify 10 dog breeds or something like that. You only have a small dataset. Here, our small dataset
only has C classes. Then what you'll typically
do is for this last fully connected layer that is going from the last layer features
to the final class scores, this now, you need to
reinitialize that matrix randomly. For ImageNet, it was a 4,096-by-1,000 dimensional matrix. Now for your new classes, it might be 4,096-by-C or by 10 or whatever. You reinitialize this
last matrix randomly, freeze the weights of
all the previous layers and now just basically
train a linear classifier, and only train the
parameters of this last layer and let it converge on your data. This tends to work pretty well if you only have a very small dataset to work with. Now if you have a little bit more data, another thing you can try is actually fine tuning the whole network. After that top layer converges and after you learn that last layer for your data, then you can consider
actually trying to update the whole network, as well. If you have more data,
then you might consider updating larger parts of the network. A general strategy here is that when you're updating the
network, you want to drop the learning rate from
its initial learning rate because probably the original parameters in this network that converged on ImageNet probably worked pretty well generally, and you just want to change
them a very small amount to tune performance for your dataset. Then when you're working
with transfer learning, you kind of imagine this two-by-two grid of scenarios where on
the one side, you have maybe very small amounts of data for your dataset, or very large amount of data for your dataset. Then maybe your data is
very similar to images. Like, ImageNet has a lot
of pictures of animals and plants and stuff like that. If you want to just classify other types of animals and plants
and other types of images like that, then you're
in pretty good shape. Then generally what you do is if your data is very similar to
something like ImageNet, if you have a very small amount of data, you can just basically
train a linear classifier on top of features, extracted using an ImageNet model. If you have a little bit
more data to work with, then you might imagine
fine tuning your data. However, you sometimes get in trouble if your data looks very
different from ImageNet. Maybe if you're working with maybe medical images that
are X-rays or CAT scans or something that looks very different from images in ImageNet, in that case, you maybe need to get a
little bit more creative. Sometimes it still works well here, but those last layer features might not be so informative. You might consider
reinitializing larger parts of the network and getting a little bit more creative and trying
more experiments here. This is somewhat mitigated if you have a large amount of data in
your very different dataset 'cause then you can actually fine tune larger parts of the network. Another point I'd like
to make is this idea of transfer learning is super pervasive. It's actually the norm,
rather than the exception. As you read computer vision papers, you'll often see system diagrams like this for different tasks. On the left, we're working
with object detection. On the right, we're working
with image captioning. Both of these models have a CNN that's kind of processing the image. In almost all applications
of computer vision these days, most people are not training these things from scratch. Almost always, that CNN will be pretrained on ImageNet, and then
potentially fine tuned for the task at hand. Also, in the captioning
sense, sometimes you can actually pretrain some word vectors relating to the language, as well. You maybe pretrain the CNN on ImageNet, pretrain some word vectors on a large text corpus, and then
fine tune the whole thing for your dataset. Although in the case
of captioning, I think this pretraining with word vectors tends to be a little bit less common and a little bit less critical. The takeaway for your projects, and more generally as you
work on different models, is that whenever you
have some large dataset, whenever you have some problem that you want to tackle, but you
don't have a large dataset, then what you should
generally do is download some pretrained model
that's relatively close to the task you care
about, and then either reinitialize parts of
that model or fine tune that model for your data. That tends to work pretty
well, even if you have only a modest amount of training data to work with. Because this is such a common strategy, all of the different deep learning software packages out there provide a model zoo where you can just download pretrained versions of various models. In summary today, we
talked about optimization, which is about how to
improve the training loss. We talked about regularization,
which is improving your performance on the test data. Model ensembling kind of fit into there. We also talked about transfer learning, which is how you can actually do better with less data. These are all super useful strategies. You should use them in
your projects and beyond. Next time, we'll talk
more concretely about some of the different deep learning software packages out there.",https://cs231n.github.io/neural-networks-3/,"




CS231n Convolutional Neural Networks for Visual Recognition









 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1\*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-46895817-2', 'auto');
 ga('send', 'pageview');
 



addBackToTop({
 backgroundColor: '#fff',
 innerHTML: 'Back to Top',
 textColor: '#333'
 })

 #back-to-top {
 border: 1px solid #ccc;
 border-radius: 0;
 font-family: sans-serif;
 font-size: 14px;
 width: 100px;
 text-align: center;
 line-height: 30px;
 height: 30px;
 }
 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io)
[Course Website](http://cs231n.stanford.edu/)





# 




Table of Contents:


* [Gradient checks](#gradcheck)
* [Sanity checks](#sanitycheck)
* [Babysitting the learning process](#baby)
	+ [Loss function](#loss)
	+ [Train/val accuracy](#accuracy)
	+ [Weights:Updates ratio](#ratio)
	+ [Activation/Gradient distributions per layer](#distr)
	+ [Visualization](#vis)
* [Parameter updates](#update)
	+ [First-order (SGD), momentum, Nesterov momentum](#sgd)
	+ [Annealing the learning rate](#anneal)
	+ [Second-order methods](#second)
	+ [Per-parameter adaptive learning rates (Adagrad, RMSProp)](#ada)
* [Hyperparameter Optimization](#hyper)
* [Evaluation](#eval)
	+ [Model Ensembles](#ensemble)
* [Summary](#summary)
* [Additional References](#add)


## Learning


In the previous sections we’ve discussed the static parts of a Neural Networks:
how we can set up the network connectivity, the data, and the loss function.
This section is devoted to the dynamics, or in other words, the process of
learning the parameters and finding good hyperparameters.



### Gradient Checks


In theory, performing a gradient check is as simple as comparing the analytic
gradient to the numerical gradient. In practice, the process is much more
involved and error prone. Here are some tips, tricks, and issues to watch out
for:


**Use the centered formula**. The formula you may have seen for the finite
difference approximation when evaluating the numerical gradient looks as
follows:



\[\frac{df(x)}{dx} = \frac{f(x + h) - f(x)}{h} \hspace{0.1in} \text{(bad, do not use)}\]

where \(h\) is a very small number, in practice approximately 1e-5 or so. In
practice, it turns out that it is much better to use the *centered* difference
formula of the form:



\[\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in} \text{(use instead)}\]

This requires you to evaluate the loss function twice to check every single
dimension of the gradient (so it is about 2 times as expensive), but the
gradient approximation turns out to be much more precise. To see this, you can
use Taylor expansion of \(f(x+h)\) and \(f(x-h)\) and verify that the first
formula has an error on order of \(O(h)\), while the second formula only has
error terms on order of \(O(h^2)\) (i.e. it is a second order approximation).


**Use relative error for the comparison**. What are the details of comparing the
numerical gradient \(f’\_n\) and analytic gradient \(f’\_a\)? That is, how do we
know if the two are not compatible? You might be temped to keep track of the
difference \(\mid f’\_a - f’\_n \mid \) or its square and define the gradient
check as failed if that difference is above a threshold. However, this is
problematic. For example, consider the case where their difference is 1e-4. This
seems like a very appropriate difference if the two gradients are about 1.0, so
we’d consider the two gradients to match. But if the gradients were both on
order of 1e-5 or lower, then we’d consider 1e-4 to be a huge difference and
likely a failure. Hence, it is always more appropriate to consider the *relative
error*:



\[\frac{\mid f'\_a - f'\_n \mid}{\max(\mid f'\_a \mid, \mid f'\_n \mid)}\]

which considers their ratio of the differences to the ratio of the absolute
values of both gradients. Notice that normally the relative error formula only
includes one of the two terms (either one), but I prefer to max (or add) both to
make it symmetric and to prevent dividing by zero in the case where one of the
two is zero (which can often happen, especially with ReLUs). However, one must
explicitly keep track of the case where both are zero and pass the gradient
check in that edge case. In practice:


* relative error > 1e-2 usually means the gradient is probably wrong
* 1e-2 > relative error > 1e-4 should make you feel uncomfortable
* 1e-4 > relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities and softmax), then 1e-4 is too high.
* 1e-7 and less you should be happy.


Also keep in mind that the deeper the network, the higher the relative errors
will be. So if you are gradient checking the input data for a 10-layer network,
a relative error of 1e-2 might be okay because the errors build up on the way.
Conversely, an error of 1e-2 for a single differentiable function likely
indicates incorrect gradient.


**Use double precision**. A common pitfall is using single precision floating
point to compute gradient check. It is often that case that you might get high
relative errors (as high as 1e-2) even with a correct gradient implementation.
In my experience I’ve sometimes seen my relative errors plummet from 1e-2 to
1e-8 by switching to double precision.


**Stick around active range of floating point**. It’s a good idea to read
through [“What Every Computer Scientist Should Know About Floating-Point
Arithmetic”](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html), as
it may demystify your errors and enable you to write more careful code. For
example, in neural nets it can be common to normalize the loss function over the
batch. However, if your gradients per datapoint are very small, then
*additionally* dividing them by the number of data points is starting to give
very small numbers, which in turn will lead to more numerical issues. This is
why I like to always print the raw numerical/analytic gradient, and make sure
that the numbers you are comparing are not extremely small (e.g. roughly 1e-10
and smaller in absolute value is worrying). If they are you may want to
temporarily scale your loss function up by a constant to bring them to a “nicer”
range where floats are more dense - ideally on the order of 1.0, where your
float exponent is 0.


**Kinks in the objective**. One source of inaccuracy to be aware of during
gradient checking is the problem of *kinks*. Kinks refer to non-differentiable
parts of an objective function, introduced by functions such as ReLU
(\(max(0,x)\)), or the SVM loss, Maxout neurons, etc. Consider gradient checking
the ReLU function at \(x = -1e6\). Since \(x < 0\), the analytic gradient at
this point is exactly zero. However, the numerical gradient would suddenly
compute a non-zero gradient because \(f(x+h)\) might cross over the kink (e.g.
if \(h > 1e-6\)) and introduce a non-zero contribution. You might think that
this is a pathological case, but in fact this case can be very common. For
example, an SVM for CIFAR-10 contains up to 450,000 \(max(0,x)\) terms because
there are 50,000 examples and each example yields 9 terms to the objective.
Moreover, a Neural Network with an SVM classifier will contain many more kinks
due to ReLUs.


Note that it is possible to know if a kink was crossed in the evaluation of the
loss. This can be done by keeping track of the identities of all “winners” in a
function of form \(max(x,y)\); That is, was x or y higher during the forward
pass. If the identity of at least one winner changes when evaluating \(f(x+h)\)
and then \(f(x-h)\), then a kink was crossed and the numerical gradient will not
be exact.


**Use only few datapoints**. One fix to the above problem of kinks is to use
fewer datapoints, since loss functions that contain kinks (e.g. due to use of
ReLUs or margin losses etc.) will have fewer kinks with fewer datapoints, so it
is less likely for you to cross one when you perform the finite different
approximation. Moreover, if your gradcheck for only ~2 or 3 datapoints then you
would almost certainly gradcheck for an entire batch. Using very few datapoints
also makes your gradient check faster and more efficient.


**Be careful with the step size h**. It is not necessarily the case that smaller
is better, because when \(h\) is much smaller, you may start running into
numerical precision problems. Sometimes when the gradient doesn’t check, it is
possible that you change \(h\) to be 1e-4 or 1e-6 and suddenly the gradient will
be correct. This [wikipedia
article](http://en.wikipedia.org/wiki/Numerical_differentiation) contains a
chart that plots the value of **h** on the x-axis and the numerical gradient
error on the y-axis.


**Gradcheck during a “characteristic” mode of operation**. It is important to
realize that a gradient check is performed at a particular (and usually random),
single point in the space of parameters. Even if the gradient check succeeds at
that point, it is not immediately certain that the gradient is correctly
implemented globally. Additionally, a random initialization might not be the
most “characteristic” point in the space of parameters and may in fact introduce
pathological situations where the gradient seems to be correctly implemented but
isn’t. For instance, an SVM with very small weight initialization will assign
almost exactly zero scores to all datapoints and the gradients will exhibit a
particular pattern across all datapoints. An incorrect implementation of the
gradient could still produce this pattern and not generalize to a more
characteristic mode of operation where some scores are larger than others.
Therefore, to be safe it is best to use a short **burn-in** time during which
the network is allowed to learn and perform the gradient check after the loss
starts to go down. The danger of performing it at the first iteration is that
this could introduce pathological edge cases and mask an incorrect
implementation of the gradient.


**Don’t let the regularization overwhelm the data**. It is often the case that a
loss function is a sum of the data loss and the regularization loss (e.g. L2
penalty on weights). One danger to be aware of is that the regularization loss
may overwhelm the data loss, in which case the gradients will be primarily
coming from the regularization term (which usually has a much simpler gradient
expression). This can mask an incorrect implementation of the data loss
gradient. Therefore, it is recommended to turn off regularization and check the
data loss alone first, and then the regularization term second and
independently. One way to perform the latter is to hack the code to remove the
data loss contribution. Another way is to increase the regularization strength
so as to ensure that its effect is non-negligible in the gradient check, and
that an incorrect implementation would be spotted.


**Remember to turn off dropout/augmentations**. When performing gradient check,
remember to turn off any non-deterministic effects in the network, such as
dropout, random data augmentations, etc. Otherwise these can clearly introduce
huge errors when estimating the numerical gradient. The downside of turning off
these effects is that you wouldn’t be gradient checking them (e.g. it might be
that dropout isn’t backpropagated correctly). Therefore, a better solution might
be to force a particular random seed before evaluating both \(f(x+h)\) and
\(f(x-h)\), and when evaluating the analytic gradient.


**Check only few dimensions**. In practice the gradients can have sizes of
million parameters. In these cases it is only practical to check some of the
dimensions of the gradient and assume that the others are correct. **Be
careful**: One issue to be careful with is to make sure to gradient check a few
dimensions for every separate parameter. In some applications, people combine
the parameters into a single large parameter vector for convenience. In these
cases, for example, the biases could only take up a tiny number of parameters
from the whole vector, so it is important to not sample at random but to take
this into account and check that all parameters receive the correct gradients.



### Before learning: sanity checks Tips/Tricks


Here are a few sanity checks you might consider running before you plunge into
expensive optimization:


* **Look for correct loss at chance performance.** Make sure you’re getting the loss you expect when you initialize with small parameters. It’s best to first check the data loss alone (so set regularization strength to zero). For example, for CIFAR-10 with a Softmax classifier we would expect the initial loss to be 2.302, because we expect a diffuse probability of 0.1 for each class (since there are 10 classes), and Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302. For The Weston Watkins SVM, we expect all desired margins to be violated (since all scores are approximately zero), and hence expect a loss of 9 (since margin is 1 for each wrong class). If you’re not seeing these losses there might be issue with initialization.
* As a second sanity check, increasing the regularization strength should increase the loss
* **Overfit a tiny subset of data**. Lastly and most importantly, before training on the full dataset try to train on a tiny portion (e.g. 20 examples) of your data and make sure you can achieve zero cost. For this experiment it’s also best to set regularization to zero, otherwise this can prevent you from getting zero cost. Unless you pass this sanity check with a small dataset it is not worth proceeding to the full dataset. Note that it may happen that you can overfit very small dataset but still have an incorrect implementation. For instance, if your datapoints’ features are random due to some bug, then it will be possible to overfit your small training set but you will never notice any generalization when you fold it your full dataset.



### Babysitting the learning process


There are multiple useful quantities you should monitor during training of a
neural network. These plots are the window into the training process and should
be utilized to get intuitions about different hyperparameter settings and how
they should be changed for more efficient learning.


The x-axis of the plots below are always in units of epochs, which measure how
many times every example has been seen during training in expectation (e.g. one
epoch means that every example has been seen once). It is preferable to track
epochs rather than iterations since the number of iterations depends on the
arbitrary setting of batch size.



#### Loss function


The first quantity that is useful to track during training is the loss, as it is
evaluated on the individual batches during the forward pass. Below is a cartoon
diagram showing the loss over time, and especially what the shape might tell you
about the learning rate:



![](/assets/nn3/learningrates.jpeg)
![](/assets/nn3/loss.jpeg)

**Left:** A cartoon depicting the effects of different learning rates. With low learning rates the improvements will be linear. With high learning rates they will start to look more exponential. Higher learning rates will decay the loss faster, but they get stuck at worse values of loss (green line). This is because there is too much ""energy"" in the optimization and the parameters are bouncing around chaotically, unable to settle in a nice spot in the optimization landscape. **Right:** An example of a typical loss function over time, while training a small network on CIFAR-10 dataset. This loss function looks reasonable (it might indicate a slightly too small learning rate based on its speed of decay, but it's hard to say), and also indicates that the batch size might be a little too low (since the cost is a little too noisy).
 

The amount of “wiggle” in the loss is related to the batch size. When the batch
size is 1, the wiggle will be relatively high. When the batch size is the full
dataset, the wiggle will be minimal because every gradient update should be
improving the loss function monotonically (unless the learning rate is set too
high).


Some people prefer to plot their loss functions in the log domain. Since
learning progress generally takes an exponential form shape, the plot appears as
a slightly more interpretable straight line, rather than a hockey stick.
Additionally, if multiple cross-validated models are plotted on the same loss
graph, the differences between them become more apparent.


Sometimes loss functions can look funny
[lossfunctions.tumblr.com](http://lossfunctions.tumblr.com/).



#### Train/Val accuracy


The second important quantity to track while training a classifier is the
validation/training accuracy. This plot can give you valuable insights into the
amount of overfitting in your model:



![](/assets/nn3/accuracies.jpeg)

 The gap between the training and validation accuracy indicates the amount of overfitting. Two possible cases are shown in the diagram on the left. The blue validation error curve shows very small validation accuracy compared to the training accuracy, indicating strong overfitting (note, it's possible for the validation accuracy to even start to go down after some point). When you see this in practice you probably want to increase regularization (stronger L2 weight penalty, more dropout, etc.) or collect more data. The other possible case is when the validation accuracy tracks the training accuracy fairly well. This case indicates that your model capacity is not high enough: make the model larger by increasing the number of parameters.
 



#### Ratio of weights:updates


The last quantity you might want to track is the ratio of the update magnitudes
to the value magnitudes. Note: *updates*, not the raw gradients (e.g. in vanilla
sgd this would be the gradient multiplied by the learning rate). You might want
to evaluate and track this ratio for every set of parameters independently. A
rough heuristic is that this ratio should be somewhere around 1e-3. If it is
lower than this then the learning rate might be too low. If it is higher then
the learning rate is likely too high. Here is a specific example:



```python
# assume parameter vector W and its gradient vector dW
param\_scale = np.linalg.norm(W.ravel())
update = -learning\_rate\*dW # simple SGD update
update\_scale = np.linalg.norm(update.ravel())
W += update # the actual update
print update\_scale / param\_scale # want ~1e-3

```

Instead of tracking the min or the max, some people prefer to compute and track
the norm of the gradients and their updates instead. These metrics are usually
correlated and often give approximately the same results.



#### Activation / Gradient distributions per layer


An incorrect initialization can slow down or even completely stall the learning
process. Luckily, this issue can be diagnosed relatively easily. One way to do
so is to plot activation/gradient histograms for all layers of the network.
Intuitively, it is not a good sign to see any strange distributions - e.g. with
tanh neurons we would like to see a distribution of neuron activations between
the full range of [-1,1], instead of seeing all neurons outputting zero, or all
neurons being completely saturated at either -1 or 1.



#### First-layer Visualizations


Lastly, when one is working with image pixels it can be helpful and satisfying
to plot the first-layer features visually:



![](/assets/nn3/weights.jpeg)
![](/assets/nn3/cnnweights.jpg)

 Examples of visualized weights for the first layer of a neural network. **Left**: Noisy features indicate could be a symptom: Unconverged network, improperly set learning rate, very low weight regularization penalty. **Right:** Nice, smooth, clean and diverse features are a good indication that the training is proceeding well.
 


### Parameter updates


Once the analytic gradient is computed with backpropagation, the gradients are
used to perform a parameter update. There are several approaches for performing
the update, which we discuss next.


We note that optimization for deep networks is currently a very active area of
research. In this section we highlight some established and common techniques
you may see in practice, briefly describe their intuition, but leave a detailed
analysis outside of the scope of the class. We provide some further pointers for
an interested reader.



#### SGD and bells and whistles


**Vanilla update**. The simplest form of update is to change the parameters
along the negative gradient direction (since the gradient indicates the
direction of increase, but we usually wish to minimize a loss function).
Assuming a vector of parameters `x` and the gradient `dx`, the simplest update
has the form:



```python
# Vanilla update
x += - learning\_rate \* dx

```

where `learning_rate` is a hyperparameter - a fixed constant. When evaluated on
the full dataset, and when the learning rate is low enough, this is guaranteed
to make non-negative progress on the loss function.


**Momentum update** is another approach that almost always enjoys better
converge rates on deep networks. This update can be motivated from a physical
perspective of the optimization problem. In particular, the loss can be
interpreted as the height of a hilly terrain (and therefore also to the
potential energy since \(U = mgh\) and therefore \( U \propto h \) ).
Initializing the parameters with random numbers is equivalent to setting a
particle with zero initial velocity at some location. The optimization process
can then be seen as equivalent to the process of simulating the parameter vector
(i.e. a particle) as rolling on the landscape.


Since the force on the particle is related to the gradient of potential energy
(i.e. \(F = - \nabla U \) ), the **force** felt by the particle is precisely the
(negative) **gradient** of the loss function. Moreover, \(F = ma \) so the
(negative) gradient is in this view proportional to the acceleration of the
particle. Note that this is different from the SGD update shown above, where the
gradient directly integrates the position. Instead, the physics view suggests an
update in which the gradient only directly influences the velocity, which in
turn has an effect on the position:



```python
# Momentum update
v = mu \* v - learning\_rate \* dx # integrate velocity
x += v # integrate position

```

Here we see an introduction of a `v` variable that is initialized at zero, and
an additional hyperparameter (`mu`). As an unfortunate misnomer, this variable
is in optimization referred to as *momentum* (its typical value is about 0.9),
but its physical meaning is more consistent with the coefficient of friction.
Effectively, this variable damps the velocity and reduces the kinetic energy of
the system, or otherwise the particle would never come to a stop at the bottom
of a hill. When cross-validated, this parameter is usually set to values such as
[0.5, 0.9, 0.95, 0.99]. Similar to annealing schedules for learning rates
(discussed later, below), optimization can sometimes benefit a little from
momentum schedules, where the momentum is increased in later stages of learning.
A typical setting is to start with momentum of about 0.5 and anneal it to 0.99
or so over multiple epochs.



> 
> With Momentum update, the parameter vector will build up velocity in any
> direction that has consistent gradient.
> 
> 
> 


**Nesterov Momentum** is a slightly different version of the momentum update
that has recently been gaining popularity. It enjoys stronger theoretical
converge guarantees for convex functions and in practice it also consistenly
works slightly better than standard momentum.


The core idea behind Nesterov momentum is that when the current parameter vector
is at some position `x`, then looking at the momentum update above, we know that
the momentum term alone (i.e. ignoring the second term with the gradient) is
about to nudge the parameter vector by `mu * v`. Therefore, if we are about to
compute the gradient, we can treat the future approximate position `x + mu * v`
as a “lookahead” - this is a point in the vicinity of where we are soon going to
end up. Hence, it makes sense to compute the gradient at `x + mu * v` instead of
at the “old/stale” position `x`.



![](/assets/nn3/nesterov.jpeg)

 Nesterov momentum. Instead of evaluating gradient at the current position (red circle), we know that our momentum is about to carry us to the tip of the green arrow. With Nesterov momentum we therefore instead evaluate the gradient at this ""looked-ahead"" position.
 

That is, in a slightly awkward notation, we would like to do the following:



```python
x\_ahead = x + mu \* v
# evaluate dx\_ahead (the gradient at x\_ahead instead of at x)
v = mu \* v - learning\_rate \* dx\_ahead
x += v

```

However, in practice people prefer to express the update to look as similar to
vanilla SGD or to the previous momentum update as possible. This is possible to
achieve by manipulating the update above with a variable transform `x_ahead = x
+ mu * v`, and then expressing the update in terms of `x_ahead` instead of `x`.
That is, the parameter vector we are actually storing is always the ahead
version. The equations in terms of `x_ahead` (but renaming it back to `x`) then
become:



```python
v\_prev = v # back this up
v = mu \* v - learning\_rate \* dx # velocity update stays the same
x += -mu \* v\_prev + (1 + mu) \* v # position update changes form

```

We recommend this further reading to understand the source of these equations
and the mathematical formulation of Nesterov’s Accelerated Momentum (NAG):


* [Advances in optimizing Recurrent Networks](http://arxiv.org/pdf/1212.0901v2.pdf) by Yoshua Bengio, Section 3.5.
* [Ilya Sutskever’s thesis](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf) (pdf) contains a longer exposition of the topic in section 7.2



#### Annealing the learning rate


In training deep networks, it is usually helpful to anneal the learning rate
over time. Good intuition to have in mind is that with a high learning rate, the
system contains too much kinetic energy and the parameter vector bounces around
chaotically, unable to settle down into deeper, but narrower parts of the loss
function. Knowing when to decay the learning rate can be tricky: Decay it slowly
and you’ll be wasting computation bouncing around chaotically with little
improvement for a long time. But decay it too aggressively and the system will
cool too quickly, unable to reach the best position it can. There are three
common types of implementing the learning rate decay:


* **Step decay**: Reduce the learning rate by some factor every few epochs. Typical values might be reducing the learning rate by a half every 5 epochs, or by 0.1 every 20 epochs. These numbers depend heavily on the type of problem and the model. One heuristic you may see in practice is to watch the validation error while training with a fixed learning rate, and reduce the learning rate by a constant (e.g. 0.5) whenever the validation error stops improving.
* **Exponential decay.** has the mathematical form \(\alpha = \alpha\_0 e^{-k t}\), where \(\alpha\_0, k\) are hyperparameters and \(t\) is the iteration number (but you can also use units of epochs).
* **1/t decay** has the mathematical form \(\alpha = \alpha\_0 / (1 + k t )\) where \(a\_0, k\) are hyperparameters and \(t\) is the iteration number.


In practice, we find that the step decay is slightly preferable because the
hyperparameters it involves (the fraction of decay and the step timings in units
of epochs) are more interpretable than the hyperparameter \(k\). Lastly, if you
can afford the computational budget, err on the side of slower decay and train
for a longer time.



#### Second order methods


A second, popular group of methods for optimization in context of deep learning
is based on [Newton’s
method](http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization), which
iterates the following update:



\[x \leftarrow x - [H f(x)]^{-1} \nabla f(x)\]

Here, \(H f(x)\) is the [Hessian
matrix](http://en.wikipedia.org/wiki/Hessian_matrix), which is a square matrix
of second-order partial derivatives of the function. The term \(\nabla f(x)\) is
the gradient vector, as seen in Gradient Descent. Intuitively, the Hessian
describes the local curvature of the loss function, which allows us to perform a
more efficient update. In particular, multiplying by the inverse Hessian leads
the optimization to take more aggressive steps in directions of shallow
curvature and shorter steps in directions of steep curvature. Note, crucially,
the absence of any learning rate hyperparameters in the update formula, which
the proponents of these methods cite this as a large advantage over first-order
methods.


However, the update above is impractical for most deep learning applications
because computing (and inverting) the Hessian in its explicit form is a very
costly process in both space and time. For instance, a Neural Network with one
million parameters would have a Hessian matrix of size [1,000,000 x 1,000,000],
occupying approximately 3725 gigabytes of RAM. Hence, a large variety of
*quasi-Newton* methods have been developed that seek to approximate the inverse
Hessian. Among these, the most popular is
[L-BFGS](http://en.wikipedia.org/wiki/Limited-memory_BFGS), which uses the
information in the gradients over time to form the approximation implicitly
(i.e. the full matrix is never computed).


However, even after we eliminate the memory concerns, a large downside of a
naive application of L-BFGS is that it must be computed over the entire training
set, which could contain millions of examples. Unlike mini-batch SGD, getting
L-BFGS to work on mini-batches is more tricky and an active area of research.


**In practice**, it is currently not common to see L-BFGS or similar
second-order methods applied to large-scale Deep Learning and Convolutional
Neural Networks. Instead, SGD variants based on (Nesterov’s) momentum are more
standard because they are simpler and scale more easily.


Additional references:


* [Large Scale Distributed Deep Networks](http://research.google.com/archive/large_deep_networks_nips2012.html) is a paper from the Google Brain team, comparing L-BFGS and SGD variants in large-scale distributed optimization.
* [SFO](http://arxiv.org/abs/1311.2115) algorithm strives to combine the advantages of SGD with advantages of L-BFGS.



#### Per-parameter adaptive learning rate methods


All previous approaches we’ve discussed so far manipulated the learning rate
globally and equally for all parameters. Tuning the learning rates is an
expensive process, so much work has gone into devising methods that can
adaptively tune the learning rates, and even do so per parameter. Many of these
methods may still require other hyperparameter settings, but the argument is
that they are well-behaved for a broader range of hyperparameter values than the
raw learning rate. In this section we highlight some common adaptive methods you
may encounter in practice:


**Adagrad** is an adaptive learning rate method originally proposed by [Duchi et
al.](http://jmlr.org/papers/v12/duchi11a.html).



```python
# Assume the gradient dx and parameter vector x
cache += dx\*\*2
x += - learning\_rate \* dx / (np.sqrt(cache) + eps)

```

Notice that the variable `cache` has size equal to the size of the gradient, and
keeps track of per-parameter sum of squared gradients. This is then used to
normalize the parameter update step, element-wise. Notice that the weights that
receive high gradients will have their effective learning rate reduced, while
weights that receive small or infrequent updates will have their effective
learning rate increased. Amusingly, the square root operation turns out to be
very important and without it the algorithm performs much worse. The smoothing
term `eps` (usually set somewhere in range from 1e-4 to 1e-8) avoids division by
zero. A downside of Adagrad is that in case of Deep Learning, the monotonic
learning rate usually proves too aggressive and stops learning too early.


**RMSprop.** RMSprop is a very effective, but currently unpublished adaptive
learning rate method. Amusingly, everyone who uses this method in their work
currently cites [slide 29 of Lecture
6](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) of
Geoff Hinton’s Coursera class. The RMSProp update adjusts the Adagrad method in
a very simple way in an attempt to reduce its aggressive, monotonically
decreasing learning rate. In particular, it uses a moving average of squared
gradients instead, giving:



```python
cache = decay\_rate \* cache + (1 - decay\_rate) \* dx\*\*2
x += - learning\_rate \* dx / (np.sqrt(cache) + eps)

```

Here, `decay_rate` is a hyperparameter and typical values are [0.9, 0.99,
0.999]. Notice that the `x+=` update is identical to Adagrad, but the `cache`
variable is a “leaky”. Hence, RMSProp still modulates the learning rate of each
weight based on the magnitudes of its gradients, which has a beneficial
equalizing effect, but unlike Adagrad the updates do not get monotonically
smaller.


**Adam.** [Adam](http://arxiv.org/abs/1412.6980) is a recently proposed update
that looks a bit like RMSProp with momentum. The (simplified) update looks as
follows:



```python
m = beta1\*m + (1-beta1)\*dx
v = beta2\*v + (1-beta2)\*(dx\*\*2)
x += - learning\_rate \* m / (np.sqrt(v) + eps)

```

Notice that the update looks exactly as RMSProp update, except the “smooth”
version of the gradient `m` is used instead of the raw (and perhaps noisy)
gradient vector `dx`. Recommended values in the paper are `eps = 1e-8`, `beta1 =
0.9`, `beta2 = 0.999`. In practice Adam is currently recommended as the default
algorithm to use, and often works slightly better than RMSProp. However, it is
often also worth trying SGD+Nesterov Momentum as an alternative. The full Adam
update also includes a *bias correction* mechanism, which compensates for the
fact that in the first few time steps the vectors `m,v` are both initialized and
therefore biased at zero, before they fully “warm up”. With the *bias
correction* mechanism, the update looks as follows:



```python
# t is your iteration counter going from 1 to infinity
m = beta1\*m + (1-beta1)\*dx
mt = m / (1-beta1\*\*t)
v = beta2\*v + (1-beta2)\*(dx\*\*2)
vt = v / (1-beta2\*\*t)
x += - learning\_rate \* mt / (np.sqrt(vt) + eps)

```

Note that the update is now a function of the iteration as well as the other
parameters. We refer the reader to the paper for the details, or the course
slides where this is expanded on.


Additional References:


* [Unit Tests for Stochastic Optimization](http://arxiv.org/abs/1312.6055) proposes a series of tests as a standardized benchmark for stochastic optimization.



![](/assets/nn3/opt2.gif)
![](/assets/nn3/opt1.gif)

 Animations that may help your intuitions about the learning process dynamics. **Left:** Contours of a loss surface and time evolution of different optimization algorithms. Notice the ""overshooting"" behavior of momentum-based methods, which make the optimization look like a ball rolling down the hill. **Right:** A visualization of a saddle point in the optimization landscape, where the curvature along different dimension has different signs (one dimension curves up and another down). Notice that SGD has a very hard time breaking symmetry and gets stuck on the top. Conversely, algorithms such as RMSprop will see very low gradients in the saddle direction. Due to the denominator term in the RMSprop update, this will increase the effective learning rate along this direction, helping RMSProp proceed. Images credit: [Alec Radford](https://twitter.com/alecrad).
 


### Hyperparameter optimization


As we’ve seen, training Neural Networks can involve many hyperparameter
settings. The most common hyperparameters in context of Neural Networks include:


* the initial learning rate
* learning rate decay schedule (such as the decay constant)
* regularization strength (L2 penalty, dropout strength)


But as we saw, there are many more relatively less sensitive hyperparameters,
for example in per-parameter adaptive learning methods, the setting of momentum
and its schedule, etc. In this section we describe some additional tips and
tricks for performing the hyperparameter search:


**Implementation**. Larger Neural Networks typically require a long time to
train, so performing hyperparameter search can take many days/weeks. It is
important to keep this in mind since it influences the design of your code base.
One particular design is to have a **worker** that continuously samples random
hyperparameters and performs the optimization. During the training, the worker
will keep track of the validation performance after every epoch, and writes a
model checkpoint (together with miscellaneous training statistics such as the
loss over time) to a file, preferably on a shared file system. It is useful to
include the validation performance directly in the filename, so that it is
simple to inspect and sort the progress. Then there is a second program which we
will call a **master**, which launches or kills workers across a computing
cluster, and may additionally inspect the checkpoints written by workers and
plot their training statistics, etc.


**Prefer one validation fold to cross-validation**. In most cases a single
validation set of respectable size substantially simplifies the code base,
without the need for cross-validation with multiple folds. You’ll hear people
say they “cross-validated” a parameter, but many times it is assumed that they
still only used a single validation set.


**Hyperparameter ranges**. Search for hyperparameters on log scale. For example,
a typical sampling of the learning rate would look as follows: `learning_rate =
10 ** uniform(-6, 1)`. That is, we are generating a random number from a uniform
distribution, but then raising it to the power of 10. The same strategy should
be used for the regularization strength. Intuitively, this is because learning
rate and regularization strength have multiplicative effects on the training
dynamics. For example, a fixed change of adding 0.01 to a learning rate has huge
effects on the dynamics if the learning rate is 0.001, but nearly no effect if
the learning rate when it is 10. This is because the learning rate multiplies
the computed gradient in the update. Therefore, it is much more natural to
consider a range of learning rate multiplied or divided by some value, than a
range of learning rate added or subtracted to by some value. Some parameters
(e.g. dropout) are instead usually searched in the original scale (e.g. `dropout
= uniform(0,1)`).


**Prefer random search to grid search**. As argued by Bergstra and Bengio in
[Random Search for Hyper-Parameter
Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf),
“randomly chosen trials are more efficient for hyper-parameter optimization than
trials on a grid”. As it turns out, this is also usually easier to implement.



![](/assets/nn3/gridsearchbad.jpeg)

 Core illustration from [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) by Bergstra and Bengio. It is very often the case that some of the hyperparameters matter much more than others (e.g. top hyperparam vs. left one in this figure). Performing random search rather than grid search allows you to much more precisely discover good values for the important ones.
 

**Careful with best values on border**. Sometimes it can happen that you’re
searching for a hyperparameter (e.g. learning rate) in a bad range. For example,
suppose we use `learning_rate = 10 ** uniform(-6, 1)`. Once we receive the
results, it is important to double check that the final learning rate is not at
the edge of this interval, or otherwise you may be missing more optimal
hyperparameter setting beyond the interval.


**Stage your search from coarse to fine**. In practice, it can be helpful to
first search in coarse ranges (e.g. 10 \*\* [-6, 1]), and then depending on
where the best results are turning up, narrow the range. Also, it can be helpful
to perform the initial coarse search while only training for 1 epoch or even
less, because many hyperparameter settings can lead the model to not learn at
all, or immediately explode with infinite cost. The second stage could then
perform a narrower search with 5 epochs, and the last stage could perform a
detailed search in the final range for many more epochs (for example).


**Bayesian Hyperparameter Optimization** is a whole area of research devoted to
coming up with algorithms that try to more efficiently navigate the space of
hyperparameters. The core idea is to appropriately balance the exploration -
exploitation trade-off when querying the performance at different
hyperparameters. Multiple libraries have been developed based on these models as
well, among some of the better known ones are
[Spearmint](https://github.com/JasperSnoek/spearmint),
[SMAC](http://www.cs.ubc.ca/labs/beta/Projects/SMAC/), and
[Hyperopt](http://jaberg.github.io/hyperopt/). However, in practical settings
with ConvNets it is still relatively difficult to beat random search in a
carefully-chosen intervals. See some additional from-the-trenches discussion
[here](http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html).



## Evaluation



### Model Ensembles


In practice, one reliable approach to improving the performance of Neural
Networks by a few percent is to train multiple independent models, and at test
time average their predictions. As the number of models in the ensemble
increases, the performance typically monotonically improves (though with
diminishing returns). Moreover, the improvements are more dramatic with higher
model variety in the ensemble. There are a few approaches to forming an
ensemble:


* **Same model, different initializations**. Use cross-validation to determine the best hyperparameters, then train multiple models with the best set of hyperparameters but with different random initialization. The danger with this approach is that the variety is only due to initialization.
* **Top models discovered during cross-validation**. Use cross-validation to determine the best hyperparameters, then pick the top few (e.g. 10) models to form the ensemble. This improves the variety of the ensemble but has the danger of including suboptimal models. In practice, this can be easier to perform since it doesn’t require additional retraining of models after cross-validation
* **Different checkpoints of a single model**. If training is very expensive, some people have had limited success in taking different checkpoints of a single network over time (for example after every epoch) and using those to form an ensemble. Clearly, this suffers from some lack of variety, but can still work reasonably well in practice. The advantage of this approach is that is very cheap.
* **Running average of parameters during training**. Related to the last point, a cheap way of almost always getting an extra percent or two of performance is to maintain a second copy of the network’s weights in memory that maintains an exponentially decaying sum of previous weights during training. This way you’re averaging the state of the network over last several iterations. You will find that this “smoothed” version of the weights over last few steps almost always achieves better validation error. The rough intuition to have in mind is that the objective is bowl-shaped and your network is jumping around the mode, so the average has a higher chance of being somewhere nearer the mode.


One disadvantage of model ensembles is that they take longer to evaluate on test
example. An interested reader may find the recent work from Geoff Hinton on
[“Dark Knowledge”](https://www.youtube.com/watch?v=EK61htlw8hY) inspiring, where
the idea is to “distill” a good ensemble back to a single model by incorporating
the ensemble log likelihoods into a modified objective.



## Summary


To train a Neural Network:


* Gradient check your implementation with a small batch of data and be aware of the pitfalls.
* As a sanity check, make sure your initial loss is reasonable, and that you can achieve 100% training accuracy on a very small portion of the data
* During training, monitor the loss, the training/validation accuracy, and if you’re feeling fancier, the magnitude of updates in relation to parameter values (it should be ~1e-3), and when dealing with ConvNets, the first-layer weights.
* The two recommended updates to use are either SGD+Nesterov Momentum or Adam.
* Decay your learning rate over the period of the training. For example, halve the learning rate after a fixed number of epochs, or whenever the validation accuracy tops off.
* Search for good hyperparameters with random search (not grid search). Stage your search from coarse (wide hyperparameter ranges, training only for 1-5 epochs), to fine (narrower rangers, training for many more epochs)
* Form model ensembles for extra performance



## Additional References


* [SGD](http://research.microsoft.com/pubs/192769/tricks-2012.pdf) tips and tricks from Leon Bottou
* [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) (pdf) from Yann LeCun
* [Practical Recommendations for Gradient-Based Training of Deep
Architectures](http://arxiv.org/pdf/1206.5533v2.pdf) from Yoshua Bengio









* [cs231n](https://github.com/cs231n)
* [cs231n](https://twitter.com/cs231n)












 // Make responsive
 MathJax.Hub.Config({
 ""HTML-CSS"": { linebreaks: { automatic: true } },
 ""SVG"": { linebreaks: { automatic: true } },
 });
 


"
