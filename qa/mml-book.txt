The fundamental mathematical tools needed to understand machine
learning include linear algebra, analytic geometry, matrix decompositions,
vector calculus, optimization, probability and statistics. These topics MATHEMATICS
are traditionally taught in disparate courses, making it hard for data FOR
science or computer science students, or professionals, to effi ciently learn
the mathematics. This self-contained textbook bridges the gap between
mathematical and machine learning texts, introducing the mathematical
concepts with a minimum of prerequisites. It uses these concepts to MACHINE LEAR NING
derive four central machine learning methods: linear regression, principal
component analysis, Gaussian mixture models and support vector machines.
For students and others with a mathematical background, these derivations
provide a starting point to machine learning texts. For those learning the
mathematics for the fi rst time, the methods help build intuition and practical
experience with applying mathematical concepts. Every chapter includes
worked examples and exercises to test understanding. Programming
tutorials are offered on the book’s web site.
MARC PETER DEISENROTH is Senior Lecturer in Statistical Machine
Learning at the Department of Computing, Împerial College London.
A. ALDO FAISAL leads the Brain & Behaviour Lab at Imperial College
London, where he is also Reader in Neurotechnology at the Department of
Bioengineering and the Department of Computing.
CHENG SOON ONG is Principal Research Scientist at the Machine Learning
Research Group, Data61, CSIRO. He is also Adjunct Associate Professor at
Australian National University.
Marc Peter Deisenroth
A. Aldo Faisal
Cheng Soon Ong
Cover image courtesy of Daniel Bosma / Moment / Getty Images
Cover design by Holly Johnson
Deisenrith
et
al.
9781108455145
Cover.
C
M
Y
K
DEISENROTH
ET
AL.
MATHEMATICS
FOR
MACHINE
LEARNING

Contents
Foreword 1
Part I Mathematical Foundations 9
1 IntroductionandMotivation 11
1.1 FindingWordsforIntuitions 12
1.2 TwoWaystoReadThisBook 13
1.3 ExercisesandFeedback 16
2 LinearAlgebra 17
2.1 SystemsofLinearEquations 19
2.2 Matrices 22
2.3 SolvingSystemsofLinearEquations 27
2.4 VectorSpaces 35
2.5 LinearIndependence 40
2.6 BasisandRank 44
2.7 LinearMappings 48
2.8 AffineSpaces 61
2.9 FurtherReading 63
Exercises 64
3 AnalyticGeometry 70
3.1 Norms 71
3.2 InnerProducts 72
3.3 LengthsandDistances 75
3.4 AnglesandOrthogonality 76
3.5 OrthonormalBasis 78
3.6 OrthogonalComplement 79
3.7 InnerProductofFunctions 80
3.8 OrthogonalProjections 81
3.9 Rotations 91
3.10 FurtherReading 94
Exercises 96
4 MatrixDecompositions 98
4.1 DeterminantandTrace 99
i
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
ii Contents
4.2 EigenvaluesandEigenvectors 105
4.3 CholeskyDecomposition 114
4.4 EigendecompositionandDiagonalization 115
4.5 SingularValueDecomposition 119
4.6 MatrixApproximation 129
4.7 MatrixPhylogeny 134
4.8 FurtherReading 135
Exercises 137
5 VectorCalculus 139
5.1 DifferentiationofUnivariateFunctions 141
5.2 PartialDifferentiationandGradients 146
5.3 GradientsofVector-ValuedFunctions 149
5.4 GradientsofMatrices 155
5.5 UsefulIdentitiesforComputingGradients 158
5.6 BackpropagationandAutomaticDifferentiation 159
5.7 Higher-OrderDerivatives 164
5.8 LinearizationandMultivariateTaylorSeries 165
5.9 FurtherReading 170
Exercises 170
6 ProbabilityandDistributions 172
6.1 ConstructionofaProbabilitySpace 172
6.2 DiscreteandContinuousProbabilities 178
6.3 SumRule,ProductRule,andBayes’Theorem 183
6.4 SummaryStatisticsandIndependence 186
6.5 GaussianDistribution 197
6.6 ConjugacyandtheExponentialFamily 205
6.7 ChangeofVariables/InverseTransform 214
6.8 FurtherReading 221
Exercises 222
7 ContinuousOptimization 225
7.1 OptimizationUsingGradientDescent 227
7.2 ConstrainedOptimizationandLagrangeMultipliers 233
7.3 ConvexOptimization 236
7.4 FurtherReading 246
Exercises 247
Part II Central Machine Learning Problems 249
8 WhenModelsMeetData 251
8.1 Data,Models,andLearning 251
8.2 EmpiricalRiskMinimization 258
8.3 ParameterEstimation 265
8.4 ProbabilisticModelingandInference 272
8.5 DirectedGraphicalModels 278
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Contents iii
8.6 ModelSelection 283
9 LinearRegression 289
9.1 ProblemFormulation 291
9.2 ParameterEstimation 292
9.3 BayesianLinearRegression 303
9.4 MaximumLikelihoodasOrthogonalProjection 313
9.5 FurtherReading 315
10 DimensionalityReductionwithPrincipalComponentAnalysis 317
10.1 ProblemSetting 318
10.2 MaximumVariancePerspective 320
10.3 ProjectionPerspective 325
10.4 EigenvectorComputationandLow-RankApproximations 333
10.5 PCAinHighDimensions 335
10.6 KeyStepsofPCAinPractice 336
10.7 LatentVariablePerspective 339
10.8 FurtherReading 343
11 DensityEstimationwithGaussianMixtureModels 348
11.1 GaussianMixtureModel 349
11.2 ParameterLearningviaMaximumLikelihood 350
11.3 EMAlgorithm 360
11.4 Latent-VariablePerspective 363
11.5 FurtherReading 368
12 ClassificationwithSupportVectorMachines 370
12.1 SeparatingHyperplanes 372
12.2 PrimalSupportVectorMachine 374
12.3 DualSupportVectorMachine 383
12.4 Kernels 388
12.5 NumericalSolution 390
12.6 FurtherReading 392
References 395
Index 407
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).

Foreword
Machine learning is the latest in a long line of attempts to distill human
knowledgeandreasoningintoaformthatissuitableforconstructingma-
chinesandengineeringautomatedsystems.Asmachinelearningbecomes
moreubiquitousanditssoftwarepackagesbecomeeasiertouse,itisnat-
uralanddesirablethatthelow-leveltechnicaldetailsareabstractedaway
and hidden from the practitioner. However, this brings with it the danger
that a practitioner becomes unaware of the design decisions and, hence,
thelimitsofmachinelearningalgorithms.
The enthusiastic practitioner who is interested to learn more about the
magic behind successful machine learning algorithms currently faces a
dauntingsetofpre-requisiteknowledge:
Programminglanguagesanddataanalysistools
Large-scalecomputationandtheassociatedframeworks
Mathematicsandstatisticsandhowmachinelearningbuildsonit
At universities, introductory courses on machine learning tend to spend
earlypartsofthecoursecoveringsomeofthesepre-requisites.Forhistori-
calreasons,coursesinmachinelearningtendtobetaughtinthecomputer
sciencedepartment,wherestudentsareoftentrainedinthefirsttwoareas
ofknowledge,butnotsomuchinmathematicsandstatistics.
Current machine learning textbooks primarily focus on machine learn-
ing algorithms and methodologies and assume that the reader is com-
petent in mathematics and statistics. Therefore, these books only spend
one or two chapters on background mathematics, either at the beginning
of the book or as appendices. We have found many people who want to
delve into the foundations of basic machine learning methods who strug-
glewiththemathematicalknowledgerequiredtoreadamachinelearning
textbook.Havingtaughtundergraduateandgraduatecoursesatuniversi-
ties,wefindthatthegapbetweenhighschoolmathematicsandthemath-
ematicslevelrequiredtoreadastandardmachinelearningtextbookistoo
bigformanypeople.
Thisbookbringsthemathematicalfoundationsofbasicmachinelearn-
ing concepts to the fore and collects the information in a single place so
thatthisskillsgapisnarrowedorevenclosed.
1
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
2 Foreword
Why Another Book on Machine Learning?
Machine learning builds upon the language of mathematics to express
concepts that seem intuitively obvious but that are surprisingly difficult
toformalize.Onceformalizedproperly,wecangaininsightsintothetask
we want to solve. One common complaint of students of mathematics
around the globe is that the topics covered seem to have little relevance
topracticalproblems.Webelievethatmachinelearningisanobviousand
directmotivationforpeopletolearnmathematics.
This book is intended to be a guidebook to the vast mathematical lit-
“Mathislinkedin erature that forms the foundations of modern machine learning. We mo-
thepopularmind tivate the need for mathematical concepts by directly pointing out their
withphobiaand
usefulness in the context of fundamental machine learning problems. In
anxiety.You’dthink
the interest of keeping the book short, many details and more advanced
we’rediscussing
spiders.”(Strogatz, concepts have been left out. Equipped with the basic concepts presented
2014,page281) here, and how they fit into the larger context of machine learning, the
readercanfindnumerousresourcesforfurtherstudy,whichweprovideat
theendoftherespectivechapters.Forreaderswithamathematicalback-
ground,thisbookprovidesabriefbutpreciselystatedglimpseofmachine
learning. In contrast to other books that focus on methods and models
of machine learning (MacKay, 2003; Bishop, 2006; Alpaydin, 2010; Bar-
ber, 2012; Murphy, 2012; Shalev-Shwartz and Ben-David, 2014; Rogers
andGirolami,2016)orprogrammaticaspectsofmachinelearning(Mu¨ller
andGuido,2016;RaschkaandMirjalili,2017;CholletandAllaire,2018),
we provide only four representative examples of machine learning algo-
rithms.Instead,wefocusonthemathematicalconceptsbehindthemodels
themselves.Wehopethatreaderswillbeabletogainadeeperunderstand-
ingofthebasicquestionsinmachinelearningandconnectpracticalques-
tions arising from the use of machine learning with fundamental choices
inthemathematicalmodel.
We do not aim to write a classical machine learning book. Instead, our
intentionistoprovidethemathematicalbackground,appliedtofourcen-
tral machine learning problems, to make it easier to read other machine
learningtextbooks.
Who Is the Target Audience?
As applications of machine learning become widespread in society, we
believethateverybodyshouldhavesomeunderstandingofitsunderlying
principles.Thisbookiswritteninanacademicmathematicalstyle,which
enables us to be precise about the concepts behind machine learning. We
encourage readers unfamiliar with this seemingly terse style to persevere
and to keep the goals of each topic in mind. We sprinkle comments and
remarks throughout the text, in the hope that it provides useful guidance
withrespecttothebigpicture.
The book assumes the reader to have mathematical knowledge commonly
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Foreword 3
covered in high school mathematics and physics. For example, the reader
should have seen derivatives and integrals before, and geometric vectors
intwoorthreedimensions.Startingfromthere,wegeneralizethesecon-
cepts. Therefore, the target audience of the book includes undergraduate
university students, evening learners and learners participating in online
machinelearningcourses.
In analogy to music, there are three types of interaction that people
havewithmachinelearning:
AstuteListener Thedemocratizationofmachinelearningbythepro-
vision of open-source software, online tutorials and cloud-based tools al-
lowsuserstonotworryaboutthespecificsofpipelines.Userscanfocuson
extracting insights from data using off-the-shelf tools. This enables non-
tech-savvy domain experts to benefit from machine learning. This is sim-
ilar to listening to music; the user is able to choose and discern between
different types of machine learning, and benefits from it. More experi-
enced users are like music critics, asking important questions about the
applicationofmachinelearninginsocietysuchasethics,fairness,andpri-
vacy of the individual. We hope that this book provides a foundation for
thinkingaboutthecertificationandriskmanagementofmachinelearning
systems, and allows them to use their domain expertise to build better
machinelearningsystems.
ExperiencedArtist Skilledpractitionersofmachinelearningcanplug
andplaydifferenttoolsandlibrariesintoananalysispipeline.Thestereo-
typicalpractitionerwouldbeadatascientistorengineerwhounderstands
machine learning interfaces and their use cases, and is able to perform
wonderfulfeatsofpredictionfromdata.Thisissimilartoavirtuosoplay-
ing music, where highly skilled practitioners can bring existing instru-
ments to life and bring enjoyment to their audience. Using the mathe-
matics presented here as a primer, practitioners would be able to under-
stand the benefits and limits of their favorite method, and to extend and
generalize existing machine learning algorithms. We hope that this book
provides the impetus for more rigorous and principled development of
machinelearningmethods.
FledglingComposer Asmachinelearningisappliedtonewdomains,
developersofmachinelearningneedtodevelopnewmethodsandextend
existing algorithms. They are often researchers who need to understand
themathematicalbasisofmachinelearninganduncoverrelationshipsbe-
tween different tasks. This is similar to composers of music who, within
therulesandstructureofmusicaltheory,createnewandamazingpieces.
Wehopethisbookprovidesahigh-leveloverviewofothertechnicalbooks
for people who want to become composers of machine learning. There is
a great need in society for new researchers who are able to propose and
explore novel approaches for attacking the many challenges of learning
fromdata.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
4 Foreword
Acknowledgments
We are grateful to many people who looked at early drafts of the book
and suffered through painful expositions of concepts. We tried to imple-
ment their ideas that we did not vehemently disagree with. We would
like to especially acknowledge Christfried Webers for his careful reading
of many parts of the book, and his detailed suggestions on structure and
presentation. Many friends and colleagues have also been kind enough
to provide their time and energy on different versions of each chapter.
We have been lucky to benefit from the generosity of the online commu-
nity,whohavesuggestedimprovementsviahttps://github.com,which
greatlyimprovedthebook.
Thefollowingpeoplehavefoundbugs,proposedclarificationsandsug-
gested relevant literature, either via https://github.com or personal
communication.Theirnamesaresortedalphabetically.
Abdul-GaniyUsman EllenBroad
AdamGaier FengkuangtianZhu
AdeleJackson FionaCondon
AdityaMenon GeorgiosTheodorou
AlasdairTran HeXin
AleksandarKrnjaic IreneRaissaKameni
AlexanderMakrigiorgos JakubNabaglo
AlfredoCanziani JamesHensman
AliShafti JamieLiu
AmrKhalifa JeanKaddour
AndrewTanggara Jean-PaulEbejer
AngusGruen JerryQiang
AntalA.Buss JiteshSindhare
AntoineToisoulLeCann JohnLloyd
AregSarvazyan JonasNgnawe
ArtemArtemev JonMartin
ArtyomStepanov JustinHsi
BillKromydas KaiArulkumaran
BobWilliamson KamilDreczkowski
BoonPingLim LilyWang
ChaoQu LionelTondjiNgoupeyou
ChengLi LydiaKnu¨fing
ChrisSherlock MahmoudAslan
ChristopherGray MarkHartenstein
DanielMcNamara MarkvanderWilk
DanielWood MarkusHegland
DarrenSiegel MartinHewing
DavidJohnston MatthewAlger
DaweiChen MatthewLee
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Foreword 5
MaximusMcCann ShakirMohamed
MengyanZhang ShawnBerry
MichaelBennett SheikhAbdulRaheemAli
MichaelPedersen ShengXue
MinjeongShin SridharThiagarajan
MohammadMalekzadeh SyedNoumanHasany
NaveenKumar SzymonBrych
NicoMontali
ThomasBu¨hler
OscarArmas
TimurSharapov
PatrickHenriksen
TomMelamed
PatrickWieschollek
VincentAdam
PattarawatChormai
VincentDutordoir
PaulKelly
VuMinh
PetrosChristodoulou
WasimAftab
PiotrJanuszewski
WenZhi
PranavSubramani
WojciechStokowiec
QuyuKong
XiaonanChong
RagibZaman
XiaoweiZhang
RuiZhang
YazhouHao
Ryan-RhysGriffiths
YichengLuo
SalomonKabongo
SamuelOgunmola YoungLee
SandeepMavadia YuLu
SarveshNikumbh YunCheng
SebastianRaschka YuxiaoHuang
SenanayakSeshKumarKarri ZacCranko
Seung-HeonBaek ZijianCao
ShahbazChaudhary ZoeNolan
ContributorsthroughGitHub,whoserealnameswerenotlistedontheir
GitHubprofile,are:
SamDataMad insad empet
bumptiousmonkey HorizonP victorBigand
idoamihai cs-maillist 17SKYE
deepakiim kudo23 jessjing1995
WearealsoverygratefultoParameswaranRamanandthemanyanony-
mous reviewers, organized by Cambridge University Press, who read one
ormorechaptersofearlierversionsofthemanuscript,andprovidedcon-
structive criticism that led to considerable improvements. A special men-
tiongoestoDineshSinghNegi,ourLATEXsupport,fordetailedandprompt
advice about LATEX-related issues. Last but not least, we are very grateful
to our editor Lauren Cowles, who has been patiently guiding us through
thegestationprocessofthisbook.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
6 Foreword
Table of Symbols
Symbol Typicalmeaning
a,b,c,α,β,γ Scalarsarelowercase
x,y,z Vectorsareboldlowercase
A,B,C Matricesarebolduppercase
x⊤,A⊤ Transposeofavectorormatrix
A−1 Inverseofamatrix
x,y Innerproductofxandy
⟨ ⟩
x⊤y Dotproductofxandy
B = (b ,b ,b ) (Ordered)tuple
1 2 3
B = [b ,b ,b ] Matrixofcolumnvectorsstackedhorizontally
1 2 3
= b ,b ,b Setofvectors(unordered)
1 2 3
ZB ,N{ }
Integersandnaturalnumbers,respectively
R,C Realandcomplexnumbers,respectively
Rn n-dimensionalvectorspaceofrealnumbers
x Universalquantifier:forallx
∀
x Existentialquantifier:thereexistsx
∃
a := b aisdefinedasb
a =: b bisdefinedasa
a b aisproportionaltob,i.e.,a = constant b
∝ ·
g f Functioncomposition:“g afterf”
◦
Ifandonlyif
⇐⇒
= Implies
⇒
, Sets
A C
a aisanelementofset
∈ A A
Emptyset
∅
without :thesetofelementsin butnotin
A\B A B A B
D Numberofdimensions;indexedbyd = 1,...,D
N Numberofdatapoints;indexedbyn = 1,...,N
I Identitymatrixofsizem m
m
×
0 Matrixofzerosofsizem n
m,n
×
1 Matrixofonesofsizem n
m,n
×
e Standard/canonicalvector(whereiisthecomponentthatis1)
i
dim Dimensionalityofvectorspace
rk(A) RankofmatrixA
Im(Φ) ImageoflinearmappingΦ
ker(Φ) Kernel(nullspace)ofalinearmappingΦ
span[b ] Span(generatingset)ofb
1 1
tr(A) TraceofA
det(A) DeterminantofA
Absolutevalueordeterminant(dependingoncontext)
|·|
Norm;Euclidean,unlessspecified
∥·∥
λ EigenvalueorLagrangemultiplier
E Eigenspacecorrespondingtoeigenvalueλ
λ
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Foreword 7
Symbol Typicalmeaning
x y Vectorsxandy areorthogonal
⊥
V Vectorspace
V⊥ OrthogonalcomplementofvectorspaceV
(cid:80)N
x Sumofthex :x +...+x
n=1 n n 1 N
(cid:81)N
x Productofthex :x ... x
n=1 n n 1 · · N
θ Parametervector
∂f Partialderivativeoff withrespecttox
∂x
df Totalderivativeoff withrespecttox
dx
Gradient
∇
f = min f(x) Thesmallestfunctionvalueoff
∗ x
x argmin f(x) Thevaluex thatminimizesf (note:argminreturnsasetofvalues)
∗ x ∗
∈
L Lagrangian
Negativelog-likelihood
L(cid:0)n(cid:1)
Binomialcoefficient,nchoosek
k
V [x] VarianceofxwithrespecttotherandomvariableX
X
E [x] ExpectationofxwithrespecttotherandomvariableX
X
Cov [x,y] Covariancebetweenxandy.
X,Y
X Y Z X isconditionallyindependentofY givenZ
⊥⊥ |
X p RandomvariableX isdistributedaccordingtop
(cid:0)∼ (cid:1)
µ, Σ GaussiandistributionwithmeanµandcovarianceΣ
N
Ber(µ) Bernoullidistributionwithparameterµ
Bin(N,µ) BinomialdistributionwithparametersN,µ
Beta(α,β) Betadistributionwithparametersα,β
Table of Abbreviations and Acronyms
Acronym Meaning
e.g. Exempligratia(Latin:forexample)
GMM Gaussianmixturemodel
i.e. Idest(Latin:thismeans)
i.i.d. Independent,identicallydistributed
MAP Maximumaposteriori
MLE Maximumlikelihoodestimation/estimator
ONB Orthonormalbasis
PCA Principalcomponentanalysis
PPCA Probabilisticprincipalcomponentanalysis
REF Row-echelonform
SPD Symmetric,positivedefinite
SVM Supportvectormachine
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).

Part I
Mathematical Foundations
9
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.

1
Introduction and Motivation
Machinelearningisaboutdesigningalgorithmsthatautomaticallyextract
valuableinformationfromdata.Theemphasishereison“automatic”,i.e.,
machinelearningisconcernedaboutgeneral-purposemethodologiesthat
canbeappliedtomanydatasets,whileproducingsomethingthatismean-
ingful. There are three concepts that are at the core of machine learning:
data,amodel,andlearning.
Since machine learning is inherently data driven, data is at the core data
of machine learning. The goal of machine learning is to design general-
purpose methodologies to extract valuable patterns from data, ideally
withoutmuchdomain-specificexpertise.Forexample,givenalargecorpus
of documents (e.g., books in many libraries), machine learning methods
can be used to automatically find relevant topics that are shared across
documents (Hoffman et al., 2010). To achieve this goal, we design mod-
els that are typically related to the process that generates data, similar to model
the dataset we are given. For example, in a regression setting, the model
would describe a function that maps inputs to real-valued outputs. To
paraphrase Mitchell (1997): A model is said to learn from data if its per-
formance on a given task improves after the data is taken into account.
The goal is to find good models that generalize well to yet unseen data,
which we may care about in the future. Learning can be understood as a learning
waytoautomaticallyfindpatternsandstructureindatabyoptimizingthe
parametersofthemodel.
While machine learning has seen many success stories, and software is
readily available to design and train rich and flexible machine learning
systems, we believe that the mathematical foundations of machine learn-
ing are important in order to understand fundamental principles upon
whichmorecomplicatedmachinelearningsystemsarebuilt.Understand-
ingtheseprinciplescanfacilitatecreatingnewmachinelearningsolutions,
understandinganddebuggingexistingapproaches,andlearningaboutthe
inherent assumptions and limitations of the methodologies we are work-
ingwith.
11
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
12 IntroductionandMotivation
1.1 Finding Words for Intuitions
A challenge we face regularly in machine learning is that concepts and
words are slippery, and a particular component of the machine learning
systemcanbeabstractedtodifferentmathematicalconcepts.Forexample,
the word “algorithm” is used in at least two different senses in the con-
text of machine learning. In the first sense, we use the phrase “machine
learningalgorithm”tomeanasystemthatmakespredictionsbasedonin-
predictor put data. We refer to these algorithms as predictors. In the second sense,
we use the exact same phrase “machine learning algorithm” to mean a
system that adapts some internal parameters of the predictor so that it
performs well on future unseen input data. Here we refer to this adapta-
training tionastrainingasystem.
This book will not resolve the issue of ambiguity, but we want to high-
light upfront that, depending on the context, the same expressions can
mean different things. However, we attempt to make the context suffi-
cientlycleartoreducethelevelofambiguity.
The first part of this book introduces the mathematical concepts and
foundationsneededtotalkaboutthethreemaincomponentsofamachine
learning system: data, models, and learning. We will briefly outline these
components here, and we will revisit them again in Chapter 8 once we
havediscussedthenecessarymathematicalconcepts.
While not all data is numerical, it is often useful to consider data in
a number format. In this book, we assume that data has already been
appropriatelyconvertedintoanumericalrepresentationsuitableforread-
dataasvectors ing into a computer program. Therefore, we think of data as vectors. As
another illustration of how subtle words are, there are (at least) three
different ways to think about vectors: a vector as an array of numbers (a
computerscienceview),avectorasanarrowwithadirectionandmagni-
tude (a physics view), and a vector as an object that obeys addition and
scaling(amathematicalview).
model Amodelistypicallyusedtodescribeaprocessforgeneratingdata,sim-
ilar to the dataset at hand. Therefore, good models can also be thought
of as simplified versions of the real (unknown) data-generating process,
capturing aspects that are relevant for modeling the data and extracting
hidden patterns from it. A good model can then be used to predict what
would happen in the real world without performing real-world experi-
ments.
learning We now come to the crux of the matter, the learning component of
machine learning. Assume we are given a dataset and a suitable model.
Training the model means to use the data available to optimize some pa-
rametersofthemodelwithrespecttoautilityfunctionthatevaluateshow
well the model predicts the training data. Most training methods can be
thought of as an approach analogous to climbing a hill to reach its peak.
In this analogy, the peak of the hill corresponds to a maximum of some
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
1.2 TwoWaystoReadThisBook 13
desired performance measure. However, in practice, we are interested in
the model to perform well on unseen data. Performing well on data that
we have already seen (training data) may only mean that we found a
goodwaytomemorizethedata.However,thismaynotgeneralizewellto
unseen data, and, in practical applications, we often need to expose our
machinelearningsystemtosituationsthatithasnotencounteredbefore.
Letussummarizethemainconceptsofmachinelearningthatwecover
inthisbook:
Werepresentdataasvectors.
We choose an appropriate model, either using the probabilistic or opti-
mizationview.
Welearnfromavailabledatabyusingnumericaloptimizationmethods
withtheaimthatthemodelperformswellondatanotusedfortraining.
1.2 Two Ways to Read This Book
We can consider two strategies for understanding the mathematics for
machinelearning:
Bottom-up: Building up the concepts from foundational to more ad-
vanced. This is often the preferred approach in more technical fields,
such as mathematics. This strategy has the advantage that the reader
at all times is able to rely on their previously learned concepts. Unfor-
tunately, for a practitioner many of the foundational concepts are not
particularlyinterestingbythemselves,andthelackofmotivationmeans
thatmostfoundationaldefinitionsarequicklyforgotten.
Top-down: Drilling down from practical needs to more basic require-
ments. This goal-driven approach has the advantage that the readers
know at all times why they need to work on a particular concept, and
thereisaclearpathofrequiredknowledge.Thedownsideofthisstrat-
egyisthattheknowledgeisbuiltonpotentiallyshakyfoundations,and
thereadershavetorememberasetofwordsthattheydonothaveany
wayofunderstanding.
We decided to write this book in a modular way to separate foundational
(mathematical) concepts from applications so that this book can be read
inbothways.Thebookissplitintotwoparts,wherePartIlaysthemath-
ematical foundations and Part II applies the concepts from Part I to a set
of fundamental machine learning problems, which form four pillars of
machine learning as illustrated in Figure 1.1: regression, dimensionality
reduction,densityestimation,andclassification.ChaptersinPartImostly
builduponthepreviousones,butitispossibletoskipachapterandwork
backward if necessary. Chapters in Part II are only loosely coupled and
canbereadinanyorder.Therearemanypointersforwardandbackward
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
14 IntroductionandMotivation
Figure1.1 The
foundationsand
fourpillarsof Machine Learning
machinelearning.
Regression
Dimensionality
Reduction
Density
Estimation
Classification
Vector Calculus Probability & Distributions Optimization
Linear Algebra Analytic Geometry Matrix Decomposition
between the two parts of the book to link mathematical concepts with
machinelearningalgorithms.
Of course there are more than two ways to read this book. Most readers
learnusingacombinationoftop-downandbottom-upapproaches,some-
times building up basic mathematical skills before attempting more com-
plex concepts, but also choosing topics based on applications of machine
learning.
Part I Is about Mathematics
Thefourpillarsofmachinelearningwecoverinthisbook(seeFigure1.1)
requireasolidmathematicalfoundation,whichislaidoutinPartI.
We represent numerical data as vectors and represent a table of such
dataasamatrix.Thestudyofvectorsandmatricesiscalledlinearalgebra,
linearalgebra which we introduce in Chapter 2. The collection of vectors as a matrix is
alsodescribedthere.
Given two vectors representing two objects in the real world, we want
to make statements about their similarity. The idea is that vectors that
are similar should be predicted to have similar outputs by our machine
learning algorithm (our predictor). To formalize the idea of similarity be-
tween vectors, we need to introduce operations that take two vectors as
inputandreturnanumericalvaluerepresentingtheirsimilarity.Thecon-
analyticgeometry struction of similarity and distances is central to analytic geometry and is
discussedinChapter3.
In Chapter 4, we introduce some fundamental concepts about matri-
matrix cesandmatrixdecomposition.Someoperationsonmatricesareextremely
decomposition useful in machine learning, and they allow for an intuitive interpretation
ofthedataandmoreefficientlearning.
We often consider data to be noisy observations of some true underly-
ingsignal.Wehopethatbyapplyingmachinelearningwecanidentifythe
signal from the noise. This requires us to have a language for quantify-
ing what “noise” means. We often would also like to have predictors that
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
1.2 TwoWaystoReadThisBook 15
allow us to express some sort of uncertainty, e.g., to quantify the confi-
dence we have about the value of the prediction at a particular test data
point. Quantification of uncertainty is the realm of probability theory and probabilitytheory
iscoveredinChapter6.
To train machine learning models, we typically find parameters that
maximize some performance measure. Many optimization techniques re-
quire the concept of a gradient, which tells us the direction in which to
search for a solution. Chapter 5 is about vector calculus and details the vectorcalculus
concept of gradients, which we subsequently use in Chapter 7, where we
talkaboutoptimizationtofindmaxima/minimaoffunctions. optimization
Part II Is about Machine Learning
The second part of the book introduces four pillars of machine learning
as shown in Figure 1.1. We illustrate how the mathematical concepts in-
troduced in the first part of the book are the foundation for each pillar.
Broadlyspeaking,chaptersareorderedbydifficulty(inascendingorder).
In Chapter 8, we restate the three components of machine learning
(data, models, and parameter estimation) in a mathematical fashion. In
addition, we provide some guidelines for building experimental set-ups
that guard against overly optimistic evaluations of machine learning sys-
tems. Recall that the goal is to build a predictor that performs well on
unseendata.
In Chapter 9, we will have a close look at linear regression, where our linearregression
objectiveistofindfunctionsthatmapinputsx RD tocorrespondingob-
servedfunctionvaluesy
R,whichwecanint∈
erpretasthelabelsoftheir
∈
respective inputs. We will discuss classical model fitting (parameter esti-
mation) via maximum likelihood and maximum a posteriori estimation,
as well as Bayesian linear regression, where we integrate the parameters
outinsteadofoptimizingthem.
Chapter10focusesondimensionalityreduction,thesecondpillarinFig- dimensionality
ure 1.1, using principal component analysis. The key objective of dimen- reduction
sionalityreductionistofindacompact,lower-dimensionalrepresentation
of high-dimensional data x RD, which is often easier to analyze than
∈
theoriginaldata.Unlikeregression,dimensionalityreductionisonlycon-
cerned about modeling the data – there are no labels associated with a
datapointx.
In Chapter 11, we will move to our third pillar: density estimation. The densityestimation
objectiveofdensityestimationistofindaprobabilitydistributionthatde-
scribesagivendataset.WewillfocusonGaussianmixturemodelsforthis
purpose,andwewilldiscussaniterativeschemetofindtheparametersof
this model. As in dimensionality reduction, there are no labels associated
withthedatapointsx RD.However,wedonotseekalow-dimensional
∈
representation of the data. Instead, we are interested in a density model
thatdescribesthedata.
Chapter12concludesthebookwithanin-depthdiscussionofthefourth
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
16 IntroductionandMotivation
classification pillar:classification.Wewilldiscussclassificationinthecontextofsupport
vectormachines.Similartoregression(Chapter9),wehaveinputsxand
correspondinglabelsy.However,unlikeregression,wherethelabelswere
real-valued,thelabelsinclassificationareintegers,whichrequiresspecial
care.
1.3 Exercises and Feedback
WeprovidesomeexercisesinPartI,whichcanbedonemostlybypenand
paper. For Part II, we provide programming tutorials (jupyter notebooks)
toexploresomepropertiesofthemachinelearningalgorithmswediscuss
inthisbook.
We appreciate that Cambridge University Press strongly supports our
aim to democratize education and learning by making this book freely
availablefordownloadat
https://mml-book.com
where tutorials, errata, and additional materials can be found. Mistakes
canbereportedandfeedbackprovidedusingtheprecedingURL.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2
Linear Algebra
Whenformalizingintuitiveconcepts,acommonapproachistoconstructa
setofobjects(symbols)andasetofrulestomanipulatetheseobjects.This
is known as an algebra. Linear algebra is the study of vectors and certain algebra
rulestomanipulatevectors.Thevectorsmanyofusknowfromschoolare
called “geometric vectors”, which are usually denoted by a small arrow
above the letter, e.g., x and y. In this book, we discuss more general
→− →−
conceptsofvectorsanduseaboldlettertorepresentthem,e.g.,xandy.
In general, vectors are special objects that can be added together and
multiplied by scalars to produce another object of the same kind. From
an abstract mathematical viewpoint, any object that satisfies these two
properties can be considered a vector. Here are some examples of such
vectorobjects:
1. Geometricvectors.Thisexampleofavectormaybefamiliarfromhigh
schoolmathematicsandphysics.Geometricvectors–seeFigure2.1(a)
– are directed segments, which can be drawn (at least in two dimen-
→ → → → →
sions).Twogeometricvectorsx, y canbeadded,suchthatx+y = z
is another geometric vector. Furthermore, multiplication by a scalar
λ→ x, λ R, is also a geometric vector. In fact, it is the original vector
∈
scaled by λ. Therefore, geometric vectors are instances of the vector
concepts introduced previously. Interpreting vectors as geometric vec-
tors enables us to use our intuitions about direction and magnitude to
reasonaboutmathematicaloperations.
2. Polynomials are also vectors; see Figure 2.1(b): Two polynomials can
→ → 4 Figure2.1
x + y Differenttypesof
2 vectors.Vectorscan
besurprising
0
objects,including
→
x
−2 (a)geometric
→ vectors
y 4
− and(b)polynomials.
6
− 2 0 2
− x
(a)Geometricvectors. (b)Polynomials.
17
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
y
18 LinearAlgebra
be added together, which results in another polynomial; and they can
be multiplied by a scalar λ R, and the result is a polynomial as
∈
well.Therefore,polynomialsare(ratherunusual)instancesofvectors.
Notethatpolynomialsareverydifferentfromgeometricvectors.While
geometric vectors are concrete “drawings”, polynomials are abstract
concepts. However, they are both vectors in the sense previously de-
scribed.
3. Audio signals are vectors. Audio signals are represented as a series of
numbers. We can add audio signals together, and their sum is a new
audiosignal.Ifwescaleanaudiosignal,wealsoobtainanaudiosignal.
Therefore,audiosignalsareatypeofvector,too.
4. Elements of Rn (tuples of n real numbers) are vectors. Rn is more
abstract than polynomials, and it is the concept we focus on in this
book.Forinstance,
 
1
a = 2 R3 (2.1)
∈
3
is an example of a triplet of numbers. Adding two vectors a,b Rn
component-wise resultsin anothervector: a+b = c Rn. More∈ over,
multiplying a Rn by λ R results in a scaled v∈ ector λa Rn.
Becarefultocheck Considering vec∈ tors as elem∈ ents of Rn has an additional benefi∈ t that
whetherarray it loosely corresponds to arrays of real numbers on a computer. Many
operationsactually
programminglanguagessupportarrayoperations,whichallowforcon-
performvector
venientimplementationofalgorithmsthatinvolvevectoroperations.
operationswhen
implementingona
computer. Linear algebra focuses on the similarities between these vector concepts.
We can add them together and multiply them by scalars. We will largely
PavelGrinfeld’s
focus on vectors in Rn since most algorithms in linear algebra are for-
seriesonlinear
algebra: mulated in Rn. We will see in Chapter 8 that we often consider data to
http://tinyurl. be represented as vectors in Rn. In this book, we will focus on finite-
com/nahclwm
dimensional vector spaces, in which case there is a 1:1 correspondence
GilbertStrang’s between any kind of vector and Rn. When it is convenient, we will use
courseonlinear
algebra: intuitionsaboutgeometricvectorsandconsiderarray-basedalgorithms.
http://tinyurl. Onemajorideainmathematicsistheideaof“closure”.Thisistheques-
com/29p5q8j tion: What is the set of all things that can result from my proposed oper-
3Blue1Brownseries ations?Inthecaseofvectors:Whatisthesetofvectorsthatcanresultby
onlinearalgebra:
starting with a small set of vectors, and adding them to each other and
https://tinyurl.
com/h5g4kps scaling them? This results in a vector space (Section 2.4). The concept of
a vector space and its properties underlie much of machine learning. The
conceptsintroducedinthischapteraresummarizedinFigure2.2.
ThischapterismostlybasedonthelecturenotesandbooksbyDrumm
and Weil (2001), Strang (2003), Hogben (2013), Liesen and Mehrmann
(2015), as well as Pavel Grinfeld’s Linear Algebra series. Other excellent
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.1 SystemsofLinearEquations 19
Figure2.2 Amind
Vector
mapoftheconcepts
composes
propertyof i cn ht ar po td eu r,ce ad loi nn gt whi is
th
wheretheyareused
VecC th oa rp ct ae lr cu5 lus
represents
Matrix
re
prese
Vectorspace
A wb ite hli +an
Group indeL pi en ne dar
ence
i bn ooo kth .erpartsofthe
Systemof
nts
linearequations
Linear/affine
mapping
solves
Basis
Matrix
inverse
Gaussian
elimination
Chapter3 Chapter12 Chapter10
Analyticgeometry Classification Dimensionality
reduction
resourcesareGilbertStrang’sLinearAlgebracourseatMITandtheLinear
AlgebraSeriesby3Blue1Brown.
Linear algebra plays an important role in machine learning and gen-
eral mathematics. The concepts introduced in this chapter are further ex-
panded to include the idea of geometry in Chapter 3. In Chapter 5, we
will discuss vector calculus, where a principled knowledge of matrix op-
erations is essential. In Chapter 10, we will use projections (to be intro-
duced in Section 3.8) for dimensionality reduction with principal compo-
nentanalysis(PCA).InChapter9,wewilldiscusslinearregression,where
linearalgebraplaysacentralroleforsolvingleast-squaresproblems.
2.1 Systems of Linear Equations
Systems of linear equations play a central part of linear algebra. Many
problems can be formulated as systems of linear equations, and linear
algebragivesusthetoolsforsolvingthem.
Example 2.1
A company produces products N ,...,N for which resources
1 n
R ,...,R are required. To produce a unit of product N , a units of
1 m j ij
resourceR areneeded,wherei = 1,...,mandj = 1,...,n.
i
The objective is to find an optimal production plan, i.e., a plan of how
many units x of product N should be produced if a total of b units of
j j i
resourceR areavailableand(ideally)noresourcesareleftover.
i
If we produce x ,...,x units of the corresponding products, we need
1 n
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
solvedby
closure
maximalset
20 LinearAlgebra
atotalof
a x + +a x (2.2)
i1 1 in n
···
manyunitsofresourceR .Anoptimalproductionplan(x ,...,x ) Rn,
i 1 n
∈
therefore,hastosatisfythefollowingsystemofequations:
a x + +a x = b
11 1 1n n 1
···
.
. . , (2.3)
a x + +a x = b
m1 1 mn n m
···
wherea Randb R.
ij i
∈ ∈
systemoflinear Equation (2.3) is the general form of a system of linear equations, and
equations x ,...,x are the unknowns of this system. Every n-tuple (x ,...,x )
1 n 1 n
solution Rn thatsatisfies(2.3)isasolutionofthelinearequationsystem. ∈
Example 2.2
Thesystemoflinearequations
x + x + x = 3 (1)
1 2 3
x x + 2x = 2 (2) (2.4)
1 2 3
−
2x + 3x = 1 (3)
1 3
hasnosolution:Addingthefirsttwoequationsyields2x +3x = 5,which
1 3
contradictsthethirdequation(3).
Letushavealookatthesystemoflinearequations
x + x + x = 3 (1)
1 2 3
x x + 2x = 2 (2) . (2.5)
1 2 3
−
x + x = 2 (3)
2 3
From the first and third equation, it follows that x = 1. From (1)+(2),
1
we get 2x +3x = 5, i.e., x = 1. From (3), we then get that x = 1.
1 3 3 2
Therefore, (1,1,1) is the only possible and unique solution (verify that
(1,1,1)isasolutionbypluggingin).
Asathirdexample,weconsider
x + x + x = 3 (1)
1 2 3
x x + 2x = 2 (2) . (2.6)
1 2 3
−
2x + 3x = 5 (3)
1 3
Since (1)+(2)=(3), we can omit the third equation (redundancy). From
(1)and(2),weget2x = 5 3x and2x = 1+x .Wedefinex = a R
1 3 2 3 3
− ∈
asafreevariable,suchthatanytriplet
(cid:18) (cid:19)
5 3 1 1
a, + a,a , a R (2.7)
2 − 2 2 2 ∈
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.1 SystemsofLinearEquations 21
Figure2.3 The
x 2 solutionspaceofa
systemoftwolinear
equationswithtwo
4x +4x =5
1 2
variablescanbe
geometrically
2x 1−4x 2=1 interpretedasthe
intersectionoftwo
lines.Everylinear
equationrepresents
aline.
x
1
is a solution of the system of linear equations, i.e., we obtain a solution
setthatcontainsinfinitelymany solutions.
Ingeneral,forareal-valuedsystemoflinearequationsweobtaineither
no,exactlyone,orinfinitelymanysolutions.Linearregression(Chapter9)
solvesaversionofExample2.1whenwecannotsolvethesystemoflinear
equations.
Remark (Geometric Interpretation of Systems of Linear Equations). In a
systemoflinearequationswithtwovariablesx ,x ,eachlinearequation
1 2
defines a line on the x x -plane. Since a solution to a system of linear
1 2
equationsmustsatisfyallequationssimultaneously,thesolutionsetisthe
intersectionoftheselines.Thisintersectionsetcanbealine(ifthelinear
equations describe the same line), a point, or empty (when the lines are
parallel).AnillustrationisgiveninFigure2.3forthesystem
4x +4x = 5
1 2
(2.8)
2x 4x = 1
1 2
−
wherethesolutionspaceisthepoint(x ,x ) = (1, 1).Similarly,forthree
1 2 4
variables, each linear equation determines a plane in three-dimensional
space. When we intersect these planes, i.e., satisfy all linear equations at
thesametime,wecanobtainasolutionsetthatisaplane,aline,apoint
orempty(whentheplaneshavenocommonintersection).
♢
For a systematic approach to solving systems of linear equations, we
will introduce a useful compact notation. We collect the coefficients a
ij
intovectorsandcollectthevectorsintomatrices.Inotherwords,wewrite
thesystemfrom(2.3)inthefollowingform:
       
a a a b
11 12 1n 1
. . . .
  . .  x 1+  . .  x 2+ ···+  . .  x n =   . .   (2.9)
a a a b
m1 m2 mn m
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
22 LinearAlgebra
    
a a x b
11 1n 1 1
···
. . . .
⇐⇒   . . . .    . .   =   . .  . (2.10)
a a x b
m1 mn n m
···
In the following, we will have a close look at these matrices and de-
fine computation rules. We will return to solving linear equations in Sec-
tion2.3.
2.2 Matrices
Matrices play a central role in linear algebra. They can be used to com-
pactlyrepresentsystemsoflinearequations,buttheyalsorepresentlinear
functions(linearmappings)aswewillseelaterinSection2.7.Beforewe
discuss some of these interesting topics, let us first define what a matrix
isandwhatkindofoperationswecandowithmatrices.Wewillseemore
propertiesofmatricesinChapter4.
matrix Definition 2.1 (Matrix). Withm,n Nareal-valued(m,n)matrix Ais
∈
anm n-tupleofelementsa ,i = 1,...,m,j = 1,...,n,whichisordered
ij
·
accordingtoarectangularschemeconsistingofmrowsandncolumns:
 
a a a
11 12 1n
···
a 21 a 22 a 2n
A =    . . . . . . ··· . . .   , a ij ∈ R. (2.11)
a a a
m1 m2 mn
···
row Byconvention(1,n)-matricesarecalledrowsand(m,1)-matricesarecalled
column columns.Thesespecialmatricesarealsocalledrow/columnvectors.
rowvector
Rm×n is the set of all real-valued (m,n)-matrices. A Rm×n can be
columnvector
Figure2.4 By equivalently represented as a Rmn by stacking all n∈ columns of the
∈
stackingits matrixintoalongvector;seeFigure2.4.
columns,amatrixA
canberepresented
asalongvectora.
2.2.1 Matrix Addition and Multiplication
A R4×2 a R8
∈ ∈ ThesumoftwomatricesA Rm×n,B Rm×nisdefinedastheelement-
∈ ∈
re-shape wisesum,i.e.,
 
a +b a +b
11 11 1n 1n
···
A+B :=   . . . . . .   ∈ Rm×n. (2.12)
a +b a +b
m1 m1 mn mn
···
Notethesizeofthe For matrices A Rm×n, B Rn×k, the elements c ij of the product
matrices. C = AB Rm×k ∈ arecomputed∈ as
C = ∈
np.einsum(’il, (cid:88)n
lj’, A, B) c ij = a ilb lj, i = 1,...,m, j = 1,...,k. (2.13)
l=1
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.2 Matrices 23
This means, to compute element c ij we multiply the elements of the ith Therearencolumns
rowofAwiththejthcolumnofBandsumthemup.LaterinSection3.2, inAandnrowsin
Bsothatwecan
we will call this the dot product of the corresponding row and column. In
cases,whereweneedtobeexplicitthatweareperformingmultiplication, computea ilb lj for
l=1,...,n.
we use the notation A B to denote multiplication (explicitly showing
Commonly,thedot
·
“ ”). productbetween
·
twovectorsa,bis
Remark. Matricescanonlybemultipliediftheir“neighboring”dimensions
denotedbya⊤bor
match. For instance, an n k-matrix A can be multiplied with a k m-
⟨a,b⟩.
× ×
matrixB,butonlyfromtheleftside:
A B = C (2.14)
(cid:124)(cid:123)(cid:122)(cid:125)(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
n×k k×m n×m
TheproductBAisnotdefinedifm = nsincetheneighboringdimensions
̸
donotmatch.
♢
Remark. Matrixmultiplicationisnotdefinedasanelement-wiseoperation
on matrix elements, i.e., c = a b (even if the size of A,B was cho-
ij ij ij
̸
senappropriately).Thiskindofelement-wisemultiplicationoftenappears
in programming languages when we multiply (multi-dimensional) arrays
witheachother,andiscalledaHadamardproduct. Hadamardproduct
♢
Example 2.3
 
(cid:20) (cid:21) 0 2
1 2 3
ForA = R2×3,B = 1 1 R3×2,weobtain
3 2 1 ∈ − ∈
0 1
 
(cid:20) (cid:21) 0 2 (cid:20) (cid:21)
1 2 3 2 3
AB = 1 1 = R2×2, (2.15)
3 2 1 − 2 5 ∈
0 1
   
0 2 (cid:20) (cid:21) 6 4 2
1 2 3
BA = 1 1 =  2 0 2 R3×3. (2.16)
− 3 2 1 − ∈
0 1 3 2 1
Figure2.5 Evenif
Fromthisexample,wecanalreadyseethatmatrixmultiplicationisnot bothmatrix
multiplicationsAB
commutative,i.e.,AB = BA;seealsoFigure2.5foranillustration.
̸ andBAare
Definition 2.2 (IdentityMatrix). InRn×n,wedefinetheidentitymatrix defined,the
dimensionsofthe
 1 0 0 0 resultscanbe
··· ··· different.
0 1 0 0
 ··· ··· 
I n :=

 
0. .
.
0. .
.
... 1. .
.
... 0. . .
 
 ∈
Rn×n (2.17)
 ··· ··· 
 . .
.
. .
.
... . .
.
... . . .

identitymatrix
0 0 0 1
··· ···
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
24 LinearAlgebra
asthen n-matrixcontaining1onthediagonaland0everywhereelse.
×
Now that we defined matrix multiplication, matrix addition and the
identitymatrix,letushavealookatsomepropertiesofmatrices:
associativity
Associativity:
A Rm×n,B Rn×p,C Rp×q : (AB)C = A(BC) (2.18)
∀ ∈ ∈ ∈
distributivity
Distributivity:
A,B Rm×n,C,D Rn×p :(A+B)C = AC +BC (2.19a)
∀ ∈ ∈
A(C +D) = AC +AD (2.19b)
Multiplicationwiththeidentitymatrix:
A Rm×n : I A = AI = A (2.20)
m n
∀ ∈
NotethatI = I form = n.
m n
̸ ̸
2.2.2 Inverse and Transpose
Asquarematrix Definition2.3(Inverse). ConsiderasquarematrixA Rn×n.Letmatrix
possessesthesame B Rn×n have the property that AB = I = BA∈ . B is called the
n
numberofcolumns inve∈ rseofAanddenotedbyA−1.
androws.
inverse Unfortunately, not every matrix A possesses an inverse A−1. If this
regular inverse does exist, A is called regular/invertible/nonsingular, otherwise
invertible singular/noninvertible.Whenthematrixinverseexists,itisunique.InSec-
nonsingular tion2.3,wewilldiscussageneralwaytocomputetheinverseofamatrix
singular bysolvingasystemoflinearequations.
noninvertible
Remark(ExistenceoftheInverseofa2 2-matrix). Consideramatrix
×
(cid:20) (cid:21)
a a
A := 11 12 R2×2. (2.21)
a 21 a 22 ∈
IfwemultiplyAwith
(cid:20) (cid:21)
a a
A′ := 22 − 12 (2.22)
a a
21 11
−
weobtain
(cid:20) (cid:21)
a a a a 0
AA′ = 11 22 − 12 21 = (a a a a )I.
0 a 11a 22 a 12a 21 11 22 − 12 21
−
(2.23)
Therefore,
(cid:20) (cid:21)
1 a a
A−1 = 22 − 12 (2.24)
a a a a a a
11 22 − 12 21 − 21 11
ifandonlyifa a a a = 0.InSection4.1,wewillseethata a
11 22 12 21 11 22
− ̸ −
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.2 Matrices 25
a a isthedeterminantofa2 2-matrix.Furthermore,wecangenerally
12 21
×
usethedeterminanttocheckwhetheramatrixisinvertible.
♢
Example 2.4 (Inverse Matrix)
Thematrices
   
1 2 1 7 7 6
− −
A = 4 4 5 , B =  2 1 1 (2.25)
−
6 7 7 4 5 4
−
areinversetoeachothersinceAB = I = BA.
Definition 2.4 (Transpose). For A Rm×n the matrix B Rn×m with
b ij = a ji iscalledthetransposeofA∈ .WewriteB = A⊤. ∈ transpose
Ingeneral,A⊤canbeobtainedbywritingthecolumnsofAastherows Themaindiagonal
(sometimescalled
ofA⊤.Thefollowingareimportantpropertiesofinversesandtransposes:
“principaldiagonal”,
“primarydiagonal”,
“leadingdiagonal”,
AA−1 = I = A−1A (2.26) or“majordiagonal”)
(AB)−1 = B−1A−1 (2.27) ofamatrixAisthe
collectionofentries
(A+B)−1 = A−1+B−1 (2.28) Aij wherei=j.
̸
(A⊤)⊤ = A (2.29) Thescalarcaseof
(2.28)is
(AB)⊤ = B⊤A⊤ (2.30) 2+1 4 = 1 6 ̸= 1 2 + 41.
(A+B)⊤ = A⊤+B⊤ (2.31)
Definition 2.5 (Symmetric Matrix). A matrix A Rn×n is symmetric if symmetricmatrix
A = A⊤. ∈
Note that only (n,n)-matrices can be symmetric. Generally, we call
(n,n)-matrices also square matrices because they possess the same num- squarematrix
ber of rows and columns. Moreover, if A is invertible, then so is A⊤, and
(A−1)⊤ = (A⊤)−1 =: A−⊤.
Remark (Sum and Product of Symmetric Matrices). The sum of symmet-
ric matrices A,B Rn×n is always symmetric. However, although their
∈
productisalwaysdefined,itisgenerallynotsymmetric:
(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)
1 0 1 1 1 1
= . (2.32)
0 0 1 1 0 0
♢
2.2.3 Multiplication by a Scalar
Let us look at what happens to matrices when they are multiplied by a
scalar λ R. Let A Rm×n and λ R. Then λA = K, K = λa .
ij ij
Practicall∈ y,λscalesea∈ chelementofA∈ .Forλ,ψ R,thefollowingholds:
∈
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
26 LinearAlgebra
associativity
Associativity:
(λψ)C = λ(ψC), C Rm×n
∈
λ(BC) = (λB)C = B(λC) = (BC)λ, B Rm×n,C Rn×k.
∈ ∈
Notethatthisallowsustomovescalarvaluesaround.
(λC)⊤ = C⊤λ⊤ = C⊤λ = λC⊤ sinceλ = λ⊤ forallλ R.
distributivity ∈
Distributivity:
(λ+ψ)C = λC +ψC, C Rm×n
λ(B+C) = λB+λC, B,∈ C Rm×n
∈
Example 2.5 (Distributivity)
Ifwedefine
(cid:20) (cid:21)
1 2
C := , (2.33)
3 4
thenforanyλ,ψ Rweobtain
∈
(cid:20) (cid:21) (cid:20) (cid:21)
(λ+ψ)1 (λ+ψ)2 λ+ψ 2λ+2ψ
(λ+ψ)C = = (2.34a)
(λ+ψ)3 (λ+ψ)4 3λ+3ψ 4λ+4ψ
(cid:20) (cid:21) (cid:20) (cid:21)
λ 2λ ψ 2ψ
= + = λC +ψC. (2.34b)
3λ 4λ 3ψ 4ψ
2.2.4 Compact Representations of Systems of Linear Equations
Ifweconsiderthesystemoflinearequations
2x +3x +5x = 1
1 2 3
4x 2x 7x = 8 (2.35)
1 2 3
− −
9x +5x 3x = 2
1 2 3
−
and use the rules for matrix multiplication, we can write this equation
systeminamorecompactformas
    
2 3 5 x 1
1
4 2 7x 2 = 8. (2.36)
− −
9 5 3 x 2
3
−
Note that x scales the first column, x the second one, and x the third
1 2 3
one.
Generally,asystemoflinearequationscanbecompactlyrepresentedin
their matrix form as Ax = b; see (2.3), and the product Ax is a (linear)
combination of the columns of A. We will discuss linear combinations in
moredetailinSection2.5.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.3 SolvingSystemsofLinearEquations 27
2.3 Solving Systems of Linear Equations
In(2.3),weintroducedthegeneralformofanequationsystem,i.e.,
a x + +a x = b
11 1 1n n 1
···
.
. (2.37)
.
a x + +a x = b ,
m1 1 mn n m
···
where a R and b R are known constants and x are unknowns,
ij i j
∈ ∈
i = 1,...,m, j = 1,...,n. Thus far, we saw that matrices can be used as
a compact way of formulating systems of linear equations so that we can
write Ax = b, see (2.10). Moreover, we defined basic matrix operations,
such as addition and multiplication of matrices. In the following, we will
focusonsolvingsystemsoflinearequationsandprovideanalgorithmfor
findingtheinverseofamatrix.
2.3.1 Particular and General Solution
Before discussing how to generally solve systems of linear equations, let
ushavealookatanexample.Considerthesystemofequations
 
x
1
(cid:20) (cid:21) (cid:20) (cid:21)
1 0 8 −4  x 2
 =
42
. (2.38)
0 1 2 12 x 3 8
x
4
The system has two equations and four unknowns. Therefore, in general
we would expect infinitely many solutions. This system of equations is
in a particularly easy form, where the first two columns consist of a 1
and a 0. Remember that we want to find scalars x ,...,x , such that
1 4
(cid:80)4
x c = b,wherewedefinec tobetheithcolumnofthematrixand
i=1 i i i
b the right-hand-side of (2.38). A solution to the problem in (2.38) can
befoundimmediatelybytaking42timesthefirstcolumnand8timesthe
secondcolumnsothat
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
42 1 0
b = = 42 +8 . (2.39)
8 0 1
Therefore, a solution is [42,8,0,0]⊤. This solution is called a particular particularsolution
solution or special solution. However, this is not the only solution of this specialsolution
system of linear equations. To capture all the other solutions, we need
to be creative in generating 0 in a non-trivial way using the columns of
the matrix: Adding 0 to our special solution does not change the special
solution.Todoso,weexpressthethirdcolumnusingthefirsttwocolumns
(whichareofthisverysimpleform)
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
8 1 0
= 8 +2 (2.40)
2 0 1
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
28 LinearAlgebra
so that 0 = 8c +2c 1c +0c and (x ,x ,x ,x ) = (8,2, 1,0). In
1 2 3 4 1 2 3 4
fact,anyscalingofthis−
solutionbyλ
Rproducesthe0vecto−
r,i.e.,
1
∈
  
8
(cid:20) (cid:21)
1 0 8 4   2 
0 1 2 − 12  λ 1  1   = λ 1(8c 1+2c 2 −c 3) = 0. (2.41)
−
0
Followingthesamelineofreasoning,weexpressthefourthcolumnofthe
matrix in (2.38) using the first two columns and generate another set of
non-trivialversionsof0as
  
4
(cid:20) (cid:21) −
1 0 8 4  12
0 1 2 − 12  λ 2  0    = λ 2( −4c 1+12c 2 −c 4) = 0 (2.42)
1
−
foranyλ R.Puttingeverythingtogether,weobtainallsolutionsofthe
2
∈
generalsolution equationsystemin(2.38),whichiscalledthegeneralsolution,astheset
       
42 8 4
  −  
 x
∈
R4 : x =   8 0  +λ 1 

2 1  +λ 2  1 02 

,λ 1,λ 2
∈
R . (2.43)
  0 − 0 1  
−
Remark. The general approach we followed consisted of the following
threesteps:
1. FindaparticularsolutiontoAx = b.
2. FindallsolutionstoAx = 0.
3. Combinethesolutionsfromsteps1.and2.tothegeneralsolution.
Neitherthegeneralnortheparticularsolutionisunique.
♢
The system of linear equations in the preceding example was easy to
solve because the matrix in (2.38) has this particularly convenient form,
which allowed us to find the particular and the general solution by in-
spection. However, general equation systems are not of this simple form.
Fortunately, there exists a constructive algorithmic way of transforming
anysystemoflinearequationsintothisparticularlysimpleform:Gaussian
elimination. Key to Gaussian elimination are elementary transformations
of systems of linear equations, which transform the equation system into
asimpleform.Then,wecanapplythethreestepstothesimpleformthat
wejustdiscussedinthecontextoftheexamplein(2.38).
2.3.2 Elementary Transformations
elementary Keytosolvingasystemoflinearequationsareelementarytransformations
transformations thatkeepthesolutionsetthesame,butthattransformtheequationsystem
intoasimplerform:
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.3 SolvingSystemsofLinearEquations 29
Exchangeoftwoequations(rowsinthematrixrepresentingthesystem
ofequations)
Multiplicationofanequation(row)withaconstantλ R 0
∈ \{ }
Additionoftwoequations(rows)
Example 2.6
Fora R,weseekallsolutionsofthefollowingsystemofequations:
∈
2x + 4x 2x x + 4x = 3
1 2 3 4 5
− − − −
4x 8x + 3x 3x + x = 2
1 − 2 3 − 4 5 . (2.44)
x 2x + x x + x = 0
1 2 3 4 5
− −
x 2x 3x + 4x = a
1 2 4 5
− −
We start by converting this system of equations into the compact matrix
notation Ax = b. We no longer mention the variables x explicitly and
(cid:2) (cid:3)
buildthe augmentedmatrix (intheform A b )
augmentedmatrix
|
 
2 4 2 1 4 3 SwapwithR
3
− − − −
 4 8 3 3 1 2 
 − − 
 1 2 1 1 1 0  SwapwithR 1
− −
1 2 0 3 4 a
− −
where we used the vertical line to separate the left-hand side from the
right-hand side in (2.44). We use ⇝ to indicate a transformation of the
augmentedmatrixusingelementarytransformations. Theaugmented
(cid:2) (cid:3)
SwappingRows1and3leadsto matrix A|b
compactly
 
1 2 1 1 1 0 representsthe
  4 −− 8 3 −− 3 1 2   −4R 1 s ey qs ut ae tm ioo nf sl Ain xea =r b.
 2 4 2 1 4 3  +2R 1
− − − −
1 2 0 3 4 a R
1
− − −
When we now apply the indicated transformations (e.g., subtract Row 1
fourtimesfromRow2),weobtain
 
1 2 1 1 1 0
− −
 0 0 1 1 3 2 
 − − 
 0 0 0 3 6 3 
− −
0 0 1 2 3 a R R
2 3
− − − −
 
1 2 1 1 1 0
− −
⇝  0 0 1 1 3 2  ( 1)
 − −  · −
 0 0 0 3 6 3  ( 1)
− − · −3
0 0 0 0 0 a+1
 
1 2 1 1 1 0
− −
⇝  0 0 1 1 3 2 
 − − 
 0 0 0 1 2 1 
−
0 0 0 0 0 a+1
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
30 LinearAlgebra
row-echelonform This (augmented) matrix is in a convenient form, the row-echelon form
(REF).Revertingthiscompactnotationbackintotheexplicitnotationwith
thevariablesweseek,weobtain
x 2x + x x + x = 0
1 2 3 4 5
− −
x x + 3x = 2
3 − 4 5 − . (2.45)
x 2x = 1
4 5
−
0 = a+1
particularsolution Onlyfora = 1thissystemcanbesolved.Aparticularsolution is
−
   
x 2
1
x 2  0 
   
x 3 =  1 . (2.46)
  − 
x 4  1 
x 0
5
generalsolution Thegeneralsolution, whichcapturesthesetofallpossiblesolutions,is
       
2 2 2
 
 
   0  1  0   
 
x R5 : x =   1 +λ 1 0 +λ 2  1  , λ 1,λ 2 R . (2.47)
   ∈  − 1    0   − 2   ∈   
 
 0 0 1 
Inthefollowing,wewilldetailaconstructivewaytoobtainaparticular
andgeneralsolutionofasystemoflinearequations.
Remark(PivotsandStaircaseStructure). Theleadingcoefficientofarow
pivot (first nonzero number from the left) is called the pivot and is always
strictly to the right of the pivot of the row above it. Therefore, any equa-
tionsysteminrow-echelonformalwayshasa“staircase”structure.
♢
row-echelonform Definition 2.6 (Row-EchelonForm). Amatrixisinrow-echelonformif
All rows that contain only zeros are at the bottom of the matrix; corre-
spondingly, all rows that contain at least one nonzero element are on
topofrowsthatcontainonlyzeros.
Looking at nonzero rows only, the first nonzero number from the left
pivot (also called the pivot or the leading coefficient) is always strictly to the
leadingcoefficient rightofthepivotoftherowaboveit.
Inothertexts,itis
Remark (Basic and Free Variables). The variables corresponding to the
sometimesrequired
thatthepivotis1. pivots in the row-echelon form are called basic variables and the other
basicvariable variables are free variables. For example, in (2.45), x 1,x 3,x 4 are basic
freevariable variables,whereasx ,x arefreevariables.
2 5
♢
Remark (Obtaining a Particular Solution). The row-echelon form makes
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.3 SolvingSystemsofLinearEquations 31
our lives easier when we need to determine a particular solution. To do
this,weexpresstheright-handsideoftheequationsystemusingthepivot
columns, such that b =
(cid:80)P
λ p , where p , i = 1,...,P, are the pivot
i=1 i i i
columns.Theλ aredeterminedeasiestifwestartwiththerightmostpivot
i
columnandworkourwaytotheleft.
Inthepreviousexample,wewouldtrytofindλ ,λ ,λ sothat
1 2 3
       
1 1 1 0
−
0 1  1  2
λ 1 0 +λ 2 0 +λ 3 −
1


=  −
1


. (2.48)
0 0 0 0
Fromhere,wefindrelativelydirectlythatλ = 1,λ = 1,λ = 2.When
3 2 1
−
we put everything together, we must not forget the non-pivot columns
for which we set the coefficients implicitly to 0. Therefore, we get the
particularsolutionx = [2,0, 1,1,0]⊤.
− ♢
Remark (Reduced Row Echelon Form). An equation system is in reduced reduced
row-echelonform(also:row-reducedechelonformorrowcanonicalform)if row-echelonform
Itisinrow-echelonform.
Everypivotis1.
Thepivotistheonlynonzeroentryinitscolumn.
♢
Thereducedrow-echelonformwillplayanimportantrolelaterinSec-
tion 2.3.3 because it allows us to determine the general solution of a sys-
temoflinearequationsinastraightforwardway.
Gaussian
Remark(GaussianElimination). Gaussianeliminationisanalgorithmthat elimination
performselementarytransformationstobringasystemoflinearequations
intoreducedrow-echelonform.
♢
Example 2.7 (Reduced Row Echelon Form)
Verifythatthefollowingmatrixisinreducedrow-echelonform(thepivots
areinbold):
 
1 3 0 0 3
A = 0 0 1 0 9  . (2.49)
0 0 0 1 4
−
The key idea for finding the solutions of Ax = 0 is to look at the non-
pivotcolumns,whichwewillneedtoexpressasa(linear)combinationof
the pivot columns. The reduced row echelon form makes this relatively
straightforward, and we express the non-pivot columns in terms of sums
and multiples of the pivot columns that are on their left: The second col-
umn is 3 times the first column (we can ignore the pivot columns on the
right of the second column). Therefore, to obtain 0, we need to subtract
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
32 LinearAlgebra
thesecondcolumnfromthreetimesthefirstcolumn.Now,welookatthe
fifthcolumn,whichisoursecondnon-pivotcolumn.Thefifthcolumncan
be expressed as 3 times the first pivot column, 9 times the second pivot
column, and 4 times the third pivot column. We need to keep track of
−
theindicesofthepivotcolumnsandtranslatethisinto3timesthefirstcol-
umn, 0 times the second column (which is a non-pivot column), 9 times
the third column (which is our second pivot column), and 4 times the
−
fourthcolumn(whichisthethirdpivotcolumn).Thenweneedtosubtract
thefifthcolumntoobtain0.Intheend,wearestillsolvingahomogeneous
equationsystem.
Tosummarize,allsolutionsofAx = 0,x R5 aregivenby
∈
     
3 3
 
 
   1  0   
 x R5 : x = λ 1 − 0  +λ 2  9   , λ 1,λ 2 R . (2.50)
   ∈   0     4  ∈   
  0 − 1  
−
2.3.3 The Minus-1 Trick
In the following, we introduce a practical trick for reading out the solu-
tions x of a homogeneous system of linear equations Ax = 0, where
A Rk×n,x Rn.
∈ ∈
Tostart,weassumethatAisinreducedrow-echelonformwithoutany
rowsthatjustcontainzeros,i.e.,
 
0 0 1 0 0
··· ∗ ··· ∗ ∗ ··· ∗ ∗ ··· ∗
. . . . .
 . . . . . 
 . . 0 0 0 1 . . . 
 ··· ∗ ··· ∗ 
. . . . . . . . . .
A =   . . . . . . . . . . 0 . . . . . . . . . .   ,
 
 . . . . . . . . . . . . . . . . . . . . 
 . . . . . . . . 0 . . 
0 0 0 0 0 0 0 0 1
··· ··· ··· ∗ ··· ∗
(2.51)
where canbeanarbitraryrealnumber,withtheconstraintsthatthefirst
∗
nonzeroentryperrowmustbe1andallotherentriesinthecorresponding
column must be 0. The columns j ,...,j with the pivots (marked in
1 k
bold)arethestandardunitvectorse ,...,e Rk.Weextendthismatrix
1 k
toann n-matrixA˜ byaddingn k rowso∈ ftheform
× −
(cid:2) (cid:3)
0 0 1 0 0 (2.52)
··· − ···
so that the diagonal of the augmented matrix A˜ contains either 1 or 1.
Then, the columns of A˜ that contain the 1 as pivots are solutions− of
−
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.3 SolvingSystemsofLinearEquations 33
the homogeneous equation system Ax = 0. To be more precise, these
columns form a basis (Section 2.6.1) of the solution space of Ax = 0,
whichwewilllatercallthekernelornullspace(seeSection2.7.3). kernel
nullspace
Example 2.8 (Minus-1 Trick)
Letusrevisitthematrixin(2.49),whichisalreadyinreducedREF:
 
1 3 0 0 3
A = 0 0 1 0 9  . (2.53)
0 0 0 1 4
−
We now augment this matrix to a 5 5 matrix by adding rows of the
×
form (2.52) at the places where the pivots on the diagonal are missing
andobtain
 
1 3 0 0 3
0 1 0 0 0 
A˜ =  0 − 0 1 0 9   . (2.54)
 
0 0 0 1 4
−
0 0 0 0 1
−
Fromthisform,wecanimmediatelyreadoutthesolutionsofAx = 0by
takingthecolumnsofA˜,whichcontain 1onthediagonal:
−
     
3 3
 
 
   1  0   
 x R5 : x = λ 1 − 0  +λ 2  9   , λ 1,λ 2 R , (2.55)
   ∈   0     4  ∈   
  0 − 1  
−
whichisidenticaltothesolutionin(2.50)thatweobtainedby“insight”.
Calculating the Inverse
To compute the inverse A−1 of A Rn×n, we need to find a matrix X
that satisfies AX = I . Then, X∈ = A−1. We can write this down as
n
a set of simultaneous linear equations AX = I , where we solve for
n
X = [x x ]. We use the augmented matrix notation for a compact
1 n
|···|
representationofthissetofsystemsoflinearequationsandobtain
(cid:2) A I (cid:3) ⇝ ⇝ (cid:2) I A−1(cid:3) . (2.56)
n n
| ··· |
This means that if we bring the augmented equation system into reduced
row-echelon form, we can read out the inverse on the right-hand side of
theequationsystem.Hence,determiningtheinverseofamatrixisequiv-
alenttosolvingsystemsoflinearequations.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
34 LinearAlgebra
Example2.9(CalculatinganInverseMatrixbyGaussianElimination)
Todeterminetheinverseof
 
1 0 2 0
1 1 0 0
A =   (2.57)
1 2 0 1
1 1 1 1
wewritedowntheaugmentedmatrix
 
1 0 2 0 1 0 0 0
 1 1 0 0 0 1 0 0 
 
 1 2 0 1 0 0 1 0 
1 1 1 1 0 0 0 1
anduseGaussianeliminationtobringitintoreducedrow-echelonform
 
1 0 0 0 1 2 2 2
− −
 0 1 0 0 1 1 2 2
 − − ,
 0 0 1 0 1 1 1 1
− −
0 0 0 1 1 0 1 2
− −
suchthatthedesiredinverseisgivenasitsright-handside:
 
1 2 2 2
− −
A−1 =   1 −1 2 −2  . (2.58)
 1 1 1 1
− −
1 0 1 2
− −
We can verify that (2.58) is indeed the inverse by performing the multi-
plicationAA−1 andobservingthatwerecoverI .
4
2.3.4 Algorithms for Solving a System of Linear Equations
In the following, we briefly discuss approaches to solving a system of lin-
ear equations of the form Ax = b. We make the assumption that a solu-
tionexists.Shouldtherebenosolution,weneedtoresorttoapproximate
solutions,whichwedonotcoverinthischapter.Onewaytosolvetheap-
proximate problem is using the approach of linear regression, which we
discussindetailinChapter9.
In special cases, we may be able to determine the inverse A−1, such
that the solution of Ax = b is given as x = A−1b. However, this is
onlypossibleifAisasquarematrixandinvertible,whichisoftennotthe
case. Otherwise, under mild assumptions (i.e., A needs to have linearly
independentcolumns)wecanusethetransformation
Ax = b A⊤Ax = A⊤b x = (A⊤A)−1A⊤b (2.59)
⇐⇒ ⇐⇒
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.4 VectorSpaces 35
and use the Moore-Penrose pseudo-inverse (A⊤A)−1A⊤ to determine the Moore-Penrose
solution (2.59) that solves Ax = b, which also corresponds to the mini- pseudo-inverse
mumnormleast-squaressolution.Adisadvantageofthisapproachisthat
itrequiresmanycomputationsforthematrix-matrixproductandcomput-
ing the inverse of A⊤A. Moreover, for reasons of numerical precision it
is generally not recommended to compute the inverse or pseudo-inverse.
In the following, we therefore briefly discuss alternative approaches to
solvingsystemsoflinearequations.
Gaussian elimination plays an important role when computing deter-
minants (Section 4.1), checking whether a set of vectors is linearly inde-
pendent(Section2.5),computingtheinverseofamatrix(Section2.2.2),
computing the rank of a matrix (Section 2.6.2), and determining a basis
ofavectorspace(Section2.6.1).Gaussianeliminationisanintuitiveand
constructive way to solve a system of linear equations with thousands of
variables. However, for systems with millions of variables, it is impracti-
calastherequirednumberofarithmeticoperationsscalescubicallyinthe
numberofsimultaneousequations.
Inpractice,systemsofmanylinearequationsaresolvedindirectly,byei-
therstationaryiterativemethods,suchastheRichardsonmethod,theJa-
cobimethod,theGauß-Seidelmethod,andthesuccessiveover-relaxation
method,orKrylovsubspacemethods,suchasconjugategradients,gener-
alized minimal residual, or biconjugate gradients. We refer to the books
byStoerandBurlirsch(2002),Strang(2003),andLiesenandMehrmann
(2015)forfurtherdetails.
Letx beasolutionofAx = b.Thekeyideaoftheseiterativemethods
∗
istosetupaniterationoftheform
x(k+1) = Cx(k)+d (2.60)
forsuitableC anddthatreducestheresidualerror x(k+1) x inevery
∗
∥ − ∥
iterationandconvergestox .Wewillintroducenorms ,whichallow
∗
∥·∥
ustocomputesimilaritiesbetweenvectors,inSection3.1.
2.4 Vector Spaces
Thus far, we have looked at systems of linear equations and how to solve
them (Section 2.3). We saw that systems of linear equations can be com-
pactly represented using matrix-vector notation (2.10). In the following,
wewillhaveacloserlookatvectorspaces,i.e.,astructuredspaceinwhich
vectorslive.
Inthebeginningofthischapter,weinformallycharacterizedvectorsas
objects that can be added together and multiplied by a scalar, and they
remain objects of the same type. Now, we are ready to formalize this,
and we will start by introducing the concept of a group, which is a set
of elements and an operation defined on these elements that keeps some
structureofthesetintact.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
36 LinearAlgebra
2.4.1 Groups
Groups play an important role in computer science. Besides providing a
fundamental framework for operations on sets, they are heavily used in
cryptography,codingtheory,andgraphics.
Definition2.7(Group). Consideraset andanoperation :
G ⊗ G×G → G
group definedon .ThenG := ( , )iscalledagroupifthefollowinghold:
G G ⊗
closure
1. Closureof under : x,y : x y
associativity G ⊗ ∀ ∈ G ⊗ ∈ G
2. Associativity: x,y,z : (x y) z = x (y z)
neutralelement ∀ ∈ G ⊗ ⊗ ⊗ ⊗
3. Neutralelement: e x : x e = xande x = x
inverseelement
∃ ∈ G∀ ∈ G ⊗ ⊗
4. Inverse element: x y : x y = e and y x = e, where e is
∀ ∈ G∃ ∈ G ⊗ ⊗
theneutralelement.Weoftenwritex−1 todenotetheinverseelement
ofx.
Remark. The inverse element is defined with respect to the operation
⊗
anddoesnotnecessarilymean 1.
x ♢
Abeliangroup If additionally x,y : x y = y x, then G = ( , ) is an Abelian
∀ ∈ G ⊗ ⊗ G ⊗
group(commutative).
Example 2.10 (Groups)
Let us have a look at some examples of sets with associated operations
andseewhethertheyaregroups:
(Z,+)isanAbeliangroup.
N 0:=N∪{0} (N 0,+) is not a group: Although (N 0,+) possesses a neutral element
(0),theinverseelementsaremissing.
(Z, )isnotagroup:Although(Z, )containsaneutralelement(1),the
inve· rseelementsforanyz Z,z =· 1,aremissing.
(R, )isnotagroupsince0∈ doesn̸ ot± possessaninverseelement.
(R · 0 , )isAbelian.
(R\ n,{ +} ),· (Zn,+),n NareAbelianif+isdefinedcomponentwise,i.e.,
∈
(x , ,x )+(y , ,y ) = (x +y , ,x +y ). (2.61)
1 n 1 n 1 1 n n
··· ··· ···
Then, (x , ,x )−1 := ( x , , x ) is the inverse element and
1 n 1 n
··· − ··· −
e = (0, ,0)istheneutralelement.
(Rm×n,· +··
), the set of m n-matrices is Abelian (with componentwise
×
additionasdefinedin(2.61)).
Letushaveacloserlookat(Rn×n, ),i.e.,thesetofn n-matriceswith
· ×
matrixmultiplicationasdefinedin(2.13).
– Closureandassociativityfollowdirectlyfromthedefinitionofmatrix
multiplication.
– Neutral element: The identity matrix I is the neutral element with
n
respecttomatrixmultiplication“ ”in(Rn×n, ).
· ·
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.4 VectorSpaces 37
– Inverse element: If the inverse exists (A is regular), then A−1 is the
inverse element of A Rn×n, and in exactly this case (Rn×n, ) is a
∈ ·
group,calledthegenerallineargroup.
Definition 2.8 (General Linear Group). The set of regular (invertible)
matrices A Rn×n is a group with respect to matrix multiplication as
defined in (2∈ .13) and is called general linear group GL(n,R). However, generallineargroup
sincematrixmultiplicationisnotcommutative,thegroupisnotAbelian.
2.4.2 Vector Spaces
When we discussed groups, we looked at sets and inner operations on
G
, i.e., mappings that only operate on elements in . In the
G G ×G → G G
following, we will consider sets that in addition to an inner operation +
also contain an outer operation , the multiplication of a vector x by
ascalarλ
R.Wecanthinkoft· heinneroperationasaformofad∈ diG
tion,
∈
and the outer operation as a form of scaling. Note that the inner/outer
operationshavenothingtodowithinner/outerproducts.
Definition 2.9 (VectorSpace). Areal-valuedvectorspaceV = ( ,+, )is vectorspace
V ·
aset withtwooperations
V
+ : (2.62)
V ×V → V
: R (2.63)
· ×V → V
where
1. ( ,+)isanAbeliangroup
V
2. Distributivity:
1. λ R,x,y : λ (x+y) = λ x+λ y
∀ ∈ ∈ V · · ·
2. λ,ψ R,x : (λ+ψ) x = λ x+ψ x
∀ ∈ ∈ V · · ·
3. Associativity(outeroperation): λ,ψ R,x : λ (ψ x) = (λψ) x
∀ ∈ ∈ V · · ·
4. Neutralelementwithrespecttotheouteroperation: x : 1 x = x
∀ ∈ V ·
The elements x V are called vectors. The neutral element of ( ,+) is vector
∈ V
thezerovector0 = [0,...,0]⊤,andtheinneroperation+iscalledvector vectoraddition
addition. The elements λ R are called scalars and the outer operation scalar
∈
is a multiplication by scalars. Note that a scalar product is something multiplicationby
·
different,andwewillgettothisinSection3.2. scalars
Remark. A “vector multiplication” ab, a,b Rn, is not defined. Theoret-
∈
ically, we could define an element-wise multiplication, such that c = ab
with c = a b . This “array multiplication” is common to many program-
j j j
ming languages but makes mathematically limited sense using the stan-
dardrulesformatrixmultiplication:Bytreatingvectorsasn 1matrices
×
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
38 LinearAlgebra
(which we usually do), we can use the matrix multiplication as defined
in(2.13).However,thenthedimensionsofthevectorsdonotmatch.Only
outerproduct the following multiplications for vectors are defined: ab⊤ Rn×n (outer
product),a⊤b R(inner/scalar/dotproduct). ∈
∈ ♢
Example 2.11 (Vector Spaces)
Letushavealookatsomeimportantexamples:
= Rn,n Nisavectorspacewithoperationsdefinedasfollows:
V ∈
– Addition:x+y = (x ,...,x )+(y ,...,y ) = (x +y ,...,x +y )
1 n 1 n 1 1 n n
forallx,y Rn
∈
– Multiplication by scalars: λx = λ(x ,...,x ) = (λx ,...,λx ) for
1 n 1 n
allλ R,x Rn
∈ ∈
= Rm×n,m,n Nisavectorspacewith
V ∈
 
a +b a +b
11 11 1n 1n
···
. .
– Addition: A + B =   . . . .   is defined ele-
a +b a +b
m1 m1 mn mn
···
mentwiseforallA,B
∈ V  
λa λa
11 1n
···
. .
– Multiplication by scalars: λA =   . . . .   as defined in
λa λa
m1 mn
···
Section2.2.RememberthatRm×n isequivalenttoRmn.
= C,withthestandarddefinitionofadditionofcomplexnumbers.
V
Remark. In the following, we will denote a vector space ( ,+, ) by V
V ·
when + and are the standard vector addition and scalar multiplication.
·
Moreover, we will use the notation x V for vectors in to simplify
∈ V
notation.
♢
Remark. The vector spaces Rn,Rn×1,R1×n are only different in the way
wewritevectors.Inthefollowing,wewillnotmakeadistinctionbetween
columnvector
Rn andRn×1,whichallowsustowriten-tuplesascolumnvectors
 
x
1
.
x =   . .  . (2.64)
x
n
This simplifies the notation regarding vector space operations. However,
rowvector wedodistinguishbetweenRn×1 andR1×n (therowvectors)toavoidcon-
fusion with matrix multiplication. By default, we write x to denote a col-
transpose
umnvector,andarowvectorisdenotedbyx⊤,thetransposeofx.
♢
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.4 VectorSpaces 39
2.4.3 Vector Subspaces
In the following, we will introduce vector subspaces. Intuitively, they are
sets contained in the original vector space with the property that when
weperformvectorspaceoperationsonelementswithinthissubspace,we
willneverleaveit.Inthissense,theyare“closed”.Vectorsubspacesarea
keyideainmachinelearning.Forexample,Chapter10demonstrateshow
tousevectorsubspacesfordimensionalityreduction.
Definition 2.10 (Vector Subspace). Let V = ( ,+, ) be a vector space
V ·
and , = . Then U = ( ,+, ) is called vector subspace of V (or vectorsubspace
U ⊆ V U ̸ ∅ U ·
linear subspace) if U is a vector space with the vector space operations + linearsubspace
and restrictedto andR .WewriteU V todenoteasubspace
· U×U ×U ⊆
U ofV.
If andV isavectorspace,thenU naturallyinheritsmanyprop-
U ⊆ V
ertiesdirectlyfromV becausetheyholdforallx ,andinparticularfor
∈ V
all x . This includes the Abelian group properties, the distribu-
∈ U ⊆ V
tivity, the associativity and the neutral element. To determine whether
( ,+, )isasubspaceofV westilldoneedtoshow
U ·
1. = ,inparticular:0
U ̸ ∅ ∈ U
2. ClosureofU:
a. Withrespecttotheouteroperation: λ R x : λx .
∀ ∈ ∀ ∈ U ∈ U
b. Withrespecttotheinneroperation: x,y : x+y .
∀ ∈ U ∈ U
Example 2.12 (Vector Subspaces)
Letushavealookatsomeexamples:
ForeveryvectorspaceV,thetrivialsubspacesareV itselfand 0 .
OnlyexampleDinFigure2.6isasubspaceofR2(withtheusua{ lin}
ner/
outeroperations).InAandC,theclosurepropertyisviolated;B does
notcontain0.
The solution set of a homogeneous system of linear equations Ax = 0
withnunknownsx = [x ,...,x ]⊤ isasubspaceofRn.
1 n
The solution of an inhomogeneous system of linear equations Ax =
b, b = 0isnotasubspaceofRn.
̸
Theintersectionofarbitrarilymanysubspacesisasubspaceitself.
B Figure2.6 Notall
A subsetsofR2are
subspaces.InAand
D
C,theclosure
0 0 0 0
C propertyisviolated;
Bdoesnotcontain
0.OnlyDisa
subspace.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
40 LinearAlgebra
Remark. Every subspace U (Rn,+, ) is the solution space of a homo-
geneoussystemoflinearequ⊆ ationsAx· = 0forx Rn.
∈ ♢
2.5 Linear Independence
Inthefollowing,wewillhaveacloselookatwhatwecandowithvectors
(elements of the vector space). In particular, we can add vectors together
and multiply them with scalars. The closure property guarantees that we
endupwithanothervectorinthesamevectorspace.Itispossibletofind
a set of vectors with which we can represent every vector in the vector
space by adding them together and scaling them. This set of vectors is
a basis, and we will discuss them in Section 2.6.1. Before we get there,
we will need to introduce the concepts of linear combinations and linear
independence.
Definition 2.11 (Linear Combination). Consider a vector space V and a
finitenumberofvectorsx ,...,x V.Then,everyv V oftheform
1 k
∈ ∈
k
(cid:88)
v = λ x + +λ x = λ x V (2.65)
1 1 k k i i
··· ∈
i=1
linearcombination withλ 1,...,λ k Risalinearcombinationofthevectorsx 1,...,x k.
∈
The 0-vector can always be written as the linear combination of k vec-
tors x ,...,x because 0 =
(cid:80)k
0x is always true. In the following,
1 k i=1 i
we are interested in non-trivial linear combinations of a set of vectors to
represent 0, i.e., linear combinations of vectors x ,...,x , where not all
1 k
coefficientsλ in(2.65)are0.
i
Definition 2.12 (Linear (In)dependence). Let us consider a vector space
V with k N and x ,...,x V. If there is a non-trivial linear com-
1 k
bination,
s∈
uch that 0 =
(cid:80)k λ∈
x with at least one λ = 0, the vectors
i=1 i i i ̸
linearlydependent x 1,...,x k are linearly dependent. If only the trivial solution exists, i.e.,
linearly λ 1 = ... = λ k = 0thevectorsx 1,...,x k arelinearlyindependent.
independent
Linear independence is one of the most important concepts in linear
algebra.Intuitively,asetoflinearlyindependentvectorsconsistsofvectors
that have no redundancy, i.e., if we remove any of those vectors from
the set, we will lose something. Throughout the next sections, we will
formalizethisintuitionmore.
Example 2.13 (Linearly Dependent Vectors)
A geographic example may help to clarify the concept of linear indepen-
dence. A person in Nairobi (Kenya) describing where Kigali (Rwanda) is
mightsay,“YoucangettoKigalibyfirstgoing506kmNorthwesttoKam-
pala(Uganda)andthen374kmSouthwest.”.Thisissufficientinformation
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.5 LinearIndependence 41
to describe the location of Kigali because the geographic coordinate sys-
temmaybeconsideredatwo-dimensionalvectorspace(ignoringaltitude
and the Earth’s curved surface). The person may add, “It is about 751km
West of here.” Although this last statement is true, it is not necessary to
find Kigali given the previous information (see Figure 2.7 for an illus-
tration). In this example, the “506km Northwest” vector (blue) and the
“374kmSouthwest”vector(purple)arelinearlyindependent.Thismeans
the Southwest vector cannot be described in terms of the Northwest vec-
tor, and vice versa. However, the third “751km West” vector (black) is a
linear combination of the other two vectors, and it makes the set of vec-
tors linearly dependent. Equivalently, given “751km West” and “374km
Southwest”canbelinearlycombinedtoobtain“506kmNorthwest”.
Kampala
Figure2.7
w
est 506
k
G (weo itg hra crp uh dic eexample
h m
Sout North
west
a cap rp dr io nx ai lm da irti eo cn tis ot no
s)
k m Nairobi oflinearly
4 dependentvectors
37 751 km West ina
t
es two-dimensional
w
h space(plane).
Kigali t
u
o
S
m
k
4
7
3
Remark. The following properties are useful to find out whether vectors
arelinearlyindependent:
k vectors are either linearly dependent or linearly independent. There
isnothirdoption.
If at least one of the vectors x ,...,x is 0 then they are linearly de-
1 k
pendent.Thesameholdsiftwovectorsareidentical.
The vectors x ,...,x : x = 0,i = 1,...,k , k ⩾ 2, are linearly
1 k i
{ ̸ }
dependent if and only if (at least) one of them is a linear combination
oftheothers.Inparticular,ifonevectorisamultipleofanothervector,
i.e., x = λx , λ R then the set x ,...,x : x = 0,i = 1,...,k
i j 1 k i
∈ { ̸ }
islinearlydependent.
Apracticalwayofcheckingwhethervectorsx ,...,x V arelinearly
1 k
∈
independentistouseGaussianelimination:Writeallvectorsascolumns
of a matrix A and perform Gaussian elimination until the matrix is in
rowechelonform(thereducedrow-echelonformisunnecessaryhere):
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
42 LinearAlgebra
– The pivot columns indicate the vectors, which are linearly indepen-
dent of the vectors on the left. Note that there is an ordering of vec-
torswhenthematrixisbuilt.
– The non-pivot columns can be expressed as linear combinations of
thepivotcolumnsontheirleft.Forinstance,therow-echelonform
(cid:20) (cid:21)
1 3 0
(2.66)
0 0 2
tells us that the first and third columns are pivot columns. The sec-
ond column is a non-pivot column because it is three times the first
column.
All column vectors are linearly independent if and only if all columns
arepivotcolumns.Ifthereisatleastonenon-pivotcolumn,thecolumns
(and,therefore,thecorrespondingvectors)arelinearlydependent.
♢
Example 2.14
ConsiderR4 with
     
1 1 1
−
 2  1  2
x 1 =   3 , x 2 =  0 , x 3 =  − 1  . (2.67)
−
4 2 1
To check whether they are linearly dependent, we follow the general ap-
proachandsolve
     
1 1 1
−
 2  1  2
λ 1x 1+λ 2x 2+λ 3x 3 = λ 1  3 +λ 2 0 +λ 3 − 1   = 0 (2.68)
−
4 2 1
for λ ,...,λ . We write the vectors x , i = 1,2,3, as the columns of a
1 3 i
matrix and apply elementary row operations until we identify the pivot
columns:
   
1 1 1 1 1 1
− −
  2 1 −2  ⇝ ⇝  0 1 0   . (2.69)
 3 0 1  ··· 0 0 1 
−
4 2 1 0 0 0
Here,everycolumnofthematrixisapivotcolumn.Therefore,thereisno
non-trivial solution, and we require λ = 0,λ = 0,λ = 0 to solve the
1 2 3
equationsystem.Hence,thevectorsx ,x ,x arelinearlyindependent.
1 2 3
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.5 LinearIndependence 43
Remark. Consider a vector space V with k linearly independent vectors
b ,...,b andmlinearcombinations
1 k
k
(cid:88)
x = λ b ,
1 i1 i
i=1
.
. . (2.70)
k
(cid:88)
x = λ b .
m im i
i=1
Defining B = [b ,...,b ] as the matrix whose columns are the linearly
1 k
independentvectorsb ,...,b ,wecanwrite
1 k
 
λ
1j
.
x j = Bλ j, λ j =   . .   , j = 1,...,m, (2.71)
λ
kj
inamorecompactform.
We want to test whether x ,...,x are linearly independent. For this
1 m
purpose,wefollowthegeneralapproachoftestingwhen(cid:80)m
ψ x = 0.
j=1 j j
With(2.71),weobtain
m m m
(cid:88) (cid:88) (cid:88)
ψ x = ψ Bλ = B ψ λ . (2.72)
j j j j j j
j=1 j=1 j=1
This means that x ,...,x are linearly independent if and only if the
1 m
{ }
columnvectors λ ,...,λ arelinearlyindependent.
1 m
{ }
♢
Remark. InavectorspaceV,mlinearcombinationsofkvectorsx ,...,x
1 k
arelinearlydependentifm > k.
♢
Example 2.15
Considerasetoflinearlyindependentvectorsb ,b ,b ,b Rn and
1 2 3 4
∈
x = b 2b + b b
1 1 2 3 4
− −
x = 4b 2b + 4b
2 − 1 − 2 4 . (2.73)
x = 2b + 3b b 3b
3 1 2 3 4
− −
x = 17b 10b + 11b + b
4 1 2 3 4
−
Are the vectors x ,...,x Rn linearly independent? To answer this
1 4
∈
question,weinvestigatewhetherthecolumnvectors
       
1 4 2 17
  −  
  2  2  3   10
− ,− , ,−  (2.74)
 1   0   1  11 
  1 4 − 3 1  
− −
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
44 LinearAlgebra
are linearly independent. The reduced row-echelon form of the corre-
spondinglinearequationsystemwithcoefficientmatrix
 
1 4 2 17
−
 2 2 3 10
A = − − −  (2.75)
 1 0 1 11 
−
1 4 3 1
− −
isgivenas
 
1 0 0 7
−
0 1 0 15
 −  . (2.76)
0 0 1 18
−
0 0 0 0
Weseethatthecorrespondinglinearequationsystemisnon-triviallysolv-
able:Thelastcolumnisnotapivotcolumn,andx = 7x 15x 18x .
4 1 2 3
− − −
Therefore, x ,...,x are linearly dependent as x can be expressed as a
1 4 4
linearcombinationofx ,...,x .
1 3
2.6 Basis and Rank
InavectorspaceV,weareparticularlyinterestedinsetsofvectors that
A
possess the property that any vector v V can be obtained by a linear
∈
combination of vectors in . These vectors are special vectors, and in the
A
following,wewillcharacterizethem.
2.6.1 Generating Set and Basis
Definition2.13(GeneratingSetandSpan). ConsideravectorspaceV =
( ,+, ) and set of vectors = x ,...,x . If every vector v
1 k
V · A { } ⊆ V ∈
can be expressed as a linear combination of x ,...,x , is called a
1 k
V A
generatingset generating set of V. The set of all linear combinations of vectors in is
A
span calledthespanof .If spansthevectorspaceV,wewriteV = span[ ]
A A A
orV = span[x ,...,x ].
1 k
Generating sets are sets of vectors that span vector (sub)spaces, i.e.,
every vector can be represented as a linear combination of the vectors
in the generating set. Now, we will be more specific and characterize the
smallestgeneratingsetthatspansavector(sub)space.
Definition 2.14 (Basis). Consider a vector space V = ( ,+, ) and
V · A ⊆
minimal . A generating set of V is called minimal if there exists no smaller set
V˜⊊ thatspaA
nsV.EverylinearlyindependentgeneratingsetofV
A A ⊆ V
basis isminimalandiscalledabasisofV.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.6 BasisandRank 45
Let V = ( ,+, ) be a vector space and , = . Then, the
V · B ⊆ V B ̸ ∅
followingstatementsareequivalent: Abasisisaminimal
generatingsetanda
isabasisofV. maximallinearly
B
isaminimalgeneratingset. independentsetof
B vectors.
isamaximallinearlyindependentsetofvectorsinV,i.e.,addingany
B
othervectortothissetwillmakeitlinearlydependent.
Everyvectorx V isalinearcombinationofvectorsfrom ,andevery
∈ B
linearcombinationisunique,i.e.,with
k k
(cid:88) (cid:88)
x = λ b = ψ b (2.77)
i i i i
i=1 i=1
andλ ,ψ R,b itfollowsthatλ = ψ , i = 1,...,k.
i i i i i
∈ ∈ B
Example 2.16
InR3,the canonical/standardbasisis canonicalbasis
     
1 0 0
 
= 0,1,0 . (2.78)
B  0 0 1 
DifferentbasesinR3 are
           
1 1 1 0.5 1.8 2.2
   − 
1
= 0,1,1 ,
2
= 0.8,0.3, 1.3 . (2.79)
B  0 0 1  B  0.4 0.3 − 3.5 
Theset
     
1 2 1
 
 
 2  1  1 
=  ,− ,  (2.80)
A 3  0   0 
 
 4 2 4 
−
is linearly independent, but not a generating set (and no basis) of R4:
Forinstance,thevector[1,0,0,0]⊤ cannotbeobtainedbyalinearcom-
binationofelementsin .
A
Remark. Every vector space V possesses a basis . The preceding exam-
B
ples show that there can be many bases of a vector space V, i.e., there is
nouniquebasis.However,allbasespossessthesamenumberofelements,
thebasisvectors. basisvector
♢
We only consider finite-dimensional vector spaces V. In this case, the
dimensionofV isthenumberofbasisvectorsofV,andwewritedim(V). dimension
If U V is a subspace of V, then dim(U) ⩽ dim(V) and dim(U) =
⊆
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
46 LinearAlgebra
dim(V) if and only if U = V. Intuitively, the dimension of a vector space
can be thought of as the number of independent directions in this vector
Thedimensionofa space.
vectorspace
Remark. The dimension of a vector space is not necessarily the number
correspondstothe (cid:20) (cid:21)
0
numberofitsbasis of elements in a vector. For instance, the vector space V = span[ ] is
vectors. 1
one-dimensional,althoughthebasisvectorpossessestwoelements.
♢
Remark. A basis of a subspace U = span[x ,...,x ] Rn can be found
1 m
⊆
byexecutingthefollowingsteps:
1. WritethespanningvectorsascolumnsofamatrixA
2. Determinetherow-echelonformofA.
3. The spanning vectors associated with the pivot columns are a basis of
U.
♢
Example 2.17 (Determining a Basis)
ForavectorsubspaceU R5,spannedbythevectors
⊆
       
1 2 3 1
−
 2   1  4  8 
x 1 =   1 , x 2 =  − 1  , x 3 =  − 3  , x 4 =   5  R5, (2.81)
−      −  ∈
 1  2   5   6
− −
1 2 3 1
− − −
weareinterestedinfindingoutwhichvectorsx ,...,x areabasisforU.
1 4
For this, we need to check whether x ,...,x are linearly independent.
1 4
Therefore,weneedtosolve
4
(cid:88)
λ x = 0, (2.82)
i i
i=1
whichleadstoahomogeneoussystemofequationswithmatrix
 
1 2 3 1
−
 2 1 4 8 
(cid:2) (cid:3)  − − 
x 1,x 2,x 3,x 4 =  1 1 3 5. (2.83)
− − 
 1 2 5 6
− −
1 2 3 1
− − −
With the basic transformation rules for systems of linear equations, we
obtaintherow-echelonform
   
1 2 3 1 1 2 3 1
− −
 2 1 4 8   0 1 2 2 
  1 − 1 − 3 5   ⇝ ⇝   0 0 0 − 1   .
− −  ···  
 1 2 5 6   0 0 0 0 
− −
1 2 3 1 0 0 0 0
− − −
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.6 BasisandRank 47
Since the pivot columns indicate which set of vectors is linearly indepen-
dent, we see from the row-echelon form that x ,x ,x are linearly inde-
1 2 4
pendent(becausethesystemoflinearequationsλ x +λ x +λ x = 0
1 1 2 2 4 4
can only be solved with λ = λ = λ = 0). Therefore, x ,x ,x is a
1 2 4 1 2 4
{ }
basisofU.
2.6.2 Rank
The number of linearly independent columns of a matrix A Rm×n
∈
equals the number of linearly independent rows and is called the rank rank
ofAandisdenotedbyrk(A).
Remark. Therankofamatrixhassomeimportantproperties:
rk(A) = rk(A⊤),i.e.,thecolumnrankequalstherowrank.
The columns of A Rm×n span a subspace U Rm with dim(U) =
∈ ⊆
rk(A). Later we will call this subspace the image or range. A basis of
U can be found by applying Gaussian elimination to A to identify the
pivotcolumns.
The rows of A Rm×n span a subspace W Rn with dim(W) =
∈ ⊆
rk(A). A basis of W can be found by applying Gaussian elimination to
A⊤.
For all A Rn×n it holds that A is regular (invertible) if and only if
∈
rk(A) = n.
For all A Rm×n and all b Rm it holds that the linear equation
∈ ∈
system Ax = b can be solved if and only if rk(A) = rk(A b), where
|
A bdenotestheaugmentedsystem.
Fo| r A Rm×n the subspace of solutions for Ax = 0 possesses dimen-
∈
sion n rk(A). Later, we will call this subspace the kernel or the null kernel
−
space. nullspace
A matrix A Rm×n has full rank if its rank equals the largest possible fullrank
∈
rank for a matrix of the same dimensions. This means that the rank of
afull-rankmatrixisthelesserofthenumberofrowsandcolumns,i.e.,
rk(A) = min(m,n). A matrix is said to be rank deficient if it does not rankdeficient
havefullrank.
♢
Example 2.18 (Rank)
 
1 0 1
A = 0 1 1.
0 0 0
Ahastwolinearlyindependentrows/columnssothatrk(A) = 2.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
48 LinearAlgebra
 
1 2 1
A =  2 3 1.
− −
3 5 0
WeuseGaussianeliminationtodeterminetherank:
   
1 2 1 1 2 1
 2 3 1 ⇝ ⇝ 0 1 3 . (2.84)
− − ···
3 5 0 0 0 0
Here,weseethatthenumberoflinearlyindependentrowsandcolumns
is2,suchthatrk(A) = 2.
2.7 Linear Mappings
In the following, we will study mappings on vector spaces that preserve
their structure, which will allow us to define the concept of a coordinate.
Inthebeginningofthechapter,wesaidthatvectorsareobjectsthatcanbe
added together and multiplied by a scalar, and the resulting object is still
a vector. We wish to preserve this property when applying the mapping:
Consider two real vector spaces V,W. A mapping Φ : V W preserves
→
thestructureofthevectorspaceif
Φ(x+y) = Φ(x)+Φ(y) (2.85)
Φ(λx) = λΦ(x) (2.86)
for all x,y V and λ R. We can summarize this in the following
∈ ∈
definition:
Definition 2.15 (Linear Mapping). For vector spaces V,W, a mapping
linearmapping Φ : V W is called a linear mapping (or vector space homomorphism/
→
vectorspace lineartransformation)if
homomorphism
x,y V λ,ψ R : Φ(λx+ψy) = λΦ(x)+ψΦ(y). (2.87)
linear
∀ ∈ ∀ ∈
transformation
It turns out that we can represent linear mappings as matrices (Sec-
tion2.7.1).Recallthatwecanalsocollectasetofvectorsascolumnsofa
matrix. When working with matrices, we have to keep in mind what the
matrixrepresents:alinearmappingoracollectionofvectors.Wewillsee
more about linear mappings in Chapter 4. Before we continue, we will
brieflyintroducespecialmappings.
Definition 2.16 (Injective,Surjective,Bijective). ConsideramappingΦ :
,where , canbearbitrarysets.ThenΦiscalled
V → W V W
injective
Injectiveif x,y : Φ(x) = Φ(y) = x = y.
surjective
∀ ∈ V ⇒
Surjectiveif Φ( ) = .
bijective
V W
Bijectiveifitisinjectiveandsurjective.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.7 LinearMappings 49
If Φ is surjective, then every element in can be “reached” from
W V
using Φ. A bijective Φ can be “undone”, i.e., there exists a mapping Ψ :
so that Ψ Φ(x) = x. This mapping Ψ is then called the inverse
W → V ◦
ofΦandnormallydenotedbyΦ−1.
Withthesedefinitions,weintroducethefollowingspecialcasesoflinear
mappingsbetweenvectorspacesV andW:
isomorphism
Isomorphism:Φ : V W linearandbijective
endomorphism
→
Endomorphism:Φ : V V linear
automorphism
→
Automorphism:Φ : V V linearandbijective
→
We define id V : V V, x x as the identity mapping or identity identitymapping
→ (cid:55)→
automorphisminV. identity
automorphism
Example 2.19 (Homomorphism)
ThemappingΦ : R2 C, Φ(x) = x +ix ,isahomomorphism:
1 2
→
(cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)
x y
Φ 1 + 1 = (x +y )+i(x +y ) = x +ix +y +iy
x y 1 1 2 2 1 2 1 2
2 2
(cid:18)(cid:20) (cid:21)(cid:19) (cid:18)(cid:20) (cid:21)(cid:19)
x y
= Φ 1 +Φ 1
x y
2 2
(cid:18) (cid:20) (cid:21)(cid:19) (cid:18)(cid:20) (cid:21)(cid:19)
x x
Φ λ 1 = λx +λix = λ(x +ix ) = λΦ 1 .
x 1 2 1 2 x
2 2
(2.88)
This also justifies why complex numbers can be represented as tuples in
R2:Thereisabijectivelinearmappingthatconvertstheelementwiseaddi-
tionoftuplesinR2 intothesetofcomplexnumberswiththecorrespond-
ingaddition.Notethatweonlyshowedlinearity,butnotthebijection.
Theorem2.17(Theorem3.59inAxler(2015)). Finite-dimensionalvector
spacesV andW areisomorphicifandonlyifdim(V) = dim(W).
Theorem 2.17 states that there exists a linear, bijective mapping be-
tween two vector spaces of the same dimension. Intuitively, this means
that vector spaces of the same dimension are kind of the same thing, as
theycanbetransformedintoeachotherwithoutincurringanyloss.
Theorem 2.17 also gives us the justification to treat Rm×n (the vector
space of m n-matrices) and Rmn (the vector space of vectors of length
×
mn) the same, as their dimensions are mn, and there exists a linear, bi-
jectivemappingthattransformsoneintotheother.
Remark. ConsidervectorspacesV,W,X.Then:
For linear mappings Φ : V W and Ψ : W X, the mapping
→ →
Ψ Φ : V X isalsolinear.
◦ →
If Φ : V W is an isomorphism, then Φ−1 : W V is an isomor-
→ →
phism,too.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
50 LinearAlgebra
Figure2.8 Two
differentcoordinate
systemsdefinedby
twosetsofbasis
vectors.Avectorx
hasdifferent
coordinate
x x
representations b
2
dependingonwhich
coordinatesystemis e 2
chosen.
b
1
e
1
IfΦ : V W, Ψ : V W arelinear,thenΦ+ΨandλΦ, λ R,are
→ → ∈
linear,too.
♢
2.7.1 Matrix Representation of Linear Mappings
Any n-dimensional vector space is isomorphic to Rn (Theorem 2.17). We
consider a basis b ,...,b of an n-dimensional vector space V. In the
1 n
{ }
following, the order of the basis vectors will be important. Therefore, we
write
B = (b ,...,b ) (2.89)
1 n
orderedbasis andcallthisn-tupleanorderedbasisofV.
Remark (Notation). We are at the point where notation gets a bit tricky.
Therefore,wesummarizesomepartshere.B = (b ,...,b )isanordered
1 n
basis, = b ,...,b isan(unordered)basis,andB = [b ,...,b ]isa
1 n 1 n
B { }
matrixwhosecolumnsarethevectorsb ,...,b .
1 n
♢
Definition2.18(Coordinates). ConsideravectorspaceV andanordered
basisB = (b ,...,b )ofV.Foranyx V weobtainauniquerepresen-
1 n
∈
tation(linearcombination)
x = α b +...+α b (2.90)
1 1 n n
coordinate of x with respect to B. Then α 1,...,α n are the coordinates of x with
respecttoB,andthevector
 
α
1
α =   . . .   ∈ Rn (2.91)
α
n
coordinatevector is the coordinate vector/coordinate representation of x with respect to the
coordinate orderedbasisB.
representation
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.7 LinearMappings 51
Abasiseffectivelydefinesacoordinatesystem.Wearefamiliarwiththe
Cartesian coordinate system in two dimensions, which is spanned by the
canonical basis vectors e ,e . In this coordinate system, a vector x R2
1 2
∈
has a representation that tells us how to linearly combine e and e to
1 2
obtain x. However, any basis of R2 defines a valid coordinate system,
and the same vector x from before may have a different coordinate rep-
resentation in the (b ,b ) basis. In Figure 2.8, the coordinates of x with
1 2
respect to the standard basis (e ,e ) is [2,2]⊤. However, with respect to
1 2
the basis (b ,b ) the same vector x is represented as [1.09,0.72]⊤, i.e.,
1 2
x = 1.09b +0.72b . In the following sections, we will discover how to
1 2
obtainthisrepresentation.
Example 2.20
Let us have a look at a geometric vector x R2 with coordinates [2,3]⊤ Figure2.9
withrespecttothestandardbasis(e ,e
)of∈R2.Thismeans,wecanwrite
Differentcoordinate
1 2
x = 2e +3e . However, we do not have to choose the standard basis to representationsofa
1 2
vectorx,depending
representthisvector.Ifweusethebasisvectorsb = [1, 1]⊤,b = [1,1]⊤
1 − 2 onthechoiceof
wewillobtainthecoordinates 1[ 1,5]⊤torepresentthesamevectorwith basis.
2 −
respectto(b ,b )(seeFigure2.9).
1 2 x=2e1+3e2
x= −1 2b1+5 2b2
Remark. For an n-dimensional vector space V and an ordered basis B
of V, the mapping Φ : Rn V, Φ(e ) = b , i = 1,...,n, is linear
i i
→
(and because of Theorem 2.17 an isomorphism), where (e 1,...,e n) is e2
thestandardbasisofRn.
b2
e1
♢
Nowwearereadytomakeanexplicitconnectionbetweenmatricesand
b1
linearmappingsbetweenfinite-dimensionalvectorspaces.
Definition 2.19 (Transformation Matrix). Consider vector spaces V,W
withcorresponding(ordered)basesB = (b ,...,b )andC = (c ,...,c ).
1 n 1 m
Moreover,weconsideralinearmappingΦ : V W.Forj 1,...,n ,
→ ∈ { }
m
(cid:88)
Φ(b ) = α c + +α c = α c (2.92)
j 1j 1 mj m ij i
···
i=1
istheuniquerepresentationofΦ(b )withrespecttoC.Then,wecallthe
j
m n-matrixA ,whoseelementsaregivenby
Φ
×
A (i,j) = α , (2.93)
Φ ij
the transformation matrix of Φ (with respect to the ordered bases B of V transformation
andC ofW). matrix
The coordinates of Φ(b ) with respect to the ordered basis C of W
j
are the j-th column of A . Consider (finite-dimensional) vector spaces
Φ
V,W with ordered bases B,C and a linear mapping Φ : V W with
→
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
52 LinearAlgebra
transformation matrix A . If xˆ is the coordinate vector of x V with
Φ
∈
respect to B and yˆ the coordinate vector of y = Φ(x) W with respect
∈
toC,then
yˆ = A xˆ. (2.94)
Φ
Thismeansthatthetransformationmatrixcanbeusedtomapcoordinates
with respect to an ordered basis in V to coordinates with respect to an
orderedbasisinW.
Example 2.21 (Transformation Matrix)
Consider a homomorphism Φ : V W and ordered bases B =
→
(b ,...,b )ofV andC = (c ,...,c )ofW.With
1 3 1 4
Φ(b ) = c c +3c c
1 1 2 3 4
− −
Φ(b ) = 2c +c +7c +2c (2.95)
2 1 2 3 4
Φ(b ) = 3c +c +4c
3 2 3 4
the transformation matrix A with respect to B and C satisfies Φ(b ) =
Φ k
(cid:80)4
α c fork = 1,...,3andisgivenas
i=1 ik i
 
1 2 0
 1 1 3
A Φ = [α 1,α 2,α 3] =  − 3 7 1  , (2.96)
1 2 4
−
wheretheα , j = 1,2,3,arethecoordinatevectorsofΦ(b )withrespect
j j
toC.
Example 2.22 (Linear Transformations of Vectors)
Figure2.10 Three
examplesoflinear
transformationsof
thevectorsshown
asdotsin(a);
(b)Rotationby45◦;
(c)Stretchingofthe
horizontal
(a)Originaldata. (b)Rotationby45◦. (c) Stretch along the (d) General linear
coordinatesby2;
horizontalaxis. mapping.
(d)Combinationof
reflection,rotation We consider three linear transformations of a set of vectors in R2 with
andstretching.
thetransformationmatrices
(cid:20) cos(π) sin(π)(cid:21) (cid:20) 2 0(cid:21) 1 (cid:20) 3 1(cid:21)
A = 4 − 4 , A = , A = − . (2.97)
1 sin(π) cos(π) 2 0 1 3 2 1 1
4 4 −
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.7 LinearMappings 53
Figure2.10givesthreeexamplesoflineartransformationsofasetofvec-
tors.Figure2.10(a)shows400vectorsinR2,eachofwhichisrepresented
by a dot at the corresponding (x ,x )-coordinates. The vectors are ar-
1 2
rangedinasquare.WhenweusematrixA in(2.97)tolinearlytransform
1
eachofthesevectors,weobtaintherotatedsquareinFigure2.10(b).Ifwe
apply the linear mapping represented by A , we obtain the rectangle in
2
Figure2.10(c)whereeachx -coordinateisstretchedby2.Figure2.10(d)
1
showstheoriginalsquarefromFigure2.10(a)whenlinearlytransformed
usingA ,whichisacombinationofareflection,arotation,andastretch.
3
2.7.2 Basis Change
Inthefollowing,wewillhaveacloserlookathowtransformationmatrices
of a linear mapping Φ : V W change if we change the bases in V and
→
W.Considertwoorderedbases
B = (b ,...,b ), B˜ = (b˜ ,...,b˜ ) (2.98)
1 n 1 n
ofV andtwoorderedbases
C = (c ,...,c ), C˜ = (c˜ ,...,c˜ ) (2.99)
1 m 1 m
of W. Moreover, A Rm×n is the transformation matrix of the linear
Φ
mappingΦ : V W w∈ ithrespecttothebasesB andC,andA˜ Rm×n
Φ
is the correspon→ ding transformation mapping with respect to B˜∈ and C˜.
In the following, we will investigate how A and A˜ are related, i.e., how/
whether we can transform A into A˜ if we choose to perform a basis
Φ Φ
changefromB,C toB˜,C˜.
Remark. We effectively get different coordinate representations of the
identity mapping id . In the context of Figure 2.9, this would mean to
V
map coordinates with respect to (e ,e ) onto coordinates with respect to
1 2
(b ,b ) without changing the vector x. By changing the basis and corre-
1 2
spondingly the representation of vectors, the transformation matrix with
respect to this new basis can have a particularly simple form that allows
forstraightforwardcomputation.
♢
Example 2.23 (Basis Change)
Consideratransformationmatrix
(cid:20) (cid:21)
2 1
A = (2.100)
1 2
withrespecttothecanonicalbasisinR2.Ifwedefineanewbasis
(cid:20) (cid:21) (cid:20) (cid:21)
1 1
B = ( , ) (2.101)
1 1
−
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
54 LinearAlgebra
weobtainadiagonaltransformationmatrix
(cid:20) (cid:21)
3 0
A˜ = (2.102)
0 1
withrespecttoB,whichiseasiertoworkwiththanA.
In the following, we will look at mappings that transform coordinate
vectors with respect to one basis into coordinate vectors with respect to
a different basis. We will state our main result first and then provide an
explanation.
Theorem2.20(BasisChange). ForalinearmappingΦ : V W,ordered
→
bases
B = (b ,...,b ), B˜ = (b˜ ,...,b˜ ) (2.103)
1 n 1 n
ofV and
C = (c ,...,c ), C˜ = (c˜ ,...,c˜ ) (2.104)
1 m 1 m
of W, and a transformation matrix A of Φ with respect to B and C, the
Φ
correspondingtransformationmatrixA˜ withrespecttothebasesB˜ andC˜
Φ
isgivenas
A˜ = T−1A S. (2.105)
Φ Φ
Here, S Rn×n is the transformation matrix of id that maps coordinates
V
withresp∈ ecttoB˜ ontocoordinateswithrespecttoB,andT Rm×m isthe
transformation matrix of id that maps coordinates with res∈ pect to C˜ onto
W
coordinateswithrespecttoC.
Proof Following Drumm and Weil (2001), we can write the vectors of
the new basis B˜ of V as a linear combination of the basis vectors of B,
suchthat
n
(cid:88)
b˜ = s b + +s b = s b , j = 1,...,n. (2.106)
j 1j 1 nj n ij i
···
i=1
Similarly, we write the new basis vectors C˜ of W as a linear combination
ofthebasisvectorsofC,whichyields
m
(cid:88)
c˜ = t c + +t c = t c , k = 1,...,m. (2.107)
k 1k 1 mk m lk l
···
l=1
We define S = ((s )) Rn×n as the transformation matrix that maps
ij
∈
coordinates with respect to B˜ onto coordinates with respect to B and
T = ((t )) Rm×m as the transformation matrix that maps coordinates
lk
withrespect∈ toC˜ ontocoordinateswithrespecttoC.Inparticular,thejth
column of S is the coordinate representation of b˜ with respect to B and
j
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.7 LinearMappings 55
thekthcolumnofT isthecoordinaterepresentationofc˜ withrespectto
k
C.NotethatbothS andT areregular.
WearegoingtolookatΦ(b˜ )fromtwoperspectives.First,applyingthe
j
mappingΦ,wegetthatforallj = 1,...,n
(cid:32) (cid:33)
m m m m m
Φ(b˜ ) = (cid:88) a˜ c˜ (2. =107) (cid:88) a˜ (cid:88) t c = (cid:88) (cid:88) t a˜ c , (2.108)
j kj k kj lk l lk kj l
(cid:124) (cid:123)(cid:122) (cid:125)
k=1 k=1 l=1 l=1 k=1
∈W
where we first expressed the new basis vectors c˜ W as linear com-
k
∈
binations of the basis vectors c W and then swapped the order of
l
∈
summation.
Alternatively, when we express the b˜ V as linear combinations of
j
∈
b V,wearriveat
j
∈
(cid:32) (cid:33)
n n n m
Φ(b˜ ) (2. =106) Φ (cid:88) s b = (cid:88) s Φ(b ) = (cid:88) s (cid:88) a c (2.109a)
j ij i ij i ij li l
i=1 i=1 i=1 l=1
(cid:32) (cid:33)
m n
(cid:88) (cid:88)
= a s c , j = 1,...,n, (2.109b)
li ij l
l=1 i=1
where we exploited the linearity of Φ. Comparing (2.108) and (2.109b),
itfollowsforallj = 1,...,nandl = 1,...,mthat
m n
(cid:88) (cid:88)
t a˜ = a s (2.110)
lk kj li ij
k=1 i=1
and,therefore,
TA˜ = A S Rm×n, (2.111)
Φ Φ
∈
suchthat
A˜ = T−1A S, (2.112)
Φ Φ
whichprovesTheorem2.20.
Theorem2.20tellsusthatwithabasischangeinV (B isreplacedwith
B˜) and W (C is replaced with C˜), the transformation matrix A of a
Φ
linearmappingΦ : V W isreplacedbyanequivalentmatrixA˜ with
Φ
→
A˜ = T−1A S. (2.113)
Φ Φ
Figure 2.11 illustrates this relation: Consider a homomorphism Φ : V
W and ordered bases B,B˜ of V and C,C˜ of W. The mapping Φ is a→ n
CB
instantiation of Φ and maps basis vectors of B onto linear combinations
ofbasisvectorsofC.AssumethatweknowthetransformationmatrixA
Φ
ofΦ withrespectto theorderedbasesB,C.Whenwe performabasis
CB
change from B to B˜ in V and from C to C˜ in W, we can determine the
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
56 LinearAlgebra
Figure2.11 Fora Φ Φ
Vectorspaces V W V W
homomorphism
Φ:V →W and B Φ CB C B Φ CB C
orderedbasesB,B˜ A Φ A Φ
ofV andC,C˜ofW Orderedbases Ψ BB˜ S T Ξ CC˜ Ψ BB˜ S T−1 Ξ C˜C =Ξ− CC1 ˜
(markedinblue), A˜ A˜
B˜ Φ C˜ B˜ Φ C˜
wecanexpressthe
Φ C˜B˜ Φ C˜B˜
mappingΦ C˜B˜ with
respecttothebases
B˜,C˜equivalentlyas correspondingtransformationmatrixA˜ asfollows:First,wefindthema-
Φ
acompositionofthe
trixrepresentationofthelinearmappingΨ : V V thatmapscoordi-
h Φo C˜m B˜om =orphisms nateswithrespecttothenewbasisB˜ ontoB thB˜ e(un→ ique)coordinateswith
Ξ C˜C◦ΦCB◦Ψ BB˜ respect to the “old” basis B (in V). Then, we use the transformation ma-
withrespecttothe trix A of Φ : V W to map these coordinates onto the coordinates
Φ CB
basesinthe →
with respect to C in W. Finally, we use a linear mapping Ξ : W W
subscripts.The C˜C →
tomapthecoordinateswithrespecttoC ontocoordinateswithrespectto
corresponding
transformation C˜.Therefore,wecanexpressthelinearmappingΦ C˜B˜ asacompositionof
matricesareinred. linearmappingsthatinvolvethe“old”basis:
Φ = Ξ Φ Ψ = Ξ−1 Φ Ψ . (2.114)
C˜B˜ C˜C ◦ CB ◦ BB˜ CC˜ ◦ CB ◦ BB˜
Concretely,weuseΨ = id andΞ = id ,i.e.,theidentitymappings
BB˜ V CC˜ W
thatmapvectorsontothemselves,butwithrespecttoadifferentbasis.
equivalent Definition2.21(Equivalence). TwomatricesA,A˜ Rm×nareequivalent
if there exist regular matrices S Rn×n and T∈ Rm×m, such that
A˜ = T−1AS. ∈ ∈
similar Definition 2.22 (Similarity). Two matrices A,A˜ Rn×n are similar if
thereexistsaregularmatrixS Rn×n withA˜ = S∈−1AS
∈
Remark. Similarmatricesarealwaysequivalent.However,equivalentma-
tricesarenotnecessarilysimilar.
♢
Remark. Consider vector spaces V,W,X. From the remark that follows
Theorem 2.17, we already know that for linear mappings Φ : V W
→
and Ψ : W X the mapping Ψ Φ : V X is also linear. With
→ ◦ →
transformation matrices A and A of the corresponding mappings, the
Φ Ψ
overalltransformationmatrixisA = A A .
Ψ◦Φ Ψ Φ
♢
In light of this remark, we can look at basis changes from the perspec-
tiveofcomposinglinearmappings:
A is the transformation matrix of a linear mapping Φ : V W
Φ CB
→
withrespecttothebasesB,C.
A˜ is the transformation matrix of the linear mapping Φ : V W
wiΦ
threspecttothebasesB˜,C˜.
C˜B˜
→
S is the transformation matrix of a linear mapping Ψ : V V
(automorphism)thatrepresentsB˜ intermsofB.NormaB llB˜ y,Ψ = i→ d is
V
theidentitymappinginV.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.7 LinearMappings 57
T is the transformation matrix of a linear mapping Ξ : W W
(automorphism)thatrepresentsC˜ intermsofC.NormC alC˜ ly,Ξ = i→ d is
W
theidentitymappinginW.
If we (informally) write down the transformations just in terms of bases,
then A : B C, A˜ : B˜ C˜, S : B˜ B, T : C˜ C and
Φ Φ
T−1 : C C˜,a→ nd → → →
→
B˜ C˜ = B˜ B C C˜ (2.115)
→ → → →
A˜ = T−1A S. (2.116)
Φ Φ
Notethattheexecutionorderin(2.116)isfromrighttoleftbecausevec-
tors are multiplied at the right-hand side so that x Sx A (Sx)
Φ
T−1(cid:0) A (Sx)(cid:1) = A˜ x. (cid:55)→ (cid:55)→ (cid:55)→
Φ Φ
Example 2.24 (Basis Change)
ConsideralinearmappingΦ : R3 R4 whosetransformationmatrixis
→
 
1 2 0
 1 1 3
A Φ =  − 3 7 1  (2.117)
1 2 4
−
withrespecttothestandardbases
       
      1 0 0 0
1 0 0
0 1 0 0
B = (0,1,0), C = ( , , , ). (2.118)
0 0 1 0
0 0 1
0 0 0 1
WeseekthetransformationmatrixA˜ ofΦwithrespecttothenewbases
Φ
       
      1 1 0 1
1 0 1
B˜ = (1,1,0) R3, C˜ = ( 1 , 0 , 1 , 0 ). (2.119)
∈ 0 1 1 0
0 1 1
0 0 0 1
Then,
 
  1 1 0 1
1 0 1
1 0 1 0
S = 1 1 0, T =   , (2.120)
0 1 1 0
0 1 1
0 0 0 1
where the ith column of S is the coordinate representation of b˜ in
i
terms of the basis vectors of B. Since B is the standard basis, the co-
ordinate representation is straightforward to find. For a general basis B,
we would need to solve a linear equation system to find the λ such that
i
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
58 LinearAlgebra
(cid:80)3 λ b = b˜ ,j = 1,...,3.Similarly,thejthcolumnofT isthecoordi-
i=1 i i j
naterepresentationofc˜ intermsofthebasisvectorsofC.
j
Therefore,weobtain
  
1 1 1 1 3 2 1
− −
A˜ Φ = T−1A ΦS = 21    1 1 − 11 11 − 11    10 0 4 8 2 4   (2.121a)
−
0 0 0 2 1 6 3
 
4 4 2
− − −
 6 0 0 
=  . (2.121b)
 4 8 4 
1 6 3
In Chapter 4, we will be able to exploit the concept of a basis change
to find a basis with respect to which the transformation matrix of an en-
domorphismhasaparticularlysimple(diagonal)form.InChapter10,we
will look at adata compression problem and finda convenient basis onto
whichwecanprojectthedatawhileminimizingthecompressionloss.
2.7.3 Image and Kernel
The image and kernel of a linear mapping are vector subspaces with cer-
tain important properties. In the following, we will characterize them
morecarefully.
Definition 2.23 (ImageandKernel).
kernel ForΦ : V W,wedefinethekernel/nullspace
→
nullspace
ker(Φ) := Φ−1(0 ) = v V : Φ(v) = 0 (2.122)
W W
{ ∈ }
image andtheimage/range
range
Im(Φ) := Φ(V) = w W v V : Φ(v) = w . (2.123)
{ ∈ |∃ ∈ }
domain WealsocallV andW alsothedomainandcodomainofΦ,respectively.
codomain
Intuitively, the kernel is the set of vectors v V that Φ maps onto the
∈
neutral element 0 W. The image is the set of vectors w W that
W
∈ ∈
can be “reached” by Φ from any vector in V. An illustration is given in
Figure2.12.
Remark. Consider a linear mapping Φ : V W, where V,W are vector
→
spaces.
It always holds that Φ(0 ) = 0 and, therefore, 0 ker(Φ). In
V W V
∈
particular,thenullspaceisneverempty.
Im(Φ) W isasubspaceofW,andker(Φ) V isasubspaceofV.
⊆ ⊆
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.7 LinearMappings 59
Φ:V W Figure2.12 Kernel
V → W
andimageofa
linearmapping
Φ:V →W.
ker(Φ) Im(Φ)
0 0
V W
Φisinjective(one-to-one)ifandonlyifker(Φ) = 0 .
{ }
♢
Remark (Null Space and Column Space). Let us consider A Rm×n and
alinearmappingΦ : Rn Rm, x Ax. ∈
→ (cid:55)→
ForA = [a ,...,a ],wherea arethecolumnsofA,weobtain
1 n i
(cid:40) (cid:41)
n
(cid:88)
Im(Φ) = Ax : x Rn = x a : x ,...,x R (2.124a)
i i 1 n
{ ∈ } ∈
i=1
= span[a ,...,a ] Rm, (2.124b)
1 n
⊆
i.e., the image is the span of the columns of A, also called the column columnspace
space.Therefore,thecolumnspace(image)isasubspaceofRm,where
misthe“height”ofthematrix.
rk(A) = dim(Im(Φ)).
The kernel/null space ker(Φ) is the general solution to the homoge-
neous system of linear equations Ax = 0 and captures all possible
linearcombinationsoftheelementsinRn thatproduce0 Rm.
ThekernelisasubspaceofRn,wherenisthe“width”oft∈
hematrix.
Thekernelfocusesontherelationshipamongthecolumns,andwecan
use it to determine whether/how we can express a column as a linear
combinationofothercolumns.
♢
Example 2.25 (Image and Kernel of a Linear Mapping)
Themapping
   
x x
1 1
(cid:20) (cid:21) (cid:20) (cid:21)
Φ : R4 R2,  x 2  1 2 −1 0  x 2  = x 1+2x 2 −x 3
→ x 3 (cid:55)→ 1 0 0 1 x 3 x 1+x 4
x x
4 4
(2.125a)
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
60 LinearAlgebra
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
1 2 1 0
= x +x +x − +x (2.125b)
1 1 2 0 3 0 4 1
islinear.TodetermineIm(Φ),wecantakethespanofthecolumnsofthe
transformationmatrixandobtain
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
1 2 1 0
Im(Φ) = span[ , , − , ]. (2.126)
1 0 0 1
To compute the kernel (null space) of Φ, we need to solve Ax = 0, i.e.,
we need to solve a homogeneous equation system. To do this, we use
GaussianeliminationtotransformAintoreducedrow-echelonform:
(cid:20) (cid:21) (cid:20) (cid:21)
1 2 1 0 1 0 0 1
− ⇝ ⇝ . (2.127)
1 0 0 1 ··· 0 1 1 1
−2 −2
Thismatrixisinreducedrow-echelonform,andwecanusetheMinus-
1Tricktocomputeabasisofthekernel(seeSection2.3.3).Alternatively,
we can express the non-pivot columns (columns 3 and 4) as linear com-
binationsofthepivotcolumns(columns1and2).Thethirdcolumna is
3
equivalentto 1 timesthesecondcolumna .Therefore,0 = a +1a .In
−2 2 3 2 2
thesameway,weseethata = a 1a and,therefore,0 = a 1a a .
4 1 −2 2 1 −2 2 − 4
Overall,thisgivesusthekernel(nullspace)as
   
0 1
−
1 1
   
ker(Φ) = span[2, 2 ]. (2.128)
1  0 
0 1
rank-nullity
theorem Theorem 2.24 (Rank-NullityTheorem). ForvectorspacesV,W andalin-
earmappingΦ : V W itholdsthat
→
dim(ker(Φ))+dim(Im(Φ)) = dim(V). (2.129)
fundamental Therank-nullitytheoremisalsoreferredtoasthefundamentaltheorem
theoremoflinear of linear mappings (Axler, 2015, theorem 3.22). The following are direct
mappings
consequencesofTheorem2.24:
If dim(Im(Φ)) < dim(V), then ker(Φ) is non-trivial, i.e., the kernel
containsmorethan0 anddim(ker(Φ)) ⩾ 1.
V
IfA isthetransformationmatrixofΦwithrespecttoanorderedbasis
Φ
anddim(Im(Φ)) < dim(V),thenthesystemoflinearequationsA x =
Φ
0hasinfinitelymanysolutions.
Ifdim(V) = dim(W),thenthethree-wayequivalence
Φisinjective Φissurjective Φisbijective
⇐⇒ ⇐⇒
holdssinceIm(Φ) W.
⊆
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.8 AffineSpaces 61
2.8 Affine Spaces
In the following, we will have a closer look at spaces that are offset from
the origin, i.e., spaces that are no longer vector subspaces. Moreover, we
will briefly discuss properties of mappings between these affine spaces,
whichresemblelinearmappings.
Remark. Inthemachinelearningliterature,thedistinctionbetweenlinear
and affine is sometimes not clear so that we can find references to affine
spaces/mappingsaslinearspaces/mappings.
♢
2.8.1 Affine Subspaces
Definition 2.25 (Affine Subspace). Let V be a vector space, x V and
0
∈
U V asubspace.Thenthesubset
⊆
L = x +U := x +u : u U (2.130a)
0 0
{ ∈ }
= v V u U : v = x +u V (2.130b)
0
{ ∈ |∃ ∈ } ⊆
is called affine subspace or linear manifold of V. U is called direction or affinesubspace
direction space, and x 0 is called support point. In Chapter 12, we refer to linearmanifold
suchasubspaceasahyperplane. direction
directionspace
Note that the definition of an affine subspace excludes 0 if x 0 / U. supportpoint
∈
Therefore,anaffinesubspaceisnota(linear)subspace(vectorsubspace) hyperplane
ofV forx / U.
0
Examples∈ ofaffinesubspacesarepoints,lines,andplanesinR3,which
donot(necessarily)gothroughtheorigin.
Remark. ConsidertwoaffinesubspacesL = x +U andL˜ = x˜ +U˜ ofa
0 0
vectorspaceV.Then,L L˜ ifandonlyifU U˜ andx x˜ U˜.
0 0
⊆ ⊆ − ∈
Affinesubspacesareoftendescribedbyparameters:Considerak-dimen-
sionalaffinespaceL = x +U ofV.If(b ,...,b )isanorderedbasisof
0 1 k
U,theneveryelementx Lcanbeuniquelydescribedas
∈
x = x +λ b +...+λ b , (2.131)
0 1 1 k k
where λ 1,...,λ k R. This representation is called parametric equation parametricequation
∈
ofLwithdirectionalvectorsb 1,...,b k andparametersλ 1,...,λ k. parameters
♢
Example 2.26 (Affine Subspaces)
One-dimensional affine subspaces are called lines and can be written line
as y = x + λb , where λ R and U = span[b ] Rn is a one-
0 1 1
dimensionalsubspaceofRn.T∈ hismeansthatalineisd⊆
efinedbyasup-
portpointx andavectorb thatdefinesthedirection.SeeFigure2.13
0 1
foranillustration.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
62 LinearAlgebra
plane Two-dimensional affine subspaces of Rn are called planes. The para-
metric equation for planes is y = x +λ b +λ b , where λ ,λ R
0 1 1 2 2 1 2
and U = span[b ,b ] Rn. This means that a plane is defined b∈ y a
1 2
⊆
support point x and two linearly independent vectors b ,b that span
0 1 2
thedirectionspace.
hyperplane InRn,the(n 1)-dimensionalaffinesubspacesarecalledhyperplanes,
and the
corre−
sponding parametric equation is y = x +
(cid:80)n−1λ
b ,
0 i=1 i i
where b ,...,b form a basis of an (n 1)-dimensional subspace
1 n−1
U of Rn. This means that a hyperplane is− defined by a support point
x and (n 1) linearly independent vectors b ,...,b that span the
0 1 n−1
directionsp− ace.InR2,alineisalsoahyperplane.InR3,aplaneisalso
ahyperplane.
F arig eu ar fe fin2 e.1 s3 ubL si pn ae cs es. L = x 0 + λb 1
Vectorsyonaline
x0+λb1lieinan y
affinesubspaceL
withsupportpoint x 0
x0anddirectionb1. b
1
0
Remark(Inhomogeneoussystemsoflinearequationsandaffinesubspaces).
For A Rm×n and x Rm, the solution of the system of linear equa-
tions A∈ λ = x is eithe∈ r the empty set or an affine subspace of Rn of
dimension n rk(A). In particular, the solution of the linear equation
−
λ b +...+λ b = x, where (λ ,...,λ ) = (0,...,0), is a hyperplane
1 1 n n 1 n
inRn. ̸
In Rn, every k-dimensional affine subspace is the solution of an inho-
mogeneous system of linear equations Ax = b, where A Rm×n,b
Rm and rk(A) = n k. Recall that for homogeneous equa∈ tion system∈ s
−
Ax = 0 the solution was a vector subspace, which we can also think of
asaspecialaffinespacewithsupportpointx = 0.
0
♢
2.8.2 Affine Mappings
Similar to linear mappings between vector spaces, which we discussed
in Section 2.7, we can define affine mappings between two affine spaces.
Linearandaffinemappingsarecloselyrelated.Therefore,manyproperties
thatwealreadyknowfromlinearmappings,e.g.,thatthecompositionof
linearmappingsisalinearmapping,alsoholdforaffinemappings.
Definition 2.26 (Affine Mapping). For two vector spaces V,W, a linear
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2.9 FurtherReading 63
mappingΦ : V W,anda W,themapping
→ ∈
ϕ : V W (2.132)
→
x a+Φ(x) (2.133)
(cid:55)→
is an affine mapping from V to W. The vector a is called the translation affinemapping
vectorofϕ. translationvector
Every affine mapping ϕ : V W is also the composition of a linear
→
mapping Φ : V W and a translation τ : W W in W, such that
→ →
ϕ = τ Φ.ThemappingsΦandτ areuniquelydetermined.
◦
The composition ϕ′ ϕ of affine mappings ϕ : V W, ϕ′ : W X is
◦ → →
affine.
Ifϕisbijective,affinemappingskeepthegeometricstructureinvariant.
Theythenalsopreservethedimensionandparallelism.
2.9 Further Reading
There are many resources for learning linear algebra, including the text-
books by Strang (2003), Golan (2007), Axler (2015), and Liesen and
Mehrmann (2015). There are also several online resources that we men-
tionedintheintroductiontothischapter.WeonlycoveredGaussianelim-
ination here, but there are many other approaches for solving systems of
linear equations, and we refer to numerical linear algebra textbooks by
Stoer and Burlirsch (2002), Golub and Van Loan (2012), and Horn and
Johnson(2013)foranin-depthdiscussion.
In this book, we distinguish between the topics of linear algebra (e.g.,
vectors, matrices, linear independence, basis) and topics related to the
geometry of a vector space. In Chapter 3, we will introduce the inner
product,whichinducesanorm.Theseconceptsallowustodefineangles,
lengths and distances, which we will use for orthogonal projections. Pro-
jections turn out to be key in many machine learning algorithms, such as
linearregressionandprincipalcomponentanalysis,bothofwhichwewill
coverinChapters9and10,respectively.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
64 LinearAlgebra
Exercises
2.1 Weconsider(R\{−1},⋆),where
a⋆b:=ab+a+b, a,b∈R\{−1} (2.134)
a. Showthat(R\{−1},⋆)isanAbeliangroup.
b. Solve
3⋆x⋆x=15
intheAbeliangroup(R\{−1},⋆),where⋆isdefinedin(2.134).
2.2 LetnbeinN\{0}.Letk,xbeinZ.Wedefinethecongruenceclassk¯ofthe
integerkastheset
k={x∈Z|x−k=0 (modn)}
={x∈Z|∃a∈Z: (x−k=n·a)}.
We now define Z/nZ (sometimes written Z n) as the set of all congruence
classesmodulon.Euclideandivisionimpliesthatthissetisafinitesetcon-
tainingnelements:
Z n ={0,1,...,n−1}
Foralla,b∈Z n,wedefine
a⊕b:=a+b
a. Showthat(Z n,⊕)isagroup.IsitAbelian?
b. Wenowdefineanotheroperation⊗forallaandbinZ n as
a⊗b=a×b, (2.135)
wherea×brepresentstheusualmultiplicationinZ.
Letn=5.DrawthetimestableoftheelementsofZ \{0}under⊗,i.e.,
5
calculatetheproductsa⊗bforallaandbinZ \{0}.
5
Hence, show that Z \{0} is closed under ⊗ and possesses a neutral
5
element for ⊗. Display the inverse of all elements in Z \{0} under ⊗.
5
Concludethat(Z \{0},⊗)isanAbeliangroup.
5
c. Showthat(Z \{0},⊗)isnotagroup.
8
d. We recall that the B´ezout theorem states that two integers a and b are
relativelyprime(i.e.,gcd(a,b)=1)ifandonlyifthereexisttwointegers
uandv suchthatau+bv =1.Showthat(Z n\{0},⊗)isagroupifand
onlyifn∈N\{0}isprime.
2.3 ConsiderthesetG of3×3matricesdefinedasfollows:
  (cid:12) 
G
= 1
0
x
1
yz ∈R3×3(cid:12) (cid:12) (cid:12)x,y,z∈R
 0 0 1 (cid:12) (cid:12) 
Wedefine·asthestandardmatrixmultiplication.
Is(G,·)agroup?Ifyes,isitAbelian?Justifyyouranswer.
2.4 Computethefollowingmatrixproducts,ifpossible:
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Exercises 65
a.
  
1 2 1 1 0
4 50 1 1
7 8 1 0 1
b.
  
1 2 3 1 1 0
4 5 60 1 1
7 8 9 1 0 1
c.
  
1 1 0 1 2 3
0 1 14 5 6
1 0 1 7 8 9
d.
 
0 3
(cid:20) (cid:21)
1 2 1 2 1 −1
 
4 1 −1 −4 2 1 
5 2
e.
 
0 3
(cid:20) (cid:21)
1 −1 1 2 1 2
 
2 1  4 1 −1 −4
5 2
2.5 Find the set S of all solutions in x of the following inhomogeneous linear
systemsAx=b,whereAandbaredefinedasfollows:
a.
   
1 1 −1 −1 1
2 5 −7 −5 −2
A=  , b= 
2 −1 1 3   4 
5 2 −4 2 6
b.
   
1 −1 0 0 1 3
 1 1 0 −3 0   6 
A=  , b= 
 2 −1 0 1 −1  5 
−1 2 0 −2 −1 −1
2.6 Using Gaussian elimination, find all solutions of the inhomogeneous equa-
tionsystemAx=bwith
   
0 1 0 0 1 0 2
A=0 0 0 1 1 0 , b=−1 .
0 1 0 0 0 1 1
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
66 LinearAlgebra
 
x
1
2.7 Find all solutions in x = x 2 ∈ R3 of the equation system Ax = 12x,
x
3
where
 
6 4 3
A=6 0 9
0 8 0
and(cid:80)3
x =1.
i=1 i
2.8 Determinetheinversesofthefollowingmatricesifpossible:
a.
 
2 3 4
A=3 4 5
4 5 6
b.
 
1 0 1 0
0 1 1 0
A= 
1 1 0 1
1 1 1 0
2.9 WhichofthefollowingsetsaresubspacesofR3?
a. A={(λ,λ+µ3,λ−µ3)|λ,µ∈R}
b. B ={(λ2,−λ2,0)|λ∈R}
c. Letγ beinR.
C ={(ξ ,ξ ,ξ )∈R3 |ξ −2ξ +3ξ =γ}
1 2 3 1 2 3
d. D={(ξ ,ξ ,ξ )∈R3 |ξ ∈Z}
1 2 3 2
2.10 Arethefollowingsetsofvectorslinearlyindependent?
a.
     
2 1 3
x 1 =−1 , x 2 = 1  , x 3 =−3
3 −2 8
b.
     
1 1 1
2 1 0
     
x
1
=1 , x
2
=0 , x
3
=0
     
0 1 1
0 1 1
2.11 Write
 
1
y=−2
5
aslinearcombinationof
     
1 1 2
x
1
=1 , x
2
=2 , x
3
=−1
1 3 1
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Exercises 67
2.12 ConsidertwosubspacesofR4:
           
1 2 −1 −1 2 −3
 1  −1  1  −2 −2  6 
U 1 =span[  ,  , ], U 2 =span[  ,  , ].
−3  0  −1  2   0  −2
1 −1 1 1 0 −1
DetermineabasisofU ∩U .
1 2
2.13 Consider two subspaces U and U , where U is the solution space of the
1 2 1
homogeneousequationsystemA x=0andU isthesolutionspaceofthe
1 2
homogeneousequationsystemA x=0with
2
   
1 0 1 3 −3 0
1 −2 −1 1 2 3
A 1 =  , A 2 =  .
2 1 3  7 −5 2
1 0 1 3 −1 2
a. DeterminethedimensionofU ,U .
1 2
b. DeterminebasesofU andU .
1 2
c. DetermineabasisofU ∩U .
1 2
2.14 ConsidertwosubspacesU andU ,whereU isspannedbythecolumnsof
1 2 1
A andU isspannedbythecolumnsofA with
1 2 2
   
1 0 1 3 −3 0
1 −2 −1 1 2 3
A 1 =  , A 2 =  .
2 1 3  7 −5 2
1 0 1 3 −1 2
a. DeterminethedimensionofU ,U
1 2
b. DeterminebasesofU andU
1 2
c. DetermineabasisofU ∩U
1 2
2.15 LetF ={(x,y,z)∈R3 |x+y−z=0}andG={(a−b,a+b,a−3b)|a,b∈R}.
a. ShowthatF andGaresubspacesofR3.
b. CalculateF ∩Gwithoutresortingtoanybasisvector.
c. FindonebasisforF andoneforG,calculateF∩Gusingthebasisvectors
previouslyfoundandcheckyourresultwiththepreviousquestion.
2.16 Arethefollowingmappingslinear?
a. Leta,b∈R.
Φ:L1([a,b])→R
(cid:90) b
f (cid:55)→Φ(f)= f(x)dx,
a
whereL1([a,b])denotesthesetofintegrablefunctionson[a,b].
b.
Φ:C1 →C0
f (cid:55)→Φ(f)=f′,
where for k ⩾ 1, Ck denotes the set of k times continuously differen-
tiablefunctions,andC0 denotesthesetofcontinuousfunctions.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
68 LinearAlgebra
c.
Φ:R→R
x(cid:55)→Φ(x)=cos(x)
d.
Φ:R3 →R2
(cid:20) (cid:21)
1 2 3
x(cid:55)→ x
1 4 3
e. Letθbein[0,2π[and
Φ:R2 →R2
(cid:20) (cid:21)
cos(θ) sin(θ)
x(cid:55)→ x
−sin(θ) cos(θ)
2.17 Considerthelinearmapping
Φ:R3 →R4
 
 x  3x 1+2x 2+x 3
Φx x1 2= 

x 1 x+ 1−x 2 3+
x
2x 3  

3 2x +3x +x
1 2 3
FindthetransformationmatrixA .
Φ
Determinerk(A ).
Φ
ComputethekernelandimageofΦ.Whataredim(ker(Φ))anddim(Im(Φ))?
2.18 LetE beavectorspace.Letf andg betwoautomorphismsonE suchthat
f ◦g = id (i.e., f ◦g is the identity mapping id ). Show that ker(f) =
E E
ker(g◦f),Im(g)=Im(g◦f)andthatker(f)∩Im(g)={0 }.
E
2.19 Consider an endomorphism Φ : R3 → R3 whose transformation matrix
(withrespecttothestandardbasisinR3)is
 
1 1 0
A
Φ
=1 −1 0 .
1 1 1
a. Determineker(Φ)andIm(Φ).
b. DeterminethetransformationmatrixA˜ withrespecttothebasis
Φ
     
1 1 1
B =(1, 2, 0),
1 1 0
i.e.,performabasischangetowardthenewbasisB.
2.20 Letusconsiderb ,b ,b′,b′,4vectorsofR2expressedinthestandardbasis
1 2 1 2
ofR2 as
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
2 −1 2 1
b = , b = , b′ = , b′ =
1 1 2 −1 1 −2 2 1
andletusdefinetwoorderedbasesB =(b ,b )andB′ =(b′,b′)ofR2.
1 2 1 2
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Exercises 69
a. ShowthatB andB′ aretwobasesofR2 anddrawthosebasisvectors.
b. ComputethematrixP thatperformsabasischangefromB′ toB.
1
c. Weconsiderc ,c ,c ,threevectorsofR3 definedinthestandardbasis
1 2 3
ofR3 as
     
1 0 1
c 1 = 2 , c 2 =−1, c 3 = 0 
−1 2 −1
andwedefineC =(c ,c ,c ).
1 2 3
(i) Show that C is a basis of R3, e.g., by using determinants (see
Section4.1).
(ii) Let us call C′ = (c′,c′,c′) the standard basis of R3. Determine
1 2 3
thematrixP thatperformsthebasischangefromC toC′.
2
d. WeconsiderahomomorphismΦ:R2 −→R3,suchthat
Φ(b +b ) = c +c
1 2 2 3
Φ(b −b ) = 2c −c +3c
1 2 1 2 3
whereB =(b ,b )andC =(c ,c ,c )areorderedbasesofR2andR3,
1 2 1 2 3
respectively.
Determine the transformation matrix A of Φ with respect to the or-
Φ
deredbasesB andC.
e. DetermineA′,thetransformationmatrixofΦwithrespecttothebases
B′ andC′.
f. Let us consider the vector x ∈ R2 whose coordinates in B′ are [2,3]⊤.
Inotherwords,x=2b′ +3b′.
1 2
(i) CalculatethecoordinatesofxinB.
(ii) Basedonthat,computethecoordinatesofΦ(x)expressedinC.
(iii) Then,writeΦ(x)intermsofc′,c′,c′.
1 2 3
(iv) Use the representation of x in B′ and the matrix A′ to find this
resultdirectly.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
3
Analytic Geometry
In Chapter 2, we studied vectors, vector spaces, and linear mappings at
a general but abstract level. In this chapter, we will add some geomet-
ric interpretation and intuition to all of these concepts. In particular, we
will look at geometric vectors and compute their lengths and distances
or angles between two vectors. To be able to do this, we equip the vec-
tor space with an inner product that induces the geometry of the vector
space.Innerproductsandtheircorrespondingnormsandmetricscapture
the intuitive notions of similarity and distances, which we use to develop
the support vector machine in Chapter 12. We will then use the concepts
of lengths and angles between vectors to discuss orthogonal projections,
whichwillplayacentralrolewhenwediscussprincipalcomponentanal-
ysis in Chapter 10 and regression via maximum likelihood estimation in
Chapter 9. Figure 3.1 gives an overview of how concepts in this chapter
arerelatedandhowtheyareconnectedtootherchaptersofthebook.
Figure3.1 Amind
Innerproduct
mapoftheconcepts
i cn ht ar po td eu r,ce ad loi nn gt whi is
th in d
uces
whentheyareused
inotherpartsofthe
Chapter12
book. Norm
Classification
Orthogonal
Lengths Angles Rotations
projection
Chapter9 Chapter4 Chapter10
Regression Matrix Dimensionality
decomposition reduction
70
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
3.1 Norms 71
(cid:107)x(cid:107) =1 (cid:107)x(cid:107) =1 Figure3.3 For
1 1 1 2
differentnorms,the
redlinesindicate
thesetofvectors
withnorm1.Left:
1 1
Manhattannorm;
Right:Euclidean
distance.
3.1 Norms
Whenwethinkofgeometricvectors,i.e.,directedlinesegmentsthatstart
at the origin, then intuitively the length of a vector is the distance of the
“end” of this directed line segment from the origin. In the following, we
willdiscussthenotionofthelengthofvectorsusingtheconceptofanorm.
Definition 3.1 (Norm). AnormonavectorspaceV isafunction norm
: V R, (3.1)
∥·∥ →
x x , (3.2)
(cid:55)→ ∥ ∥
which assigns each vector x its length x R, such that for all λ R length
∥ ∥ ∈ ∈
andx,y V thefollowinghold:
∈
absolutely
Absolutelyhomogeneous: λx = λ x homogeneous
∥ ∥ | |∥ ∥
Triangleinequality: x+y ⩽ x + y triangleinequality
∥ ∥ ∥ ∥ ∥ ∥
Positivedefinite: x ⩾ 0and x = 0 x = 0 positivedefinite
∥ ∥ ∥ ∥ ⇐⇒
Figure3.2 Triangle
In geometric terms, the triangle inequality states that for any triangle,
inequality.
the sum of the lengths of any two sides must be greater than or equal
to the length of the remaining side; see Figure 3.2 for an illustration. a b
Definition 3.1 is in terms of a general vector space V (Section 2.4), but c≤a+b
in this book we will only consider a finite-dimensional vector space Rn.
Recallthatforavectorx Rn wedenotetheelementsofthevectorusing
∈
asubscript,thatis,x istheith elementofthevectorx.
i
Example 3.1 (Manhattan Norm)
The ManhattannormonRn isdefinedforx Rn as Manhattannorm
∈
n
(cid:88)
x := x , (3.3)
1 i
∥ ∥ | |
i=1
where is the absolute value. The left panel of Figure 3.3 shows all
vectors| x· |
∈
R2 with ∥x ∥1 = 1. The Manhattan norm is also called ℓ 1 ℓ1norm
norm.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
72 AnalyticGeometry
Example 3.2 (Euclidean Norm)
Euclideannorm The Euclideannormofx Rn isdefinedas
∈
(cid:118)
(cid:117) n
(cid:117)(cid:88)
x := (cid:116) x2 = √x⊤x (3.4)
∥ ∥2 i
i=1
Euclideandistance andcomputesthe Euclideandistanceofxfromtheorigin.Therightpanel
of Figure 3.3 shows all vectors x R2 with x = 1. The Euclidean
2
∈ ∥ ∥
ℓ2norm normisalsocalled ℓ
2
norm.
Remark. Throughout this book, we will use the Euclidean norm (3.4) by
defaultifnotstatedotherwise.
♢
3.2 Inner Products
Inner products allow for the introduction of intuitive geometrical con-
cepts, such as the length of a vector and the angle or distance between
two vectors. A major purpose of inner products is to determine whether
vectorsareorthogonaltoeachother.
3.2.1 Dot Product
We may already be familiar with a particular type of inner product, the
scalarproduct
scalarproduct/dotproductinRn,whichisgivenby
dotproduct n
(cid:88)
x⊤y = x y . (3.5)
i i
i=1
We will refer to this particular inner product as the dot product in this
book. However, inner products are more general concepts with specific
properties,whichwewillnowintroduce.
3.2.2 General Inner Products
Recall the linear mapping from Section 2.7, where we can rearrange the
bilinearmapping mapping with respect to addition and multiplication with a scalar. A bi-
linear mapping Ω is a mapping with two arguments, and it is linear in
each argument, i.e., when we look at a vector space V then it holds that
forallx,y,z V, λ,ψ Rthat
∈ ∈
Ω(λx+ψy,z) = λΩ(x,z)+ψΩ(y,z) (3.6)
Ω(x,λy+ψz) = λΩ(x,y)+ψΩ(x,z). (3.7)
Here,(3.6)assertsthatΩislinearinthefirstargument,and(3.7)asserts
thatΩislinearinthesecondargument(seealso(2.87)).
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
3.2 InnerProducts 73
Definition3.2. LetV beavectorspaceandΩ : V V Rbeabilinear
× →
mappingthattakestwovectorsandmapsthemontoarealnumber.Then
Ω is called symmetric if Ω(x,y) = Ω(y,x) for all x,y V, i.e., the symmetric
∈
orderoftheargumentsdoesnotmatter.
Ωiscalledpositivedefiniteif positivedefinite
x V 0 : Ω(x,x) > 0, Ω(0,0) = 0. (3.8)
∀ ∈ \{ }
Definition3.3. LetV beavectorspaceandΩ : V V Rbeabilinear
× →
mappingthattakestwovectorsandmapsthemontoarealnumber.Then
Apositivedefinite,symmetricbilinearmappingΩ : V V Riscalled
× →
aninnerproductonV.Wetypicallywrite x,y insteadofΩ(x,y). innerproduct
⟨ ⟩
The pair (V, , ) is called an inner product space or (real) vector space innerproductspace
⟨· ·⟩
with inner product. If we use the dot product defined in (3.5), we call vectorspacewith
(V, , )aEuclideanvectorspace. innerproduct
⟨· ·⟩
Euclideanvector
Wewillrefertothesespacesasinnerproductspacesinthisbook. space
Example 3.3 (Inner Product That Is Not the Dot Product)
ConsiderV = R2.Ifwedefine
x,y := x y (x y +x y )+2x y (3.9)
1 1 1 2 2 1 2 2
⟨ ⟩ −
then , isaninnerproductbutdifferentfromthedotproduct.Theproof
⟨· ·⟩
willbeanexercise.
3.2.3 Symmetric, Positive Definite Matrices
Symmetric, positive definite matrices play an important role in machine
learning, and they are defined via the inner product. In Section 4.3, we
willreturntosymmetric,positivedefinitematricesinthecontextofmatrix
decompositions. The idea of symmetric positive semidefinite matrices is
keyinthedefinitionofkernels(Section12.4).
Considerann-dimensionalvectorspaceV withaninnerproduct , :
V V R(seeDefinition3.3)andanorderedbasisB = (b ,...,b⟨· )·⟩ of
1 n
× →
V. Recall from Section 2.6.1 that any vectors x,y V can be written as
linear combinations of the basis vectors so that x
=∈ (cid:80)n
ψ b V and
y = (cid:80)n λ b V for suitable ψ ,λ R. Due to thei b= i1 lini eai ri∈ ty of the
j=1 j j ∈ i j ∈
innerproduct,itholdsforallx,y V that
∈
(cid:42) (cid:43)
n n n n
(cid:88) (cid:88) (cid:88)(cid:88)
x,y = ψ b , λ b = ψ b ,b λ = xˆ⊤Ayˆ, (3.10)
i i j j i i j j
⟨ ⟩ ⟨ ⟩
i=1 j=1 i=1 j=1
whereA := b ,b andxˆ,yˆ arethecoordinatesofxandywithrespect
ij i j
⟨ ⟩
to the basis B. This implies that the inner product , is uniquely deter-
⟨· ·⟩
mined through A. The symmetry of the inner product also means that A
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
74 AnalyticGeometry
is symmetric. Furthermore, the positive definiteness of the inner product
impliesthat
x V 0 : x⊤Ax > 0. (3.11)
∀ ∈ \{ }
Definition3.4(Symmetric,PositiveDefiniteMatrix). Asymmetricmatrix
symmetric,positive A Rn×n that satisfies (3.11) is called symmetric, positive definite, or
definite just∈ positivedefinite.Ifonly⩾holdsin(3.11),thenAiscalledsymmetric,
positivedefinite positivesemidefinite.
symmetric,positive
semidefinite
Example 3.4 (Symmetric, Positive Definite Matrices)
Considerthematrices
(cid:20) (cid:21) (cid:20) (cid:21)
9 6 9 6
A = , A = . (3.12)
1 6 5 2 6 3
A ispositivedefinitebecauseitissymmetricand
1
(cid:20) (cid:21)(cid:20) (cid:21)
x⊤A x = (cid:2) x x (cid:3) 9 6 x 1 (3.13a)
1 1 2 6 5 x
2
= 9x2+12x x +5x2 = (3x +2x )2+x2 > 0 (3.13b)
1 1 2 2 1 2 2
for all x V 0 . In contrast, A is symmetric but not positive definite
2
∈ \{ }
because x⊤A x = 9x2 +12x x +3x2 = (3x +2x )2 x2 can be less
2 1 1 2 2 1 2 − 2
than0,e.g.,forx = [2, 3]⊤.
−
IfA Rn×n issymmetric,positivedefinite,then
∈
x,y = xˆ⊤Ayˆ (3.14)
⟨ ⟩
definesaninnerproductwithrespecttoanorderedbasisB,wherexˆ and
yˆ arethecoordinaterepresentationsofx,y V withrespecttoB.
∈
Theorem 3.5. For a real-valued, finite-dimensional vector space V and an
orderedbasisB ofV,itholdsthat , : V V Risaninnerproductif
andonlyifthereexistsasymmetric⟨ ,· po·⟩ sitive× defin→ itematrixA Rn×n with
∈
x,y = xˆ⊤Ayˆ. (3.15)
⟨ ⟩
The following properties hold if A Rn×n is symmetric and positive
∈
definite:
The null space (kernel) of A consists only of 0 because x⊤Ax > 0 for
allx = 0.ThisimpliesthatAx = 0ifx = 0.
̸ ̸ ̸
The diagonal elements a of A are positive because a = e⊤Ae > 0,
ii ii i i
wheree istheithvectorofthestandardbasisinRn.
i
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
3.3 LengthsandDistances 75
3.3 Lengths and Distances
In Section 3.1, we already discussed norms that we can use to compute
thelengthofavector.Innerproductsandnormsarecloselyrelatedinthe
sensethatanyinnerproductinducesanorm Innerproducts
(cid:113) inducenorms.
x := x,x (3.16)
∥ ∥ ⟨ ⟩
inanaturalway,suchthatwecancomputelengthsofvectorsusingthein-
nerproduct.However,noteverynormisinducedbyaninnerproduct.The
Manhattan norm (3.3) is an example of a norm without a corresponding
inner product. In the following, we will focus on norms that are induced
byinnerproductsandintroducegeometricconcepts,suchaslengths,dis-
tances,andangles.
Remark (Cauchy-Schwarz Inequality). For an inner product vector space
(V, , )theinducednorm satisfiestheCauchy-Schwarzinequality Cauchy-Schwarz
⟨· ·⟩ ∥·∥
inequality
x,y ⩽ x y . (3.17)
|⟨ ⟩| ∥ ∥∥ ∥
♢
Example 3.5 (Lengths of Vectors Using Inner Products)
Ingeometry,weareofteninterestedinlengthsofvectors.Wecannowuse
aninnerproducttocomputethemusing(3.16).Letustakex = [1,1]⊤
R2.Ifweusethedotproductastheinnerproduct,with(3.16)weobtai∈
n
x = √x⊤x = √12+12 = √2 (3.18)
∥ ∥
asthelengthofx.Letusnowchooseadifferentinnerproduct:
(cid:20)
1
1(cid:21)
1
x,y := x⊤ −2 y = x y (x y +x y )+x y . (3.19)
⟨ ⟩ 1 1 1 1 − 2 1 2 2 1 2 2
−2
Ifwecomputethenormofavector,thenthisinnerproductreturnssmaller
values than the dot product if x and x have the same sign (and x x >
1 2 1 2
0); otherwise, it returns greater values than the dot product. With this
innerproduct,weobtain
x,x = x2 x x +x2 = 1 1+1 = 1 = x = √1 = 1, (3.20)
⟨ ⟩ 1− 1 2 2 − ⇒ ∥ ∥
suchthatxis“shorter”withthisinnerproductthanwiththedotproduct.
Definition 3.6 (Distance and Metric). Consider an inner product space
(V, , ).Then
⟨· ·⟩
(cid:113)
d(x,y) := x y = x y,x y (3.21)
∥ − ∥ ⟨ − − ⟩
is called the distance between x and y for x,y V. If we use the dot distance
∈
productastheinnerproduct,thenthedistanceiscalledEuclideandistance. Euclideandistance
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
76 AnalyticGeometry
Themapping
d : V V R (3.22)
× →
(x,y) d(x,y) (3.23)
(cid:55)→
metric iscalledametric.
Remark. Similar to the length of a vector, the distance between vectors
doesnotrequireaninnerproduct:anormissufficient.Ifwehaveanorm
induced by an inner product, the distance may vary depending on the
choiceoftheinnerproduct.
♢
Ametricdsatisfiesthefollowing:
positivedefinite 1. d is positive definite, i.e., d(x,y) ⩾ 0 for all x,y V and d(x,y) =
∈
0 x = y.
⇐⇒
symmetric 2. dissymmetric,i.e.,d(x,y) = d(y,x)forallx,y V.
∈
triangleinequality 3. Triangleinequality:d(x,z) ⩽ d(x,y)+d(y,z)forallx,y,z V.
∈
Remark. At first glance, the lists of properties of inner products and met-
rics look very similar. However, by comparing Definition 3.3 with Defini-
tion3.6weobservethat x,y andd(x,y)behaveinoppositedirections.
⟨ ⟩
Verysimilarxandy willresultinalargevaluefortheinnerproductand
asmallvalueforthemetric.
♢
3.4 Angles and Orthogonality
Figure3.4 When
restrictedto[0,π] In addition to enabling the definition of lengths of vectors, as well as the
thenf(ω)=cos(ω) distance between two vectors, inner products also capture the geometry
returnsaunique of a vector space by defining the angle ω between two vectors. We use
numberinthe
the Cauchy-Schwarz inequality (3.17) to define angles ω in inner prod-
interval[−1,1].
uct spaces between two vectors x,y, and this notion coincides with our
1 intuitioninR2 andR3.Assumethatx = 0,y = 0.Then
̸ ̸
0 x,y
1 ⩽ ⟨ ⟩ ⩽ 1. (3.24)
− x y
1 ∥ ∥∥ ∥
− 0 π/2 π
ω Therefore,thereexistsauniqueω [0,π],illustratedinFigure3.4,with
∈
x,y
cosω = ⟨ ⟩ . (3.25)
x y
∥ ∥∥ ∥
angle The number ω is the angle between the vectors x and y. Intuitively, the
angle between two vectors tells us how similar their orientations are. For
example, using the dot product, the angle between x and y = 4x, i.e., y
isascaledversionofx,is0:Theirorientationisthesame.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
)ω(soc
3.4 AnglesandOrthogonality 77
Example 3.6 (Angle between Vectors)
Letuscomputetheanglebetweenx = [1,1]⊤ R2 andy = [1,2]⊤ R2; Figure3.5 The
∈ ∈
see Figure 3.5, where we use the dot product as the inner product. Then angleωbetween
weget twovectorsx,yis
computedusingthe
x,y x⊤y 3 innerproduct.
cosω = ⟨ ⟩ = = , (3.26)
(cid:112) (cid:112)
x,x y,y x⊤xy⊤y √10
⟨ ⟩⟨ ⟩ y
and the angle between the two vectors is arccos(√3 ) 0.32rad, which
10 ≈
correspondstoabout18◦.
1
ω x
Akeyfeatureoftheinnerproductisthatitalsoallowsustocharacterize
vectorsthatareorthogonal.
0 1
Definition3.7(Orthogonality). Twovectorsxandyareorthogonalifand orthogonal
only if x,y = 0, and we write x y. If additionally x = 1 = y ,
⟨ ⟩ ⊥ ∥ ∥ ∥ ∥
i.e.,thevectorsareunitvectors,thenxandy areorthonormal. orthonormal
An implication of this definition is that the 0-vector is orthogonal to
everyvectorinthevectorspace.
Remark. Orthogonality is the generalization of the concept of perpendic-
ularity to bilinear forms that do not have to be the dot product. In our
context, geometrically, we can think of orthogonal vectors as having a
rightanglewithrespecttoaspecificinnerproduct.
♢
Example 3.7 (Orthogonal Vectors)
Figure3.6 The
1 angleωbetween
y x twovectorsx,ycan
changedepending
ω
ontheinner
product.
−1 0 1
Consider two vectors x = [1,1]⊤,y = [ 1,1]⊤ R2; see Figure 3.6.
− ∈
We are interested in determining the angle ω between them using two
differentinnerproducts.Usingthedotproductastheinnerproductyields
an angle ω between x and y of 90◦, such that x y. However, if we
⊥
choosetheinnerproduct
(cid:20) (cid:21)
2 0
x,y = x⊤ y, (3.27)
⟨ ⟩ 0 1
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
78 AnalyticGeometry
wegetthattheangleω betweenxandy isgivenby
x,y 1
cosω = ⟨ ⟩ = = ω 1.91rad 109.5◦, (3.28)
x y −3 ⇒ ≈ ≈
∥ ∥∥ ∥
and x and y are not orthogonal. Therefore, vectors that are orthogonal
with respect to one inner product do not have to be orthogonal with re-
specttoadifferentinnerproduct.
Definition 3.8 (Orthogonal Matrix). A square matrix A Rn×n is an
∈
orthogonalmatrix orthogonalmatrix ifandonlyifitscolumnsareorthonormalsothat
AA⊤ = I = A⊤A, (3.29)
whichimpliesthat
A−1 = A⊤, (3.30)
Itisconventionto i.e.,theinverseisobtainedbysimplytransposingthematrix.
callthesematrices
Transformations by orthogonal matrices are special because the length
“orthogonal”buta
moreprecise of a vector x is not changed when transforming it using an orthogonal
descriptionwould matrixA.Forthedotproduct,weobtain
be“orthonormal”.
Transformations Ax 2 = (Ax)⊤(Ax) = x⊤A⊤Ax = x⊤Ix = x⊤x = x 2 . (3.31)
∥ ∥ ∥ ∥
withorthogonal
matricespreserve Moreover, the angle between any two vectors x,y, as measured by their
distancesand inner product, is also unchanged when transforming both of them using
angles. an orthogonal matrix A. Assuming the dot product as the inner product,
theangleoftheimagesAxandAy isgivenas
(Ax)⊤(Ay) x⊤A⊤Ay x⊤y
cosω = = = , (3.32)
(cid:113)
∥Ax ∥∥Ay
∥
x⊤A⊤Axy⊤A⊤Ay ∥x ∥∥y
∥
which gives exactly the angle between x and y. This means that orthog-
onal matrices A with A⊤ = A−1 preserve both angles and distances. It
turns out that orthogonal matrices define transformations that are rota-
tions (with the possibility of flips). In Section 3.9, we will discuss more
detailsaboutrotations.
3.5 Orthonormal Basis
In Section 2.6.1, we characterized properties of basis vectors and found
that in an n-dimensional vector space, we need n basis vectors, i.e., n
vectors that are linearly independent. In Sections 3.3 and 3.4, we used
inner products to compute the length of vectors and the angle between
vectors. In the following, we will discuss the special case where the basis
vectors are orthogonal to each other and where the length of each basis
vectoris1.Wewillcallthisbasisthenanorthonormalbasis.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
3.6 OrthogonalComplement 79
Letusintroducethismoreformally.
Definition 3.9 (Orthonormal Basis). Consider an n-dimensional vector
spaceV andabasis b ,...,b ofV.If
1 n
{ }
b ,b = 0 fori = j (3.33)
i j
⟨ ⟩ ̸
b ,b = 1 (3.34)
i i
⟨ ⟩
for all i,j = 1,...,n then the basis is called an orthonormal basis (ONB). orthonormalbasis
Ifonly(3.33)issatisfied,thenthebasisiscalledanorthogonalbasis.Note ONB
that(3.34)impliesthateverybasisvectorhaslength/norm1. orthogonalbasis
RecallfromSection2.6.1thatwecanuseGaussianeliminationtofinda
basis for a vector space spanned by a set of vectors. Assume we are given
a set b˜ ,...,b˜ of non-orthogonal and unnormalized basis vectors. We
1 n
conca{ tenatethem} intoamatrixB˜ = [b˜ ,...,b˜ ]andapplyGaussianelim-
1 n
ination to the augmented matrix (Section 2.3.2) [B˜B˜⊤ B˜] to obtain an
|
orthonormalbasis.Thisconstructivewaytoiterativelybuildanorthonor-
malbasis b ,...,b iscalledtheGram-Schmidtprocess(Strang,2003).
1 n
{ }
Example 3.8 (Orthonormal Basis)
The canonical/standard basis for a Euclidean vector space Rn is an or-
thonormalbasis,wheretheinnerproductisthedotproductofvectors.
InR2,thevectors
(cid:20) (cid:21) (cid:20) (cid:21)
1 1 1 1
b = , b = (3.35)
1 √2 1 2 √2 1
−
formanorthonormalbasissinceb⊤b = 0and b = 1 = b .
1 2 ∥ 1 ∥ ∥ 2 ∥
We will exploit the concept of an orthonormal basis in Chapter 12 and
Chapter 10 when we discuss support vector machines and principal com-
ponentanalysis.
3.6 Orthogonal Complement
Having defined orthogonality, we will now look at vector spaces that are
orthogonal to each other. This will play an important role in Chapter 10,
when we discuss linear dimensionality reduction from a geometric per-
spective.
Consider a D-dimensional vector space V and an M-dimensional sub-
spaceU V.ThenitsorthogonalcomplementU⊥isa(D M)-dimensional orthogonal
⊆ −
subspace of V and contains all vectors in V that are orthogonal to every complement
vectorinU.Furthermore,U U⊥ = 0 sothatanyvectorx V canbe
∩ { } ∈
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
80 AnalyticGeometry
Figure3.7 Aplane
e
U ina 3
three-dimensional
vectorspacecanbe w
describedbyits
normalvector,
e
whichspansits 2
orthogonal
complementU⊥.
e
1
U
uniquelydecomposedinto
M D−M
(cid:88) (cid:88)
x = λ b + ψ b⊥, λ , ψ R, (3.36)
m m j j m j ∈
m=1 j=1
where(b ,...,b )isabasisofU and(b⊥,...,b⊥ )isabasisofU⊥.
1 M 1 D−M
Therefore, the orthogonal complement can also be used to describe a
planeU (two-dimensionalsubspace)inathree-dimensionalvectorspace.
More specifically, the vector w with w = 1, which is orthogonal to the
∥ ∥
plane U, is the basis vector of U⊥. Figure 3.7 illustrates this setting. All
vectors that are orthogonal to w must (by construction) lie in the plane
normalvector U.Thevectorw iscalledthenormalvectorofU.
Generally,orthogonalcomplementscanbeusedtodescribehyperplanes
inn-dimensionalvectorandaffinespaces.
3.7 Inner Product of Functions
Thus far, we looked at properties of inner products to compute lengths,
angles and distances. We focused on inner products of finite-dimensional
vectors. In the following, we will look at an example of inner products of
adifferenttypeofvectors:innerproductsoffunctions.
The inner products we discussed so far were defined for vectors with a
finite number of entries. We can think of a vector x Rn as a function
∈
withnfunctionvalues.Theconceptofaninnerproductcanbegeneralized
tovectorswith aninfinitenumberofentries(countably infinite)andalso
continuous-valued functions (uncountably infinite). Then the sum over
individual components of vectors (see Equation (3.5) for example) turns
intoanintegral.
An inner product of two functions u : R R and v : R R can be
→ →
definedasthedefiniteintegral
(cid:90) b
u,v := u(x)v(x)dx (3.37)
⟨ ⟩
a
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
3.8 OrthogonalProjections 81
forlowerandupperlimitsa,b < ,respectively.Aswithourusualinner
∞
product, we can define norms and orthogonality by looking at the inner
product.If(3.37)evaluatesto0,thefunctionsuandv areorthogonal.To
maketheprecedinginnerproductmathematicallyprecise,weneedtotake
careofmeasuresandthedefinitionofintegrals,leadingtothedefinitionof
aHilbertspace.Furthermore,unlikeinnerproductsonfinite-dimensional
vectors,innerproductsonfunctionsmaydiverge(haveinfinitevalue).All
thisrequiresdivingintosomemoreintricatedetailsofrealandfunctional
analysis,whichwedonotcoverinthisbook.
Example 3.9 (Inner Product of Functions)
If we choose u = sin(x) and v = cos(x), the integrand f(x) = u(x)v(x) Figure3.8 f(x)=
of (3.37), is shown in Figure 3.8. We see that this function is odd, i.e., sin(x)cos(x).
f( x) = f(x).Therefore,theintegralwithlimitsa = π,b = π ofthis
− − − 0.5
productevaluatesto0.Therefore,sinandcosareorthogonalfunctions.
0.0
Remark. Italsoholdsthatthecollectionoffunctions
0.5
− 2.5 0.0 2.5
1,cos(x),cos(2x),cos(3x),... (3.38) − x
{ }
is orthogonal if we integrate from π to π, i.e., any pair of functions are
−
orthogonal to each other. The collection of functions in (3.38) spans a
largesubspaceofthefunctionsthatareevenandperiodicon[ π,π),and
−
projecting functions onto this subspace is the fundamental idea behind
Fourierseries.
♢
InSection6.4.6,wewillhavealookatasecondtypeofunconventional
innerproducts:theinnerproductofrandomvariables.
3.8 Orthogonal Projections
Projectionsareanimportantclassoflineartransformations(besidesrota-
tions and reflections) and play an important role in graphics, coding the-
ory, statistics and machine learning. In machine learning, we often deal
with data that is high-dimensional. High-dimensional data is often hard
to analyze or visualize. However, high-dimensional data quite often pos-
sessesthepropertythatonlyafewdimensionscontainmostinformation,
and most other dimensions are not essential to describe key properties
of the data. When we compress or visualize high-dimensional data, we
will lose information. To minimize this compression loss, we ideally find
the most informative dimensions in the data. As discussed in Chapter 1, “Feature”isa
data can be represented as vectors, and in this chapter, we will discuss commonexpression
fordata
someofthefundamentaltoolsfordatacompression.Morespecifically,we
representation.
can project the original high-dimensional data onto a lower-dimensional
feature space and work in this lower-dimensional space to learn more
about the dataset and extract relevant patterns. For example, machine
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
)x(soc)x(nis
82 AnalyticGeometry
Figure3.9
Orthogonal
2
projection(orange
dots)ofa
1
two-dimensional
dataset(bluedots)
0
ontoa
one-dimensional
1
subspace(straight −
line). 2
−
4 2 0 2 4
− − x
1
learningalgorithms,suchasprincipalcomponentanalysis(PCA)byPear-
son (1901) and Hotelling (1933) and deep neural networks (e.g., deep
auto-encoders(Dengetal.,2010)),heavilyexploittheideaofdimension-
ality reduction. In the following, we will focus on orthogonal projections,
which we will use in Chapter 10 for linear dimensionality reduction and
in Chapter 12 for classification. Even linear regression, which we discuss
inChapter9,canbeinterpretedusingorthogonalprojections.Foragiven
lower-dimensional subspace, orthogonal projections of high-dimensional
dataretainasmuchinformationaspossibleandminimizethedifference/
error between the original data and the corresponding projection. An il-
lustration of such an orthogonal projection is given in Figure 3.9. Before
we detail how to obtain these projections, let us define what a projection
actuallyis.
Definition 3.10 (Projection). Let V be a vector space and U V a
⊆
projection subspace of V. A linear mapping π : V U is called a projection if
→
π2 = π π = π.
◦
Sincelinearmappingscanbeexpressedbytransformationmatrices(see
Section 2.7), the preceding definition applies equally to a special kind
projectionmatrix of transformation matrices, the projection matrices P π, which exhibit the
propertythatP2 = P .
π π
Inthefollowing,wewillderiveorthogonalprojectionsofvectorsinthe
inner product space (Rn, , ) onto subspaces. We will start with one-
⟨· ·⟩
line dimensional subspaces, which are also called lines. If not mentioned oth-
erwise,weassumethedotproduct x,y = x⊤y astheinnerproduct.
⟨ ⟩
3.8.1 Projection onto One-Dimensional Subspaces (Lines)
Assume we are given a line (one-dimensional subspace) through the ori-
gin with basis vector b Rn. The line is a one-dimensional subspace
U Rn spanned by b. W∈ hen we project x Rn onto U, we seek the
⊆ ∈
vector π (x) U that is closest to x. Using geometric arguments, let
U
∈
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
x 2
3.8 OrthogonalProjections 83
Figure3.10
x Examplesof
projectionsonto
one-dimensional
subspaces.
b x
π (x)
U
ω sinω
ω cosω b
(a)Projectionofx ∈ R2 ontoasubspaceU (b)Projectionofatwo-dimensionalvector
withbasisvectorb. x with ∥x∥ = 1 onto a one-dimensional
subspacespannedbyb.
us characterize some properties of the projection π (x) (Figure 3.10(a)
U
servesasanillustration):
The projection π (x) is closest to x, where “closest” implies that the
U
distance x π (x) isminimal.Itfollowsthatthesegmentπ (x) x
U U
∥ − ∥ −
fromπ (x)toxisorthogonaltoU,andthereforethebasisvectorbof
U
U. The orthogonality condition yields π (x) x,b = 0 since angles
U
⟨ − ⟩
betweenvectorsaredefinedviatheinnerproduct.
λisthenthe
Theprojectionπ U(x)ofxontoU mustbeanelementofU and,there- coordinateofπU(x)
fore,amultipleofthebasisvectorbthatspansU.Hence,π (x) = λb, withrespecttob.
U
forsomeλ R.
∈
Inthefollowingthreesteps,wedeterminethecoordinateλ,theprojection
π (x) U,andtheprojectionmatrixP thatmapsanyx Rn ontoU:
U π
∈ ∈
1. Findingthecoordinateλ.Theorthogonalityconditionyields
x π (x),b = 0
πU(x)=λb
x λb,b = 0. (3.39)
U
⟨ − ⟩ ⇐⇒ ⟨ − ⟩
Wecannowexploitthebilinearityoftheinnerproductandarriveat Withageneralinner
product,weget
x,b b,x λ=⟨x,b⟩if
x,b λ b,b = 0 λ = ⟨ ⟩ = ⟨ ⟩ . (3.40)
⟨ ⟩− ⟨ ⟩ ⇐⇒ b,b b 2 ∥b∥=1.
⟨ ⟩ ∥ ∥
In the last step, we exploited the fact that inner products are symmet-
ric.Ifwechoose , tobethedotproduct,weobtain
⟨· ·⟩
b⊤x b⊤x
λ = = . (3.41)
b⊤b b 2
∥ ∥
If b = 1,thenthecoordinateλoftheprojectionisgivenbyb⊤x.
∥ ∥
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
84 AnalyticGeometry
2. Findingtheprojectionpointπ (x) U.Sinceπ (x) = λb,weimme-
U U
∈
diatelyobtainwith(3.40)that
x,b b⊤x
π (x) = λb = ⟨ ⟩b = b, (3.42)
U b 2 b 2
∥ ∥ ∥ ∥
where the last equality holds for the dot product only. We can also
computethelengthofπ (x)bymeansofDefinition3.1as
U
π (x) = λb = λ b . (3.43)
U
∥ ∥ ∥ ∥ | |∥ ∥
Hence, our projection is of length λ times the length of b. This also
| |
addstheintuitionthatλisthecoordinateofπ (x)withrespecttothe
U
basisvectorbthatspansourone-dimensionalsubspaceU.
Ifweusethedotproductasaninnerproduct,weget
b⊤x b
π (x) (3 =.42) | | b (3 =.25) cosω x b ∥ ∥ = cosω x .
∥ U ∥ b 2 ∥ ∥ | |∥ ∥∥ ∥ b 2 | |∥ ∥
∥ ∥ ∥ ∥
(3.44)
Here,ω istheanglebetweenxandb.Thisequationshouldbefamiliar
fromtrigonometry:If x = 1,thenxliesontheunitcircle.Itfollows
∥ ∥
Thehorizontalaxis that the projection onto the horizontal axis spanned by b is exactly
isaone-dimensional cosω, and the length of the corresponding vector π (x) = cosω . An
U
subspace. | |
illustrationisgiveninFigure3.10(b).
3. Finding the projection matrix P . We know that a projection is a lin-
π
earmapping(seeDefinition3.10).Therefore,thereexistsaprojection
matrix P , such that π (x) = P x. With the dot product as inner
π U π
productand
b⊤x bb⊤
π (x) = λb = bλ = b = x, (3.45)
U b 2 b 2
∥ ∥ ∥ ∥
weimmediatelyseethat
bb⊤
P = . (3.46)
π b 2
∥ ∥
Projectionmatrices Note that bb⊤ (and, consequently, P π) is a symmetric matrix (of rank
arealways 1),and b 2 = b,b isascalar.
symmetric. ∥ ∥ ⟨ ⟩
TheprojectionmatrixP projectsanyvectorx Rnontothelinethrough
π
∈
theoriginwithdirectionb(equivalently,thesubspaceU spannedbyb).
Remark. The projection π (x) Rn is still an n-dimensional vector and
U
∈
notascalar.However,wenolongerrequirencoordinatestorepresentthe
projection, but only a single one if we want to express it with respect to
thebasisvectorbthatspansthesubspaceU:λ.
♢
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
3.8 OrthogonalProjections 85
x Figure3.11
Projectionontoa
two-dimensional
subspaceU with
basisb1,b2.The
x π (x)
projectionπU(x)of
− U x∈R3ontoU can
beexpressedasa
linearcombination
U
b
ofb1,b2andthe
2
displacementvector
π (x)
x−πU(x)is
U orthogonaltoboth
b1andb2.
0 b
1
Example 3.10 (Projection onto a Line)
Find the projection matrix P onto the line through the origin spanned
π
(cid:2) (cid:3)⊤
by b = 1 2 2 . b is a direction and a basis of the one-dimensional
subspace(linethroughorigin).
With(3.46),weobtain
   
1 1 2 2
bb⊤ 1 (cid:2) (cid:3) 1
P
π
=
b⊤b
=
9
2 1 2 2 =
9
2 4 4 . (3.47)
2 2 4 4
Let us now choose a particular x and see whether it lies in the subspace
(cid:2) (cid:3)⊤
spannedbyb.Forx = 1 1 1 ,theprojectionis
      
1 2 2 1 5 1
1 1
π U(x) = P πx = 2 4 41 = 10 span[2]. (3.48)
9 9 ∈
2 4 4 1 10 2
Note that the application of P to π (x) does not change anything, i.e.,
π U
P π (x) = π (x).ThisisexpectedbecauseaccordingtoDefinition3.10,
π U U
weknowthataprojectionmatrixP satisfiesP2x = P xforallx.
π π π
Remark. With the results from Chapter 4, we can show that π (x) is an
U
eigenvectorofP ,andthecorrespondingeigenvalueis1.
π
♢
3.8.2 Projection onto General Subspaces
IfU isgivenbyaset
In the following, we look at orthogonal projections of vectors x Rn ofspanningvectors,
onto lower-dimensional subspaces U Rn with dim(U) = m ⩾∈ 1. An whicharenota
⊆ basis,makesure
illustrationisgiveninFigure3.11.
youdeterminea
Assumethat(b ,...,b )isanorderedbasisofU.Anyprojectionπ (x)
1 m U basisb1,...,bm
ontoU isnecessarilyanelementofU.Therefore,theycanberepresented beforeproceeding.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
86 AnalyticGeometry
as linear combinations of the basis vectors b ,...,b of U, such that
1 m
Thebasisvectors π U(x) =
(cid:80)m
i=1λ ib i.
formthecolumnsof As in the 1D case, we follow a three-step procedure to find the projec-
B∈Rn×m,where
tionπ (x)andtheprojectionmatrixP :
B=[b1,...,bm]. U π
1. Find the coordinates λ ,...,λ of the projection (with respect to the
1 m
basisofU),suchthatthelinearcombination
m
(cid:88)
π (x) = λ b = Bλ, (3.49)
U i i
i=1
B = [b ,...,b ] Rn×m, λ = [λ ,...,λ ]⊤ Rm, (3.50)
1 m 1 m
∈ ∈
is closest to x Rn. As in the 1D case, “closest” means “minimum
∈
distance”, which implies that the vector connecting π (x) U and
U
x Rn must be orthogonal to all basis vectors of U. There∈ fore, we
∈
obtain m simultaneous conditions (assuming the dot product as the
innerproduct)
b ,x π (x) = b⊤(x π (x)) = 0 (3.51)
⟨ 1 − U ⟩ 1 − U
.
.
.
b ,x π (x) = b⊤(x π (x)) = 0 (3.52)
⟨ m − U ⟩ m − U
which,withπ (x) = Bλ,canbewrittenas
U
b⊤(x Bλ) = 0 (3.53)
1 −
.
.
.
b⊤(x Bλ) = 0 (3.54)
m −
suchthatweobtainahomogeneouslinearequationsystem
 b⊤
 
1
  . . .  x −Bλ = 0 ⇐⇒ B⊤(x −Bλ) = 0 (3.55)
b⊤
m
B⊤Bλ = B⊤x. (3.56)
⇐⇒
normalequation The last expression is called normal equation. Since b 1,...,b m are a
basisofU and,therefore,linearlyindependent,B⊤B Rm×m isreg-
∈
ular and can be inverted. This allows us to solve for the coefficients/
coordinates
λ = (B⊤B)−1B⊤x. (3.57)
pseudo-inverse The matrix (B⊤B)−1B⊤ is also called the pseudo-inverse of B, which
canbecomputedfornon-squarematricesB.ItonlyrequiresthatB⊤B
is positive definite, which is the case if B is full rank. In practical ap-
plications (e.g., linear regression), we often add a “jitter term” ϵI to
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
3.8 OrthogonalProjections 87
B⊤B toguaranteeincreasednumericalstabilityandpositivedefinite-
ness. This “ridge” can be rigorously derived using Bayesian inference.
SeeChapter9fordetails.
2. Find the projection π (x) U. We already established that π (x) =
U U
∈
Bλ.Therefore,with(3.57)
π (x) = B(B⊤B)−1B⊤x. (3.58)
U
3. Find the projection matrix P . From (3.58), we can immediately see
π
thattheprojectionmatrixthatsolvesP x = π (x)mustbe
π U
P = B(B⊤B)−1B⊤. (3.59)
π
Remark. The solution for projecting onto general subspaces includes the
1D case as a special case: If dim(U) = 1, then B⊤B R is a scalar and
we can rewrite the projection matrix in (3.59) P =∈ B(B⊤B)−1B⊤ as
π
P =
BB⊤
,whichisexactlytheprojectionmatrixin(3.46).
π B⊤B ♢
Example 3.11 (Projection onto a Two-dimensional Subspace)
     
1 0 6
For a subspace U = span[1,1] R3 and x = 0 R3 find the
⊆ ∈
1 2 0
coordinatesλofxintermsofthesubspaceU,theprojectionpointπ (x)
U
andtheprojectionmatrixP .
π
First, we see that the generating set of U is a basis (linear indepen-
 
1 0
dence)andwritethebasisvectorsofU intoamatrixB = 1 1.
1 2
Second,wecomputethematrixB⊤B andthevectorB⊤xas
   
(cid:20) (cid:21) 1 0 (cid:20) (cid:21) (cid:20) (cid:21) 6 (cid:20) (cid:21)
1 1 1 3 3 1 1 1 6
B⊤B = 1 1 = , B⊤x = 0 = .
0 1 2 3 5 0 1 2 0
1 2 0
(3.60)
Third,wesolvethenormalequationB⊤Bλ = B⊤xtofindλ:
(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
3 3 λ 6 5
1 = λ = . (3.61)
3 5 λ 2 0 ⇐⇒ 3
−
Fourth,theprojectionπ (x)ofxontoU,i.e.,intothecolumnspaceof
U
B,canbedirectlycomputedvia
 
5
π U(x) = Bλ =  2  . (3.62)
1
−
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
88 AnalyticGeometry
projectionerror The corresponding projection error is the norm of the difference vector
Theprojectionerror betweentheoriginalvectoranditsprojectionontoU,i.e.,
isalsocalledthe (cid:13) (cid:13)
reconstructionerror. x π (x) = (cid:13)(cid:2) 1 2 1(cid:3)⊤(cid:13) = √6. (3.63)
U (cid:13) (cid:13)
∥ − ∥ −
Fifth,theprojectionmatrix(foranyx R3)isgivenby
∈
 
5 2 1
P π = B(B⊤B)−1B⊤ = 1  2 2 − 2  . (3.64)
6
1 2 5
−
Toverifytheresults,wecan(a)checkwhetherthedisplacementvector
π (x) x is orthogonal to all basis vectors of U, and (b) verify that
U
P =
P−2
(seeDefinition3.10).
π π
Remark. Theprojectionsπ (x)arestillvectorsinRn althoughtheyliein
U
an m-dimensional subspace U Rn. However, to represent a projected
⊆
vector we only need the m coordinates λ ,...,λ with respect to the
1 m
basisvectorsb ,...,b ofU.
1 m
♢
Remark. In vector spaces with general inner products, we have to pay
attention when computing angles and distances, which are defined by
meansoftheinnerproduct.
Wecanfind ♢
approximate Projectionsallowustolookatsituationswherewehavealinearsystem
solutionsto Ax = b without a solution. Recall that this means that b does not lie in
unsolvablelinear
the span of A, i.e., the vector b does not lie in the subspace spanned by
equationsystems
thecolumnsofA.Giventhatthelinearequationcannotbesolvedexactly,
usingprojections.
we can find an approximate solution. The idea is to find the vector in the
subspacespannedbythecolumnsofAthatisclosesttob,i.e.,wecompute
theorthogonalprojectionofbontothesubspacespannedbythecolumns
of A. This problem arises often in practice, and the solution is called the
least-squares least-squares solution (assuming the dot product as the inner product) of
solution an overdetermined system. This is discussed further in Section 9.4. Using
reconstruction errors (3.63) is one possible approach to derive principal
componentanalysis(Section10.3).
Remark. WejustlookedatprojectionsofvectorsxontoasubspaceU with
basis vectors b ,...,b . If this basis is an ONB, i.e., (3.33) and (3.34)
1 k
{ }
aresatisfied,theprojectionequation(3.58)simplifiesgreatlyto
π (x) = BB⊤x (3.65)
U
sinceB⊤B = I withcoordinates
λ = B⊤x. (3.66)
This means that we no longer have to compute the inverse from (3.58),
whichsavescomputationtime.
♢
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
3.8 OrthogonalProjections 89
3.8.3 Gram-Schmidt Orthogonalization
Projections are at the core of the Gram-Schmidt method that allows us to
constructivelytransformanybasis(b ,...,b )ofann-dimensionalvector
1 n
space V into an orthogonal/orthonormal basis (u ,...,u ) of V. This
1 n
basisalwaysexists(LiesenandMehrmann,2015)andspan[b ,...,b ] =
1 n
span[u 1,...,u n].TheGram-Schmidtorthogonalizationmethoditeratively Gram-Schmidt
constructsanorthogonalbasis(u ,...,u )fromanybasis(b ,...,b )of orthogonalization
1 n 1 n
V asfollows:
u := b (3.67)
1 1
u := b π (b ), k = 2,...,n. (3.68)
k k
−
span[u1,...,uk−1] k
In (3.68), the kth basis vector b is projected onto the subspace spanned
k
by the first k 1 constructed orthogonal vectors u ,...,u ; see Sec-
1 k−1
−
tion 3.8.2. This projection is then subtracted from b and yields a vector
k
u that is orthogonal to the (k 1)-dimensional subspace spanned by
k
−
u ,...,u . Repeating this procedure for all n basis vectors b ,...,b
1 k−1 1 n
yields an orthogonal basis (u ,...,u ) of V. If we normalize the u , we
1 n k
obtainanONBwhere u = 1fork = 1,...,n.
k
∥ ∥
Example 3.12 (Gram-Schmidt Orthogonalization)
Figure3.12
b2 b2 u2 b2 Gram-Schmidt
orthogonalization.
(a)non-orthogonal
basis(b1,b2)ofR2;
0 b1 0 π span[u1](b2) u1 0 π span[u1](b2) u1 (b)firstconstructed
(a) Original non-orthogonal (b) First new basis vector (c) Orthogonal basis vectors u1
basisvectoru1and
orthogonal
basisvectorsb1,b2. u on1 to= thb e1 sa un bd spp ar co eje sc pt aio nn neo dfb b2
y
andu2=b2−π span[u1](b2).
projectionofb2
u1.
ontospan[u1];
(c)orthogonalbasis
Considerabasis(b 1,b 2)ofR2,where (u1,u2)ofR2.
(cid:20) (cid:21) (cid:20) (cid:21)
2 1
b = , b = ; (3.69)
1 0 2 1
seealsoFigure3.12(a).UsingtheGram-Schmidtmethod,weconstructan
orthogonal basis (u ,u ) of R2 as follows (assuming the dot product as
1 2
theinnerproduct):
(cid:20) (cid:21)
2
u := b = , (3.70)
1 1 0
u u⊤ (cid:20) 1(cid:21) (cid:20) 1 0(cid:21)(cid:20) 1(cid:21) (cid:20) 0(cid:21)
u := b π (b ) (3 =.45) b 1 1 b = = .
2 2 − span[u1] 2 2 − u 2 2 1 − 0 0 1 1
1
∥ ∥
(3.71)
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
90 AnalyticGeometry
Figure3.13 x x
Projectionontoan
affinespace.
(a)originalsetting;
( bb y) −se xt 0tin sog ts hh aif tted L x −x 0
π L(x)
L
x x
x−x0canbe 0 0
projectedontothe
b b U =L x b
directionspaceU; 2 2 − 0 2
π (x x )
(c)projectionis U − 0
translatedbackto 0 b 0 b 0 b
1 1 1
x0+πU(x−x0),
(a)Setting. (b) Reduce problem to pro- (c)Addsupportpointbackin
whichgivesthefinal
jection πU onto vector sub- togetaffineprojectionπL.
orthogonal
space.
projectionπL(x).
ThesestepsareillustratedinFigures3.12(b)and(c).Weimmediatelysee
thatu andu areorthogonal,i.e.,u⊤u = 0.
1 2 1 2
3.8.4 Projection onto Affine Subspaces
Thus far, we discussed how to project a vector onto a lower-dimensional
subspaceU.Inthefollowing,weprovideasolutiontoprojectingavector
ontoanaffinesubspace.
ConsiderthesettinginFigure3.13(a).WearegivenanaffinespaceL =
x +U, where b ,b are basis vectors of U. To determine the orthogonal
0 1 2
projection π (x) of x onto L, we transform the problem into a problem
L
that we know how to solve: the projection onto a vector subspace. In
order to get there, we subtract the support point x from x and from L,
0
sothatL x = U isexactlythevectorsubspaceU.Wecannowusethe
0
−
orthogonalprojectionsontoasubspacewediscussedinSection3.8.2and
obtain the projection π (x x ), which is illustrated in Figure 3.13(b).
U 0
−
ThisprojectioncannowbetranslatedbackintoLbyaddingx ,suchthat
0
weobtaintheorthogonalprojectionontoanaffinespaceLas
π (x) = x +π (x x ), (3.72)
L 0 U 0
−
where π ( ) is the orthogonal projection onto the subspace U, i.e., the
U
·
directionspaceofL;seeFigure3.13(c).
FromFigure3.13,itisalsoevidentthatthedistanceofxfromtheaffine
spaceLisidenticaltothedistanceofx x fromU,i.e.,
0
−
d(x,L) = x π (x) = x (x +π (x x )) (3.73a)
L 0 U 0
∥ − ∥ ∥ − − ∥
= d(x x ,π (x x )) = d(x x ,U). (3.73b)
0 U 0 0
− − −
Wewilluseprojectionsontoanaffinesubspacetoderivetheconceptof
aseparatinghyperplaneinSection12.1.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
3.9 Rotations 91
Figure3.14 A
rotationrotates
objectsinaplane
abouttheorigin.If
Original
therotationangleis
Rotatedby112.5◦
positive,werotate
counterclockwise.
Figure3.15 The
roboticarmneedsto
rotateitsjointsin
ordertopickup
objectsortoplace
themcorrectly.
Figuretaken
from(Deisenroth
etal.,2015).
3.9 Rotations
Length and angle preservation, as discussed in Section 3.4, are the two
characteristics of linear mappings with orthogonal transformation matri-
ces. In the following, we will have a closer look at specific orthogonal
transformationmatrices,whichdescriberotations.
A rotation is a linear mapping (more specifically, an automorphism of rotation
a Euclidean vector space) that rotates a plane by an angle θ about the
origin, i.e., the origin is a fixed point. For a positive angle θ > 0, by com-
monconvention,werotateinacounterclockwisedirection.Anexampleis
showninFigure3.14,wherethetransformationmatrixis
(cid:20) (cid:21)
0.38 0.92
R = − − . (3.74)
0.92 0.38
−
Important application areas of rotations include computer graphics and
robotics. For example, in robotics, it is often important to know how to
rotate the joints of a robotic arm in order to pick up or place an object,
seeFigure3.15.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
92 AnalyticGeometry
Figure3.16 Φ(e )=[ sinθ,cosθ]⊤
Rotationofthe 2 − cosθ
standardbasisinR2
byanangleθ. e 2
Φ(e )=[cosθ,sinθ]⊤
1
sinθ
θ
θ
sinθ e cosθ
− 1
3.9.1 Rotations in R2
(cid:26) (cid:20) (cid:21) (cid:20) (cid:21)(cid:27)
1 0
Consider the standard basis e = , e = of R2, which defines
1 0 2 1
the standard coordinate system in R2. We aim to rotate this coordinate
system by an angle θ as illustrated in Figure 3.16. Note that the rotated
vectorsarestilllinearlyindependentand,therefore,areabasisofR2.This
meansthattherotationperformsabasischange.
Rotations Φ are linear mappings so that we can express them by a
rotationmatrix rotation matrix R(θ). Trigonometry (see Figure 3.16) allows us to de-
terminethecoordinatesoftherotatedaxes(theimageofΦ)withrespect
tothestandardbasisinR2.Weobtain
(cid:20) (cid:21) (cid:20) (cid:21)
cosθ sinθ
Φ(e ) = , Φ(e ) = − . (3.75)
1 sinθ 2 cosθ
Therefore, the rotation matrix that performs the basis change into the
rotatedcoordinatesR(θ)isgivenas
(cid:20) (cid:21)
(cid:2) (cid:3) cosθ sinθ
R(θ) = Φ(e 1) Φ(e 2) =
sinθ
−
cosθ
. (3.76)
3.9.2 Rotations in R3
IncontrasttotheR2 case,inR3 wecanrotateanytwo-dimensionalplane
aboutaone-dimensionalaxis.Theeasiestwaytospecifythegeneralrota-
tionmatrixistospecifyhowtheimagesofthestandardbasise ,e ,e are
1 2 3
supposedtoberotated,andmakingsuretheseimagesRe ,Re ,Re are
1 2 3
orthonormal to each other. We can then obtain a general rotation matrix
Rbycombiningtheimagesofthestandardbasis.
To have a meaningful rotation angle, we have to define what “coun-
terclockwise” means when we operate in more than two dimensions. We
use the convention that a “counterclockwise” (planar) rotation about an
axis refers to a rotation about an axis when we look at the axis “head on,
fromtheendtowardtheorigin”.InR3,therearethereforethree(planar)
rotationsaboutthethreestandardbasisvectors(seeFigure3.17):
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
3.9 Rotations 93
e Figure3.17
3
Rotationofavector
(gray)inR3byan
angleθaboutthe
e3-axis.Therotated
vectorisshownin
blue.
e
2
e
θ 1
Rotationaboutthee -axis
1
 
1 0 0
(cid:2) (cid:3)
R 1(θ) = Φ(e 1) Φ(e 2) Φ(e 3) = 0 cosθ sinθ . (3.77)
−
0 sinθ cosθ
Here, the e coordinate is fixed, and the counterclockwise rotation is
1
performedinthee e plane.
2 3
Rotationaboutthee -axis
2
 
cosθ 0 sinθ
R 2(θ) =  0 1 0  . (3.78)
sinθ 0 cosθ
−
Ifwerotatethee e planeaboutthee axis,weneedtolookatthee
1 3 2 2
axisfromits“tip”towardtheorigin.
Rotationaboutthee -axis
3
 
cosθ sinθ 0
−
R 3(θ) = sinθ cosθ 0 . (3.79)
0 0 1
Figure3.17illustratesthis.
3.9.3 Rotations in n Dimensions
The generalization of rotations from 2D and 3D to n-dimensional Eu-
clidean vector spaces can be intuitively described as fixing n 2 dimen-
−
sionsandrestricttherotationtoatwo-dimensionalplaneinthen-dimen-
sional space. As in the three-dimensional case, we can rotate any plane
(two-dimensionalsubspaceofRn).
Definition 3.11 (GivensRotation). LetV beann-dimensionalEuclidean
vector space and Φ : V V an automorphism with transformation ma-
→
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
94 AnalyticGeometry
trix
 
I 0 0
i−1
··· ···
 0 cosθ 0 sinθ 0 
R ij(θ) :=   0 0 I j−i−1 − 0 0   Rn×n, (3.80)
  ∈
 0 sinθ 0 cosθ 0 
0 0 I
n−j
··· ···
Givensrotation for 1 ⩽ i < j ⩽ n and θ R. Then R ij(θ) is called a Givens rotation.
∈
Essentially,R (θ)istheidentitymatrixI with
ij n
r = cosθ, r = sinθ, r = sinθ, r = cosθ. (3.81)
ii ij ji jj
−
Intwodimensions(i.e.,n = 2),weobtain(3.76)asaspecialcase.
3.9.4 Properties of Rotations
Rotations exhibit a number of useful properties, which can be derived by
consideringthemasorthogonalmatrices(Definition3.8):
Rotationspreservedistances,i.e., x y = R (x) R (y) .Inother
θ θ
∥ − ∥ ∥ − ∥
words,rotationsleavethedistancebetweenanytwopointsunchanged
afterthetransformation.
Rotationspreserveangles,i.e.,theanglebetweenR xandR y equals
θ θ
theanglebetweenxandy.
Rotations in three (or more) dimensions are generally not commuta-
tive. Therefore, the order in which rotations are applied is important,
eveniftheyrotateaboutthesamepoint.Onlyintwodimensionsvector
rotations are commutative, such that R(ϕ)R(θ) = R(θ)R(ϕ) for all
ϕ,θ [0,2π).TheyformanAbeliangroup(withmultiplication)onlyif
∈
theyrotateaboutthesamepoint(e.g.,theorigin).
3.10 Further Reading
Inthischapter,wegaveabriefoverviewofsomeoftheimportantconcepts
of analytic geometry, which we will use in later chapters of the book.
For a broader and more in-depth overview of some of the concepts we
presented, we refer to the following excellent books: Axler (2015) and
BoydandVandenberghe(2018).
Innerproductsallowustodeterminespecificbasesofvector(sub)spaces,
whereeachvectorisorthogonaltoallothers(orthogonalbases)usingthe
Gram-Schmidt method. These bases are important in optimization and
numerical algorithms for solving linear equation systems. For instance,
Krylov subspace methods, such as conjugate gradients or the generalized
minimal residual method (GMRES), minimize residual errors that are or-
thogonaltoeachother(StoerandBurlirsch,2002).
In machine learning, inner products are important in the context of
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
3.10 FurtherReading 95
kernelmethods(Scho¨lkopfandSmola,2002).Kernelmethodsexploitthe
fact that many linear algorithms can be expressed purely by inner prod-
uct computations. Then, the “kernel trick” allows us to compute these
inner products implicitly in a (potentially infinite-dimensional) feature
space,withoutevenknowingthisfeaturespaceexplicitly.Thisallowedthe
“non-linearization”ofmanyalgorithmsusedinmachinelearning,suchas
kernel-PCA (Scho¨lkopf et al., 1997) for dimensionality reduction. Gaus-
sianprocesses(RasmussenandWilliams,2006)alsofallintothecategory
of kernel methods and are the current state of the art in probabilistic re-
gression (fitting curves to data points). The idea of kernels is explored
furtherinChapter12.
Projectionsareoftenusedincomputergraphics,e.g.,togenerateshad-
ows.Inoptimization,orthogonalprojectionsareoftenusedto(iteratively)
minimize residual errors. This also has applications in machine learning,
e.g., in linear regression where we want to find a (linear) function that
minimizes the residual errors, i.e., the lengths of the orthogonal projec-
tions of the data onto the linear function (Bishop, 2006). We will investi-
gate this further in Chapter 9. PCA (Pearson, 1901; Hotelling, 1933) also
uses projections to reduce the dimensionality of high-dimensional data.
WewilldiscussthisinmoredetailinChapter10.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
96 AnalyticGeometry
Exercises
3.1 Showthat⟨·,·⟩definedforallx=[x ,x ]⊤ ∈R2 andy=[y ,y ]⊤ ∈R2 by
1 2 1 2
⟨x,y⟩:=x y −(x y +x y )+2(x y )
1 1 1 2 2 1 2 2
isaninnerproduct.
3.2 ConsiderR2 with⟨·,·⟩definedforallxandyinR2 as
(cid:20) (cid:21)
2 0
⟨x,y⟩:=x⊤ y.
1 2
(cid:124) (cid:123)(cid:122) (cid:125)
=:A
Is⟨·,·⟩aninnerproduct?
3.3 Computethedistancebetween
   
1 −1
x=2 , y=−1
3 0
using
a. ⟨x,y⟩:=x⊤y
 
2 1 0
b. ⟨x,y⟩:=x⊤Ay, A:=1 3 −1
0 −1 2
3.4 Computetheanglebetween
(cid:20) (cid:21) (cid:20) (cid:21)
1 −1
x= , y=
2 −1
using
a. ⟨x,y⟩:=x⊤y
(cid:20) (cid:21)
2 1
b. ⟨x,y⟩:=x⊤By, B:=
1 3
3.5 Consider the Euclidean vector space R5 with the dot product. A subspace
U ⊆R5 andx∈R5 aregivenby
         
0 1 −3 −1 −1
−1 −3  4  −3 −9
         
U =span[ 2 ,  1 ,  1 ,  5 ], x=−1 .
         
 0  −1  2   0   4 
2 2 1 7 1
a. Determinetheorthogonalprojectionπ (x)ofxontoU
U
b. Determinethedistanced(x,U)
3.6 ConsiderR3 withtheinnerproduct
 
2 1 0
⟨x,y⟩:=x⊤ 1 2 −1y.
0 −1 2
Furthermore,wedefinee ,e ,e asthestandard/canonicalbasisinR3.
1 2 3
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Exercises 97
a. Determinetheorthogonalprojectionπ (e )ofe onto
U 2 2
U =span[e ,e ].
1 3
Hint:Orthogonalityisdefinedthroughtheinnerproduct.
b. Computethedistanced(e ,U).
2
c. Drawthescenario:standardbasisvectorsandπ (e )
U 2
3.7 LetV beavectorspaceandπanendomorphismofV.
a. Provethatπ isaprojectionifandonlyifid −π isaprojection,where
V
id istheidentityendomorphismonV.
V
b. Assumenowthatπisaprojection.CalculateIm(id −π)andker(id −π)
V V
asafunctionofIm(π)andker(π).
3.8 Using the Gram-Schmidt method, turn the basis B = (b ,b ) of a two-
1 2
dimensionalsubspaceU ⊆R3 intoanONBC =(c ,c )ofU,where
1 2
   
1 −1
b 1 :=1, b 2 := 2  .
1 0
3.9 Let n ∈ N and let x 1,...,xn > 0 be n positive real numbers so that x
1
+
...+xn =1.UsetheCauchy-Schwarzinequalityandshowthat
a. (cid:80)n x2 ⩾ 1
i=1 i n
b. (cid:80)n 1 ⩾n2
i=1 xi
Hint: Think about the dot product on Rn. Then, choose specific vectors
x,y∈Rn andapplytheCauchy-Schwarzinequality.
3.10 Rotatethevectors
(cid:20) (cid:21) (cid:20) (cid:21)
2 0
x := , x :=
1 2
3 −1
by30◦.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
4
Matrix Decompositions
InChapters2and3,westudiedwaystomanipulateandmeasurevectors,
projections of vectors, and linear mappings. Mappings and transforma-
tionsofvectorscanbeconvenientlydescribedasoperationsperformedby
matrices.Moreover,dataisoftenrepresentedinmatrixformaswell,e.g.,
where the rows of the matrix represent different people and the columns
describedifferentfeaturesofthepeople,suchasweight,height,andsocio-
economicstatus.Inthischapter,wepresentthreeaspectsofmatrices:how
tosummarizematrices,howmatricescanbedecomposed,andhowthese
decompositionscanbeusedformatrixapproximations.
We first consider methods that allow us to describe matrices with just
a few numbers that characterize the overall properties of matrices. We
will do this in the sections on determinants (Section 4.1) and eigenval-
ues(Section4.2)fortheimportantspecialcaseofsquarematrices.These
characteristic numbers have important mathematical consequences and
allow us to quickly grasp what useful properties a matrix has. From here
we will proceed to matrix decomposition methods: An analogy for ma-
trix decomposition is the factoring of numbers, such as the factoring of
21 into prime numbers 7 3. For this reason matrix decomposition is also
·
matrixfactorization often referred to as matrix factorization. Matrix decompositions are used
to describe a matrix by means of a different representation using factors
ofinterpretablematrices.
We will first cover a square-root-like operation for symmetric, positive
definite matrices, the Cholesky decomposition (Section 4.3). From here
we will look at two related methods for factorizing matrices into canoni-
cal forms. The first one is known as matrix diagonalization (Section 4.4),
which allows us to represent the linear mapping using a diagonal trans-
formation matrix if we choose an appropriate basis. The second method,
singular value decomposition (Section 4.5), extends this factorization to
non-squarematrices,anditisconsideredoneofthefundamentalconcepts
inlinearalgebra.Thesedecompositionsarehelpful,asmatricesrepresent-
ingnumericaldataareoftenverylargeandhardtoanalyze.Weconclude
the chapter with a systematic overview of the types of matrices and the
characteristicpropertiesthatdistinguishthemintheformofamatrixtax-
onomy(Section4.7).
The methods that we cover in this chapter will become important in
98
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
4.1 DeterminantandTrace 99
tests usedin Figure4.1 Amind
Determinant Invertibility Cholesky mapoftheconcepts
introducedinthis
chapter,alongwith
wheretheyareused
inotherpartsofthe
book.
Eigenvalues Chapter6
Probability
&distributions
usedin
constructs usedin
Eigenvectors Orthogonalmatrix Diagonalization
use d
in
n
us
e d
i
SVD
usedi
n
Chapter10
Dimensionality
reduction
both subsequent mathematical chapters, such as Chapter 6, but also in
appliedchapters,suchasdimensionalityreductioninChapters10orden-
sity estimation in Chapter 11. This chapter’s overall structure is depicted
inthemindmapofFigure4.1.
4.1 Determinant and Trace
Thedeterminant
Determinants are important concepts in linear algebra. A determinant is notation|A|must
a mathematical object in the analysis and solution of systems of linear notbeconfused
equations. Determinants are only defined for square matrices A Rn×n, withtheabsolute
∈ value.
i.e., matrices with the same number of rows and columns. In this book,
wewritethedeterminantasdet(A)orsometimesas A sothat
| |
(cid:12) (cid:12)
(cid:12) a 11 a 12 ... a 1n (cid:12)
(cid:12) (cid:12)
(cid:12) a 21 a 22 ... a 2n (cid:12)
det(A) = (cid:12) (cid:12)
(cid:12)
. .
.
... . .
.
(cid:12) (cid:12)
(cid:12)
. (4.1)
(cid:12) (cid:12)
(cid:12) a a ... a (cid:12)
n1 n2 nn
The determinant of a square matrix A Rn×n is a function that maps A determinant
∈
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
usedin
determines
usedin
usedin
100 MatrixDecompositions
onto a real number. Before providing a definition of the determinant for
general n n matrices, let us have a look at some motivating examples,
×
anddefinedeterminantsforsomespecialmatrices.
Example 4.1 (Testing for Matrix Invertibility)
Let us begin with exploring if a square matrix A is invertible (see Sec-
tion 2.2.2). For the smallest cases, we already know when a matrix
is invertible. If A is a 1 1 matrix, i.e., it is a scalar number, then
A = a = A−1 = 1.Thu× sa 1 = 1holds,ifandonlyifa = 0.
⇒ a a ̸
For 2 2 matrices, by the definition of the inverse (Definition 2.3), we
knowth× atAA−1 = I.Then,with(2.24),theinverseofAis
(cid:20) (cid:21)
1 a a
A−1 = 22 − 12 . (4.2)
a a a a a a
11 22 − 12 21 − 21 11
Hence,Aisinvertibleifandonlyif
a a a a = 0. (4.3)
11 22 12 21
− ̸
ThisquantityisthedeterminantofA R2×2,i.e.,
∈
(cid:12) (cid:12)
(cid:12)a a (cid:12)
det(A) = (cid:12) 11 12(cid:12) = a a a a . (4.4)
(cid:12) (cid:12)a 21 a 22(cid:12) (cid:12) 11 22 − 12 21
Example 4.1 points already at the relationship between determinants
and the existence of inverse matrices. The next theorem states the same
resultforn nmatrices.
×
Theorem4.1. ForanysquarematrixA Rn×nitholdsthatAisinvertible
∈
ifandonlyifdet(A) = 0.
̸
We have explicit (closed-form) expressions for determinants of small
matricesintermsoftheelementsofthematrix.Forn = 1,
det(A) = det(a ) = a . (4.5)
11 11
Forn = 2,
(cid:12) (cid:12)
det(A) = (cid:12) (cid:12) (cid:12)a a1 21
1
a a1 22 2(cid:12) (cid:12)
(cid:12)
= a 11a 22 −a 12a 21, (4.6)
whichwehaveobservedintheprecedingexample.
Forn = 3(knownasSarrus’rule),
(cid:12) (cid:12)
(cid:12)a 11 a 12 a 13(cid:12)
(cid:12) (cid:12)
(cid:12)a 21 a 22 a 23(cid:12) = a 11a 22a 33+a 21a 32a 13+a 31a 12a 23 (4.7)
(cid:12) (cid:12)
(cid:12)a a a (cid:12)
31 32 33
a a a a a a a a a .
31 22 13 11 32 23 21 12 33
− − −
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.1 DeterminantandTrace 101
For a memory aid of the product terms in Sarrus’ rule, try tracing the
elementsofthetripleproductsinthematrix.
We call a square matrix T an upper-triangular matrix if T ij = 0 for upper-triangular
i > j,i.e.,thematrixiszerobelowitsdiagonal.Analogously,wedefinea matrix
lower-triangularmatrixasamatrixwithzerosaboveitsdiagonal.Foratri- lower-triangular
angularmatrixT Rn×n,thedeterminantistheproductofthediagonal matrix
∈
elements,i.e.,
n
(cid:89)
det(T) = T . (4.8)
ii
i=1
Thedeterminantis
thesignedvolume
oftheparallelepiped
Example 4.2 (Determinants as Measures of Volume) formedbythe
The notion of a determinant is natural when we consider it as a mapping columnsofthe
from a set of n vectors spanning an object in Rn. It turns out that the de- matrix.
Figure4.2 Thearea
terminantdet(A)isthesignedvolumeofann-dimensionalparallelepiped
oftheparallelogram
formedbycolumnsofthematrixA. (shadedregion)
For n = 2, the columns of the matrix form a parallelogram; see Fig- spannedbythe
vectorsbandgis
ure 4.2. As the angle between vectors gets smaller, the area of a parallel-
|det([b,g])|.
ogram shrinks, too. Consider two vectors b,g that form the columns of a
matrixA = [b,g].Then,theabsolutevalueofthedeterminantofAisthe
area of the parallelogram with vertices 0,b,g,b+g. In particular, if b,g b
are linearly dependent so that b = λg for some λ R, they no longer
∈ g
formatwo-dimensionalparallelogram.Therefore,thecorrespondingarea
is 0. On the contrary, if b,g are linearly independent and are multiples of Figure4.3 The
(cid:20) b(cid:21) volumeofthe
thecanonicalbasisvectorse ,e thentheycanbewrittenasb = and parallelepiped
1 2 0
(shadedvolume)
(cid:20) (cid:21) (cid:12) (cid:12)
0 (cid:12)b 0(cid:12) spannedbyvectors
g = ,andthedeterminantis(cid:12) (cid:12) = bg 0 = bg.
g (cid:12)0 g(cid:12) − r,b,gis
|det([r,b,g])|.
The sign of the determinant indicates the orientation of the spanning
vectors b,g with respect to the standard basis (e ,e ). In our figure, flip-
1 2
pingtheordertog,bswapsthecolumnsofAandreversestheorientation
of the shaded area. This becomes the familiar formula: area = height
length. This intuition extends to higher dimensions. In R3, we conside× r b
three vectors r,b,g R3 spanning the edges of a parallelepiped, i.e., a r
∈ g
solidwithfacesthatareparallelparallelograms(seeFigure4.3). Theab- Thesignofthe
solutevalueofthedeterminantofthe3 3matrix[r, b, g]isthevolume determinant
× indicatesthe
of the solid. Thus, the determinant acts as a function that measures the
orientationofthe
signedvolumeformedbycolumnvectorscomposedinamatrix. spanningvectors.
Considerthethreelinearlyindependentvectorsr,g,b R3 givenas
∈
     
2 6 1
r =  0 , g = 1, b =  4  . (4.9)
8 0 1
− −
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
102 MatrixDecompositions
Writingthesevectorsasthecolumnsofamatrix
 
2 6 1
A = [r, g, b] =  0 1 4  (4.10)
8 0 1
− −
allowsustocomputethedesiredvolumeas
V = det(A) = 186. (4.11)
| |
Computingthedeterminantofann nmatrixrequiresageneralalgo-
×
rithmtosolvethecasesforn > 3,whichwearegoingtoexploreinthefol-
lowing. Theorem 4.2 below reduces the problem of computing the deter-
minantofann nmatrixtocomputingthedeterminantof(n 1) (n 1)
× − × −
matrices. By recursively applying the Laplace expansion (Theorem 4.2),
we can therefore compute determinants of n n matrices by ultimately
×
computingdeterminantsof2 2matrices.
Laplaceexpansion ×
Theorem 4.2 (Laplace Expansion). Consider a matrix A Rn×n. Then,
∈
forallj = 1,...,n:
det(A k,j)iscalled 1. Expansionalongcolumnj
aminorand
n
(−1)k+jdet(A k,j) det(A) = (cid:88) ( 1)k+ja det(A ). (4.12)
acofactor. − kj k,j
k=1
2. Expansionalongrowj
n
(cid:88)
det(A) = ( 1)k+ja det(A ). (4.13)
jk j,k
−
k=1
Here A R(n−1)×(n−1) is the submatrix of A that we obtain when delet-
k,j
∈
ingrowk andcolumnj.
Example 4.3 (Laplace Expansion)
Letuscomputethedeterminantof
 
1 2 3
A = 3 1 2 (4.14)
0 0 1
usingtheLaplaceexpansionalongthefirstrow.Applying(4.13)yields
(cid:12) (cid:12)
(cid:12)1 2 3(cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)1 2(cid:12)
(cid:12)3 1 2(cid:12) = ( 1)1+1 1(cid:12) (cid:12)
(cid:12) (cid:12) − · (cid:12)0 1(cid:12)
(cid:12)0 0 1(cid:12) (4.15)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)3 2(cid:12) (cid:12)3 1(cid:12)
+( 1)1+2 2(cid:12) (cid:12)+( 1)1+3 3(cid:12) (cid:12) .
− · (cid:12)0 1(cid:12) − · (cid:12)0 0(cid:12)
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.1 DeterminantandTrace 103
Weuse(4.6)tocomputethedeterminantsofall2 2matricesandobtain
×
det(A) = 1(1 0) 2(3 0)+3(0 0) = 5. (4.16)
− − − − −
Forcompletenesswecancomparethisresulttocomputingthedetermi-
nantusingSarrus’rule(4.7):
det(A) = 1 1 1+3 0 3+0 2 2 0 1 3 1 0 2 3 2 1 = 1 6 = 5. (4.17)
· · · · · · − · · − · · − · · − −
ForA Rn×n thedeterminantexhibitsthefollowingproperties:
∈
Thedeterminantofamatrixproductistheproductofthecorresponding
determinants,det(AB) = det(A)det(B).
Determinantsareinvarianttotransposition,i.e.,det(A) = det(A⊤).
IfAisregular(invertible),thendet(A−1) = 1 .
det(A)
Similarmatrices(Definition2.22)possessthesamedeterminant.There-
fore, for a linear mapping Φ : V V all transformation matrices A
Φ
→
of Φ have the same determinant. Thus, the determinant is invariant to
thechoiceofbasisofalinearmapping.
Adding a multiple of a column/row to another one does not change
det(A).
Multiplication of a column/row with λ R scales det(A) by λ. In
∈
particular,det(λA) = λndet(A).
Swappingtworows/columnschangesthesignofdet(A).
Becauseofthelastthreeproperties,wecanuseGaussianelimination(see
Section 2.1) to compute det(A) by bringing A into row-echelon form.
We can stop Gaussian elimination when we have A in a triangular form
wheretheelementsbelowthediagonalareall0.Recallfrom(4.8)thatthe
determinantofatriangularmatrixistheproductofthediagonalelements.
Theorem 4.3. A square matrix A Rn×n has det(A) = 0 if and only if
∈ ̸
rk(A) = n.Inotherwords,Aisinvertibleifandonlyifitisfullrank.
When mathematics was mainly performed by hand, the determinant
calculation was considered an essential way to analyze matrix invertibil-
ity. However, contemporary approaches in machine learning use direct
numerical methods that superseded the explicit calculation of the deter-
minant. For example, in Chapter 2, we learned that inverse matrices can
be computed by Gaussian elimination. Gaussian elimination can thus be
usedtocomputethedeterminantofamatrix.
Determinants will play an important theoretical role for the following
sections, especially when we learn about eigenvalues and eigenvectors
(Section4.2)throughthecharacteristicpolynomial.
Definition 4.4. ThetraceofasquarematrixA Rn×n isdefinedas trace
∈
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
104 MatrixDecompositions
n
(cid:88)
tr(A) := a , (4.18)
ii
i=1
i.e.,thetraceisthesumofthediagonalelementsofA.
Thetracesatisfiesthefollowingproperties:
tr(A+B) = tr(A)+tr(B)forA,B Rn×n
∈
tr(αA) = αtr(A),α RforA Rn×n
∈ ∈
tr(I ) = n
n
tr(AB) = tr(BA)forA Rn×k,B Rk×n
∈ ∈
It can be shown that only one function satisfies these four properties to-
gether–thetrace(Gohbergetal.,2012).
Thepropertiesofthetraceofmatrixproductsaremoregeneral.Specif-
Thetraceis ically,thetraceisinvariantundercyclicpermutations,i.e.,
invariantunder
cyclicpermutations. tr(AKL) = tr(KLA) (4.19)
formatricesA Ra×k,K Rk×l,L Rl×a.Thispropertygeneralizesto
∈ ∈ ∈
productsofanarbitrarynumberofmatrices.Asaspecialcaseof(4.19),it
followsthatfortwovectorsx,y Rn
∈
tr(xy⊤) = tr(y⊤x) = y⊤x R. (4.20)
∈
Given a linear mapping Φ : V V, where V is a vector space, we
→
define the trace of this map by using the trace of matrix representation
of Φ. For a given basis of V, we can describe Φ by means of the transfor-
mation matrix A. Then the trace of Φ is the trace of A. For a different
basis of V, it holds that the corresponding transformation matrix B of Φ
canbeobtainedbyabasischangeoftheformS−1AS forsuitableS (see
Section2.7.2).ForthecorrespondingtraceofΦ,thismeans
tr(B) = tr(S−1AS) (4 =.19) tr(ASS−1) = tr(A). (4.21)
Hence, while matrix representations of linear mappings are basis depen-
dentthetraceofalinearmappingΦisindependentofthebasis.
In this section, we covered determinants and traces as functions char-
acterizingasquarematrix.Takingtogetherourunderstandingofdetermi-
nants and traces we can now define an important equation describing a
matrix A in terms of a polynomial, which we will use extensively in the
followingsections.
Definition 4.5 (Characteristic Polynomial). For λ R and a square ma-
trixA Rn×n ∈
∈
p (λ) := det(A λI) (4.22a)
A
−
= c +c λ+c λ2+ +c λn−1+( 1)nλn, (4.22b)
0 1 2 n−1
··· −
characteristic c 0,...,c n−1 R,isthecharacteristicpolynomialofA.Inparticular,
∈
polynomial
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.2 EigenvaluesandEigenvectors 105
c = det(A), (4.23)
0
c = ( 1)n−1tr(A). (4.24)
n−1
−
The characteristic polynomial (4.22a) will allow us to compute eigen-
valuesandeigenvectors,coveredinthenextsection.
4.2 Eigenvalues and Eigenvectors
Wewillnowgettoknowanewwaytocharacterizeamatrixanditsassoci-
ated linear mapping. Recall from Section 2.7.1 that every linear mapping
has a unique transformation matrix given an ordered basis. We can in-
terpret linear mappings and their associated transformation matrices by
performing an “eigen” analysis. As we will see, the eigenvalues of a lin- EigenisaGerman
ear mapping will tell us how a special set of vectors, the eigenvectors, is wordmeaning
“characteristic”,
transformedbythelinearmapping.
“self”,or“own”.
Definition 4.6. Let A Rn×n be a square matrix. Then λ R is an
eigenvalueofAandx
∈Rn
0
isthecorrespondingeigenvect∈
orofAif eigenvalue
∈ \{ }
eigenvector
Ax = λx. (4.25)
Wecall(4.25)theeigenvalueequation. eigenvalueequation
Remark. Inthelinearalgebraliteratureandsoftware,itisoftenaconven-
tion that eigenvalues are sorted in descending order, so that the largest
eigenvalue and associated eigenvector are called the first eigenvalue and
itsassociatedeigenvector,andthesecondlargestcalledthesecondeigen-
value and its associated eigenvector, and so on. However, textbooks and
publicationsmayhavedifferentornonotionoforderings.Wedonotwant
topresumeanorderinginthisbookifnotstatedexplicitly.
♢
Thefollowingstatementsareequivalent:
λisaneigenvalueofA Rn×n.
There exists an x
Rn∈
0 with Ax = λx, or equivalently, (A
∈ \{ } −
λI )x = 0canbesolvednon-trivially,i.e.,x = 0.
n
̸
rk(A λI ) < n.
n
−
det(A λI ) = 0.
n
−
Definition 4.7 (Collinearity and Codirection). Two vectors that point in
the same direction are called codirected. Two vectors are collinear if they codirected
pointinthesameortheoppositedirection. collinear
Remark (Non-uniqueness of eigenvectors). If x is an eigenvector of A
associated with eigenvalue λ, then for any c R 0 it holds that cx is
∈ \{ }
aneigenvectorofAwiththesameeigenvaluesince
A(cx) = cAx = cλx = λ(cx). (4.26)
Thus,allvectorsthatarecollineartoxarealsoeigenvectorsofA.
♢
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
106 MatrixDecompositions
Theorem 4.8. λ R is an eigenvalue of A Rn×n if and only if λ is a
∈ ∈
rootofthecharacteristicpolynomialp (λ)ofA.
A
algebraic Definition4.9. LetasquarematrixAhaveaneigenvalueλ i.Thealgebraic
multiplicity multiplicityofλ isthenumberoftimestherootappearsinthecharacter-
i
isticpolynomial.
Definition 4.10 (EigenspaceandEigenspectrum). ForA Rn×n,theset
∈
ofalleigenvectorsofAassociatedwithaneigenvalueλspansasubspace
eigenspace
ofRn,whichiscalledtheeigenspaceofAwithrespecttoλandisdenoted
eigenspectrum by E λ. The set of all eigenvalues of A is called the eigenspectrum, or just
spectrum spectrum,ofA.
If λ is an eigenvalue of A Rn×n, then the corresponding eigenspace
∈
E is the solution space of the homogeneous system of linear equations
λ
(A λI)x = 0.Geometrically,theeigenvectorcorrespondingtoanonzero
−
eigenvalue points in a direction that is stretched by the linear mapping.
The eigenvalue is the factor by which it is stretched. If the eigenvalue is
negative,thedirectionofthestretchingisflipped.
Example 4.4 (The Case of the Identity Matrix)
The identity matrix I Rn×n has characteristic polynomial p (λ) =
I
∈
det(I λI) = (1 λ)n = 0,whichhasonlyoneeigenvalueλ = 1thatoc-
cursn− times.Mor− eover,Ix = λx = 1xholdsforallvectorsx Rn 0 .
∈ \{ }
Because of this, the sole eigenspace E of the identity matrix spans n di-
1
mensions,andallnstandardbasisvectorsofRn areeigenvectorsofI.
Useful properties regarding eigenvalues and eigenvectors include the
following:
AmatrixAanditstransposeA⊤ possessthesameeigenvalues,butnot
necessarilythesameeigenvectors.
TheeigenspaceE isthenullspaceofA λI since
λ
−
Ax = λx Ax λx = 0 (4.27a)
⇐⇒ −
(A λI)x = 0 x ker(A λI). (4.27b)
⇐⇒ − ⇐⇒ ∈ −
Similar matrices (see Definition 2.22) possess the same eigenvalues.
Therefore,alinearmappingΦhaseigenvaluesthatareindependentof
thechoiceofbasisofitstransformationmatrix.Thismakeseigenvalues,
together with the determinant and the trace, key characteristic param-
etersofalinearmappingastheyareallinvariantunderbasischange.
Symmetric, positive definite matrices always have positive, real eigen-
values.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.2 EigenvaluesandEigenvectors 107
Example 4.5 (Computing Eigenvalues, Eigenvectors, and
Eigenspaces)
Letusfindtheeigenvaluesandeigenvectorsofthe2 2matrix
×
(cid:20) (cid:21)
4 2
A = . (4.28)
1 3
Step 1: Characteristic Polynomial. From our definition of the eigen-
vector x = 0 and eigenvalue λ of A, there will be a vector such that
̸
Ax = λx,i.e.,(A λI)x = 0.Sincex = 0,thisrequiresthatthekernel
− ̸
(null space) of A λI contains more elements than just 0. This means
−
that A λI is not invertible and therefore det(A λI) = 0. Hence, we
− −
needtocomputetherootsofthecharacteristicpolynomial(4.22a)tofind
theeigenvalues.
Step 2: Eigenvalues.Thecharacteristicpolynomialis
p (λ) = det(A λI) (4.29a)
A
−
(cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) (cid:12) (cid:12)
4 2 λ 0 (cid:12)4 λ 2 (cid:12)
= det = (cid:12) − (cid:12) (4.29b)
1 3 − 0 λ (cid:12) 1 3 λ(cid:12)
−
= (4 λ)(3 λ) 2 1. (4.29c)
− − − ·
Wefactorizethecharacteristicpolynomialandobtain
p(λ) = (4 λ)(3 λ) 2 1 = 10 7λ+λ2 = (2 λ)(5 λ) (4.30)
− − − · − − −
givingtherootsλ = 2andλ = 5.
1 2
Step 3: Eigenvectors and Eigenspaces. We find the eigenvectors that
correspondtotheseeigenvaluesbylookingatvectorsxsuchthat
(cid:20) (cid:21)
4 λ 2
− x = 0. (4.31)
1 3 λ
−
Forλ = 5weobtain
(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21)
4 5 2 x 1 2 x
− 1 = − 1 = 0. (4.32)
1 3 5 x 1 2 x
2 2
− −
Wesolvethishomogeneoussystemandobtainasolutionspace
(cid:20) (cid:21)
2
E = span[ ]. (4.33)
5 1
Thiseigenspaceisone-dimensionalasitpossessesasinglebasisvector.
Analogously, we find the eigenvector for λ = 2 by solving the homoge-
neoussystemofequations
(cid:20) (cid:21) (cid:20) (cid:21)
4 2 2 2 2
− x = x = 0. (4.34)
1 3 2 1 1
−
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
108 MatrixDecompositions
(cid:20) (cid:21) (cid:20) (cid:21)
x 1
This means any vector x = 1 , where x = x , such as , is an
x 2 2 − 1 1
−
eigenvectorwitheigenvalue2.Thecorrespondingeigenspaceisgivenas
(cid:20) (cid:21)
1
E = span[ ]. (4.35)
2 1
−
The two eigenspaces E and E in Example 4.5 are one-dimensional
5 2
as they are each spanned by a single vector. However, in other cases
we may have multiple identical eigenvalues (see Definition 4.9) and the
eigenspacemayhavemorethanonedimension.
Definition 4.11. Let λ be an eigenvalue of a square matrix A. Then the
i
geometric geometric multiplicity of λ i is the number of linearly independent eigen-
multiplicity vectors associated with λ . In other words, it is the dimensionality of the
i
eigenspacespannedbytheeigenvectorsassociatedwithλ .
i
Remark. A specific eigenvalue’s geometric multiplicity must be at least
one because every eigenvalue has at least one associated eigenvector. An
eigenvalue’s geometric multiplicity cannot exceed its algebraic multiplic-
ity,butitmaybelower.
♢
Example 4.6
(cid:20) (cid:21)
2 1
ThematrixA = hastworepeatedeigenvaluesλ = λ = 2andan
0 2 1 2
algebraicmultiplicityof2.Theeigenvaluehas,however,onlyonedistinct
(cid:20) (cid:21)
1
uniteigenvectorx = and,thus,geometricmultiplicity1.
1 0
Graphical Intuition in Two Dimensions
Let us gain some intuition for determinants, eigenvectors, and eigenval-
uesusingdifferentlinearmappings.Figure4.4depictsfivetransformation
matricesA ,...,A andtheirimpactonasquaregridofpoints,centered
1 5
Ingeometry,the attheorigin:
area-preserving (cid:20)1 0(cid:21)
propertiesofthis A = 2 . The direction of the two eigenvectors correspond to the
typeofshearing 1 0 2
paralleltoanaxisis canonicalbasisvectorsinR2,i.e.,totwocardinalaxes.Theverticalaxis
alsoknownas isextendedbyafactorof2(eigenvalueλ = 2),andthehorizontalaxis
Cavalieri’sprinciple 1
is compressed by factor 1 (eigenvalue λ = 1). The mapping is area
ofequalareasfor 2 2 2
p (Ka ara tzll ,e 2lo 0g 0r 4a )m .s preserv (cid:20)in 1g( 1d (cid:21)et(A 1) = 1 = 2
·
1 2).
A = 2 corresponds to a shearing mapping , i.e., it shears the
2 0 1
points along the horizontal axis to the right if they are on the positive
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.2 EigenvaluesandEigenvectors 109
Figure4.4
Determinantsand
eigenspaces.
Overviewoffive
λ1=2.0 linearmappingsand
λ2=0.5 theirassociated
det(A)=1.0
transformation
matrices
Ai∈R2×2
projecting400
color-codedpoints
x∈R2(left
λ1=1.0
column)ontotarget
λ2=1.0
det(A)=1.0 pointsAix(right
column).The
centralcolumn
depictsthefirst
eigenvector,
stretchedbyits
λ1=(0.87-0.5j) associated
λ2=(0.87+0.5j) eigenvalueλ1,and
det(A)=1.0
thesecond
eigenvector
stretchedbyits
eigenvalueλ2.Each
rowdepictsthe
effectofoneoffive
λ1=0.0
transformation
λ2=2.0
det(A)=0.0 matricesAiwith
respecttothe
standardbasis.
λ1=0.5
λ2=1.5
det(A)=0.75
half of the vertical axis, and to the left vice versa. This mapping is area
preserving (det(A ) = 1). The eigenvalue λ = 1 = λ is repeated
2 1 2
and the eigenvectors are collinear (drawn here for emphasis in two
opposite directions). This indicates that the mapping acts only along
onedirection(thehorizontalaxis).
(cid:20) cos(π) sin(π)(cid:21) (cid:20)√3 1(cid:21)
A = 6 − 6 = 1 − The matrix A rotates the
3 sin(π) cos(π) 2 1 √3 3
6 6
points by π rad = 30◦ counter-clockwise and has only complex eigen-
6
values,reflectingthatthemappingisarotation(hence,noeigenvectors
are drawn). A rotation has to be volume preserving, and so the deter-
minantis1.Formoredetailsonrotations,werefertoSection3.9.
(cid:20) (cid:21)
1 1
A = − represents a mapping in the standard basis that col-
4 1 1
−
lapsesatwo-dimensionaldomainontoonedimension.Sinceoneeigen-
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
110 MatrixDecompositions
valueis0,thespaceindirectionofthe(blue)eigenvectorcorresponding
to λ = 0 collapses, while the orthogonal (red) eigenvector stretches
1
spacebyafactorλ = 2.Therefore,theareaoftheimageis0.
2
(cid:20)
1
1(cid:21)
A = 2 is a shear-and-stretch mapping that scales space by 75%
5 1 1
2
since det(A ) = 3. It stretches space along the (red) eigenvector
| 5 | 4
of λ by a factor 1.5 and compresses it along the orthogonal (blue)
2
eigenvectorbyafactor0.5.
Example 4.7 (Eigenspectrum of a Biological Neural Network)
Figure4.5
0
Caenorhabditis 25
elegansneural
50 20
network(Kaiserand
Hilgetag, 15
100 2006).(a)Sym- 10
metrized
150 5
connectivitymatrix;
0
(b)Eigenspectrum.
200
5
−
250 10
−
0 50 100 150 200 250 0 100 200
neuronindex indexofsortedeigenvalue
(a)Connectivitymatrix. (b)Eigenspectrum.
Methods to analyze and learn from network data are an essential com-
ponentofmachinelearningmethods.Thekeytounderstandingnetworks
is the connectivity between network nodes, especially if two nodes are
connected to each other or not. In data science applications, it is often
usefultostudythematrixthatcapturesthisconnectivitydata.
Webuildaconnectivity/adjacencymatrixA R277×277ofthecomplete
∈
neural network of the worm C.Elegans. Each row/column represents one
of the 277 neurons of this worm’s brain. The connectivity matrix A has
a value of a = 1 if neuron i talks to neuron j through a synapse, and
ij
a = 0 otherwise. The connectivity matrix is not symmetric, which im-
ij
plies that eigenvalues may not be real valued. Therefore, we compute a
symmetrizedversionoftheconnectivitymatrixasA := A+A⊤.This
sym
newmatrixA isshowninFigure4.5(a)andhasanonzerovaluea if
sym ij
and only if two neurons are connected (white pixels), irrespective of the
direction of the connection. In Figure 4.5(b), we show the correspond-
ing eigenspectrum of A . The horizontal axis shows the index of the
sym
eigenvalues,sortedindescendingorder.Theverticalaxisshowsthecorre-
spondingeigenvalue.TheS-likeshapeofthiseigenspectrumistypicalfor
many biological neural networks. The underlying mechanism responsible
forthisisanareaofactiveneuroscienceresearch.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
xedninoruen eulavnegie
4.2 EigenvaluesandEigenvectors 111
Theorem 4.12. Theeigenvectorsx ,...,x ofamatrixA Rn×n withn
1 n
∈
distincteigenvaluesλ ,...,λ arelinearlyindependent.
1 n
This theorem states that eigenvectors of a matrix with n distinct eigen-
valuesformabasisofRn.
Definition 4.13. A square matrix A Rn×n is defective if it possesses defective
∈
fewerthannlinearlyindependenteigenvectors.
A non-defective matrix A Rn×n does not necessarily require n dis-
∈
tincteigenvalues,butitdoesrequirethattheeigenvectorsformabasisof
Rn. Looking at the eigenspaces of a defective matrix, it follows that the
sumofthedimensionsoftheeigenspacesislessthann.Specifically,ade-
fectivematrixhasatleastoneeigenvalueλ withanalgebraicmultiplicity
i
m > 1andageometricmultiplicityoflessthanm.
Remark. Adefectivematrixcannothavendistincteigenvalues,asdistinct
eigenvalueshavelinearlyindependenteigenvectors(Theorem4.12).
♢
Theorem 4.14. Given a matrix A Rm×n, we can always obtain a sym-
metric,positivesemidefinitematrixS∈ Rn×n bydefining
∈
S := A⊤A. (4.36)
Remark. If rk(A) = n, then S := A⊤A is symmetric, positive definite.
♢
Understanding why Theorem 4.14 holds is insightful for how we can
use symmetrized matrices: Symmetry requires S = S⊤, and by insert-
ing (4.36) we obtain S = A⊤A = A⊤(A⊤)⊤ = (A⊤A)⊤ = S⊤. More-
over, positive semidefiniteness (Section 3.2.3) requires that x⊤Sx ⩾ 0
and inserting (4.36) we obtain x⊤Sx = x⊤A⊤Ax = (x⊤A⊤)(Ax) =
(Ax)⊤(Ax) ⩾ 0, because the dot product computes a sum of squares
(whicharethemselvesnon-negative).
spectraltheorem
Theorem 4.15 (Spectral Theorem). If A Rn×n is symmetric, there ex-
∈
ists an orthonormal basis of the corresponding vector space V consisting of
eigenvectorsofA,andeacheigenvalueisreal.
A direct implication of the spectral theorem is that the eigendecompo-
sition of a symmetric matrix A exists (with real eigenvalues), and that
we can find an ONB of eigenvectors so that A = PDP⊤, where D is
diagonalandthecolumnsofP containtheeigenvectors.
Example 4.8
Considerthematrix
 
3 2 2
A = 2 3 2 . (4.37)
2 2 3
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
112 MatrixDecompositions
ThecharacteristicpolynomialofAis
p (λ) = (λ 1)2(λ 7), (4.38)
A
− − −
so that we obtain the eigenvalues λ = 1 and λ = 7, where λ is a
1 2 1
repeated eigenvalue. Following our standard procedure for computing
eigenvectors,weobtaintheeigenspaces
     
1 1 1
− −
E
1
= span[ 1 , 0 ], E
7
= span[1]. (4.39)
0 1 1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
=:x1 =:x2 =:x3
We see that x is orthogonal to both x and x . However, since x⊤x =
3 1 2 1 2
1 = 0, they are not orthogonal. The spectral theorem (Theorem 4.15)
̸
states that there exists an orthogonal basis, but the one we have is not
orthogonal.However,wecanconstructone.
To construct such a basis, we exploit the fact that x ,x are eigenvec-
1 2
torsassociatedwiththesameeigenvalueλ.Therefore,foranyα,β Rit
∈
holdsthat
A(αx +βx ) = Ax α+Ax β = λ(αx +βx ), (4.40)
1 2 1 2 1 2
i.e., any linear combination of x and x is also an eigenvector of A as-
1 2
sociatedwithλ.TheGram-Schmidtalgorithm(Section3.8.3)isamethod
foriterativelyconstructinganorthogonal/orthonormalbasisfromasetof
basisvectorsusingsuchlinearcombinations.Therefore,evenifx andx
1 2
are not orthogonal, we can apply the Gram-Schmidt algorithm and find
eigenvectors associated with λ = 1 that are orthogonal to each other
1
(andtox ).Inourexample,wewillobtain
3
   
1 1
− 1 −
x′ =  1 , x′ =  1 , (4.41)
1 2 2 −
0 2
whichareorthogonaltoeachother,orthogonaltox ,andeigenvectorsof
3
Aassociatedwithλ = 1.
1
Beforeweconcludeourconsiderationsofeigenvaluesandeigenvectors
itisusefultotiethesematrixcharacteristicstogetherwiththeconceptsof
thedeterminantandthetrace.
Theorem 4.16. The determinant of a matrix A Rn×n is the product of
∈
itseigenvalues,i.e.,
n
(cid:89)
det(A) = λ , (4.42)
i
i=1
whereλ Care(possiblyrepeated)eigenvaluesofA.
i
∈
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.2 EigenvaluesandEigenvectors 113
Figure4.6
Geometric
x A interpretationof
2
eigenvalues.The
v
2 eigenvectorsofA
x v getstretchedbythe
1 1
corresponding
eigenvalues.The
Theorem 4.17. The trace of a matrix A Rn×n is the sum of its eigenval- areaoftheunit
∈ squarechangesby
ues,i.e.,
|λ1λ2|,the
n perimeterchanges
(cid:88)
tr(A) = λ , (4.43) byafactorof
i
i=1
1 2(|λ1|+|λ2|).
whereλ Care(possiblyrepeated)eigenvaluesofA.
i
∈
Let us provide a geometric intuition of these two theorems. Consider
a matrix A R2×2 that possesses two linearly independent eigenvectors
x ,x .Forth∈ isexample,weassume(x ,x )areanONBofR2sothatthey
1 2 1 2
are orthogonal and the area of the square they span is 1; see Figure 4.6.
FromSection4.1, weknowthatthedeterminantcomputes thechangeof
area of unit square under the transformation A. In this example, we can
compute the change of area explicitly: Mapping the eigenvectors using
A gives us vectors v = Ax = λ x and v = Ax = λ x , i.e., the
1 1 1 1 2 2 2 2
new vectors v are scaled versions of the eigenvectors x , and the scaling
i i
factors are the corresponding eigenvalues λ . v ,v are still orthogonal,
i 1 2
andtheareaoftherectangletheyspanis λ λ .
1 2
| |
Given that x ,x (in our example) are orthonormal, we can directly
1 2
computetheperimeteroftheunitsquareas2(1+1).Mappingtheeigen-
vectors using A creates a rectangle whose perimeter is 2( λ + λ ).
1 2
| | | |
Therefore, the sum of the absolute values of the eigenvalues tells us how
theperimeteroftheunitsquarechangesunderthetransformationmatrix
A.
Example 4.9 (Google’s PageRank – Webpages as Eigenvectors)
Google uses the eigenvector corresponding to the maximal eigenvalue of
a matrix A to determine the rank of a page for search. The idea for the
PageRank algorithm, developed at Stanford University by Larry Page and
SergeyBrinin1996,wasthattheimportanceofanywebpagecanbeap-
proximated by the importance of pages that link to it. For this, they write
down all web sites as a huge directed graph that shows which page links
to which. PageRank computes the weight (importance) x ⩾ 0 of a web
i
site a by counting the number of pages pointing to a . Moreover, PageR-
i i
anktakesintoaccounttheimportanceofthewebsitesthatlinktoa .The
i
navigationbehaviorofauseristhenmodeledbyatransitionmatrixAof
thisgraphthattellsuswithwhat(click)probabilitysomebodywillendup
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
114 MatrixDecompositions
on a different web site. The matrix A has the property that for any ini-
tialrank/importancevectorxofawebsitethesequencex,Ax,A2x,...
PageRank converges to a vector x∗. This vector is called the PageRank and satisfies
Ax∗ = x∗, i.e., it is an eigenvector (with corresponding eigenvalue 1) of
A.Afternormalizingx∗,suchthat x∗ = 1,wecaninterprettheentries
∥ ∥
as probabilities. More details and different perspectives on PageRank can
befoundintheoriginaltechnicalreport(Pageetal.,1999).
4.3 Cholesky Decomposition
There are many ways to factorize special types of matrices that we en-
counter often in machine learning. In the positive real numbers, we have
the square-root operation that gives us a decomposition of the number
into identical components, e.g., 9 = 3 3. For matrices, we need to be
·
careful that we compute a square-root-like operation on positive quanti-
ties. For symmetric, positive definite matrices (see Section 3.2.3), we can
Cholesky choosefromanumberofsquare-rootequivalentoperations.TheCholesky
decomposition decomposition/Choleskyfactorizationprovidesasquare-rootequivalentop-
Cholesky erationonsymmetric,positivedefinitematricesthatisusefulinpractice.
factorization
Theorem 4.18 (Cholesky Decomposition). A symmetric, positive definite
matrix A can be factorized into a product A = LL⊤, where L is a lower-
triangularmatrixwithpositivediagonalelements:
    
a a l 0 l l
11 1n 11 11 n1
··· ··· ···
  . . . ... . . .   =   . . . ... . . .    . . . ... . . .   . (4.44)
a a l l 0 l
n1 nn n1 nn nn
··· ··· ···
Choleskyfactor LiscalledtheCholeskyfactorofA,andLisunique.
Example 4.10 (Cholesky Factorization)
Consider a symmetric, positive definite matrix A R3×3. We are inter-
estedinfindingitsCholeskyfactorizationA =
LL⊤∈
,i.e.,
    
a a a l 0 0 l l l
11 21 31 11 11 21 31
A = a 21 a 22 a 32 = LL⊤ = l 21 l 22 0  0 l 22 l 32 . (4.45)
a a a l l l 0 0 l
31 32 33 31 32 33 33
Multiplyingouttheright-handsideyields
 l2 l l l l 
11 21 11 31 11
A = l 21l 11 l 22 1+l 22 2 l 31l 21+l 32l 22 . (4.46)
l l l l +l l l2 +l2 +l2
31 11 31 21 32 22 31 32 33
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.4 EigendecompositionandDiagonalization 115
Comparing the left-hand side of (4.45) and the right-hand side of (4.46)
showsthatthereisasimplepatterninthediagonalelementsl :
ii
(cid:113) (cid:113)
l = √a , l = a l2 , l = a (l2 +l2 ). (4.47)
11 11 22 22 − 21 33 33 − 31 32
Similarly for the elements below the diagonal (l , where i > j), there is
ij
alsoarepeatingpattern:
1 1 1
l = a , l = a , l = (a l l ). (4.48)
21 l 21 31 l 31 32 l 32 − 31 21
11 11 22
Thus,weconstructedtheCholeskydecompositionforanysymmetric,pos-
itive definite 3 3 matrix. The key realization is that we can backward
×
calculate what the components l for the L should be, given the values
ij
a forAandpreviouslycomputedvaluesofl .
ij ij
The Cholesky decomposition is an important tool for the numerical
computationsunderlyingmachinelearning.Here,symmetricpositivedef-
inite matrices require frequent manipulation, e.g., the covariance matrix
ofamultivariateGaussianvariable(seeSection6.5)issymmetric,positive
definite.TheCholeskyfactorizationofthiscovariancematrixallowsusto
generatesamplesfromaGaussiandistribution.Italsoallowsustoperform
a linear transformation of random variables, which is heavily exploited
when computing gradients in deep stochastic models, such as the varia-
tional auto-encoder (Jimenez Rezende et al., 2014; Kingma and Welling,
2014). The Cholesky decomposition also allows us to compute determi-
nants very efficiently. Given the Cholesky decomposition A = LL⊤, we
know that det(A) = det(L)det(L⊤) = det(L)2. Since L is a triangular
matrix, the determinant is simply the product of its diagonal entries so
that det(A) = (cid:81) l2. Thus, many numerical software packages use the
i ii
Choleskydecompositiontomakecomputationsmoreefficient.
4.4 Eigendecomposition and Diagonalization
A diagonal matrix is a matrix that has value zero on all off-diagonal ele- diagonalmatrix
ments,i.e.,theyareoftheform
 
c 0
1
···
D =   . . . ... . . .  . (4.49)
0 c
n
···
They allow fast computation of determinants, powers, and inverses. The
determinant is the product of its diagonal entries, a matrix power Dk is
given by each diagonal element raised to the power k, and the inverse
D−1 isthereciprocalofitsdiagonalelementsifallofthemarenonzero.
Inthissection,wewilldiscusshowtotransformmatricesintodiagonal
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
116 MatrixDecompositions
form.Thisisanimportantapplicationofthebasischangewediscussedin
Section2.7.2andeigenvaluesfromSection4.2.
Recall that two matrices A,D are similar (Definition 2.22) if there ex-
istsaninvertiblematrixP,suchthatD = P−1AP.Morespecifically,we
will look at matrices A that are similar to diagonal matrices D that con-
taintheeigenvaluesofAonthediagonal.
diagonalizable Definition 4.19 (Diagonalizable). A matrix A Rn×n is diagonalizable
∈
ifitissimilartoadiagonalmatrix,i.e.,ifthereexistsaninvertiblematrix
P Rn×n suchthatD = P−1AP.
∈
In the following, we will see that diagonalizing a matrix A Rn×n is
∈
a way of expressing the same linear mapping but in another basis (see
Section2.6.1),whichwillturnouttobeabasisthatconsistsoftheeigen-
vectorsofA.
LetA Rn×n,letλ ,...,λ beasetofscalars,andletp ,...,p bea
set of vec∈ tors in Rn. W1 e definen P := [p ,...,p ] and let D1 Rn×nn be a
1 n ∈
diagonalmatrixwithdiagonalentriesλ ,...,λ .Thenwecanshowthat
1 n
AP = PD (4.50)
if and only if λ ,...,λ are the eigenvalues of A and p ,...,p are cor-
1 n 1 n
respondingeigenvectorsofA.
Wecanseethatthisstatementholdsbecause
AP = A[p ,...,p ] = [Ap ,...,Ap ], (4.51)
1 n 1 n
 
λ 0
1
PD = [p 1,...,p n]

... 

= [λ 1p 1,...,λ np n]. (4.52)
0 λ
n
Thus,(4.50)impliesthat
Ap = λ p (4.53)
1 1 1
.
.
.
Ap = λ p . (4.54)
n n n
Therefore,thecolumnsofP mustbeeigenvectorsofA.
Our definition of diagonalization requires that P Rn×n is invertible,
∈
i.e., P has full rank (Theorem 4.3). This requires us to have n linearly
independenteigenvectorsp ,...,p ,i.e.,thep formabasisofRn.
1 n i
Theorem 4.20 (Eigendecomposition). A square matrix A Rn×n can be
∈
factoredinto
A = PDP−1, (4.55)
where P Rn×n and D is a diagonal matrix whose diagonal entries are
theeigenva∈ luesofA,ifandonlyiftheeigenvectorsofAformabasisofRn.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.4 EigendecompositionandDiagonalization 117
Ae Figure4.7 Intuition
2
behindthe
eigendecomposition
e 2 e 2 assequential
p 2 A e 1 transformations.
e
p 1 Top-leftto
1 Ae bottom-left:P−1
1
performsabasis
change(heredrawn
P−1 inR2anddepicted
P asarotation-like
operation)fromthe
λ p
p p2 2 standardbasisinto
2 2
e 2 pe 1 D p 1 λ 1p 1 t Bh oe tte oi mge -n leb fa ts ti os.
1
bottom-right:D
performsascaling
alongtheremapped
orthogonal
Theorem4.20impliesthatonlynon-defectivematricescanbediagonal- eigenvectors,
izedandthatthecolumnsofP aretheneigenvectorsofA.Forsymmetric depictedherebya
circlebeing
matriceswecanobtainevenstrongeroutcomesfortheeigenvaluedecom-
stretchedtoan
position. ellipse.Bottom-right
totop-right:P
Theorem4.21. AsymmetricmatrixS Rn×n canalwaysbediagonalized. undoesthebasis
∈
change(depictedas
Theorem 4.21 follows directly from the spectral theorem 4.15. More- areverserotation)
over,thespectraltheoremstatesthatwecanfindanONBofeigenvectors andrestoresthe
ofRn.ThismakesP anorthogonalmatrixsothatD = P⊤AP. originalcoordinate
frame.
Remark. TheJordannormalformofamatrixoffersadecompositionthat
works for defective matrices (Lang, 1987) but is beyond the scope of this
book.
♢
Geometric Intuition for the Eigendecomposition
We can interpret the eigendecomposition of a matrix as follows (see also
Figure4.7):LetAbethetransformationmatrixofalinearmappingwith
respect to the standard basis e (blue arrows). P−1 performs a basis
i
change from the standard basis into the eigenbasis. Then, the diagonal
D scales the vectors along these axes by the eigenvalues λ . Finally, P
i
transforms these scaled vectors back into the standard/canonical coordi-
natesyieldingλ p .
i i
Example 4.11 (Eigendecomposition)
(cid:20) (cid:21)
5 2
LetuscomputetheeigendecompositionofA = 1 − .
2 2 5
−
Step 1: Compute eigenvalues and eigenvectors. The characteristic
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
118 MatrixDecompositions
polynomialofAis
(cid:18)(cid:20)5
λ 1
(cid:21)(cid:19)
det(A λI) = det 2 − − (4.56a)
− 1 5 λ
− 2 −
= (5 λ)2 1 = λ2 5λ+ 21 = (λ 7)(λ 3). (4.56b)
2 − − − 4 − 2 − 2
Therefore, the eigenvalues of A are λ = 7 and λ = 3 (the roots of the
1 2 2 2
characteristic polynomial), and the associated (normalized) eigenvectors
areobtainedvia
7 3
Ap = p , Ap = p . (4.57)
1 2 1 2 2 2
Thisyields
(cid:20) (cid:21) (cid:20) (cid:21)
1 1 1 1
p = , p = . (4.58)
1 √2 1 2 √2 1
−
Step 2: Check for existence. The eigenvectors p ,p form a basis of R2.
1 2
Therefore,Acanbediagonalized.
Step 3: Construct the matrix P to diagonalize A.Wecollecttheeigen-
vectorsofAinP sothat
(cid:20) (cid:21)
1 1 1
P = [p , p ] = . (4.59)
1 2 √2 1 1
−
Wethenobtain
(cid:20)7 0(cid:21)
P−1AP = 2 = D. (4.60)
0 3
2
Figure4.7visualizes Equivalently, we get (exploiting that P−1 = P⊤ since the eigenvectors
the p andp inthisexampleformanONB)
1 2
eigendecomposition
(cid:20) 5 −2(cid:21) 1 (cid:20) 5 2(cid:21) 1 (cid:20) 1 1(cid:21)(cid:20)7 0(cid:21) 1 (cid:20) 1 1(cid:21)
ofA= −2 5 2 2 − 5 = √2 1 1 02 3 √2 1 − 1 . (4.61)
asasequenceof − − 2
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
linear
A P D P−1
transformations.
Diagonal matrices D can efficiently be raised to a power. Therefore,
we can find a matrix power for a matrix A Rn×n via the eigenvalue
∈
decomposition(ifitexists)sothat
Ak = (PDP−1)k = PDkP−1. (4.62)
ComputingDk isefficientbecauseweapplythisoperationindividually
toanydiagonalelement.
AssumethattheeigendecompositionA = PDP−1 exists.Then,
det(A) = det(PDP−1) = det(P)det(D)det(P−1) (4.63a)
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.5 SingularValueDecomposition 119
(cid:89)
= det(D) = d (4.63b)
ii
i
allowsforanefficientcomputationofthedeterminantofA.
The eigenvalue decomposition requires square matrices. It would be
useful to perform a decomposition on general matrices. In the next sec-
tion, we introduce a more general matrix decomposition technique, the
singularvaluedecomposition.
4.5 Singular Value Decomposition
The singular value decomposition (SVD) of a matrix is a central matrix
decomposition method in linear algebra. It has been referred to as the
“fundamentaltheoremoflinearalgebra”(Strang,1993)becauseitcanbe
applied to all matrices, not only to square matrices, and it always exists.
Moreover, as we will explore in the following, the SVD of a matrix A,
which represents a linear mapping Φ : V W, quantifies the change
→
between the underlying geometry of these two vector spaces. We recom-
mend the work by Kalman (1996) and Roy and Banerjee (2014) for a
deeperoverviewofthemathematicsoftheSVD.
SVDtheorem
Theorem 4.22 (SVDTheorem). LetA Rm×n bearectangularmatrixof
∈
rankr [0,min(m,n)].TheSVDofAisadecompositionoftheform SVD
∈
singularvalue
n m n decomposition
n
A = U Σ V ⊤
(4.64)
withanorthogonalmatrixU Rm×mwithcolumnvectorsu ,i = 1,...,m,
i
andanorthogonalmatrixV
∈Rn×n
withcolumnvectorsv ,j = 1,...,n.
j
Moreover,Σisanm nmat∈ rixwithΣ = σ ⩾ 0andΣ = 0, i = j.
ii i ij
× ̸
Thediagonalentriesσ i,i = 1,...,r,ofΣarecalledthesingularvalues, singularvalues
u i are called the left-singular vectors, and v j are called the right-singular left-singularvectors
vectors. By convention, the singular values are ordered, i.e., σ 1 ⩾ σ 2 ⩾ right-singular
σ ⩾ 0. vectors
r
The singular value matrix Σ is unique, but it requires some attention. singularvalue
ObservethattheΣ Rm×n isrectangular.Inparticular,Σisofthesame matrix
∈
size as A. This means that Σ has a diagonal submatrix that contains the
singularvaluesandneedsadditionalzeropadding.Specifically,ifm > n,
thenthematrixΣhasdiagonalstructureuptorownandthenconsistsof
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
m m m n
120 MatrixDecompositions
Figure4.8 Intuition
behindtheSVDofa v
matrixA∈R3×2 2
assequential v 1 A σ 2u 2
transformations.
Top-leftto
bottom-left:V⊤ σ u
1 1
performsabasis
changeinR2. V ⊤ U
Bottom-leftto
bottom-right:Σ
scalesandmaps e 2
fromR2toR3.The σ 2e 2
Σ
ellipseinthe
bottom-rightlivesin e 1 σ 1e 1
R3.Thethird
dimensionis
orthogonaltothe
surfaceofthe 0⊤ rowvectorsfromn+1tombelowsothat
ellipticaldisk.
 
Bottom-rightto σ 0 0
1
pto ep r- fr oi rg mht s: aU
basis

0
...
0


changewithinR3.
Σ =
  0 0 σ n 
 . (4.65)
0 ... 0 
 
 . . . . 
 . . 
0 ... 0
If m < n, the matrix Σ has a diagonal structure up to column m and
columnsthatconsistof0fromm+1ton:
 
σ 0 0 0 ... 0
1
Σ =  0 ... 0 . . . . . .  . (4.66)
0 0 σ 0 ... 0
m
Remark. TheSVDexistsforanymatrixA Rm×n.
∈ ♢
4.5.1 Geometric Intuitions for the SVD
The SVD offers geometric intuitions to describe a transformation matrix
A. In the following, we will discuss the SVD as sequential linear trans-
formations performed on the bases. In Example 4.12, we will then apply
transformationmatricesoftheSVDtoasetofvectorsinR2,whichallows
ustovisualizetheeffectofeachtransformationmoreclearly.
The SVD of a matrix can be interpreted as a decomposition of a corre-
sponding linear mapping (recall Section 2.7.1) Φ : Rn Rm into three
→
operations; see Figure 4.8. The SVD intuition follows superficially a simi-
larstructuretooureigendecompositionintuition,seeFigure4.7:Broadly
speaking, the SVD performs a basis change via V⊤ followed by a scal-
ing and augmentation (or reduction) in dimensionality via the singular
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.5 SingularValueDecomposition 121
valuematrixΣ.Finally,itperformsasecondbasischangeviaU.TheSVD
entails a number of important details and caveats, which is why we will
reviewourintuitioninmoredetail. Itisusefultoreview
Assume we are given a transformation matrix of a linear mapping Φ : basischanges
Rn Rm with respect to the standard bases B and C of Rn and Rm, (Section2.7.2),
resp→ ectively.Moreover,assumeasecondbasisB˜ ofRn andC˜ ofRm.Then orthogonalmatrices
(Definition3.8)and
orthonormalbases
1. ThematrixV performsabasischangeinthedomainRn fromB˜ (rep-
(Section3.5).
resentedbytheredandorangevectorsv andv inthetop-leftofFig-
1 2
ure 4.8) to the standard basis B. V⊤ = V−1 performs a basis change
from B to B˜. The red and orange vectors are now aligned with the
canonicalbasisinthebottom-leftofFigure4.8.
2. Having changed the coordinate system to B˜, Σ scales the new coordi-
nates by the singular values σ (and adds or deletes dimensions), i.e.,
i
Σ is the transformation matrix of Φ with respect to B˜ and C˜, rep-
resented by the red and orange vectors being stretched and lying in
the e -e plane, which is now embedded in a third dimension in the
1 2
bottom-rightofFigure4.8.
3. U performsabasischangeinthecodomainRmfromC˜intothecanoni-
calbasisofRm,representedbyarotationoftheredandorangevectors
outofthee -e plane.Thisisshowninthetop-rightofFigure4.8.
1 2
The SVD expresses a change of basis in both the domain and codomain.
This is in contrast with the eigendecomposition that operates within the
same vector space, where the same basis change is applied and then un-
done. What makes the SVD special is that these two different bases are
simultaneouslylinkedbythesingularvaluematrixΣ.
Example 4.12 (Vectors and the SVD)
Consideramappingofasquaregridofvectors R2 thatfitinaboxof
X ∈
size 2 2 centered at the origin. Using the standard basis, we map these
×
vectorsusing
 
1 0.8
A = 0 − 1  = UΣV⊤ (4.67a)
1 0
  
0.79 0 0.62 1.62 0 (cid:20) (cid:21)
− − 0.78 0.62
=  0.38 0.78 0.49 0 1.0 − . (4.67b)
− − 0.62 0.78
0.48 0.62 0.62 0 0 − −
− −
We start with a set of vectors (colored dots; see top-left panel of Fig-
ure 4.9) arranged in a grid. WeX then apply V⊤ R2×2, which rotates .
∈ X
The rotated vectors are shown in the bottom-left panel of Figure 4.9. We
nowmapthesevectorsusingthesingularvaluematrixΣtothecodomain
R3 (see the bottom-right panel in Figure 4.9). Note that all vectors lie in
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
122 MatrixDecompositions
thex -x plane.Thethirdcoordinateisalways0.Thevectorsinthex -x
1 2 1 2
planehavebeenstretchedbythesingularvalues.
The direct mapping of the vectors by A to the codomain R3 equals
the transformation of by UΣV⊤, wX here U performs a rotation within
the codomain R3 so thX at the mapped vectors are no longer restricted to
the x -x plane; they still are on a plane as shown in the top-right panel
1 2
ofFigure4.9.
Figure4.9 SVDand
1.5
mappingofvectors
(representedby
1.0
discs).Thepanels 1.0
followthesame
0.5 0.5
anti-clockwise
structureof
0.0x3
0.0
Figure4.8.
-0.5
0.5
− -1 1.0 .5
−1.0 0.5
-1.5
−1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5 -0.5 0.5 -0.5 x2
− − − x1 x 1 1.5
-1.5
1.5
1.0
0.5
0
x3
0.0
0.5
−
1.5
1.0
− 0.5
-1.5
−1. −5 1.5 −1.0 −0.5 0 x. 10 0.5 1.0 1.5
-0 x. 15
0.5 1.5
-1.5-0.5 x2
4.5.2 Construction of the SVD
We will next discuss why the SVD exists and show how to compute it
in detail. The SVD of a general matrix shares some similarities with the
eigendecompositionofasquarematrix.
Remark. ComparetheeigendecompositionofanSPDmatrix
S = S⊤ = PDP⊤ (4.68)
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2x
2x
4.5 SingularValueDecomposition 123
withthecorrespondingSVD
S = UΣV⊤. (4.69)
Ifweset
U = P = V , D = Σ, (4.70)
weseethattheSVDofSPDmatricesistheireigendecomposition.
♢
In the following, we will explore why Theorem 4.22 holds and how
the SVD is constructed. Computing the SVD of A Rm×n is equivalent
∈
to finding two sets of orthonormal bases U = (u ,...,u ) and V =
1 m
(v ,...,v ) of the codomain Rm and the domain Rn, respectively. From
1 n
theseorderedbases,wewillconstructthematricesU andV.
Our plan is to start with constructing the orthonormal set of right-
singular vectors v ,...,v Rn. We then construct the orthonormal set
1 n
ofleft-singularvectorsu ,.∈ ..,u Rm.Thereafter,wewilllinkthetwo
1 m
∈
and require that the orthogonality of the v is preserved under the trans-
i
formation of A. This is important because we know that the images Av
i
form a set of orthogonal vectors. We will then normalize these images by
scalarfactors,whichwillturnouttobethesingularvalues.
Let us begin with constructing the right-singular vectors. The spectral
theorem (Theorem 4.15) tells us that the eigenvectors of a symmetric
matrix form an ONB, which also means it can be diagonalized. More-
over, from Theorem 4.14 we can always construct a symmetric, positive
semidefinite matrix A⊤A Rn×n from any rectangular matrix A
Rm×n.Thus,wecanalways∈ diagonalizeA⊤Aandobtain ∈
 
λ 0
1
···
A⊤A = PDP⊤ = P   . . . ... . . .  P⊤, (4.71)
0 λ
n
···
where P is an orthogonal matrix, which is composed of the orthonormal
eigenbasis. The λ ⩾ 0 are the eigenvalues of A⊤A. Let us assume the
i
SVDofAexistsandinject(4.64)into(4.71).Thisyields
A⊤A = (UΣV⊤)⊤(UΣV⊤) = VΣ⊤U⊤UΣV⊤, (4.72)
where U,V are orthogonal matrices. Therefore, with U⊤U = I we ob-
tain
 σ2 0 0 
1
A⊤A = VΣ⊤ΣV⊤ = V  0 ... 0  V⊤. (4.73)
0 0 σ2
n
Comparingnow(4.71)and(4.73),weidentify
V⊤ = P⊤, (4.74)
σ2 = λ . (4.75)
i i
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
124 MatrixDecompositions
Therefore,theeigenvectorsofA⊤AthatcomposeP aretheright-singular
vectors V of A (see (4.74)). The eigenvalues of A⊤A are the squared
singularvaluesofΣ(see(4.75)).
To obtain the left-singular vectors U, we follow a similar procedure.
We start by computing the SVD of the symmetric matrix AA⊤ Rm×m
(insteadofthepreviousA⊤A Rn×n).TheSVDofAyields ∈
∈
AA⊤ = (UΣV⊤)(UΣV⊤)⊤ = UΣV⊤VΣ⊤U⊤ (4.76a)
 σ2 0 0 
1
= U  0 ... 0  U⊤. (4.76b)
0 0 σ2
m
The spectral theorem tells us that AA⊤ = SDS⊤ can be diagonalized
and we can find an ONB of eigenvectors of AA⊤, which are collected in
S. The orthonormal eigenvectors of AA⊤ are the left-singular vectors U
andformanorthonormalbasisinthecodomainoftheSVD.
This leaves the question of the structure of the matrix Σ. Since AA⊤
andA⊤Ahavethesamenonzeroeigenvalues(seepage106),thenonzero
entriesoftheΣmatricesintheSVDforbothcaseshavetobethesame.
Thelaststepistolinkupallthepartswetoucheduponsofar.Wehave
an orthonormal set of right-singular vectors in V. To finish the construc-
tion of the SVD, we connect them with the orthonormal vectors U. To
reach this goal, we use the fact the images of the v under A have to be
i
orthogonal, too. We can show this by using the results from Section 3.4.
We require that the inner product between Av and Av must be 0 for
i j
i = j.Foranytwoorthogonaleigenvectorsv ,v ,i = j,itholdsthat
i j
̸ ̸
(Av )⊤(Av ) = v⊤(A⊤A)v = v⊤(λ v ) = λ v⊤v = 0. (4.77)
i j i j i j j j i j
For the case m ⩾ r, it holds that Av ,...,Av is a basis of an r-
1 r
dimensionalsubspaceofRm. { }
To complete the SVD construction, we need left-singular vectors that
are orthonormal: We normalize the images of the right-singular vectors
Av andobtain
i
Av 1 1
u := i = Av = Av , (4.78)
i Av √λ i σ i
∥ i ∥ i i
where the last equality was obtained from (4.75) and (4.76b), showing
usthattheeigenvaluesofAA⊤ aresuchthatσ2 = λ .
i i
Therefore, the eigenvectors of A⊤A, which we know are the right-
singularvectorsv ,andtheirnormalizedimagesunderA,theleft-singular
i
vectorsu ,formtwoself-consistentONBsthatareconnectedthroughthe
i
singularvaluematrixΣ.
singularvalue Letusrearrange(4.78)toobtainthesingularvalueequation
equation
Av = σ u , i = 1,...,r. (4.79)
i i i
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.5 SingularValueDecomposition 125
This equation closely resembles the eigenvalue equation (4.25), but the
vectorsontheleft-andtheright-handsidesarenotthesame.
For n < m, (4.79) holds only for i ⩽ n, but (4.79) says nothing about
the u for i > n. However, we know by construction that they are or-
i
thonormal.Conversely,form < n,(4.79)holdsonlyfori ⩽ m.Fori > m,
wehaveAv = 0andwestillknowthatthev formanorthonormalset.
i i
This means that the SVD also supplies an orthonormal basis of the kernel
(nullspace)ofA,thesetofvectorsxwithAx = 0(seeSection2.7.3).
Concatenatingthev asthecolumnsofV andtheu asthecolumnsof
i i
U yields
AV = UΣ, (4.80)
whereΣhasthesamedimensionsasAandadiagonalstructureforrows
1,...,r. Hence, right-multiplying with V⊤ yields A = UΣV⊤, which is
theSVDofA.
Example 4.13 (Computing the SVD)
Letusfindthesingularvaluedecompositionof
(cid:20) (cid:21)
1 0 1
A = . (4.81)
2 1 0
−
TheSVDrequiresustocomputetheright-singularvectorsv ,thesingular
j
valuesσ ,andtheleft-singularvectorsu .
k i
Step 1: Right-singular vectors as the eigenbasis of A⊤A.
Westartbycomputing
   
1 2 (cid:20) (cid:21) 5 2 1
A⊤A = 0 − 1  1 0 1 =  2 − 1 0 . (4.82)
2 1 0 −
1 0 − 1 0 1
We compute the singular values and right-singular vectors v through
j
theeigenvaluedecompositionofA⊤A,whichisgivenas
   
√5 0 √−1  6 0 0 √5 √−2 √1
30 6 30 30 30
A⊤A = √−2 √1 √−2 0 1 0 0 √1 √2  = PDP⊤,
 30 5 6  5 5 
√1 √2 √1 0 0 0 √−1 √−2 √1
30 5 6 6 6 6
(4.83)
andweobtaintheright-singularvectorsasthecolumnsofP sothat
 
√5 0 √−1
30 6
V = P = √−2 √1 √−2 . (4.84)
 30 5 6
√1 √2 √1
30 5 6
Step 2: Singular-value matrix.
As the singular values σ are the square roots of the eigenvalues of
i
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
126 MatrixDecompositions
A⊤A we obtain them straight from D. Since rk(A) = 2, there are only
two nonzero singular values: σ = √6 and σ = 1. The singular value
1 2
matrixmustbethesamesizeasA,andweobtain
(cid:20) (cid:21)
√6 0 0
Σ = . (4.85)
0 1 0
Step 3: Left-singular vectors as the normalized image of the right-
singular vectors.
We find the left-singular vectors by computing the image of the right-
singularvectorsunderAandnormalizingthembydividingthembytheir
correspondingsingularvalue.Weobtain
 
√5 (cid:34) (cid:35)
1 1 (cid:20) 1 0 1(cid:21) 30 √1
u = Av = √−2  = 5 , (4.86)
1 σ 1 √6 2 1 0  30 √2
1 − √1 − 5
30
 0  (cid:34) (cid:35)
1 1 (cid:20) 1 0 1(cid:21) √2
u 2 = σ Av 2 = 1 2 1 0 √1 5 = √15 , (4.87)
2 − √2 5
5
(cid:20) (cid:21)
1 1 2
U = [u ,u ] = . (4.88)
1 2 √5 2 1
−
Notethatonacomputertheapproachillustratedherehaspoornumerical
behavior,andtheSVDofAisnormallycomputedwithoutresortingtothe
eigenvaluedecompositionofA⊤A.
4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition
Let us consider the eigendecomposition A = PDP−1 and the SVD A =
UΣV⊤ andreviewthecoreelementsofthepastsections.
TheSVDalwaysexistsforanymatrixRm×n.Theeigendecompositionis
only defined for square matrices Rn×n and only exists if we can find a
basisofeigenvectorsofRn.
The vectors in the eigendecomposition matrix P are not necessarily
orthogonal,i.e.,thechangeofbasisisnotasimplerotationandscaling.
Ontheotherhand,thevectorsinthematricesU andV intheSVDare
orthonormal,sotheydorepresentrotations.
Both the eigendecomposition and the SVD are compositions of three
linearmappings:
1. Changeofbasisinthedomain
2. Independentscalingofeachnewbasisvectorandmappingfromdo-
maintocodomain
3. Changeofbasisinthecodomain
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.5 SingularValueDecomposition 127
Figure4.10 Movie
ratingsofthree
peopleforfour
moviesanditsSVD
 
 
StarWars 5 4 1 0.6710 0.0236 0.4647 0.5774 decomposition.
BladeR Au mn en le ier
 
5
0
5
0
0
5


= 
 
−
−
00. .7 01 99 37
9
0 0. .2 70 75 04
5
−0 0. .4 57 25 69
8
− 0 0. .4 36 41 69
4
 
 
− − − −
Delicatessen 1 0 4 0.1515 0.6030 0.5293 0.5774
− − −
 
9.6438 0 0
 0 6.3639 0 
 
 0 0 0.7056 
0 0 0
 
0.7367 0.6515 0.1811
  − 0.0852 − 0.1762 − 0.9807  
−
0.6708 0.7379 0.0743
− −
A key difference between the eigendecomposition and the SVD is that
in the SVD, domain and codomain can be vector spaces of different
dimensions.
In the SVD, the left- and right-singular vector matrices U and V are
generally not inverse of each other (they perform basis changes in dif-
ferent vector spaces). In the eigendecomposition, the basis change ma-
tricesP andP−1 areinversesofeachother.
In the SVD, the entries in the diagonal matrix Σ are all real and non-
negative, which is not generally true for the diagonal matrix in the
eigendecomposition.
The SVD and the eigendecomposition are closely related through their
projections
– Theleft-singularvectorsofAareeigenvectorsofAA⊤
– Theright-singularvectorsofAareeigenvectorsofA⊤A.
– ThenonzerosingularvaluesofAarethesquarerootsofthenonzero
eigenvaluesofbothAA⊤ andA⊤A.
For symmetric matrices A Rn×n, the eigenvalue decomposition and
∈
the SVD are one and the same, which follows from the spectral theo-
rem4.15.
Example 4.14 (Finding Structure in Movie Ratings and Consumers)
Let us add a practical interpretation of the SVD by analyzing data on
people and their preferred movies. Consider three viewers (Ali, Beatrix,
Chandra) rating four different movies (Star Wars, Blade Runner, Amelie,
Delicatessen).Theirratingsarevaluesbetween0(worst)and5(best)and
encoded in a data matrix A R4×3 as shown in Figure 4.10. Each row
∈
represents a movie and each column a user. Thus, the column vectors of
movieratings,oneforeachviewer,arex ,x ,x .
Ali Beatrix Chandra
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
ilA
xirtaeB
ardnahC
128 MatrixDecompositions
Factoring A using the SVD offers us a way to capture the relationships
of how people rate movies, and especially if there is a structure linking
which people like which movies. Applying the SVD to our data matrix A
makesanumberofassumptions:
1. Allviewersratemoviesconsistentlyusingthesamelinearmapping.
2. Therearenoerrorsornoiseintheratings.
3. We interpret the left-singular vectors u as stereotypical movies and
i
theright-singularvectorsv asstereotypicalviewers.
j
Wethenmaketheassumptionthatanyviewer’sspecificmoviepreferences
canbeexpressedasalinearcombinationofthev .Similarly,anymovie’s
j
like-abilitycanbeexpressedasalinearcombinationoftheu .Therefore,
i
a vector in the domain of the SVD can be interpreted as a viewer in the
“space”ofstereotypicalviewers,andavectorinthecodomainoftheSVD
Thesetwo“spaces” correspondingly as a movie in the “space” of stereotypical movies. Let us
areonly inspecttheSVDofourmovie-usermatrix.Thefirstleft-singularvectoru
1
meaningfully
has large absolute values for the two science fiction movies and a large
spannedbythe
first singular value (red shading in Figure 4.10). Thus, this groups a type
respectiveviewer
andmoviedataif ofuserswithaspecificsetofmovies(sciencefictiontheme).Similarly,the
thedataitselfcovers firstright-singularv showslargeabsolutevaluesforAliandBeatrix,who
asufficientdiversity 1
givehighratingstosciencefictionmovies(greenshadinginFigure4.10).
ofviewersand
movies. Thissuggeststhatv reflectsthenotionofasciencefictionlover.
1
Similarly,u ,seemstocaptureaFrencharthousefilmtheme,andv in-
2 2
dicatesthatChandraisclosetoanidealizedloverofsuchmovies.Anide-
alizedsciencefictionloverisapuristandonlylovessciencefictionmovies,
soasciencefictionloverv givesaratingofzerotoeverythingbutscience
1
fiction themed—this logic is implied by the diagonal substructure for the
singularvaluematrixΣ.Aspecificmovieisthereforerepresentedbyhow
it decomposes (linearly) into its stereotypical movies. Likewise, a person
would be represented by how they decompose (via linear combination)
intomoviethemes.
ItisworthtobrieflydiscussSVDterminologyandconventions,asthere
are different versions used in the literature. While these differences can
beconfusing,themathematicsremainsinvarianttothem.
For convenience in notation and abstraction, we use an SVD notation
wheretheSVDisdescribedashavingtwosquareleft-andright-singular
vector matrices, but a non-square singular value matrix. Our defini-
fullSVD tion(4.64)fortheSVDissometimescalledthefullSVD.
Some authors define the SVD a bit differently and focus on square sin-
gularmatrices.Then,forA Rm×n andm ⩾ n,
∈
A = U Σ V⊤ . (4.89)
m×n m×nn×nn×n
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.6 MatrixApproximation 129
SometimesthisformulationiscalledthereducedSVD(e.g.,Datta(2010)) reducedSVD
or the SVD (e.g., Press et al. (2007)). This alternative format changes
merely how the matrices are constructed but leaves the mathematical
structure of the SVD unchanged. The convenience of this alternative
formulationisthatΣisdiagonal,asintheeigenvaluedecomposition.
In Section 4.6, we will learn about matrix approximation techniques
usingtheSVD,whichisalsocalledthetruncatedSVD. truncatedSVD
It is possible to define the SVD of a rank-r matrix A so that U is an
m r matrix, Σ a diagonal matrix r r, and V an r n matrix.
× × ×
This construction is very similar to our definition, and ensures that the
diagonal matrix Σ has only nonzero entries along the diagonal. The
main convenience of this alternative notation is that Σ is diagonal, as
intheeigenvaluedecomposition.
A restriction that the SVD for A only applies to m n matrices with
×
m > nispracticallyunnecessary.Whenm < n,theSVDdecomposition
will yield Σ with more zero columns than rows and, consequently, the
singularvaluesσ ,...,σ are0.
m+1 n
The SVD is used in a variety of applications in machine learning from
least-squares problems in curve fitting to solving systems of linear equa-
tions.TheseapplicationsharnessvariousimportantpropertiesoftheSVD,
itsrelationtotherankofamatrix,anditsabilitytoapproximatematrices
of a given rank with lower-rank matrices. Substituting a matrix with its
SVD has often the advantage of making calculation more robust to nu-
merical roundingerrors. As wewill explore inthe next section,the SVD’s
ability to approximate matrices with “simpler” matrices in a principled
manneropensupmachinelearningapplicationsrangingfromdimension-
alityreductionandtopicmodelingtodatacompressionandclustering.
4.6 Matrix Approximation
We considered the SVD as a way to factorize A = UΣV⊤ Rm×n into
the product of three matrices, where U Rm×m and V ∈Rn×n are or-
∈ ∈
thogonalandΣcontainsthesingularvaluesonitsmaindiagonal.Instead
of doing the full SVD factorization, we will now investigate how the SVD
allowsustorepresentamatrixAasasumofsimpler(low-rank)matrices
A , which lends itself to a matrix approximation scheme that is cheaper
i
tocomputethanthefullSVD.
Weconstructarank-1matrixA Rm×n as
i
∈
A := u v⊤, (4.90)
i i i
whichisformedbytheouterproductoftheithorthogonalcolumnvector
of U and V. Figure 4.11 shows an image of Stonehenge, which can be
represented by a matrix A R1432×1910, and some outer products A , as
i
∈
definedin(4.90).
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
130 MatrixDecompositions
Figure4.11 Image
processingwiththe
SVD.(a)The
originalgrayscale
imageisa
1,432×1,910
matrixofvalues
between0(black) (a)OriginalimageA. (b)A1, σ1≈228,052. (c)A2, σ2≈40,647.
and1(white).
(b)–(f)Rank-1
matrices
A1,...,A5and
theircorresponding
singularvalues
σ1,...,σ5.The
grid-likestructureof
eachrank-1matrix
(d)A3, σ3≈26,125. (e)A4, σ4≈20,232. (f)A5, σ5≈15,436.
isimposedbythe
outer-productofthe
leftand
right-singular AmatrixA Rm×nofrankrcanbewrittenasasumofrank-1matrices
vectors. ∈
A sothat
i
r r
(cid:88) (cid:88)
A = σ u v⊤ = σ A , (4.91)
i i i i i
i=1 i=1
where the outer-product matrices A are weighted by the ith singular
i
value σ . We can see why (4.91) holds: The diagonal structure of the
i
singular value matrix Σ multiplies only matching left- and right-singular
vectors u v⊤ and scales them by the corresponding singular value σ . All
i i i
termsΣ u v⊤ vanishfori = j becauseΣisadiagonalmatrix.Anyterms
ij i j ̸
i > r vanishbecausethecorrespondingsingularvaluesare0.
In (4.90), we introduced rank-1 matrices A . We summed up the r in-
i
dividual rank-1 matrices to obtain a rank-r matrix A; see (4.91). If the
sum does not run over all matrices A , i = 1,...,r, but only up to an
i
rank-k intermediatevaluek < r,weobtainarank-k approximation
approximation
k k
(cid:88) (cid:88)
A(cid:98)(k) := σ u v⊤ = σ A (4.92)
i i i i i
i=1 i=1
of A with rk(A(cid:98)(k)) = k. Figure 4.12 shows low-rank approximations
A(cid:98)(k) of an original image A of Stonehenge. The shape of the rocks be-
comes increasingly visible and clearly recognizable in the rank-5 approx-
imation. While the original image requires 1,432 1,910 = 2,735,120
·
numbers, the rank-5 approximation requires us only to store the five sin-
gularvaluesandthefiveleft-andright-singularvectors(1,432and1,910-
dimensionaleach)foratotalof5 (1,432+1,910+1) = 16,715numbers
·
–justabove0.6%oftheoriginal.
Tomeasurethedifference(error)betweenAanditsrank-kapproxima-
tionA(cid:98)(k),weneedthenotionofanorm.InSection3.1,wealreadyused
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.6 MatrixApproximation 131
Figure4.12 Image
reconstructionwith
theSVD.(a)
Originalimage.
(b)–(f)Image
reconstructionusing
thelow-rank
(a)OriginalimageA. (b)Rank-1approximationA(cid:98)(1).(c)Rank-2approximationA(cid:98)(2). approximationof
theSVD,wherethe
rank-k
approximationis
givenbyA(cid:98)(k)=
(cid:80)k i=1σiAi.
(d)Rank-3approximationA(cid:98)(3).(e)Rank-4approximationA(cid:98)(4).(f)Rank-5approximationA(cid:98)(5).
norms on vectors that measure the length of a vector. By analogy we can
alsodefinenormsonmatrices.
Definition4.23(SpectralNormofaMatrix). Forx Rn 0 ,thespectral spectralnorm
normofamatrixA Rm×n isdefinedas ∈ \{ }
∈
Ax
A := max ∥ ∥2 . (4.93)
∥ ∥2 x x
∥ ∥2
We introduce the notation of a subscript in the matrix norm (left-hand
side), similar to the Euclidean norm for vectors (right-hand side), which
hassubscript2.Thespectralnorm(4.93)determineshowlonganyvector
xcanatmostbecomewhenmultipliedbyA.
Theorem 4.24. ThespectralnormofAisitslargestsingularvalueσ .
1
Weleavetheproofofthistheoremasanexercise.
Eckart-Young
Theorem 4.25 (Eckart-Young Theorem (Eckart and Young, 1936)). Con- theorem
sidera matrixA Rm×n ofrankr andletB Rm×n bea matrixofrank
k.Foranyk ⩽ r w∈ ithA(cid:98)(k) = (cid:80)k σ u v⊤ it∈ holdsthat
i=1 i i i
A(cid:98)(k) = argmin A B , (4.94)
rk(B)=k∥ − ∥2
(cid:13) (cid:13)
(cid:13)A A(cid:98)(k)(cid:13) = σ . (4.95)
(cid:13) (cid:13) k+1
− 2
The Eckart-Young theorem states explicitly how much error we intro-
duce by approximating A using a rank-k approximation. We can inter-
pret the rank-k approximation obtained with the SVD as a projection of
the full-rank matrix A onto a lower-dimensional space of rank-at-most-k
matrices. Of all possible projections, the SVD minimizes the error (with
respecttothespectralnorm)betweenAandanyrank-k approximation.
Wecanretracesomeofthestepstounderstandwhy(4.95)shouldhold.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
132 MatrixDecompositions
We observe that the difference between A A(cid:98)(k) is a matrix containing
−
thesumoftheremainingrank-1matrices
r
(cid:88)
A A(cid:98)(k) = σ u v⊤. (4.96)
− i i i
i=k+1
ByTheorem4.24,weimmediatelyobtainσ asthespectralnormofthe
k+1
difference matrix. Let us have a closer look at (4.94). If we assume that
thereisanothermatrixB withrk(B) ⩽ k,suchthat
(cid:13) (cid:13)
A B < (cid:13)A A(cid:98)(k)(cid:13) , (4.97)
∥ − ∥2 (cid:13) − (cid:13) 2
thenthereexistsanatleast(n k)-dimensionalnullspaceZ Rn,such
− ⊆
thatx Z impliesthatBx = 0.Thenitfollowsthat
∈
Ax = (A B)x , (4.98)
∥ ∥2 ∥ − ∥2
and by using a version of the Cauchy-Schwartz inequality (3.17) that en-
compassesnormsofmatrices,weobtain
Ax ⩽ A B x < σ x . (4.99)
∥ ∥2 ∥ − ∥2∥ ∥2 k+1 ∥ ∥2
However, there exists a (k + 1)-dimensional subspace where Ax ⩾
σ x ,whichisspannedbytheright-singularvectorsv ,j
⩽∥ k+∥2
1of
k+1 ∥ ∥2 j
A.Addingupdimensionsofthesetwospacesyieldsanumbergreaterthan
n,astheremustbeanonzerovectorinbothspaces.Thisisacontradiction
oftherank-nullitytheorem(Theorem2.24)inSection2.7.3.
The Eckart-Young theorem implies that we can use SVD to reduce a
rank-r matrix A to a rank-k matrix A(cid:98) in a principled, optimal (in the
spectralnormsense)manner.WecaninterprettheapproximationofAby
a rank-k matrix as a form of lossy compression. Therefore, the low-rank
approximationofamatrixappearsinmanymachinelearningapplications,
e.g.,imageprocessing,noisefiltering,andregularizationofill-posedprob-
lems. Furthermore, it plays a key role in dimensionality reduction and
principalcomponentanalysis,aswewillseeinChapter10.
Example 4.15 (Finding Structure in Movie Ratings and Consumers
(continued))
Coming back to our movie-rating example, we can now apply the con-
ceptoflow-rankapproximationstoapproximatetheoriginaldatamatrix.
Recall that our first singular value captures the notion of science fiction
theme in movies and science fiction lovers. Thus, by using only the first
singularvalueterminarank-1decompositionofthemovie-ratingmatrix,
weobtainthepredictedratings
 
0.6710
−
A 1 = u 1v⊤ 1 =   −0 0. .7 01 99 37 9  (cid:2) −0.7367 −0.6515 −0.1811(cid:3) (4.100a)
−
0.1515
−
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.7 MatrixPhylogeny 133
 
0.4943 0.4372 0.1215
0.5302 0.4689 0.1303
=   . (4.100b)
0.0692 0.0612 0.0170
0.1116 0.0987 0.0274
This first rank-1 approximation A is insightful: it tells us that Ali and
1
Beatrix like science fiction movies, such as Star Wars and Bladerunner
(entries have values > 0.4), but fails to capture the ratings of the other
movies by Chandra. This is not surprising, as Chandra’s type of movies is
not captured by the first singular value. The second singular value gives
usabetterrank-1approximationforthosemovie-themelovers:
 
0.0236
A 2 = u 2v⊤ 2 =    0 0.2 .70 75 04 5  (cid:2) 0.0852 0.1762 −0.9807(cid:3) (4.101a)
−
0.6030
−
 
0.0020 0.0042 0.0231
−
 0.0175 0.0362 0.2014
=  −  . (4.101b)
 0.0656 0.1358 0.7556 
− −
0.0514 0.1063 0.5914
− −
In this second rank-1 approximation A , we capture Chandra’s ratings
2
and movie types well, but not the science fiction movies. This leads us to
considertherank-2approximationA(cid:98)(2),wherewecombinethefirsttwo
rank-1approximations
 
4.7801 4.2419 1.0244
5.2252 4.7522 0.0250
A(cid:98)(2) = σ 1A 1+σ 2A 2 =  0.2493 0.2743 − 4.9724   . (4.102)
−
0.7495 0.2756 4.0278
A(cid:98)(2)issimilartotheoriginalmovieratingstable
 
5 4 1
5 5 0
A =   , (4.103)
0 0 5
1 0 4
and this suggests that we can ignore the contribution of A . We can in-
3
terpretthissothatinthedatatablethereisnoevidenceofathirdmovie-
theme/movie-lovers category. This also means that the entire space of
movie-themes/movie-lovers in our example is a two-dimensional space
spannedbysciencefictionandFrencharthousemoviesandlovers.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
134 MatrixDecompositions
Figure4.13 A
functional Realmatrices
Pseudo-inverse
phylogenyof ∃ SVD
matrices ∃
encounteredin
Rn×n Rn×m
machinelearning. Square
Determinant Nonsquare
∃ Trace
∃
Nobasisof det=0
eigenvectors
Defective eigB ea ns vis eco tf ors
ddeett̸
≠ = 00
SSiinngguullaarr
Non-defective
(diagonalizable)
A⊤A=AA⊤ A⊤A̸=AA⊤
Normal Non-normal
A⊤
A
=
Symmetric A A⊤ ∃ Inverse Matrix
eigenvalues R =I Regular
∈ (invertible)
Diagonal Columnsare
orthogonal
eigenvectors
Positive definite
Cholesky
Identity eigenvalues > 0 Rotation Orthogonal
matrix
4.7 Matrix Phylogeny
Theword
In Chapters 2 and 3, we covered the basics of linear algebra and analytic “phylogenetic”
geometry.Inthischapter,welookedatfundamentalcharacteristicsofma- describeshowwe
capturethe
trices and linear mappings. Figure 4.13 depicts the phylogenetic tree of
relationshipsamong
relationshipsbetweendifferenttypesofmatrices(blackarrowsindicating
individualsor
“is a subset of”) and the covered operations we can perform on them (in groupsandderived
blue). We consider all real matrices A Rn×m. For non-square matrices fromtheGreek
(where n = m), the SVD always exists,∈ as we saw in this chapter. Focus- wordsfor“tribe”
ing on squ̸ are matrices A Rn×n, the determinant informs us whether a and“source”.
∈
square matrix possesses an inverse matrix, i.e., whether it belongs to the
classofregular,invertiblematrices.Ifthesquaren nmatrixpossessesn
×
linearlyindependenteigenvectors,thenthematrixisnon-defectiveandan
eigendecomposition exists (Theorem 4.12). We know that repeated eigen-
valuesmayresultindefectivematrices,whichcannotbediagonalized.
Non-singular and non-defective matrices are not the same. For exam-
ple, a rotation matrix will be invertible (determinant is nonzero) but not
diagonalizableintherealnumbers(eigenvaluesarenotguaranteedtobe
realnumbers).
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
4.8 FurtherReading 135
Wedivefurtherintothebranchofnon-defectivesquaren nmatrices.
A is normal if the condition A⊤A = AA⊤ holds. Moreove× r, if the more
restrictive condition holds that A⊤A = AA⊤ = I, then A is called or-
thogonal(seeDefinition3.8).Thesetoforthogonalmatricesisasubsetof
theregular(invertible)matricesandsatisfiesA⊤ = A−1.
Normal matrices have a frequently encountered subset, the symmetric
matricesS Rn×n,whichsatisfyS = S⊤.Symmetricmatriceshaveonly
∈
real eigenvalues. A subset of the symmetric matrices consists of the pos-
itive definite matrices P that satisfy the condition of x⊤Px > 0 for all
x Rn 0 . In this case, a unique Cholesky decomposition exists (Theo-
∈ \{ }
rem 4.18). Positive definite matrices have only positive eigenvalues and
arealwaysinvertible(i.e.,haveanonzerodeterminant).
Another subset of symmetric matrices consists of the diagonal matrices
D.Diagonalmatricesareclosedundermultiplicationandaddition,butdo
not necessarily form a group (this is only the case if all diagonal entries
are nonzero so that the matrix is invertible). A special diagonal matrix is
theidentitymatrixI.
4.8 Further Reading
Most of the content in this chapter establishes underlying mathematics
andconnectsthemtomethodsforstudyingmappings,manyofwhichare
attheheartofmachinelearningatthelevelofunderpinningsoftwareso-
lutionsandbuildingblocksforalmostallmachinelearningtheory.Matrix
characterization using determinants, eigenspectra, and eigenspaces pro-
videsfundamentalfeaturesandconditionsforcategorizingandanalyzing
matrices. This extends to all forms of representations of data and map-
pings involving data, as well as judging the numerical stability of compu-
tationaloperationsonsuchmatrices(Pressetal.,2007).
Determinants are fundamental tools in order to invert matrices and
compute eigenvalues “by hand”. However, for almost all but the smallest
instances, numerical computation by Gaussian elimination outperforms
determinants (Press et al., 2007). Determinants remain nevertheless a
powerful theoretical concept, e.g., to gain intuition about the orientation
of a basis based on the sign of the determinant. Eigenvectors can be used
to perform basis changes to transform data into the coordinates of mean-
ingful orthogonal, feature vectors. Similarly, matrix decomposition meth-
ods, such as the Cholesky decomposition, reappear often when we com-
puteorsimulaterandomevents(RubinsteinandKroese,2016).Therefore,
the Cholesky decomposition enables us to compute the reparametrization
trick where we want to perform continuous differentiation over random
variables,e.g.,invariationalautoencoders(JimenezRezendeetal.,2014;
KingmaandWelling,2014).
Eigendecomposition is fundamental in enabling us to extract mean-
ingful and interpretable information that characterizes linear mappings.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
136 MatrixDecompositions
Therefore, the eigendecomposition underlies a general class of machine
learningalgorithmscalledspectralmethodsthatperformeigendecomposi-
tion of a positive-definite kernel. These spectral decomposition methods
encompass classical approaches to statistical data analysis, such as the
following:
principalcomponent
analysis Principalcomponentanalysis(PCA(Pearson,1901),seealsoChapter10),
in which a low-dimensional subspace, which explains most of the vari-
abilityinthedata,issought.
Fisherdiscriminant
analysis Fisher discriminant analysis, which aims to determine a separating hy-
perplanefordataclassification(Mikaetal.,1999).
multidimensional
scaling Multidimensionalscaling(MDS)(CarrollandChang,1970).
Thecomputationalefficiencyofthesemethodstypicallycomesfromfind-
ing the best rank-k approximation to a symmetric, positive semidefinite
matrix. More contemporary examples of spectral methods have different
origins, but each of them requires the computation of the eigenvectors
Isomap andeigenvaluesofapositive-definitekernel,suchasIsomap(Tenenbaum
Laplacian et al., 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003), Hessian
eigenmaps eigenmaps (Donoho and Grimes, 2003), and spectral clustering (Shi and
Hessianeigenmaps Malik, 2000). The core computations of these are generally underpinned
spectralclustering bylow-rankmatrixapproximationtechniques(BelabbasandWolfe,2009)
asweencounteredhereviatheSVD.
TheSVDallowsustodiscoversomeofthesamekindofinformationas
the eigendecomposition. However, the SVD is more generally applicable
to non-square matrices and data tables. These matrix factorization meth-
ods become relevant whenever we want to identify heterogeneity in data
when we want to perform data compression by approximation, e.g., in-
steadofstoringn mvaluesjuststoring(n+m)kvalues,orwhenwewant
×
to perform data pre-processing, e.g., to decorrelate predictor variables of
a design matrix (Ormoneit et al., 2001). The SVD operates on matrices,
which we can interpret as rectangular arrays with two indices (rows and
columns). The extension of matrix-like structure to higher-dimensional
arrays are called tensors. It turns out that the SVD is the special case of
a more general family of decompositions that operate on such tensors
(Kolda and Bader, 2009). SVD-like operations and low-rank approxima-
Tucker tionsontensorsare,forexample,theTuckerdecomposition(Tucker,1966)
decomposition ortheCPdecomposition(CarrollandChang,1970).
CPdecomposition The SVD low-rank approximation is frequently used in machine learn-
ing for computational efficiency reasons. This is because it reduces the
amount ofmemory and operationswith nonzero multiplicationswe need
toperformonpotentiallyverylargematricesofdata(TrefethenandBauIII,
1997). Moreover, low-rank approximations are used to operate on ma-
trices that may contain missing values as well as for purposes of lossy
compression and dimensionality reduction (Moonen and De Moor, 1995;
Markovsky,2011).
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Exercises 137
Exercises
4.1 ComputethedeterminantusingtheLaplaceexpansion(usingthefirstrow)
andtheSarrusrulefor
 
1 3 5
A= 2 4 6  .
0 2 4
4.2 Computethefollowingdeterminantefficiently:
 
2 0 1 2 0
 2 −1 0 1 1
 
 0 1 2 1 2 .
 
−2 0 2 −1 2
2 0 0 1 1
4.3 Computetheeigenspacesof
a.
(cid:20) (cid:21)
1 0
A:=
1 1
b.
(cid:20) (cid:21)
−2 2
B:=
2 1
4.4 Computealleigenspacesof
 
0 −1 1 1
−1 1 −2 3
A=  .
 2 −1 0 0
1 −1 1 0
4.5 Diagonalizability of a matrix is unrelated to its invertibility. Determine for
thefollowingfourmatriceswhethertheyarediagonalizableand/orinvert-
ible
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
1 0 1 0 1 1 0 1
, , , .
0 1 0 0 0 1 0 0
4.6 Computetheeigenspacesofthefollowingtransformationmatrices.Arethey
diagonalizable?
a. For
 
2 3 0
A=1 4 3
0 0 1
b. For
 
1 1 0 0
0 0 0 0
A= 
0 0 0 0
0 0 0 0
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
138 MatrixDecompositions
4.7 Arethefollowingmatricesdiagonalizable?Ifyes,determinetheirdiagonal
form and a basis with respect to which the transformation matrices are di-
agonal.Ifno,givereasonswhytheyarenotdiagonalizable.
a.
(cid:20) (cid:21)
0 1
A=
−8 4
b.
 
1 1 1
A=1 1 1
1 1 1
c.
 
5 4 2 1
 0 1 −1 −1
A= 
−1 −1 3 0 
1 1 −1 2
d.
 
5 −6 −6
A=−1 4 2 
3 −6 −4
4.8 FindtheSVDofthematrix
(cid:20) (cid:21)
3 2 2
A= .
2 3 −2
4.9 Findthesingularvaluedecompositionof
(cid:20) (cid:21)
2 2
A= .
−1 1
4.10 Findtherank-1approximationof
(cid:20) (cid:21)
3 2 2
A=
2 3 −2
4.11 Show that for any A ∈ Rm×n the matrices A⊤A and AA⊤ possess the
samenonzeroeigenvalues.
4.12 Showthatforx̸=0Theorem4.24holds,i.e.,showthat
∥Ax∥
max 2 =σ ,
1
x ∥x∥
2
whereσ isthelargestsingularvalueofA∈Rm×n.
1
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5
Vector Calculus
Manyalgorithmsinmachinelearningoptimizeanobjectivefunctionwith
respecttoasetofdesiredmodelparametersthatcontrolhowwellamodel
explains the data: Finding good parameters can be phrased as an opti-
mization problem (see Sections 8.2 and 8.3). Examples include: (i) lin-
ear regression (see Chapter 9), where we look at curve-fitting problems
and optimize linear weight parameters to maximize the likelihood; (ii)
neural-networkauto-encodersfordimensionalityreductionanddatacom-
pression, where the parameters are the weights and biases of each layer,
andwhereweminimizeareconstructionerrorbyrepeatedapplicationof
the chain rule; and (iii) Gaussian mixture models (see Chapter 11) for
modeling data distributions, where we optimize the location and shape
parameters of each mixture component to maximize the likelihood of the
model. Figure 5.1 illustrates some of these problems, which we typically
solve by using optimization algorithms that exploit gradient information
(Section 7.1). Figure 5.2 gives an overview of how concepts in this chap-
terarerelatedandhowtheyareconnectedtootherchaptersofthebook.
Central to this chapter is the concept of a function. A function f is
a quantity that relates two quantities to each other. In this book, these
quantitiesaretypicallyinputsx RD andtargets(functionvalues)f(x),
which we assume are real-value∈ d if not stated otherwise. Here RD is the
domain of f, and the function values f(x) are the image/codomain of f. domain
image/codomain
10 Figure5.1 Vector
4 Trainingdata calculusplaysa
MLE 5 centralrolein(a)
2
regression(curve
fitting)and(b)
0 0
densityestimation,
2 i.e.,modelingdata
− 5
− distributions.
4
−
10
4 2 0 2 4 − 10 5 0 5 10
− − x − − x1
(a) Regression problem: Find parameters, (b)DensityestimationwithaGaussianmixture
such that the curve explains the observations model: Find means and covariances, such that
(crosses)well. thedata(dots)canbeexplainedwell.
139
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
y 2x
140 VectorCalculus
Figure5.2 Amind Chapter9
Differencequotient
mapoftheconcepts Regression
introducedinthis
chapter,alongwith
w inh oe tn heth re py ara tr se ou fs te hd
e
use d
in
book.
Chapter10
Chapter7 usedin usedin
Partialderivatives Dimensionality
Optimization
reduction
use
d
i
n
u
Chapter6 usedin Jacobian s e Chapter11
Probability Hessian d i Densityestimation
n
Chapter12
Taylorseries
Classification
Section 2.7.3 provides much more detailed discussion in the context of
linearfunctions.Weoftenwrite
f : RD R (5.1a)
→
x f(x) (5.1b)
(cid:55)→
to specify a function, where (5.1a) specifies that f is a mapping from
RD to R and (5.1b) specifies the explicit assignment of an input x to
a function value f(x). A function f assigns every input x exactly one
functionvaluef(x).
Example 5.1
Recallthedotproductasaspecialcaseofaninnerproduct(Section3.2).
In the previous notation, the function f(x) = x⊤x, x R2, would be
∈
specifiedas
f : R2 R (5.2a)
→
x x2+x2. (5.2b)
(cid:55)→ 1 2
In this chapter, we will discuss how to compute gradients of functions,
which is often essential to facilitate learning in machine learning models
since the gradient points in the direction of steepest ascent. Therefore,
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
defines
collectedin
usedin
5.1 DifferentiationofUnivariateFunctions 141
Figure5.3 The
y f(x) averageinclineofa
functionf between
x0andx0+δxis
theinclineofthe
secant(blue)
throughf(x0)and
f(x +δx)
0 f(x0+δx)and
δy givenbyδy/δx.
f(x )
0
δx
x
vector calculus is one of the fundamental mathematical tools we need in
machine learning. Throughout this book, we assume that functions are
differentiable. With some additional technical definitions, which we do
not cover here, many of the approaches presented can be extended to
sub-differentials (functions that are continuous but not differentiable at
certain points). We will look at an extension to the case of functions with
constraintsinChapter7.
5.1 Differentiation of Univariate Functions
Inthefollowing,webrieflyrevisitdifferentiationofaunivariatefunction,
which may be familiar from high school mathematics. We start with the
differencequotientofaunivariatefunctiony = f(x), x,y R,whichwe
∈
willsubsequentlyusetodefinederivatives.
Definition 5.1 (DifferenceQuotient). Thedifferencequotient differencequotient
δy f(x+δx) f(x)
:= − (5.3)
δx δx
computes the slope of the secant line through two points on the graph of
f.InFigure5.3,thesearethepointswithx-coordinatesx andx +δx.
0 0
The difference quotient can also be considered the average slope of f
betweenxandx+δxifweassumef tobealinearfunction.Inthelimit
for δx 0, we obtain the tangent of f at x, if f is differentiable. The
→
tangentisthenthederivativeoff atx.
Definition 5.2 (Derivative). More formally, for h > 0 the derivative of f derivative
atxisdefinedasthelimit
df f(x+h) f(x)
:= lim − , (5.4)
dx h→0 h
andthesecantinFigure5.3becomesatangent.
Thederivativeoff pointsinthedirectionofsteepestascentoff.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
142 VectorCalculus
Example 5.2 (Derivative of a Polynomial)
Wewanttocomputethederivativeoff(x) = xn,n N.Wemayalready
∈
know that the answer will be nxn−1, but we want to derive this result
usingthedefinitionofthederivativeasthelimitofthedifferencequotient.
Usingthedefinitionofthederivativein(5.4),weobtain
df f(x+h) f(x)
= lim − (5.5a)
dx h→0 h
(x+h)n xn
= lim − (5.5b)
h→0 h
(cid:80)n (cid:0)n(cid:1) xn−ihi xn
= lim i=0 i − . (5.5c)
h→0 h
Weseethatxn =
(cid:0)n(cid:1)
xn−0h0.Bystartingthesumat1,thexn-termcancels,
0
andweobtain
df (cid:80)n (cid:0)n(cid:1) xn−ihi
= lim i=1 i (5.6a)
dx h→0 h
(cid:32) (cid:33)
(cid:88)n n
= lim xn−ihi−1 (5.6b)
h→0 i
i=1
(cid:32) (cid:33) (cid:32) (cid:33)
n (cid:88)n n
= lim xn−1+ xn−ihi−1 (5.6c)
h→0 1 i
i=2
(cid:124) (cid:123)(cid:122) (cid:125)
→0ash→0
n!
= xn−1 = nxn−1. (5.6d)
1!(n 1)!
−
5.1.1 Taylor Series
The Taylor series is a representation of a function f as an infinite sum of
terms.Thesetermsaredeterminedusingderivativesoff evaluatedatx .
0
Taylorpolynomial Definition 5.3 (TaylorPolynomial). TheTaylorpolynomialofdegreenof
Wedefinet0:=1 f : R Ratx
0
isdefinedas
→
forallt∈R.
T (x) :=
(cid:88)n f(k)(x 0)
(x x )k, (5.7)
n k! − 0
k=0
where f(k)(x ) is the kth derivative of f at x (which we assume exists)
0 0
and
f(k)(x0)
arethecoefficientsofthepolynomial.
k!
Definition5.4(TaylorSeries). Forasmoothfunctionf ∞,f : R R,
∈ C →
Taylorseries theTaylorseriesoff atx 0 isdefinedas
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.1 DifferentiationofUnivariateFunctions 143
T (x) =
(cid:88)∞ f(k)(x 0)
(x x )k. (5.8)
∞ k! − 0
k=0
For x 0 = 0, we obtain the Maclaurin series as a special instance of the f ∈C∞meansthat
Taylorseries.Iff(x) = T (x),thenf iscalledanalytic. f iscontinuously
∞
differentiable
Remark. In general, a Taylor polynomial of degree n is an approximation infinitelymany
times.
of a function, which does not need to be a polynomial. The Taylor poly-
Maclaurinseries
nomial is similar to f in a neighborhood around x 0. However, a Taylor analytic
polynomial of degree n is an exact representation of a polynomial f of
degreek ⩽ nsinceallderivativesf(i),i > k vanish.
♢
Example 5.3 (Taylor Polynomial)
Weconsiderthepolynomial
f(x) = x4 (5.9)
andseektheTaylorpolynomialT ,evaluatedatx = 1.Westartbycom-
6 0
putingthecoefficientsf(k)(1)fork = 0,...,6:
f(1) = 1 (5.10)
f′(1) = 4 (5.11)
f′′(1) = 12 (5.12)
f(3)(1) = 24 (5.13)
f(4)(1) = 24 (5.14)
f(5)(1) = 0 (5.15)
f(6)(1) = 0 (5.16)
Therefore,thedesiredTaylorpolynomialis
T (x) =
(cid:88)6 f(k)(x 0)
(x x )k (5.17a)
6 k! − 0
k=0
= 1+4(x 1)+6(x 1)2+4(x 1)3+(x 1)4+0. (5.17b)
− − − −
Multiplyingoutandre-arrangingyields
T (x) = (1 4+6 4+1)+x(4 12+12 4)
6
− − − −
+x2(6 12+6)+x3(4 4)+x4 (5.18a)
− −
= x4 = f(x), (5.18b)
i.e.,weobtainanexactrepresentationoftheoriginalfunction.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
144 VectorCalculus
Figure5.4 Taylor
polynomials.The f
originalfunction T
0
4
f(x)= T
1
sin(x)+cos(x) T
5
(black,solid)is
2 T
10
approximatedby
Taylorpolynomials
(dashed)around 0
x0=0.
Higher-orderTaylor
2
polynomials −
approximatethe
functionf better 4 2 0 2 4
andmoreglobally. − − x
T10isalready
similartof in
[−4,4].
Example 5.4 (Taylor Series)
ConsiderthefunctioninFigure5.4givenby
f(x) = sin(x)+cos(x) ∞. (5.19)
∈ C
We seek a Taylor series expansion of f at x = 0, which is the Maclaurin
0
seriesexpansionoff.Weobtainthefollowingderivatives:
f(0) = sin(0)+cos(0) = 1 (5.20)
f′(0) = cos(0) sin(0) = 1 (5.21)
−
f′′(0) = sin(0) cos(0) = 1 (5.22)
− − −
f(3)(0) = cos(0)+sin(0) = 1 (5.23)
− −
f(4)(0) = sin(0)+cos(0) = f(0) = 1 (5.24)
.
.
.
We can see a pattern here: The coefficients in our Taylor series are only
1(sincesin(0) = 0),eachofwhichoccurstwicebeforeswitchingtothe
±
otherone.Furthermore,f(k+4)(0) = f(k)(0).
Therefore,thefullTaylorseriesexpansionoff atx = 0isgivenby
0
T (x) =
(cid:88)∞ f(k)(x 0)
(x x )k (5.25a)
∞ k! − 0
k=0
1 1 1 1
= 1+x x2 x3+ x4+ x5 (5.25b)
− 2! − 3! 4! 5! −···
1 1 1 1
= 1 x2+ x4 +x x3+ x5 (5.25c)
− 2! 4! ∓··· − 3! 5! ∓···
∞ ∞
(cid:88) 1 (cid:88) 1
= ( 1)k x2k + ( 1)k x2k+1 (5.25d)
− (2k)! − (2k+1)!
k=0 k=0
= cos(x)+sin(x), (5.25e)
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
y
5.1 DifferentiationofUnivariateFunctions 145
whereweusedthe powerseriesrepresentations powerseries
representation
∞
(cid:88) 1
cos(x) = ( 1)k x2k, (5.26)
− (2k)!
k=0
∞
(cid:88) 1
sin(x) = ( 1)k x2k+1. (5.27)
− (2k+1)!
k=0
Figure 5.4 shows the corresponding first Taylor polynomials T for n =
n
0,1,5,10.
Remark. ATaylorseriesisaspecialcaseofapowerseries
∞
(cid:88)
f(x) = a (x c)k (5.28)
k
−
k=0
where a are coefficients and c is a constant, which has the special form
k
inDefinition5.4.
♢
5.1.2 Differentiation Rules
In the following, we briefly state basic differentiation rules, where we
denotethederivativeoff byf′.
Productrule: (f(x)g(x))′ = f′(x)g(x)+f(x)g′(x) (5.29)
(cid:18)f(x)(cid:19)′ f′(x)g(x) f(x)g′(x)
Quotientrule: = − (5.30)
g(x) (g(x))2
Sumrule: (f(x)+g(x))′ = f′(x)+g′(x) (5.31)
Chainrule: (cid:0) g(f(x))(cid:1)′ = (g f)′(x) = g′(f(x))f′(x) (5.32)
◦
Here,g f denotesfunctioncompositionx f(x) g(f(x)).
◦ (cid:55)→ (cid:55)→
Example 5.5 (Chain Rule)
Let us compute the derivative of the function h(x) = (2x+1)4 using the
chainrule.With
h(x) = (2x+1)4 = g(f(x)), (5.33)
f(x) = 2x+1, (5.34)
g(f) = f4, (5.35)
weobtainthederivativesoff andg as
f′(x) = 2, (5.36)
g′(f) = 4f3, (5.37)
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
146 VectorCalculus
suchthatthederivativeofhisgivenas
h′(x) = g′(f)f′(x) = (4f3) 2 (5 =.34) 4(2x+1)3 2 = 8(2x+1)3, (5.38)
· ·
where we used the chain rule (5.32) and substituted the definition of f
in(5.34)ing′(f).
5.2 Partial Differentiation and Gradients
Differentiation as discussed in Section 5.1 applies to functions f of a
scalar variable x R. In the following, we consider the general case
where the function∈ f depends on one or more variables x Rn, e.g.,
∈
f(x) = f(x ,x ).Thegeneralizationofthederivativetofunctionsofsev-
1 2
eralvariablesisthegradient.
We find the gradient of the function f with respect to x by varying one
variable at a time and keeping the others constant. The gradient is then
thecollectionofthesepartialderivatives.
Definition 5.5 (Partial Derivative). For a function f : Rn R, x
partialderivative f(x), x Rn ofnvariablesx 1,...,x n wedefinethepartiald→ erivatives(cid:55)→ as
∈
∂f f(x +h,x ,...,x ) f(x)
1 2 n
= lim −
∂x 1 h→0 h
.
. . (5.39)
∂f f(x ,...,x ,x +h) f(x)
1 n−1 n
= lim −
∂x n h→0 h
andcollectthemintherowvector
df (cid:20) ∂f(x) ∂f(x) ∂f(x)(cid:21)
f = gradf = = R1×n, (5.40)
x
∇ dx ∂x ∂x ··· ∂x ∈
1 2 n
where n is the number of variables and 1 is the dimension of the image/
range/codomainoff.Here,wedefinedthecolumnvectorx = [x ,...,x ]⊤
1 n
gradient Rn. The row vector in (5.40) is called the gradient of f or the Jacobian
∈
Jacobian andisthegeneralizationofthederivativefromSection5.1.
Remark. This definition of the Jacobian is a special case of the general
definition of the Jacobian for vector-valued functions as the collection of
partialderivatives.WewillgetbacktothisinSection5.3.
Wecanuseresults ♢
fromscalar
differentiation:Each
Example 5.6 (Partial Derivatives Using the Chain Rule)
partialderivativeis
Forf(x,y) = (x+2y3)2,weobtainthepartialderivatives
aderivativewith
respecttoascalar.
∂f(x,y) ∂
= 2(x+2y3) (x+2y3) = 2(x+2y3), (5.41)
∂x ∂x
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.2 PartialDifferentiationandGradients 147
∂f(x,y) ∂
= 2(x+2y3) (x+2y3) = 12(x+2y3)y2. (5.42)
∂y ∂y
whereweusedthechainrule(5.32)tocomputethepartialderivatives.
Remark (Gradient as a Row Vector). It is not uncommon in the literature
to define the gradient vector as a column vector, following the conven-
tionthatvectorsaregenerallycolumnvectors.Thereasonwhywedefine
the gradient vector as a row vector is twofold: First, we can consistently
generalize the gradient to vector-valued functions f : Rn Rm (then
→
the gradient becomes a matrix). Second, we can immediately apply the
multi-variate chain rule without paying attention to the dimension of the
gradient.WewilldiscussbothpointsinSection5.3.
♢
Example 5.7 (Gradient)
For f(x ,x ) = x2x +x x3 R, the partial derivatives (i.e., the deriva-
1 2 1 2 1 2 ∈
tivesoff withrespecttox andx )are
1 2
∂f(x ,x )
1 2 = 2x x +x3 (5.43)
∂x 1 2 2
1
∂f(x ,x )
1 2 = x2+3x x2 (5.44)
∂x 1 1 2
2
andthegradientisthen
df = (cid:20) ∂f(x 1,x 2) ∂f(x 1,x 2)(cid:21) = (cid:2) 2x x +x3 x2+3x x2(cid:3) R1×2.
dx ∂x ∂x 1 2 2 1 1 2 ∈
1 2
(5.45)
5.2.1 Basic Rules of Partial Differentiation
Productrule:
Inthemultivariatecase,wherex Rn,thebasicdifferentiationrulesthat (fg)′=f′g+fg′,
∈
we know from school (e.g., sum rule, product rule, chain rule; see also Sumrule:
(f+g)′=f′+g′,
Section5.1.2)stillapply.However,whenwecomputederivativeswithre-
Chainrule:
spect to vectors x Rn we need to pay attention: Our gradients now
(g(f))′=g′(f)f′
∈
involve vectors and matrices, and matrix multiplication is not commuta-
tive(Section2.2.1),i.e.,theordermatters.
Herearethegeneralproductrule,sumrule,andchainrule:
∂ ∂f ∂g
(cid:0) (cid:1)
Productrule: f(x)g(x) = g(x)+f(x) (5.46)
∂x ∂x ∂x
∂ ∂f ∂g
(cid:0) (cid:1)
Sumrule: f(x)+g(x) = + (5.47)
∂x ∂x ∂x
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
148 VectorCalculus
∂ ∂ ∂g ∂f
(cid:0) (cid:1)
Chainrule: (g f)(x) = g(f(x)) = (5.48)
∂x ◦ ∂x ∂f ∂x
Thisisonlyan Let us have a closer look at the chain rule. The chain rule (5.48) resem-
intuition,butnot blestosomedegreetherulesformatrixmultiplicationwherewesaidthat
mathematically
neighboringdimensionshavetomatchformatrixmultiplicationtobede-
correctsincethe
fined;seeSection2.2.1.Ifwegofromlefttoright,thechainruleexhibits
partialderivativeis
notafraction. similar properties: ∂f shows up in the “denominator” of the first factor
andinthe“numerator”ofthesecondfactor.Ifwemultiplythefactorsto-
gether,multiplicationisdefined,i.e.,thedimensionsof∂f match,and∂f
“cancels”,suchthat∂g/∂xremains.
5.2.2 Chain Rule
Consider a function f : R2 R of two variables x ,x . Furthermore,
1 2
→
x (t) and x (t) are themselves functions of t. To compute the gradient of
1 2
f withrespecttot,weneedtoapplythechainrule(5.48)formultivariate
functionsas
(cid:34) (cid:35)
df (cid:104) (cid:105) ∂x1(t) ∂f ∂x ∂f ∂x
= ∂f ∂f ∂t = 1 + 2 , (5.49)
dt ∂x1 ∂x2 ∂x2(t) ∂x ∂t ∂x ∂t
∂t 1 2
whereddenotesthegradientand∂ partialderivatives.
Example 5.8
Considerf(x ,x ) = x2+2x ,wherex = sintandx = cost,then
1 2 1 2 1 2
df ∂f ∂x ∂f ∂x
1 2
= + (5.50a)
dt ∂x ∂t ∂x ∂t
1 2
∂sint ∂cost
= 2sint +2 (5.50b)
∂t ∂t
= 2sintcost 2sint = 2sint(cost 1) (5.50c)
− −
isthecorrespondingderivativeoff withrespecttot.
If f(x ,x ) is a function of x and x , where x (s,t) and x (s,t) are
1 2 1 2 1 2
themselves functions of two variables s and t, the chain rule yields the
partialderivatives
∂f ∂f ∂x ∂f ∂x
= 1 + 2 , (5.51)
∂s ∂x ∂s ∂x ∂s
1 2
∂f ∂f ∂x ∂f ∂x
= 1 + 2 , (5.52)
∂t ∂x ∂t ∂x ∂t
1 2
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.3 GradientsofVector-ValuedFunctions 149
andthegradientisobtainedbythematrixmultiplication
∂x ∂x 
1 1
df = ∂f ∂x = (cid:104) ∂f ∂f (cid:105)  ∂s ∂t  . (5.53)
d(s,t) ∂x∂(s,t) ∂x
1
∂x
2
∂x
2
∂x 2
(cid:124) (cid:123)(cid:122) (cid:125)
∂s ∂t
∂f
(cid:124) (cid:123)(cid:122) (cid:125)
= ∂x ∂x
=
∂(s,t)
Thiscompactwayofwritingthechainruleasamatrixmultiplicationonly Thechainrulecan
makes sense if the gradient is defined as a row vector. Otherwise, we will bewrittenasa
matrix
need to start transposing gradients for the matrix dimensions to match.
multiplication.
This may still be straightforward as long as the gradient is a vector or a
matrix;however,whenthegradientbecomesatensor(wewilldiscussthis
inthefollowing),thetransposeisnolongeratriviality.
Remark (Verifying the Correctness of a Gradient Implementation). The
definition of the partial derivatives as the limit of the corresponding dif-
ferencequotient(see(5.39))canbeexploitedwhennumericallychecking
the correctness of gradients in computer programs: When we compute Gradientchecking
gradients and implement them, we can use finite differences to numer-
ically test our computation and implementation: We choose the value h
tobesmall(e.g.,h = 10−4)andcomparethefinite-differenceapproxima-
tionfrom(5.39)withour(analytic)implementationofthegradient.Ifthe
error is small, our gradient implementation is probably correct. “Small”
could mean that
(cid:113)(cid:80)
i(dhi−dfi)2 < 10−6, where dh is the finite-difference
(cid:80) i(dhi+dfi)2 i
approximationanddf istheanalyticgradientoff withrespecttotheith
i
variablex .
i
♢
5.3 Gradients of Vector-Valued Functions
Thus far, we discussed partial derivatives and gradients of functions f :
Rn Rmappingtotherealnumbers.Inthefollowing,wewillgeneralize
→
the concept of the gradient to vector-valued functions (vector fields) f :
Rn Rm,wheren ⩾ 1andm > 1.
→
For a function f : Rn Rm and a vector x = [x ,...,x ]⊤ Rn, the
1 n
→ ∈
correspondingvectoroffunctionvaluesisgivenas
 
f (x)
1
f(x) =   . . .   ∈ Rm. (5.54)
f (x)
m
Writing the vector-valued function in this way allows us to view a vector-
valued function f : Rn Rm as a vector of functions [f ,...,f ]⊤,
1 m
f : Rn R that map on→ to R. The differentiation rules for every f are
i i
→
exactlytheoneswediscussedinSection5.2.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
150 VectorCalculus
Therefore, the partial derivative of a vector-valued function f : Rn
Rm withrespecttox R,i = 1,...n,isgivenasthevector →
i
∈
∂f1   lim f1(x1,...,xi−1,xi+h,xi+1,...xn)−f1(x) 
∂∂ xf =  ∂x . . .i   =   h→0 . . . h   ∈ Rm.
i ∂fm lim fm(x1,...,xi−1,xi+h,xi+1,...xn)−fm(x)
∂xi h→0 h
(5.55)
From (5.40), we know that the gradient of f with respect to a vector is
therowvectorofthepartialderivatives.In(5.55),everypartialderivative
∂f/∂x is itself a column vector. Therefore, we obtain the gradient of f :
i
Rn Rm withrespecttox Rn bycollectingthesepartialderivatives:
→ ∈
(cid:20) (cid:21)
df(x)
= ∂f(x) ∂f(x) (5.56a)
dx ∂x 1 ··· ∂x n
 
∂f (x) ∂f (x)
1 1
 ∂x 1 ··· ∂x n 
=   . . . . . .   ∈ Rm×n. (5.56b)
 
 ∂f (x) ∂f (x) 
m m
∂x 1 ··· ∂x n
Definition 5.6 (Jacobian). The collection of all first-order partial deriva-
Jacobian tivesofavector-valuedfunctionf : Rn Rm iscalledtheJacobian.The
→
Thegradientofa JacobianJ isanm nmatrix,whichwedefineandarrangeasfollows:
×
function (cid:20) (cid:21)
f :Rn→Rmisa J = f = df(x) = ∂f(x) ∂f(x) (5.57)
x
matrixofsize ∇ dx ∂x ··· ∂x
1 n
m×n.  
∂f (x) ∂f (x)
1 1
 ∂x ··· ∂x 
 1 n 
. .
=   . . . .   , (5.58)
 
∂f m(x) ∂f m(x)
∂x ··· ∂x
1 n
 
x
1
x =   . . .   , J(i,j) = ∂∂ xf i . (5.59)
j
x
n
As a special case of (5.58), a function f : Rn R1, which maps a
vector x Rn onto a scalar (e.g., f(x) = (cid:80)n x )→ , possesses a Jacobian
∈ i=1 i
thatisarowvector(matrixofdimension1 n);see(5.40).
×
numeratorlayout Remark. In this book, we use the numerator layout of the derivative, i.e.,
the derivative df/dx of f Rm with respect to x Rn is an m
∈ ∈ ×
n matrix, where the elements of f define the rows and the elements of
x define the columns of the corresponding Jacobian; see (5.58). There
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.3 GradientsofVector-ValuedFunctions 151
Figure5.5 The
determinantofthe
f() Jacobianoff can
·
b c c beusedtocompute
2 1 2
themagnifier
b betweentheblue
1
andorangearea.
existsalsothedenominatorlayout,whichisthetransposeofthenumerator denominatorlayout
layout.Inthisbook,wewillusethenumeratorlayout.
♢
We will see how the Jacobian is used in the change-of-variable method
for probability distributions in Section 6.7. The amount of scaling due to
thetransformationofavariableisprovidedbythedeterminant.
In Section 4.1, we saw that the determinant can be used to compute
the area of a parallelogram. If we are given two vectors b = [1,0]⊤,
1
b = [0,1]⊤ asthesidesoftheunitsquare(blue;seeFigure5.5),thearea
2
ofthissquareis
(cid:12) (cid:18)(cid:20) (cid:21)(cid:19)(cid:12)
(cid:12) 1 0 (cid:12)
(cid:12)det (cid:12) = 1. (5.60)
(cid:12) 0 1 (cid:12)
If we take a parallelogram with the sides c = [ 2,1]⊤, c = [1,1]⊤
1 2
−
(orangeinFigure5.5),itsareaisgivenastheabsolutevalueofthedeter-
minant(seeSection4.1)
(cid:12) (cid:18)(cid:20) (cid:21)(cid:19)(cid:12)
(cid:12) 2 1 (cid:12)
(cid:12)det − (cid:12) = 3 = 3, (5.61)
(cid:12) 1 1 (cid:12) |− |
i.e., the area of this is exactly three times the area of the unit square.
We can find this scaling factor by finding a mapping that transforms the
unit square into the other square. In linear algebra terms, we effectively
perform a variable transformation from (b ,b ) to (c ,c ). In our case,
1 2 1 2
the mapping is linear and the absolute value of the determinant of this
mappinggivesusexactlythescalingfactorwearelookingfor.
We will describe two approaches to identify this mapping. First, we ex-
ploitthatthemappingislinearsothatwecanusethetoolsfromChapter2
to identify this mapping. Second, we will find the mapping using partial
derivativesusingthetoolswehavebeendiscussinginthischapter.
Approach 1 To get started with the linear algebra approach, we
identify both b ,b and c ,c as bases of R2 (see Section 2.6.1 for a
1 2 1 2
{ } { }
recap). What we effectively perform is a change of basis from (b ,b ) to
1 2
(c ,c ),andwearelookingforthetransformationmatrixthatimplements
1 2
thebasischange.UsingresultsfromSection2.7.2,weidentifythedesired
basischangematrixas
(cid:20) (cid:21)
2 1
J = − , (5.62)
1 1
such that Jb = c and Jb = c . The absolute value of the determi-
1 1 2 2
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
152 VectorCalculus
nant of J, which yields the scaling factor we are looking for, is given as
det(J) = 3,i.e.,theareaofthesquarespannedby(c ,c )isthreetimes
1 2
| |
greaterthantheareaspannedby(b ,b ).
1 2
Approach 2 The linear algebra approach works for linear trans-
formations;fornonlineartransformations(whichbecomerelevantinSec-
tion6.7),wefollowamoregeneralapproachusingpartialderivatives.
Forthisapproach,weconsiderafunctionf : R2 R2 thatperformsa
→
variabletransformation.Inourexample,f mapsthecoordinaterepresen-
tation of any vector x R2 with respect to (b ,b ) onto the coordinate
1 2
representation y
R2∈
with respect to (c ,c ). We want to identify the
1 2
∈
mappingsothatwecancomputehowanarea(orvolume)changeswhen
it is being transformed by f. For this, we need to find out how f(x)
changes if we modify x a bit. This question is exactly answered by the
Jacobianmatrix df R2×2.Sincewecanwrite
dx ∈
y = 2x +x (5.63)
1 1 2
−
y = x +x (5.64)
2 1 2
we obtain the functional relationship between x and y, which allows us
togetthepartialderivatives
∂y ∂y ∂y ∂y
1 = 2, 1 = 1, 2 = 1, 2 = 1 (5.65)
∂x − ∂x ∂x ∂x
1 2 1 2
andcomposetheJacobianas
∂y ∂y 
1 1
(cid:20) (cid:21)
∂x ∂x 2 1
J =  1 2 = − . (5.66)
∂y 2 ∂y 2 1 1
∂x ∂x
1 2
Geometrically,the The Jacobian represents the coordinate transformation we are looking
Jacobian for. It is exact if the coordinate transformation is linear (as in our case),
determinantgives
and (5.66) recovers exactly the basis change matrix in (5.62). If the co-
themagnification/
ordinatetransformationisnonlinear,theJacobianapproximatesthisnon-
scalingfactorwhen
wetransforman linear transformation locally with a linear one. The absolute value of the
areaorvolume. Jacobiandeterminant det(J) isthefactorbywhichareasorvolumesare
| |
Jacobian scaledwhencoordinatesaretransformed.Ourcaseyields det(J) = 3.
determinant | |
The Jacobian determinant and variable transformations will become
relevant in Section 6.7 when we transform random variables and prob-
Figure5.6 ability distributions. These transformations are extremely relevant in ma-
Dimensionalityof chine learning in the context of training deep neural networks using the
(partial)derivatives.
reparametrizationtrick,alsocalledinfiniteperturbationanalysis.
x Inthischapter,weencounteredderivativesoffunctions.Figure5.6sum-
f(x) marizes the dimensions of those derivatives. If f : R R the gradient is
simply a scalar (top-left entry). For f : RD R the→ gradient is a 1 D
∂f
∂x row vector (top-right entry). For f : R R→ E, the gradient is an E× 1
columnvector,andforf : RD RE the→ gradientisanE D matrix× .
→ ×
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.3 GradientsofVector-ValuedFunctions 153
Example 5.9 (Gradient of a Vector-Valued Function)
Wearegiven
f(x) = Ax, f(x) RM, A RM×N, x RN .
∈ ∈ ∈
To compute the gradient df/dx we first determine the dimension of
df/dx: Since f : RN RM, it follows that df/dx RM×N. Second,
→ ∈
to compute the gradient we determine the partial derivatives of f with
respecttoeveryx :
j
f (x) =
(cid:88)N
A x =
∂f
i = A (5.67)
i ij j ⇒ ∂x ij
j=1 j
WecollectthepartialderivativesintheJacobianandobtainthegradient
∂f1 ∂f1   A A 
d df
x
=  ∂x . . .1 ··· ∂x . . .N 

= 

. . .11 ··· 1 . . .N 

= A
∈
RM×N . (5.68)
∂fM ∂fM A A
∂x1 ··· ∂xN M1 ··· MN
Example 5.10 (Chain Rule)
Considerthefunctionh : R R,h(t) = (f g)(t)with
→ ◦
f : R2 R (5.69)
→
g : R R2 (5.70)
→
f(x) = exp(x x2), (5.71)
1 2
(cid:20) (cid:21) (cid:20) (cid:21)
x tcost
x = 1 = g(t) = (5.72)
x tsint
2
and compute the gradient of h with respect to t. Since f : R2 R and
g : R R2 wenotethat →
→
∂f ∂g
R1×2, R2×1. (5.73)
∂x ∈ ∂t ∈
Thedesiredgradientiscomputedbyapplyingthechainrule:
∂x 
dh ∂f ∂x (cid:20) ∂f ∂f (cid:21) 1
= =  ∂t  (5.74a)
dt ∂x ∂t ∂x
1
∂x
2
∂x 2
∂t
(cid:20) (cid:21)
(cid:2) (cid:3) cost tsint
= exp(x 1x2 2)x2 2 2exp(x 1x2 2)x 1x 2 sint+− tcost (5.74b)
= exp(x x2)(cid:0) x2(cost tsint)+2x x (sint+tcost)(cid:1) , (5.74c)
1 2 2 − 1 2
wherex = tcostandx = tsint;see(5.72).
1 2
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
154 VectorCalculus
Example 5.11 (Gradient of a Least-Squares Loss in a Linear Model)
Wewilldiscussthis Letusconsiderthelinearmodel
modelinmuch
moredetailin y = Φθ, (5.75)
Chapter9inthe
contextoflinear where θ RD is a parameter vector, Φ RN×D are input features and
regression,where y RN a∈ rethecorrespondingobservatio∈ ns.Wedefinethefunctions
weneedderivatives ∈
oftheleast-squares L(e) := e 2, (5.76)
lossLwithrespect ∥ ∥
totheparametersθ. e(θ) := y Φθ. (5.77)
−
We seek ∂L, and we will use the chain rule for this purpose. L is called a
∂θ
least-squaresloss least-squareslossfunction.
Beforewestartourcalculation,wedeterminethedimensionalityofthe
gradientas
∂L
R1×D. (5.78)
∂θ ∈
Thechainruleallowsustocomputethegradientas
∂L ∂L∂e
= , (5.79)
∂θ ∂e ∂θ
dLdtheta = wherethedthelementisgivenby
np.einsum(
’n,nd’, ∂L (cid:88)N ∂L ∂e
dLde,dedtheta) [1,d] = [n] [n,d]. (5.80)
∂θ ∂e ∂θ
n=1
Weknowthat e 2 = e⊤e(seeSection3.2)anddetermine
∥ ∥
∂L
= 2e⊤ R1×N . (5.81)
∂e ∈
Furthermore,weobtain
∂e
= Φ RN×D, (5.82)
∂θ − ∈
suchthatourdesiredderivativeis
∂L
= 2e⊤Φ (5 =.77) 2(y⊤ θ⊤Φ⊤) Φ R1×D. (5.83)
∂θ − − (cid:124) − (cid:123)(cid:122) (cid:125)(cid:124)(cid:123)(cid:122)(cid:125) ∈
1×N N×D
Remark. Wewouldhaveobtainedthesameresultwithoutusingthechain
rulebyimmediatelylookingatthefunction
L (θ) := y Φθ 2 = (y Φθ)⊤(y Φθ). (5.84)
2
∥ − ∥ − −
This approach is still practical for simple functions like L but becomes
2
impracticalfordeepfunctioncompositions.
♢
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.4 GradientsofMatrices 155
A R4×2 x R3 Figure5.7
∈ ∈
x Visualizationof
1
x gradient
2
x computationofa
3
matrixwithrespect
toavector.Weare
interestedin
computingthe
Partialderivatives:
gradientof
∂A
R4×2 A∈R4×2with
∂x 3 ∈ dA R4×2×3 respecttoavector
∂A R4×2 dx ∈ x∈R3.Weknow
∂x 2 ∈ collate thatgradient
∂A R4×2 dA ∈R4×2×3.We
∂x 1 ∈ fd ox llowtwo
equivalent
approachestoarrive
4
there:(a)collating
3 partialderivatives
intoaJacobian
2 tensor;
(a) Approach 1: We compute the partial derivative (b)flatteningofthe
∂A, ∂A, ∂A,eachofwhichisa4×2matrix,andcol- matrixintoavector,
l∂ ax te1 th∂ ex m2 in∂x a3 4×2×3tensor. computingthe
Jacobianmatrix,
re-shapingintoa
Jacobiantensor.
A R4×2 x R3
∈ ∈
x
1
x
2
x
3
dA˜ dA
R8×3 R4×2×3
A R4×2 A˜ R8 dx ∈ dx ∈
∈ ∈
re-shape gradient re-shape
(b)Approach2:Were-shape(flatten)A∈R4×2intoavec-
torA˜ ∈R8.Then,wecomputethegradient dA˜ ∈R8×3.
dx
Weobtainthegradienttensorbyre-shapingthisgradientas
illustratedabove.
5.4 Gradients of Matrices
Wecanthinkofa
Wewillencountersituationswhereweneedtotakegradientsofmatrices tensorasa
multidimensional
withrespecttovectors(orothermatrices),whichresultsinamultidimen-
array.
sionaltensor.Wecanthinkofthistensorasamultidimensionalarraythat
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
156 VectorCalculus
collectspartialderivatives.Forexample,ifwecomputethegradientofan
m n matrix A with respect to a p q matrix B, the resulting Jacobian
× ×
wouldbe(m n) (p q),i.e.,afour-dimensionaltensorJ,whoseentries
× × ×
aregivenasJ = ∂A /∂B .
ijkl ij kl
Since matrices represent linear mappings, we can exploit the fact that
there is a vector-space isomorphism (linear, invertible mapping) between
the space Rm×n of m n matrices and the space Rmn of mn vectors.
×
Therefore, we can re-shape our matrices into vectors of lengths mn and
pq,respectively.ThegradientusingthesemnvectorsresultsinaJacobian
Matricescanbe of size mn pq. Figure 5.7 visualizes both approaches. In practical ap-
×
transformedinto plications, it is often desirable to re-shape the matrix into a vector and
vectorsbystacking
continue working with this Jacobian matrix: The chain rule (5.48) boils
thecolumnsofthe
down to simple matrix multiplication, whereas in the case of a Jacobian
matrix
(“flattening”). tensor, we will need to pay more attention to what dimensions we need
tosumout.
Example 5.12 (Gradient of Vectors with Respect to Matrices)
Letusconsiderthefollowingexample,where
f = Ax, f RM, A RM×N, x RN (5.85)
∈ ∈ ∈
andwhereweseekthegradientdf/dA.Letusstartagainbydetermining
thedimensionofthegradientas
df
RM×(M×N). (5.86)
dA ∈
Bydefinition,thegradientisthecollectionofthepartialderivatives:
∂f1 
dd Af = 

∂ . . .A 

, ∂∂ Af i
∈
R1×(M×N). (5.87)
∂fM
∂A
Tocomputethepartialderivatives,itwillbehelpfultoexplicitlywriteout
thematrixvectormultiplication:
N
(cid:88)
f = A x , i = 1,...,M , (5.88)
i ij j
j=1
andthepartialderivativesarethengivenas
∂f
i = x . (5.89)
∂A q
iq
This allows us to compute the partial derivatives of f with respect to a
i
rowofA,whichisgivenas
∂f
i = x⊤ R1×1×N , (5.90)
∂A ∈
i,:
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.4 GradientsofMatrices 157
∂f
i = 0⊤ R1×1×N (5.91)
∂A ∈
k̸=i,:
where we have to pay attention to the correct dimensionality. Since f
i
mapsontoRandeachrowofAisofsize1 N,weobtaina1 1 N-
× × ×
sizedtensorasthepartialderivativeoff withrespecttoarowofA.
i
We stack the partial derivatives (5.91) and get the desired gradient
in(5.87)via
0⊤
.
 . 
 . 
 0⊤

∂f i =  x⊤  R1×(M×N). (5.92)
∂A   ∈
0⊤
 
 . . 
 . 
0⊤
Example 5.13 (Gradient of Matrices with Respect to Matrices)
ConsideramatrixR RM×N andf : RM×N RN×N with
∈ →
f(R) = R⊤R =: K RN×N , (5.93)
∈
whereweseekthegradientdK/dR.
To solve this hard problem, let us first write down what we already
know:Thegradienthasthedimensions
dK
R(N×N)×(M×N), (5.94)
dR ∈
whichisatensor.Moreover,
dK
pq R1×M×N (5.95)
dR ∈
for p,q = 1,...,N, where K is the (p,q)th entry of K = f(R). De-
pq
noting the ith column of R by r , every entry of K is given by the dot
i
productoftwocolumnsofR,i.e.,
M
(cid:88)
K = r⊤r = R R . (5.96)
pq p q mp mq
m=1
Whenwenowcomputethepartialderivative ∂Kpq weobtain
∂Rij
∂K
pq =
(cid:88)M ∂
R R = ∂ , (5.97)
∂R ∂R mp mq pqij
ij m=1 ij
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
158 VectorCalculus

R ifj = p, p = q
  iq ̸
 R ifj = q, p = q
∂ = ip ̸ . (5.98)
pqij 2R ifj = p, p = q
 iq

 0 otherwise
From (5.94), we know that the desired gradient has the dimension (N
×
N) (M N), and every single entry of this tensor is given by ∂
pqij
× ×
in(5.98),wherep,q,j = 1,...,N andi = 1,...,M.
5.5 Useful Identities for Computing Gradients
Inthefollowing,welistsomeusefulgradientsthatarefrequentlyrequired
in a machine learning context (Petersen and Pedersen, 2012). Here, we
use tr( ) as the trace (see Definition 4.4), det( ) as the determinant (see
· ·
Section4.1)andf(X)−1 astheinverseoff(X),assumingitexists.
∂ (cid:18)∂f(X)(cid:19)⊤
f(X)⊤ = (5.99)
∂X ∂X
∂ (cid:18)∂f(X)(cid:19)
tr(f(X)) = tr (5.100)
∂X ∂X
∂ (cid:18) ∂f(X)(cid:19)
det(f(X)) = det(f(X))tr f(X)−1 (5.101)
∂X ∂X
∂ ∂f(X)
f(X)−1 = f(X)−1 f(X)−1 (5.102)
∂X − ∂X
∂a⊤X−1b
= (X−1)⊤ab⊤(X−1)⊤ (5.103)
∂X −
∂x⊤a
= a⊤ (5.104)
∂x
∂a⊤x
= a⊤ (5.105)
∂x
∂a⊤Xb
= ab⊤ (5.106)
∂X
∂x⊤Bx
= x⊤(B+B⊤) (5.107)
∂x
∂
(x As)⊤W(x As) = 2(x As)⊤WA forsymmetricW
∂s − − − −
(5.108)
Remark. In this book, we only cover traces and transposes of matrices.
However, we have seen that derivatives can be higher-dimensional ten-
sors,inwhichcasetheusualtraceandtransposearenotdefined.Inthese
cases,thetraceofaD D E F tensorwouldbeanE F-dimensional
× × × ×
matrix. This is a special case of a tensor contraction. Similarly, when we
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.6 BackpropagationandAutomaticDifferentiation 159
“transpose” a tensor, we mean swapping the first two dimensions. Specif-
ically, in (5.99) through (5.102), we require tensor-related computations
when we work with multivariate functions f( ) and compute derivatives
·
withrespecttomatrices(andchoosenottovectorizethemasdiscussedin
Section5.4).
♢
5.6 Backpropagation and Automatic Differentiation
Agooddiscussion
In many machine learning applications, we find good model parameters about
by performing gradient descent (Section 7.1), which relies on the fact backpropagation
andthechainruleis
that we can compute the gradient of a learning objective with respect
availableatablog
to the parameters of the model. For a given objective function, we can
byTimVieiraat
obtain the gradient with respect to the model parameters using calculus https://tinyurl.
and applying the chain rule; see Section 5.2.2. We already had a taste in com/ycfm2yrw.
Section5.3whenwelookedatthegradientofasquaredlosswithrespect
totheparametersofalinearregressionmodel.
Considerthefunction
(cid:113)
f(x) =
x2+exp(x2)+cos(cid:0) x2+exp(x2)(cid:1)
. (5.109)
By application of the chain rule, and noting that differentiation is linear,
wecomputethegradient
df
=
2x+2xexp(x2)
sin(cid:0) x2+exp(x2)(cid:1)(cid:0) 2x+2xexp(x2)(cid:1)
(cid:112)
dx 2 x2+exp(x2) −
(cid:32) (cid:33)
= 2x
1 sin(cid:0) x2+exp(x2)(cid:1) (cid:0) 1+exp(x2)(cid:1)
.
(cid:112)
2 x2+exp(x2) −
(5.110)
Writing out the gradient in this explicit way is often impractical since it
often results in a very lengthy expression for a derivative. In practice,
it means that, if we are not careful, the implementation of the gradient
couldbesignificantlymoreexpensivethancomputingthefunction,which
imposes unnecessary overhead. For training deep neural network mod-
els, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, backpropagation
1962;Rumelhartetal.,1986)isanefficientwaytocomputethegradient
ofanerrorfunctionwithrespecttotheparametersofthemodel.
5.6.1 Gradients in a Deep Network
Anareawherethechainruleisusedtoanextremeisdeeplearning,where
thefunctionvaluey iscomputedasamany-levelfunctioncomposition
y = (f f f )(x) = f (f ( (f (x)) )), (5.111)
K K−1 1 K K−1 1
◦ ◦···◦ ··· ···
where x are the inputs (e.g., images), y are the observations (e.g., class
labels),andeveryfunctionf ,i = 1,...,K,possessesitsownparameters.
i
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
160 VectorCalculus
Figure5.8 Forward
passinamulti-layer
neuralnetworkto x f 1 f K−1 f K L
computethelossL
asafunctionofthe
inputsxandthe
parametersAi, bi. A0,b0 A1,b1 AK−2,bK−2 AK−1,bK−1
Wediscussthecase, In neural networks with multiple layers, we have functions f i(x i−1) =
wheretheactivation σ(A x +b )intheithlayer.Herex istheoutputoflayeri 1
i−1 i−1 i−1 i−1
functionsare andσ anactivationfunction,suchasthelogisticsigmoid 1 ,tanho− ra
identicalineach 1+e−x
rectifiedlinearunit(ReLU).Inordertotrainthesemodels,werequirethe
layertounclutter
notation. gradient of a loss function L with respect to all model parameters A j,b j
for j = 1,...,K. This also requires us to compute the gradient of L with
respect to the inputs of each layer. For example, if we have inputs x and
observationsy andanetworkstructuredefinedby
f := x (5.112)
0
f := σ (A f +b ), i = 1,...,K, (5.113)
i i i−1 i−1 i−1
see also Figure 5.8 for a visualization, we may be interested in finding
A ,b forj = 0,...,K 1,suchthatthesquaredloss
j j
−
L(θ) = y f (θ,x) 2 (5.114)
∥ − K ∥
isminimized,whereθ = A ,b ,...,A ,b .
0 0 K−1 K−1
{ }
Toobtainthegradientswithrespecttotheparametersetθ,werequire
the partial derivatives of L with respect to the parameters θ = A ,b
j j j
{ }
ofeachlayerj = 0,...,K 1.Thechainruleallowsustodeterminethe
−
Amorein-depth partialderivativesas
discussionabout
∂L ∂L ∂f
gradientsofneural = K (5.115)
networkscanbe ∂θ ∂f ∂θ
K−1 K K−1
foundinJustin
Domke’slecture ∂L ∂L ∂f ∂f
= K K−1 (5.116)
notes ∂θ ∂f ∂f ∂θ
https://tinyurl. K−2 K K−1 K−2
com/yalcxgtv.
∂L ∂L ∂f ∂f ∂f
= K K−1 K−2 (5.117)
∂θ ∂f ∂f ∂f ∂θ
K−3 K K−1 K−2 K−3
∂L ∂L ∂f ∂f ∂f
= K i+2 i+1 (5.118)
∂θ ∂f ∂f ··· ∂f ∂θ
i K K−1 i+1 i
The orange terms are partial derivatives of the output of a layer with
respect to its inputs, whereas the blue terms are partial derivatives of
the output of a layer with respect to its parameters. Assuming, we have
alreadycomputedthepartialderivatives∂L/∂θ ,thenmostofthecom-
i+1
putationcanbereusedtocompute∂L/∂θ .Theadditionaltermsthatwe
i
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.6 BackpropagationandAutomaticDifferentiation 161
Figure5.9
Backwardpassina
x f 1 f K−1 f K L multi-layerneural
networktocompute
thegradientsofthe
lossfunction.
A0,b0 A1,b1 AK−2,bK−2 AK−1,bK−1
x a b y Figure5.10 Simple
graphillustrating
theflowofdata
fromxtoyvia
needtocomputeareindicatedbytheboxes.Figure5.9visualizesthatthe someintermediate
gradientsarepassedbackwardthroughthenetwork. variablesa,b.
5.6.2 Automatic Differentiation
It turns out that backpropagation is a special case of a general technique
innumericalanalysiscalledautomaticdifferentiation.Wecanthinkofau- automatic
tomaticdifferentationasasetoftechniquestonumerically(incontrastto differentiation
symbolically) evaluate the exact (up to machine precision) gradient of a
function by working with intermediate variables and applying the chain
rule. Automatic differentiation applies a series of elementary arithmetic Automatic
operations, e.g., addition and multiplication and elementary functions, differentiationis
e.g., sin,cos,exp,log. By applying the chain rule to these operations, the differentfrom
symbolic
gradient of quite complicated functions can be computed automatically.
differentiationand
Automatic differentiation applies to general computer programs and has numerical
forwardandreversemodes.Baydinetal.(2018)giveagreatoverviewof approximationsof
automaticdifferentiationinmachinelearning. thegradient,e.g.,by
usingfinite
Figure 5.10 shows a simple graph representing the data flow from in-
differences.
puts x to outputs y via some intermediate variables a,b. If we were to
computethederivativedy/dx,wewouldapplythechainruleandobtain
dy dy db da
= . (5.119)
dx dbdadx
Intuitively, the forward and reverse mode differ in the order of multipli- Inthegeneralcase,
cation. Due to the associativity of matrix multiplication, we can choose weworkwith
Jacobians,which
between
canbevectors,
dy (cid:18) dy db(cid:19) da matrices,ortensors.
= , (5.120)
dx dbda dx
dy dy (cid:18) db da(cid:19)
= . (5.121)
dx db dadx
Equation (5.120) would be the reverse mode because gradients are prop- reversemode
agated backward through the graph, i.e., reverse to the data flow. Equa-
tion (5.121) would be the forward mode, where the gradients flow with forwardmode
thedatafromlefttorightthroughthegraph.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
162 VectorCalculus
In the following, we will focus on reverse mode automatic differentia-
tion, which is backpropagation. In the context of neural networks, where
the input dimensionality is often much higher than the dimensionality of
thelabels,thereversemodeiscomputationallysignificantlycheaperthan
theforwardmode.Letusstartwithaninstructiveexample.
Example 5.14
Considerthefunction
(cid:113)
f(x) =
x2+exp(x2)+cos(cid:0) x2+exp(x2)(cid:1)
(5.122)
from (5.109). If we were to implement a function f on a computer, we
intermediate wouldbeabletosavesomecomputationbyusing intermediatevariables:
variables
a = x2, (5.123)
b = exp(a), (5.124)
c = a+b, (5.125)
d = √c, (5.126)
e = cos(c), (5.127)
f = d+e. (5.128)
Figure5.11 exp() b √ d
· ·
Computationgraph
withinputsx, x ()2 a + c + f
functionvaluesf, ·
andintermediate
variablesa,b,c,d,e. cos() e
·
This is the same kind of thinking process that occurs when applying
the chain rule. Note that the preceding set of equations requires fewer
operations than a direct implementation of the function f(x) as defined
in (5.109). The corresponding computation graph in Figure 5.11 shows
the flow of data and computations required to obtain the function value
f.
Thesetofequationsthatincludeintermediatevariablescanbethought
of as a computation graph, a representation that is widely used in imple-
mentationsofneuralnetworksoftwarelibraries.Wecandirectlycompute
the derivatives of the intermediate variables with respect to their corre-
spondinginputsbyrecallingthedefinitionofthederivativeofelementary
functions.Weobtainthefollowing:
∂a
= 2x (5.129)
∂x
∂b
= exp(a) (5.130)
∂a
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.6 BackpropagationandAutomaticDifferentiation 163
∂c ∂c
= 1 = (5.131)
∂a ∂b
∂d 1
= (5.132)
∂c 2√c
∂e
= sin(c) (5.133)
∂c −
∂f ∂f
= 1 = . (5.134)
∂d ∂e
By looking at the computation graph in Figure 5.11, we can compute
∂f/∂xbyworkingbackwardfromtheoutputandobtain
∂f ∂f ∂d ∂f ∂e
= + (5.135)
∂c ∂d ∂c ∂e ∂c
∂f ∂f ∂c
= (5.136)
∂b ∂c ∂b
∂f ∂f ∂b ∂f ∂c
= + (5.137)
∂a ∂b ∂a ∂c ∂a
∂f ∂f ∂a
= . (5.138)
∂x ∂a∂x
Notethatweimplicitlyappliedthechainruletoobtain∂f/∂x.Bysubsti-
tutingtheresultsofthederivativesoftheelementaryfunctions,weget
∂f 1
= 1 +1 ( sin(c)) (5.139)
∂c · 2√c · −
∂f ∂f
= 1 (5.140)
∂b ∂c ·
∂f ∂f ∂f
= exp(a)+ 1 (5.141)
∂a ∂b ∂c ·
∂f ∂f
= 2x. (5.142)
∂x ∂a ·
By thinking of each of the derivatives above as a variable, we observe
that the computation required for calculating the derivative is of similar
complexityasthecomputationofthefunctionitself.Thisisquitecounter-
intuitive since the mathematical expression for the derivative ∂f (5.110)
∂x
issignificantlymorecomplicatedthanthemathematicalexpressionofthe
functionf(x)in(5.109).
AutomaticdifferentiationisaformalizationofExample5.14.Letx ,...,x
1 d
betheinputvariablestothefunction,x ,...,x betheintermediate
d+1 D−1
variables,andx theoutputvariable.Thenthecomputationgraphcanbe
D
expressedasfollows:
Fori = d+1,...,D : x = g (x ), (5.143)
i i Pa(xi)
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
164 VectorCalculus
wheretheg ( )areelementaryfunctionsandx aretheparentnodes
i
·
Pa(xi)
of the variable x in the graph. Given a function defined in this way, we
i
canusethechainruletocomputethederivativeofthefunctioninastep-
by-stepfashion.Recallthatbydefinitionf = x andhence
D
∂f
= 1. (5.144)
∂x
D
Forothervariablesx ,weapplythechainrule
i
∂f = (cid:88) ∂f ∂x j = (cid:88) ∂f ∂g j , (5.145)
∂x ∂x ∂x ∂x ∂x
i j i j i
xj:xi∈Pa(xj) xj:xi∈Pa(xj)
where Pa(x ) is the set of parent nodes of x in the computation graph.
j j
Auto-differentiation Equation(5.143)istheforwardpropagationofafunction,whereas(5.145)
inreversemode is the backpropagation of the gradient through the computation graph.
requiresaparse
Forneuralnetworktraining,webackpropagatetheerroroftheprediction
tree.
withrespecttothelabel.
Theautomaticdifferentiationapproachaboveworkswheneverwehave
a function that can be expressed as a computation graph, where the ele-
mentaryfunctionsaredifferentiable.Infact,thefunctionmaynotevenbe
a mathematical function but a computer program. However, not all com-
puterprogramscanbeautomaticallydifferentiated,e.g.,ifwecannotfind
differential elementary functions. Programming structures, such as for
loopsandifstatements,requiremorecareaswell.
5.7 Higher-Order Derivatives
So far, we have discussed gradients, i.e., first-order derivatives. Some-
times,weareinterestedinderivativesofhigherorder,e.g.,whenwewant
to use Newton’s Method for optimization, which requires second-order
derivatives (Nocedal and Wright, 2006). In Section 5.1.1, we discussed
the Taylor series to approximate functions using polynomials. In the mul-
tivariate case, we can do exactly the same. In the following, we will do
exactlythis.Butletusstartwithsomenotation.
Consider a function f : R2 R of two variables x,y. We use the
→
followingnotationforhigher-orderpartialderivatives(andforgradients):
∂2f isthesecondpartialderivativeoff withrespecttox.
∂x2
∂nf isthenthpartialderivativeoff withrespecttox.
∂xn
∂2f = ∂ (cid:0)∂f(cid:1) is the partial derivative obtained by first partial differ-
∂y∂x ∂y ∂x
entiatingwithrespecttoxandthenwithrespecttoy.
∂2f
is the partial derivative obtained by first partial differentiating by
∂x∂y
y andthenx.
Hessian TheHessianisthecollectionofallsecond-orderpartialderivatives.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.8 LinearizationandMultivariateTaylorSeries 165
Figure5.12 Linear
approximationofa
function.The
1 f(x) originalfunctionf
islinearizedat
0
x0=−2usinga
first-orderTaylor
seriesexpansion.
−1 f(x 0) f(x 0)+f 0(x 0)(x −x 0)
2
−
4 2 0 2 4
− −
x
Iff(x,y)isatwice(continuously)differentiablefunction,then
∂2f ∂2f
= , (5.146)
∂x∂y ∂y∂x
i.e., the order of differentiation does not matter, and the corresponding
Hessianmatrix Hessianmatrix
 ∂2f ∂2f 
 ∂x2 ∂x∂y
H =   (5.147)

∂2f ∂2f

∂x∂y ∂y2
issymmetric.TheHessianisdenotedas 2 f(x,y).Generally,forx Rn
and f : Rn R, the Hessian is an n ∇ nx,y matrix. The Hessian mea∈ sures
→ ×
thecurvatureofthefunctionlocallyaround(x,y).
Remark (Hessian of a Vector Field). If f : Rn Rm is a vector field, the
→
Hessianisan(m n n)-tensor.
× × ♢
5.8 Linearization and Multivariate Taylor Series
Thegradient f ofafunctionf isoftenusedforalocallylinearapproxi-
∇
mationoff aroundx :
0
f(x) f(x )+( f)(x )(x x ). (5.148)
0 x 0 0
≈ ∇ −
Here ( f)(x ) is the gradient of f with respect to x, evaluated at x .
x 0 0
∇
Figure5.12illustratesthelinearapproximationofafunctionf ataninput
x . The original function is approximated by a straight line. This approx-
0
imation is locally accurate, but the farther we move away from x the
0
worsetheapproximationgets.Equation(5.148)isaspecialcaseofamul-
tivariate Taylor series expansion of f at x , where we consider only the
0
first two terms. We discuss the more general case in the following, which
willallowforbetterapproximations.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
)x(f
166 VectorCalculus
Figure5.13
Visualizingouter
products.Outer
productsofvectors
increasethe
dimensionalityof
thearrayby1per
term.(a)Theouter (a)Givenavectorδ∈R4,weobtaintheouterproductδ2:=δ⊗δ=δδ⊤∈
productoftwo R4×4asamatrix.
vectorsresultsina
matrix;(b)the
outerproductof
threevectorsyields
athird-ordertensor.
(b) An outer product δ3 := δ⊗δ⊗δ ∈ R4×4×4 results in a third-order tensor (“three-
dimensionalmatrix”),i.e.,anarraywiththreeindexes.
Definition 5.7 (MultivariateTaylorSeries). Weconsiderafunction
f : RD R (5.149)
→
x f(x), x RD, (5.150)
(cid:55)→ ∈
that is smooth at x . When we define the difference vector δ := x x ,
0 0
−
multivariateTaylor themultivariateTaylorseriesoff at(x 0)isdefinedas
series
f(x) =
(cid:88)∞ D xkf(x 0)
δk, (5.151)
k!
k=0
where Dkf(x ) is the k-th (total) derivative of f with respect to x, eval-
x 0
uatedatx .
0
Taylorpolynomial Definition 5.8 (TaylorPolynomial). TheTaylorpolynomialofdegreenof
f atx containsthefirstn+1componentsoftheseriesin(5.151)andis
0
definedas
T (x) =
(cid:88)n D xkf(x 0)
δk. (5.152)
n k!
k=0
In (5.151) and (5.152), we used the slightly sloppy notation of δk,
which is not defined for vectors x RD, D > 1, and k > 1. Note that
Avectorcanbe both Dkf and δk are k-th order te∈ nsors, i.e., k-dimensional arrays. The
x
implementedasa ktimes
one-dimensional kth-order tensor δk R(cid:122) D×D(cid:125) ×(cid:124) ...×D(cid:123) is obtained as a k-fold outer product,
a twrr oa -y d, ia mm ena st ir oix naa lsa denotedby ,ofthe∈ vectorδ RD.Forexample,
⊗ ∈
array.
δ2 := δ δ = δδ⊤, δ2[i,j] = δ[i]δ[j] (5.153)
⊗
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.8 LinearizationandMultivariateTaylorSeries 167
δ3 := δ δ δ, δ3[i,j,k] = δ[i]δ[j]δ[k]. (5.154)
⊗ ⊗
Figure 5.13 visualizes two such outer products. In general, we obtain the
terms
D D
(cid:88) (cid:88)
Dkf(x )δk = Dkf(x )[i ,...,i ]δ[i ] δ[i ] (5.155)
x 0 ··· x 0 1 k 1 ··· k
i1=1 ik=1
intheTaylorseries,whereDkf(x )δk containsk-thorderpolynomials.
x 0
Now that we defined the Taylor series for vector fields, let us explicitly
write down the first terms Dkf(x )δk of the Taylor series expansion for
x 0
k = 0,...,3andδ := x x :
0
− np.einsum(
k = 0 : D0f(x )δ0 = f(x ) R (5.156) ’i,i’,Df1,d)
x 0 0 ∈
np.einsum(
D
(cid:88) ’ij,i,j’,
k = 1 : D1f(x )δ1 = f(x ) δ = f(x )[i]δ[i] R (5.157)
x 0 (cid:124)∇x (cid:123)(cid:122) 0 (cid:125)(cid:124)(cid:123)(cid:122)(cid:125) ∇x 0 ∈ Df2,d,d)
1×D D×1 i=1 np.einsum(
k = 2 : D2f(x )δ2 = tr(cid:0) H(x ) δ δ⊤ (cid:1) = δ⊤H(x )δ (5.158) ’ijk,i,j,k’,
x 0 (cid:124) (cid:123)(cid:122)0 (cid:125)(cid:124)(cid:123)(cid:122)(cid:125)(cid:124)(cid:123)(cid:122)(cid:125) 0 Df3,d,d,d)
D×D D×1 1×D
D D
(cid:88)(cid:88)
= H[i,j]δ[i]δ[j] R (5.159)
∈
i=1 j=1
D D D
(cid:88)(cid:88)(cid:88)
k = 3 : D3f(x )δ3 = D3f(x )[i,j,k]δ[i]δ[j]δ[k] R
x 0 x 0 ∈
i=1 j=1k=1
(5.160)
Here,H(x )istheHessianoff evaluatedatx .
0 0
Example 5.15 (Taylor Series Expansion of a Function with Two Vari-
ables)
Considerthefunction
f(x,y) = x2+2xy+y3. (5.161)
We want to compute the Taylor series expansion of f at (x ,y ) = (1,2).
0 0
Before we start, let us discuss what to expect: The function in (5.161) is
a polynomial of degree 3. We are looking for a Taylor series expansion,
which itself is a linear combination of polynomials. Therefore, we do not
expect the Taylor series expansion to contain terms of fourth or higher
order to express a third-order polynomial. This means that it should be
sufficienttodeterminethefirstfourtermsof(5.151)foranexactalterna-
tiverepresentationof(5.161).
To determine the Taylor series expansion, we start with the constant
termandthefirst-orderderivatives,whicharegivenby
f(1,2) = 13 (5.162)
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
168 VectorCalculus
∂f ∂f
= 2x+2y = (1,2) = 6 (5.163)
∂x ⇒ ∂x
∂f ∂f
= 2x+3y2 = (1,2) = 14. (5.164)
∂y ⇒ ∂y
Therefore,weobtain
(cid:104) (cid:105)
D1 f(1,2) = f(1,2) = ∂f(1,2) ∂f(1,2) = (cid:2) 6 14(cid:3) R1×2
x,y ∇x,y ∂x ∂y ∈
(5.165)
suchthat
D x1 ,yf(1,2)
δ =
(cid:2)
6
14(cid:3)(cid:20) x −1(cid:21)
= 6(x 1)+14(y 2). (5.166)
1! y 2 − −
−
Note that D1 f(1,2)δ contains only linear terms, i.e., first-order polyno-
x,y
mials.
Thesecond-orderpartialderivativesaregivenby
∂2f ∂2f
= 2 = (1,2) = 2 (5.167)
∂x2 ⇒ ∂x2
∂2f ∂2f
= 6y = (1,2) = 12 (5.168)
∂y2 ⇒ ∂y2
∂2f ∂2f
= 2 = (1,2) = 2 (5.169)
∂y∂x ⇒ ∂y∂x
∂2f ∂2f
= 2 = (1,2) = 2. (5.170)
∂x∂y ⇒ ∂x∂y
When we collect the second-order partial derivatives, we obtain the Hes-
sian
(cid:34) ∂2f ∂2f (cid:35) (cid:20) (cid:21)
2 2
H = ∂x2 ∂x∂y = , (5.171)
∂2f ∂2f 2 6y
∂y∂x ∂y2
suchthat
(cid:20) (cid:21)
2 2
H(1,2) = R2×2. (5.172)
2 12 ∈
Therefore,thenexttermoftheTaylor-seriesexpansionisgivenby
D2 f(1,2) 1
x,y δ2 = δ⊤H(1,2)δ (5.173a)
2! 2
(cid:20) (cid:21)(cid:20) (cid:21)
1 (cid:2) (cid:3) 2 2 x 1
= x 1 y 2 − (5.173b)
2 − − 2 12 y 2
−
= (x 1)2+2(x 1)(y 2)+6(y 2)2. (5.173c)
− − − −
Here,D2 f(1,2)δ2containsonlyquadraticterms,i.e.,second-orderpoly-
x,y
nomials.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
5.8 LinearizationandMultivariateTaylorSeries 169
Thethird-orderderivativesareobtainedas
(cid:104) (cid:105)
D3 f = ∂H ∂H R2×2×2, (5.174)
x,y ∂x ∂y ∈
∂H
(cid:34) ∂3f ∂3f (cid:35)
D3 f[:,:,1] = = ∂x3 ∂x2∂y , (5.175)
x,y ∂x ∂3f ∂3f
∂x∂y∂x ∂x∂y2
∂H
(cid:34) ∂3f ∂3f (cid:35)
D3 f[:,:,2] = = ∂y∂x2 ∂y∂x∂y . (5.176)
x,y ∂y ∂3f ∂3f
∂y2∂x ∂y3
Since most second-order partial derivatives in the Hessian in (5.171) are
constant,theonlynonzerothird-orderpartialderivativeis
∂3f ∂3f
= 6 = (1,2) = 6. (5.177)
∂y3 ⇒ ∂y3
Higher-order derivatives and the mixed derivatives of degree 3 (e.g.,
∂f3
)vanish,suchthat
∂x2∂y
(cid:20) (cid:21) (cid:20) (cid:21)
0 0 0 0
D3 f[:,:,1] = , D3 f[:,:,2] = (5.178)
x,y 0 0 x,y 0 6
and
D3 f(1,2)
x,y δ3 = (y 2)3, (5.179)
3! −
which collects all cubic terms of the Taylor series. Overall, the (exact)
Taylorseriesexpansionoff at(x ,y ) = (1,2)is
0 0
D2 f(1,2) D3 f(1,2)
f(x) =f(1,2)+D1 f(1,2)δ+ x,y δ2+ x,y δ3
x,y 2! 3!
(5.180a)
∂f(1,2) ∂f(1,2)
=f(1,2)+ (x 1)+ (y 2)
∂x − ∂y −
1 (cid:18)∂2f(1,2) ∂2f(1,2)
+ (x 1)2+ (y 2)2
2! ∂x2 − ∂y2 −
∂2f(1,2) (cid:19) 1∂3f(1,2)
+2 (x 1)(y 2) + (y 2)3 (5.180b)
∂x∂y − − 6 ∂y3 −
=13+6(x 1)+14(y 2)
− −
+(x 1)2+6(y 2)2+2(x 1)(y 2)+(y 2)3. (5.180c)
− − − − −
In this case, we obtained an exact Taylor series expansion of the polyno-
mialin(5.161),i.e.,thepolynomialin(5.180c)isidenticaltotheoriginal
polynomial in (5.161). In this particular example, this result is not sur-
prising since the original function was a third-order polynomial, which
we expressed through a linear combination of constant terms, first-order,
second-order,andthird-orderpolynomialsin(5.180c).
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
170 VectorCalculus
5.9 Further Reading
Further details of matrix differentials, along with a short review of the
required linear algebra, can be found in Magnus and Neudecker (2007).
Automaticdifferentiationhashadalonghistory,andwerefertoGriewank
and Walther (2003), Griewank and Walther (2008), and Elliott (2009)
andthereferencestherein.
In machine learning (and other disciplines), we often need to compute
expectations,i.e.,weneedtosolveintegralsoftheform
(cid:90)
E [f(x)] = f(x)p(x)dx. (5.181)
x
Even if p(x) is in a convenient form (e.g., Gaussian), this integral gen-
erally cannot be solved analytically. The Taylor series expansion of f is
(cid:0) (cid:1)
one way of finding an approximate solution: Assuming p(x) = µ, Σ
N
is Gaussian, then the first-order Taylor series expansion around µ locally
linearizes the nonlinear function f. For linear functions, we can compute
themean(andthecovariance)exactlyifp(x)isGaussiandistributed(see
extendedKalman Section 6.5). This property is heavily exploited by the extended Kalman
filter filter (Maybeck, 1979) for online state estimation in nonlinear dynami-
cal systems (also called “state-space models”). Other deterministic ways
unscentedtransform toapproximatetheintegralin(5.181)aretheunscentedtransform(Julier
Laplace andUhlmann,1997),whichdoesnotrequireanygradients,ortheLaplace
approximation approximation(MacKay,2003;Bishop,2006;Murphy,2012),whichuses
asecond-orderTaylorseriesexpansion(requiringtheHessian)foralocal
Gaussianapproximationofp(x)arounditsmode.
Exercises
5.1 Computethederivativef′(x)for
f(x)=log(x4)sin(x3).
5.2 Computethederivativef′(x)ofthelogisticsigmoid
1
f(x)= .
1+exp(−x)
5.3 Computethederivativef′(x)ofthefunction
f(x)=exp(− 1 (x−µ)2),
2σ2
whereµ, σ∈Rareconstants.
5.4 Compute the Taylor polynomials Tn, n = 0,...,5 of f(x) = sin(x)+cos(x)
atx =0.
0
5.5 Considerthefollowingfunctions:
f (x)=sin(x )cos(x ), x∈R2
1 1 2
f (x,y)=x⊤y, x,y∈Rn
2
f (x)=xx⊤, x∈Rn
3
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Exercises 171
a. Whatarethedimensionsof ∂fi ?
∂x
b. ComputetheJacobians.
5.6 Differentiatef withrespecttotandgwithrespecttoX,where
f(t)=sin(log(t⊤t)), t∈RD
g(X)=tr(AXB), A∈RD×E,X ∈RE×F,B∈RF×D,
wheretr(·)denotesthetrace.
5.7 Computethederivativesdf/dxofthefollowingfunctionsbyusingthechain
rule.Providethedimensionsofeverysinglepartialderivative.Describeyour
stepsindetail.
a.
f(z)=log(1+z), z=x⊤x, x∈RD
b.
f(z)=sin(z), z=Ax+b, A∈RE×D,x∈RD,b∈RE
wheresin(·)isappliedtoeveryelementofz.
5.8 Compute the derivatives df/dx of the following functions. Describe your
stepsindetail.
a. Usethechainrule.Providethedimensionsofeverysinglepartialderiva-
tive.
f(z)=exp(−1z)
2
z=g(y)=y⊤S−1y
y=h(x)=x−µ
wherex,µ∈RD,S ∈RD×D.
b.
f(x)=tr(xx⊤+σ2I), x∈RD
Heretr(A)isthetraceofA,i.e.,thesumofthediagonalelementsA .
ii
Hint:Explicitlywriteouttheouterproduct.
c. Usethechainrule.Providethedimensionsofeverysinglepartialderiva-
tive.Youdonotneedtocomputetheproductofthepartialderivatives
explicitly.
f =tanh(z)∈RM
z=Ax+b, x∈RN,A∈RM×N,b∈RM.
Here,tanhisappliedtoeverycomponentofz.
5.9 Wedefine
g(x,z,ν):=logp(x,z)−logq(z,ν)
z:=t(ϵ,ν)
fordifferentiablefunctionsp,q,tandx∈RD,z ∈RE,ν ∈RF,ϵ∈RG.By
usingthechainrule,computethegradient
d
g(x,z,ν).
dν
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
6
Probability and Distributions
Probability, loosely speaking, concerns the study of uncertainty. Probabil-
itycanbethoughtofasthefractionoftimesaneventoccurs,orasadegree
ofbeliefaboutanevent.Wethenwouldliketousethisprobabilitytomea-
sure the chance of something occurring in an experiment. As mentioned
inChapter1,weoftenquantifyuncertaintyinthedata,uncertaintyinthe
machine learning model, and uncertainty in the predictions produced by
randomvariable themodel.Quantifyinguncertaintyrequirestheideaofarandomvariable,
whichisafunctionthatmapsoutcomesofrandomexperimentstoasetof
properties that we are interested in. Associated with the random variable
is a function that measures the probability that a particular outcome (or
probability setofoutcomes)willoccur;thisiscalledtheprobabilitydistribution.
distribution Probability distributions are used as a building block for other con-
cepts,suchasprobabilisticmodeling(Section8.4),graphicalmodels(Sec-
tion8.5),andmodelselection(Section8.6).Inthenextsection,wepresent
the three concepts that define a probability space (the sample space, the
events, and the probability of an event) and how they are related to a
fourth concept called the random variable. The presentation is deliber-
ately slightly hand wavy since a rigorous presentation may occlude the
intuitionbehindtheconcepts.Anoutlineoftheconceptspresentedinthis
chapterareshowninFigure6.1.
6.1 Construction of a Probability Space
The theory of probability aims at defining a mathematical structure to
describe random outcomes of experiments. For example, when tossing a
singlecoin,wecannotdeterminetheoutcome,butbydoingalargenum-
ber of coin tosses, we can observe a regularity in the average outcome.
Using this mathematical structure of probability, the goal is to perform
automated reasoning, and in this sense, probability generalizes logical
reasoning(Jaynes,2003).
6.1.1 Philosophical Issues
When constructing automated reasoning systems, classical Boolean logic
doesnotallowustoexpresscertainformsofplausiblereasoning.Consider
172
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
6.1 ConstructionofaProbabilitySpace 173
Figure6.1 Amind
Mean Variance Bayes’Theorem
mapoftheconcepts
relatedtorandom
variablesand
probability
distributions,as
Chapter9 describedinthis
Summarystatistics Productrule Sumrule Regression
chapter.
Property
Example
Randomvariable
Transformations Gaussian
&distribution
Chapter10
Exa
Dimensionality mple
reduction
Independence
arity
Bernoulli
mil
Si Sufficientstatistics
Chapter11
Densityestimation
Innerproduct Beta
Exponentialfamily
the following scenario: We observe that A is false. We find B becomes
less plausible, although no conclusion can be drawn from classical logic.
We observe that B is true. It seems A becomes more plausible. We use
this form of reasoning daily. We are waiting for a friend, and consider
threepossibilities:H1,sheisontime;H2,shehasbeendelayedbytraffic;
and H3, she has been abducted by aliens. When we observe our friend
is late, we must logically rule out H1. We also tend to consider H2 to be
morelikely,thoughwearenotlogicallyrequiredtodoso.Finally,wemay
consider H3 to be possible, but we continue to consider it quite unlikely.
How do we conclude H2 is the most plausible answer? Seen in this way, “Forplausible
probabilitytheorycanbeconsideredageneralizationofBooleanlogic.In reasoningitis
necessarytoextend
thecontextofmachinelearning,itisoftenappliedinthiswaytoformalize
thediscretetrueand
thedesignofautomatedreasoningsystems.Furtherargumentsabouthow
falsevaluesoftruth
probability theory is the foundation of reasoning systems can be found tocontinuous
inPearl(1988). plausibilities”
The philosophical basis of probability and how it should be somehow (Jaynes,2003).
relatedtowhatwethinkshouldbetrue(inthelogicalsense)wasstudied
by Cox (Jaynes, 2003). Another way to think about it is that if we are
precise about our common sense we end up constructing probabilities.
E. T. Jaynes (1922–1998) identified three mathematical criteria, which
mustapplytoallplausibilities:
1. Thedegreesofplausibilityarerepresentedbyrealnumbers.
2. Thesenumbersmustbebasedontherulesofcommonsense.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
Property
Finite
Conjugate
174 ProbabilityandDistributions
3. The resulting reasoning must be consistent, with the three following
meaningsoftheword“consistent”:
(a) Consistency or non-contradiction: When the same result can be
reachedthroughdifferentmeans,thesameplausibilityvaluemust
befoundinallcases.
(b) Honesty:Allavailabledatamustbetakenintoaccount.
(c) Reproducibility:Ifourstateofknowledgeabouttwoproblemsare
the same, then we must assign the same degree of plausibility to
bothofthem.
The Cox–Jaynes theorem proves these plausibilities to be sufficient to
define the universal mathematical rules that apply to plausibility p, up to
transformation by an arbitrary monotonic function. Crucially, these rules
aretherulesofprobability.
Remark. Inmachinelearningandstatistics,therearetwomajorinterpre-
tationsofprobability:theBayesianandfrequentistinterpretations(Bishop,
2006;EfronandHastie,2016).TheBayesianinterpretationusesprobabil-
itytospecifythedegreeofuncertaintythattheuserhasaboutanevent.It
is sometimes referred to as “subjective probability” or “degree of belief”.
Thefrequentistinterpretationconsiderstherelativefrequenciesofevents
of interest to the total number of events that occurred. The probability of
aneventisdefinedastherelativefrequencyoftheeventinthelimitwhen
onehasinfinitedata.
♢
Some machine learning texts on probabilistic models use lazy notation
andjargon,whichisconfusing.Thistextisnoexception.Multipledistinct
concepts are all referred to as “probability distribution”, and the reader
has to often disentangle the meaning from the context. One trick to help
make sense of probability distributions is to check whether we are trying
to model something categorical (a discrete random variable) or some-
thing continuous (a continuous random variable). The kinds of questions
we tackle in machine learning are closely related to whether we are con-
sideringcategoricalorcontinuousmodels.
6.1.2 Probability and Random Variables
There are three distinct ideas that are often confused when discussing
probabilities. First is the idea of a probability space, which allows us to
quantifytheideaofaprobability.However,wemostlydonotworkdirectly
withthisbasicprobabilityspace.Instead,weworkwithrandomvariables
(the second idea), which transfers the probability to a more convenient
(oftennumerical)space.Thethirdideaistheideaofadistributionorlaw
associated with a random variable. We will introduce the first two ideas
inthissectionandexpandonthethirdideainSection6.2.
ModernprobabilityisbasedonasetofaxiomsproposedbyKolmogorov
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.1 ConstructionofaProbabilitySpace 175
(Grinstead and Snell, 1997; Jaynes, 2003) that introduce the three con-
cepts of sample space, event space, and probability measure. The prob-
ability space models a real-world process (referred to as an experiment)
withrandomoutcomes.
The sample space Ω
The sample space is the set of all possible outcomes of the experiment, samplespace
usually denoted by Ω. For example, two successive coin tosses have
a sample space of hh, tt, ht, th , where “h” denotes “heads” and “t”
{ }
denotes“tails”.
The event space
A
The event space is the space of potential results of the experiment. A eventspace
subset A of the sample space Ω is in the event space if at the end
A
oftheexperimentwecanobservewhetheraparticularoutcomeω Ω
∈
is in A. The event space is obtained by considering the collection of
A
subsets of Ω, and for discrete probability distributions (Section 6.2.1)
isoftenthepowersetofΩ.
A
The probability P
WitheacheventA ,weassociateanumberP(A)thatmeasuresthe
∈ A
probability or degree of belief that the event will occur. P(A) is called
theprobability ofA. probability
The probability of a single event must lie in the interval [0,1], and the
total probability over all outcomes in the sample space Ω must be 1, i.e.,
P(Ω) = 1.Givenaprobabilityspace(Ω, ,P),wewanttouseittomodel
A
somereal-worldphenomenon.Inmachinelearning,weoftenavoidexplic-
itlyreferringtotheprobabilityspace,butinsteadrefertoprobabilitieson
quantities of interest, which we denote by . In this book, we refer to
T T
as the target space and refer to elements of as states. We introduce a targetspace
T
functionX : Ω thattakesanelementofΩ(anoutcome)andreturns
→ T
aparticularquantityofinterestx,avaluein .Thisassociation/mapping
T
fromΩto iscalledarandomvariable.Forexample,inthecaseoftossing randomvariable
T
two coins and counting the number of heads, a random variable X maps
to the three possible outcomes: X(hh) = 2, X(ht) = 1, X(th) = 1, and
X(tt) = 0.Inthisparticularcase, = 0,1,2 ,anditistheprobabilities
T { }
onelementsof thatweareinterestedin.ForafinitesamplespaceΩand Thename“random
T
finite , the function corresponding to a random variable is essentially a variable”isagreat
lookupT table. For any subset S , we associate P (S) [0,1] (the sourceof
X
⊆ T ∈ misunderstanding
probability) to a particular event occurring corresponding to the random
asitisneither
variable X. Example 6.1 provides a concrete illustration of the terminol- randomnorisita
ogy. variable.Itisa
function.
Remark. The aforementioned sample space Ω unfortunately is referred
to by different names in different books. Another common name for Ω
is “state space” (Jacod and Protter, 2004), but state space is sometimes
reserved for referring to states in a dynamical system (Hasselblatt and
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
176 ProbabilityandDistributions
Katok, 2003). Other names sometimes used to describe Ω are: “sample
descriptionspace”,“possibilityspace,”and“eventspace”.
♢
Example 6.1
Thistoyexampleis Weassumethatthereaderisalreadyfamiliarwithcomputingprobabilities
essentiallyabiased of intersections and unions of sets of events. A gentler introduction to
coinflipexample.
probability with many examples can be found in chapter 2 of Walpole
etal.(2011).
Consider a statistical experiment where we model a funfair game con-
sisting of drawing two coins from a bag (with replacement). There are
coins from USA (denoted as $) and UK (denoted as £) in the bag, and
since we draw two coins from the bag, there are four outcomes in total.
The state space or sample space Ω of this experiment is then ($, $), ($,
£),(£,$),(£,£).Letusassumethatthecompositionofthebagofcoinsis
suchthatadrawreturnsatrandoma$withprobability0.3.
Theeventweareinterestedinisthetotalnumberoftimestherepeated
draw returns $. Let us define a random variable X that maps the sample
space Ω to , which denotes the number of times we draw $ out of the
T
bag.Wecanseefromtheprecedingsamplespacewecangetzero$,one$,
ortwo$s,andtherefore = 0,1,2 .TherandomvariableX (afunction
T { }
orlookuptable)canberepresentedasatablelikethefollowing:
X(($,$)) = 2 (6.1)
X(($,£)) = 1 (6.2)
X((£,$)) = 1 (6.3)
X((£,£)) = 0. (6.4)
Since we return the first coin we draw before drawing the second, this
implies that the two draws are independent of each other, which we will
discuss in Section 6.4.5. Note that there are two experimental outcomes,
which map to the same event, where only one of the draws returns $.
Therefore,theprobabilitymassfunction(Section6.2.1)ofX isgivenby
P(X = 2) = P(($,$))
= P($) P($)
·
= 0.3 0.3 = 0.09 (6.5)
·
P(X = 1) = P(($,£) (£,$))
∪
= P(($,£))+P((£,$))
= 0.3 (1 0.3)+(1 0.3) 0.3 = 0.42 (6.6)
· − − ·
P(X = 0) = P((£,£))
= P(£) P(£)
·
= (1 0.3) (1 0.3) = 0.49. (6.7)
− · −
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.1 ConstructionofaProbabilitySpace 177
In the calculation, we equated two different concepts, the probability
of the output of X and the probability of the samples in Ω. For example,
in (6.7) we say P(X = 0) = P((£,£)). Consider the random variable
X : Ω and a subset S (for example, a single element of ,
→ T ⊆ T T
such as the outcome that one head is obtained when tossing two coins).
LetX−1(S)bethepre-imageofS byX,i.e.,thesetofelementsofΩthat
map to S under X; ω Ω : X(ω) S . One way to understand the
{ ∈ ∈ }
transformation of probability from events in Ω via the random variable
X is to associate it with the probability of the pre-image of S (Jacod and
Protter,2004).ForS ,wehavethenotation
⊆ T
P (S) = P(X S) = P(X−1(S)) = P( ω Ω : X(ω) S ). (6.8)
X
∈ { ∈ ∈ }
Theleft-handsideof(6.8)istheprobabilityofthesetofpossibleoutcomes
(e.g.,numberof$=1)thatweareinterestedin.Viatherandomvariable
X, which maps states to outcomes, we see in the right-hand side of (6.8)
thatthisistheprobabilityofthesetofstates(inΩ)thathavetheproperty
(e.g., $£, £$). We say that a random variable X is distributed according
to a particular probability distribution P , which defines the probability
X
mapping between the event and the probability of the outcome of the
randomvariable.Inotherwords,thefunctionP orequivalentlyP X−1
X
◦
isthelawordistributionofrandomvariableX. law
distribution
Remark. Thetargetspace,thatis,therange oftherandomvariableX,
T
isusedtoindicatethekindofprobabilityspace,i.e.,a randomvariable.
T
When is finite or countably infinite, this is called a discrete random
T
variable(Section6.2.1).Forcontinuousrandomvariables(Section6.2.2),
weonlyconsider = Ror = RD.
T T ♢
6.1.3 Statistics
Probabilitytheoryandstatisticsareoftenpresentedtogether,buttheycon-
cerndifferentaspectsofuncertainty.Onewayofcontrastingthemisbythe
kinds of problemsthat are considered. Using probability,we can consider
a model of some process, where the underlying uncertainty is captured
by random variables, and we use the rules of probability to derive what
happens. In statistics, we observe that something has happened and try
tofigureouttheunderlyingprocessthatexplainstheobservations.Inthis
sense, machine learning is close to statistics in its goals to construct a
modelthatadequatelyrepresentstheprocessthatgeneratedthedata.We
can use the rules of probability to obtain a “best-fitting” model for some
data.
Another aspect of machine learning systems is that we are interested
in generalization error (see Chapter 8). This means that we are actually
interested in the performance of our system on instances that we will
observe in future, which are not identical to the instances that we have
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
178 ProbabilityandDistributions
seen so far. This analysis of future performance relies on probability and
statistics, most of which is beyond what will be presented in this chapter.
The interested reader is encouraged to look at the books by Boucheron
etal.(2013)andShalev-ShwartzandBen-David(2014).Wewillseemore
aboutstatisticsinChapter8.
6.2 Discrete and Continuous Probabilities
Let us focus our attention on ways to describe the probability of an event
asintroducedinSection6.1.Dependingonwhetherthetargetspaceisdis-
crete or continuous, the natural way to refer to distributions is different.
When the target space is discrete, we can specify the probability that a
T
randomvariableX takesaparticularvaluex ,denotedasP(X = x).
∈ T
The expression P(X = x) for a discrete random variable X is known as
probabilitymass theprobabilitymassfunction.Whenthetargetspace iscontinuous,e.g.,
function
thereallineR,itismorenaturaltospecifytheprobT
abilitythatarandom
variable X is in an interval, denoted by P(a ⩽ X ⩽ b) for a < b. By con-
vention, we specify the probability that a random variable X is less than
aparticularvaluex,denotedbyP(X ⩽ x).TheexpressionP(X ⩽ x)for
cumulative a continuous random variable X is known as the cumulative distribution
distributionfunction function. We will discuss continuous random variables in Section 6.2.2.
We will revisit the nomenclature and contrast discrete and continuous
randomvariablesinSection6.2.3.
univariate Remark. Wewillusethephraseunivariatedistributiontorefertodistribu-
tions of a single random variable (whose states are denoted by non-bold
x). We will refer to distributions of more than one random variable as
multivariate multivariate distributions, and will usually consider a vector of random
variables(whosestatesaredenotedbyboldx).
♢
6.2.1 Discrete Probabilities
When the target space is discrete, we can imagine the probability distri-
bution of multiple random variables as filling out a (multidimensional)
array of numbers. Figure 6.2 shows an example. The target space of the
joint probability is the Cartesian product of the target spaces of each of
jointprobability the random variables. We define the joint probability as the entry of both
valuesjointly
n
P(X = x ,Y = y ) = ij , (6.9)
i j N
where n is the number of events with state x and y and N the total
ij i j
number of events. The joint probability is the probability of the intersec-
tion of both events, that is, P(X = x ,Y = y ) = P(X = x Y = y ).
i j i j
∩
probabilitymass Figure6.2illustratestheprobabilitymassfunction(pmf)ofadiscreteprob-
function ability distribution. For two random variables X and Y, the probability
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.2 DiscreteandContinuousProbabilities 179
c Figure6.2
i
(cid:122)(cid:125)(cid:124)(cid:123) Visualizationofa
y 1 discretebivariate
(cid:111) probabilitymass
Y y 2 n ij r j function,with
randomvariablesX
y
3 andY.This
x x x x x diagramisadapted
1 2 3 4 5
fromBishop(2006).
X
that X = x and Y = y is (lazily) written as p(x,y) and is called the joint
probability. One can think of a probability as a function that takes state
x and y and returns a real number, which is the reason we write p(x,y).
ThemarginalprobabilitythatX takesthevaluexirrespectiveofthevalue marginalprobability
of random variable Y is (lazily) written as p(x). We write X p(x) to
∼
denotethattherandomvariableX isdistributedaccordingtop(x).Ifwe
consider only the instances where X = x, then the fraction of instances
(theconditionalprobability)forwhichY = yiswritten(lazily)asp(y x). conditional
|
probability
Example 6.2
ConsidertworandomvariablesX andY,whereX hasfivepossiblestates
andY hasthreepossiblestates,asshowninFigure6.2.Wedenotebyn
ij
the number of events with state X = x and Y = y , and denote by
i j
N the total number of events. The value c is the sum of the individual
i
frequenciesfortheithcolumn,thatis,c =
(cid:80)3
n .Similarly,thevalue
i j=1 ij
r is the row sum, that is, r =
(cid:80)5
n . Using these definitions, we can
j j i=1 ij
compactlyexpressthedistributionofX andY.
The probability distribution of each random variable, the marginal
probability,canbeseenasthesumoveraroworcolumn
c
(cid:80)3
n
P(X = x ) = i = j=1 ij (6.10)
i N N
and
r
(cid:80)5
n
P(Y = y ) = j = i=1 ij, (6.11)
j N N
where c and r are the ith column and jth row of the probability table,
i j
respectively. By convention, for discrete random variables with a finite
numberofevents,weassumethatprobabiltiessumuptoone,thatis,
5 3
(cid:88) (cid:88)
P(X = x ) = 1 and P(Y = y ) = 1. (6.12)
i j
i=1 j=1
The conditional probability is the fraction of a row or column in a par-
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
180 ProbabilityandDistributions
ticularcell.Forexample,theconditionalprobabilityofY givenX is
n
P(Y = y X = x ) = ij , (6.13)
j | i c
i
andtheconditionalprobabilityofX givenY is
n
P(X = x Y = y ) = ij . (6.14)
i | j r
j
In machine learning, we use discrete probability distributions to model
categoricalvariable categorical variables, i.e., variables that take a finite set of unordered val-
ues. They could be categorical features, such as the degree taken at uni-
versity when used for predicting the salary of a person, or categorical la-
bels, such as letters of the alphabet when doing handwriting recognition.
Discretedistributionsarealsooftenusedtoconstructprobabilisticmodels
thatcombineafinitenumberofcontinuousdistributions(Chapter11).
6.2.2 Continuous Probabilities
Weconsiderreal-valuedrandomvariablesinthissection,i.e.,weconsider
targetspacesthatareintervalsofthereallineR.Inthisbook,wepretend
thatwecanperformoperationsonrealrandomvariablesasifwehavedis-
crete probability spaces with finite states. However, this simplification is
notprecisefortwosituations:whenwerepeatsomethinginfinitelyoften,
and when we want to draw a point from an interval. The first situation
arises when we discuss generalization errors in machine learning (Chap-
ter 8). The second situation arises when we want to discuss continuous
distributions, such as the Gaussian (Section 6.5). For our purposes, the
lackofprecisionallowsforabrieferintroductiontoprobability.
Remark. In continuous spaces, there are two additional technicalities,
which are counterintuitive. First, the set of all subsets (used to define
the event space in Section 6.1) is not well behaved enough. needs
A A
to be restricted to behave well under set complements, set intersections,
and set unions. Second, the size of a set (which in discrete spaces can be
obtained by counting the elements) turns out to be tricky. The size of a
measure set is called its measure. For example, the cardinality of discrete sets, the
length of an interval in R, and the volume of a region in Rd are all mea-
sures. Sets that behave well under set operations and additionally have
Borelσ-algebra a topology are called a Borel σ-algebra. Betancourt details a careful con-
structionofprobabilityspacesfromsettheorywithoutbeingboggeddown
in technicalities; see https://tinyurl.com/yb3t6mfd. For a more pre-
cise construction, we refer to Billingsley (1995) and Jacod and Protter
(2004).
In this book, we consider real-valued random variables with their cor-
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.2 DiscreteandContinuousProbabilities 181
respondingBorelσ-algebra.Weconsiderrandomvariableswithvaluesin
RD tobeavectorofreal-valuedrandomvariables.
♢
Definition 6.1 (Probability Density Function). A function f : RD R is
→
calledaprobabilitydensityfunction(pdf)if probabilitydensity
function
1. x RD : f(x) ⩾ 0
pdf
∀ ∈
2. Itsintegralexistsand
(cid:90)
f(x)dx = 1. (6.15)
RD
For probability mass functions (pmf) of discrete random variables, the
integralin(6.15)isreplacedwithasum(6.12).
Observe that the probability density function is any function f that is
non-negative and integrates to one. We associate a random variable X
withthisfunctionf by
(cid:90) b
P(a ⩽ X ⩽ b) = f(x)dx, (6.16)
a
where a,b R and x R are outcomes of the continuous random vari-
able X.
Sta∈
tes x
RD∈
are defined analogously by considering a vector
of x R. This as∈ sociation (6.16) is called the law or distribution of the law
∈
randomvariableX.
P(X=x)isasetof
Remark. Incontrasttodiscreterandomvariables,theprobabilityofacon- measurezero.
tinuous random variable X taking a particular value P(X = x) is zero.
Thisisliketryingtospecifyanintervalin(6.16)wherea = b.
♢
Definition6.2(CumulativeDistributionFunction). Acumulativedistribu- cumulative
tion function (cdf) of a multivariate real-valued random variable X with distributionfunction
statesx RD isgivenby
∈
F (x) = P(X ⩽ x ,...,X ⩽ x ), (6.17)
X 1 1 D D
where X = [X ,...,X ]⊤, x = [x ,...,x ]⊤, and the right-hand side
1 D 1 D
representstheprobabilitythatrandomvariableX takesthevaluesmaller
i
thanorequaltox .
i
Therearecdfs,
The cdf can be expressed also as the integral of the probability density whichdonothave
functionf(x)sothat correspondingpdfs.
(cid:90) x1 (cid:90) xD
F (x) = f(z ,...,z )dz dz . (6.18)
X 1 D 1 D
··· ···
−∞ −∞
Remark. We reiterate that there are in fact two distinct concepts when
talking about distributions. First is the idea of a pdf (denoted by f(x)),
which is a nonnegative function that sums to one. Second is the law of a
random variable X, that is, the association of a random variable X with
thepdff(x).
♢
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
182 ProbabilityandDistributions
Figure6.3 2.0 2.0
Examplesof
(a)discreteand 1.5 1.5
(b)continuous
uniform 1.0 1.0
distributions.See
0.5 0.5
Example6.3for
detailsofthe
0.0 0.0
distributions. 1 0 1 2 1 0 1 2
− −
z x
(a)Discretedistribution (b)Continuousdistribution
For most of this book, we will not use the notation f(x) and F (x) as
X
we mostly do not need to distinguish between the pdf and cdf. However,
wewillneedtobecarefulaboutpdfsandcdfsinSection6.7.
6.2.3 Contrasting Discrete and Continuous Distributions
RecallfromSection6.1.2thatprobabilitiesarepositiveandthetotalprob-
ability sums up to one. For discrete random variables (see (6.12)), this
implies that the probability of each state must lie in the interval [0,1].
However,forcontinuousrandomvariablesthenormalization(see(6.15))
does not imply that the value of the density is less than or equal to 1 for
uniformdistribution all values. We illustrate this in Figure 6.3 using the uniform distribution
forbothdiscreteandcontinuousrandomvariables.
Example 6.3
Weconsidertwoexamplesoftheuniformdistribution,whereeachstateis
equally likely to occur. This example illustrates some differences between
discreteandcontinuousprobabilitydistributions.
Let Z be a discrete uniform random variable with three states z =
{
Theactualvaluesof 1.1,z = 0.3,z = 1.5 . Theprobabilitymassfunctioncanberepresented
− }
thesestatesarenot asatableofprobabilityvalues:
meaningfulhere,
andwedeliberately z 1.1 0.3 1.5
−
chosenumbersto
drivehomethe P(Z =z) 1 1 1
3 3 3
pointthatwedonot
wanttouse(and
Alternatively, we can think of this as a graph (Figure 6.3(a)), where we
shouldignore)the
orderingofthe use the fact that the states can be located on the x-axis, and the y-axis
states. representstheprobabilityofaparticularstate.They-axisinFigure6.3(a)
isdeliberatelyextendedsothatisitthesameasinFigure6.3(b).
LetX beacontinuousrandomvariabletakingvaluesintherange0.9 ⩽
X ⩽ 1.6, as represented by Figure 6.3(b). Observe that the height of the
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
)z=
Z(P
)x(p
6.3 SumRule,ProductRule,andBayes’Theorem 183
Type “Pointprobability” “Intervalprobability” Table6.1
Nomenclaturefor
Discrete P(X =x) Notapplicable probability
Probabilitymassfunction distributions.
Continuous p(x) P(X ⩽x)
Probabilitydensityfunction Cumulativedistributionfunction
densitycanbegreaterthan1.However,itneedstoholdthat
(cid:90) 1.6
p(x)dx = 1. (6.19)
0.9
Remark. There is an additional subtlety with regards to discrete prob-
ability distributions. The states z ,...,z do not in principle have any
1 d
structure, i.e., there is usually no way to compare them, for example
z = red,z = green,z = blue. However, in many machine learning
1 2 3
applications discrete states take numerical values, e.g., z = 1.1,z =
1 2
−
0.3,z = 1.5, where we could say z < z < z . Discrete states that as-
3 1 2 3
sume numerical values are particularly useful because we often consider
expectedvalues(Section6.4.1)ofrandomvariables.
♢
Unfortunately, machine learning literature uses notation and nomen-
clature that hides the distinction between the sample space Ω, the target
space , and the random variable X. For a value x of the set of possible
T
outcomes of the random variable X, i.e., x , p(x) denotes the prob- Wethinkofthe
∈ T
ability that random variable X has the outcome x. For discrete random outcomexasthe
variables, this is written as P(X = x), which is known as the probabil- argumentthat
resultsinthe
ity mass function. The pmf is often referred to as the “distribution”. For
probabilityp(x).
continuousvariables,p(x)iscalledtheprobabilitydensityfunction(often
referred to as a density). To muddy things even further, the cumulative
distribution function P(X ⩽ x) is often also referred to as the “distribu-
tion”.Inthischapter,wewillusethenotationX torefertobothunivariate
and multivariate random variables, and denote the states by x and x re-
spectively.WesummarizethenomenclatureinTable6.1.
Remark. We will be using the expression “probability distribution” not
onlyfordiscreteprobabilitymassfunctionsbutalsoforcontinuousproba-
bility density functions, although this is technically incorrect. In line with
most machine learning literature, we also rely on context to distinguish
thedifferentusesofthephraseprobabilitydistribution.
♢
6.3 Sum Rule, Product Rule, and Bayes’ Theorem
Wethinkofprobabilitytheoryasanextensiontologicalreasoning.Aswe
discussed in Section 6.1.1, the rules of probability presented here follow
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
184 ProbabilityandDistributions
naturally from fulfilling the desiderata (Jaynes, 2003, chapter 2). Prob-
abilistic modeling (Section 8.4) provides a principled foundation for de-
signingmachinelearningmethods.Oncewehavedefinedprobabilitydis-
tributions(Section6.2)correspondingtotheuncertaintiesofthedataand
our problem, it turns out that there are only two fundamental rules, the
sumruleandtheproductrule.
Recall from (6.9) that p(x,y) is the joint distribution of the two ran-
dom variables x,y. The distributions p(x) and p(y) are the correspond-
ingmarginaldistributions,andp(y x)istheconditionaldistributionofy
|
givenx.Giventhedefinitionsofthemarginalandconditionalprobability
for discrete and continuous random variables in Section 6.2, we can now
Thesetworules presentthetwofundamentalrulesinprobabilitytheory.
arise Thefirstrule,thesumrule,statesthat
naturally(Jaynes,
 (cid:88)
2003)fromthe  p(x,y) ify isdiscrete

requirementswe 
discussedin p(x) = y (cid:90)∈Y , (6.20)
Section6.1.1.   p(x,y)dy ify iscontinuous

sumrule Y
where are the states of the target space of random variable Y. This
Y
meansthatwesumout(orintegrateout)thesetofstatesyoftherandom
marginalization variable Y. The sum rule is also known as the marginalization property.
property The sum rule relates the joint distribution to a marginal distribution. In
general,whenthejointdistributioncontainsmorethantworandomvari-
ables, the sum rule can be applied to any subset of the random variables,
resulting in a marginal distribution of potentially more than one random
variable.Moreconcretely,ifx = [x ,...,x ]⊤,weobtainthemarginal
1 D
(cid:90)
p(x ) = p(x ,...,x )dx (6.21)
i 1 D \i
by repeated application of the sum rule where we integrate/sum out all
random variables except x , which is indicated by i, which reads “all
i
\
excepti.”
Remark. Many of the computational challenges of probabilistic modeling
areduetotheapplicationofthesumrule.Whentherearemanyvariables
or discrete variables with many states, the sum rule boils down to per-
formingahigh-dimensionalsumorintegral.Performinghigh-dimensional
sumsorintegralsisgenerallycomputationallyhard,inthesensethatthere
isnoknownpolynomial-timealgorithmtocalculatethemexactly.
♢
productrule Thesecondrule,knownastheproductrule,relatesthejointdistribution
totheconditionaldistributionvia
p(x,y) = p(y x)p(x). (6.22)
|
The product rule can be interpreted as the fact that every joint distribu-
tion of two random variables can be factorized (written as a product)
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.3 SumRule,ProductRule,andBayes’Theorem 185
of two other distributions. The two factors are the marginal distribu-
tion of the first random variable p(x), and the conditional distribution
of the second random variable given the first p(y x). Since the ordering
|
of random variables is arbitrary in p(x,y), the product rule also implies
p(x,y) = p(x y)p(y). To be precise, (6.22) is expressed in terms of the
|
probability mass functions for discrete random variables. For continuous
randomvariables,theproductruleisexpressedintermsoftheprobability
densityfunctions(Section6.2.3).
In machine learning and Bayesian statistics, we are often interested in
makinginferencesofunobserved(latent)randomvariablesgiventhatwe
haveobservedotherrandomvariables.Letusassumewehavesomeprior
knowledge p(x) about an unobserved random variable x and some rela-
tionship p(y x) between x and a second random variable y, which we
|
can observe. If we observe y, we can use Bayes’ theorem to draw some
conclusions about x given the observed values of y. Bayes’ theorem (also Bayes’theorem
Bayes’ruleorBayes’law) Bayes’rule
Bayes’law
likelihood prior
(cid:122) (cid:125)(cid:124) (cid:123)(cid:122)(cid:125)(cid:124)(cid:123)
p(y x)p(x)
p(x y) = | (6.23)
| p(y)
(cid:124) (cid:123)(cid:122) (cid:125)
posterior (cid:124)(cid:123)(cid:122)(cid:125)
evidence
isadirectconsequenceoftheproductrulein(6.22)since
p(x,y) = p(x y)p(y) (6.24)
|
and
p(x,y) = p(y x)p(x) (6.25)
|
sothat
p(y x)p(x)
p(x y)p(y) = p(y x)p(x) p(x y) = | . (6.26)
| | ⇐⇒ | p(y)
In (6.23), p(x) is the prior, which encapsulates our subjective prior prior
knowledge of the unobserved (latent) variable x before observing any
data. We can choose any prior that makes sense to us, but it is critical to
ensure that the prior has a nonzero pdf (or pmf) on all plausible x, even
iftheyareveryrare.
The likelihood p(y x) describes how x and y are related, and in the likelihood
|
caseofdiscreteprobabilitydistributions,itistheprobabilityofthedatay Thelikelihoodis
ifweweretoknowthelatentvariablex.Notethatthelikelihoodisnota sometimesalso
calledthe
distribution in x, but only in y. We call p(y x) either the “likelihood of
| “measurement
x (given y)” or the “probability of y given x” but never the likelihood of model”.
y (MacKay,2003).
The posterior p(x y) is the quantity of interest in Bayesian statistics posterior
|
becauseitexpressesexactlywhatweareinterestedin,i.e.,whatweknow
aboutxafterhavingobservedy.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
186 ProbabilityandDistributions
Thequantity
(cid:90)
p(y) := p(y x)p(x)dx = E [p(y x)] (6.27)
X
| |
marginallikelihood isthemarginallikelihood/evidence.Theright-handsideof(6.27)usesthe
evidence expectation operator which we define in Section 6.4.1. By definition, the
marginallikelihoodintegratesthenumeratorof(6.23)withrespecttothe
latent variable x. Therefore, the marginal likelihood is independent of
x, and it ensures that the posterior p(x y) is normalized. The marginal
|
likelihood can also be interpreted as the expected likelihood where we
taketheexpectationwithrespecttothepriorp(x).Beyondnormalization
of the posterior, the marginal likelihood also plays an important role in
Bayesian model selection, as we will discuss in Section 8.6. Due to the
Bayes’theoremis integrationin(8.44),theevidenceisoftenhardtocompute.
alsocalledthe Bayes’ theorem (6.23) allows us to invert the relationship between x
“probabilistic
and y given by the likelihood. Therefore, Bayes’ theorem is sometimes
inverse.”
called the probabilistic inverse. We will discuss Bayes’ theorem further in
probabilisticinverse
Section8.4.
Remark. In Bayesian statistics, the posterior distribution is the quantity
of interest as it encapsulates all available information from the prior and
the data. Instead of carrying the posterior around, it is possible to focus
on some statistic of the posterior, such as the maximum of the posterior,
which we will discuss in Section 8.3. However, focusing on some statistic
of the posterior leads to loss of information. If we think in a bigger con-
text,thentheposteriorcanbeusedwithinadecision-makingsystem,and
havingthefullposteriorcanbeextremelyusefulandleadtodecisionsthat
arerobusttodisturbances.Forexample,inthecontextofmodel-basedre-
inforcement learning, Deisenroth et al. (2015) show that using the full
posterior distribution of plausible transition functions leads to very fast
(data/sample efficient) learning, whereas focusing on the maximum of
the posterior leads to consistent failures. Therefore, having the full pos-
terior can be very useful for a downstream task. In Chapter 9, we will
continuethisdiscussioninthecontextoflinearregression.
♢
6.4 Summary Statistics and Independence
Weareofteninterestedinsummarizingsetsofrandomvariablesandcom-
paring pairs of random variables. A statistic of a random variable is a de-
terministic function of that random variable. The summary statistics of a
distribution provide one useful view of how a random variable behaves,
and as the name suggests, provide numbers that summarize and charac-
terize the distribution. We describe the mean and the variance, two well-
known summary statistics. Then we discuss two ways to compare a pair
ofrandomvariables:first,howtosaythattworandomvariablesareinde-
pendent;andsecond,howtocomputeaninnerproductbetweenthem.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.4 SummaryStatisticsandIndependence 187
6.4.1 Means and Covariances
Meanand(co)varianceareoftenusefultodescribepropertiesofprobabil-
ity distributions (expected values and spread). We will see in Section 6.6
that there is a useful family of distributions (called the exponential fam-
ily),wherethestatisticsoftherandomvariablecaptureallpossibleinfor-
mation.
The concept of the expected value is central to machine learning, and
the foundational concepts of probability itself can be derived from the
expectedvalue(Whittle,2000).
Definition6.3(ExpectedValue). Theexpectedvalueofafunctiong : R expectedvalue
RofaunivariatecontinuousrandomvariableX p(x)isgivenby →
∼
(cid:90)
E [g(x)] = g(x)p(x)dx. (6.28)
X
X
Correspondingly, the expected value of a function g of a discrete random
variableX p(x)isgivenby
∼
(cid:88)
E [g(x)] = g(x)p(x), (6.29)
X
x∈X
where is the set of possible outcomes (the target space) of the random
X
variableX.
Inthissection,weconsiderdiscreterandomvariablestohavenumerical
outcomes. This can be seen by observing that the function g takes real
numbersasinputs. Theexpectedvalue
ofafunctionofa
Remark. We consider multivariate random variables X as a finite vector
randomvariableis
of univariate random variables [X 1,...,X D]⊤. For multivariate random sometimesreferred
variables,wedefinetheexpectedvalueelementwise toasthelawofthe
unconscious
E
[g(x
)]
statistician(Casella
E X[g(x)] =  
X1
. . .
1
  ∈ RD, (6.30) a Sn ecd tiB oe nrg 2e .2r, ).2002,
E [g(x )]
XD D
where the subscript E indicates that we are taking the expected value
Xd
withrespecttothedthelementofthevectorx.
♢
Definition 6.3 defines the meaning of the notation E as the operator
X
indicating that we should take the integral with respect to the probabil-
ity density (for continuous distributions) or the sum over all states (for
discrete distributions). The definition of the mean (Definition 6.4), is a
specialcaseoftheexpectedvalue,obtainedbychoosingg tobetheiden-
tityfunction.
Definition 6.4 (Mean). The mean of a random variable X with states mean
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
188 ProbabilityandDistributions
x RD isanaverageandisdefinedas
∈
E
[x
]
X1 1
E X[x] =   . . .   ∈ RD, (6.31)
E [x ]
XD D
where
 (cid:90)
   x dp(x d)dx d ifX isacontinuousrandomvariable
E Xd[x d] := (cid:88)X
x p(x = x ) ifX isadiscreterandomvariable
  i d i

xi∈X
(6.32)
for d = 1,...,D, where the subscript d indicates the corresponding di-
mension of x. The integral and sum are over the states of the target
X
spaceoftherandomvariableX.
In one dimension, there are two other intuitive notions of “average”,
median which are the median and the mode. The median is the “middle” value if
wesortthevalues,i.e.,50%ofthevaluesaregreaterthanthemedianand
50%aresmallerthanthemedian.Thisideacanbegeneralizedtocontin-
uousvaluesbyconsideringthevaluewherethecdf(Definition6.2)is0.5.
For distributions, which are asymmetric or have long tails, the median
provides an estimate of a typical value that is closer to human intuition
than the mean value. Furthermore, the median is more robust to outliers
than the mean. The generalization of the median to higher dimensions is
non-trivial as there is no obvious way to “sort” in more than one dimen-
mode sion (Hallin et al., 2010; Kong and Mizera, 2012). The mode is the most
frequently occurring value. For a discrete random variable, the mode is
defined as the value of x having the highest frequency of occurrence. For
acontinuousrandomvariable,themodeisdefinedasapeakinthedensity
p(x). A particular density p(x) may have more than one mode, and fur-
thermoretheremaybeaverylargenumberofmodesinhigh-dimensional
distributions. Therefore, finding all the modes of a distribution can be
computationallychallenging.
Example 6.4
Considerthetwo-dimensionaldistributionillustratedinFigure6.4:
(cid:18) (cid:12)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) (cid:18) (cid:12)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)
(cid:12) 10 1 0 (cid:12) 0 8.4 2.0
p(x) = 0.4 x (cid:12) , +0.6 x (cid:12) , .
N (cid:12) 2 0 1 N (cid:12) 0 2.0 1.7
(6.33)
(cid:0) (cid:1)
We will define the Gaussian distribution µ, σ2 in Section 6.5. Also
N
shown is its corresponding marginal distribution in each dimension. Ob-
serve that the distribution is bimodal (has two modes), but one of the
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.4 SummaryStatisticsandIndependence 189
marginal distributions is unimodal (has one mode). The horizontal bi-
modal univariate distribution illustrates that the mean and median can
be different from each other. While it is tempting to define the two-
dimensional median to be the concatenation of the medians in each di-
mension, the fact that we cannot define an ordering of two-dimensional
points makes it difficult. When we say “cannot define an ordering”, we
mean that there is more than one way to define the relation < so that
(cid:20) (cid:21) (cid:20) (cid:21)
3 2
< .
0 3
Figure6.4
Mean Illustrationofthe
Modes mean,mode,and
Median medianfora
two-dimensional
dataset,aswellas
itsmarginal
densities.
Remark. The expected value (Definition 6.3) is a linear operator. For ex-
ample,givenareal-valuedfunctionf(x) = ag(x)+bh(x)wherea,b R
andx RD,weobtain ∈
∈
(cid:90)
E [f(x)] = f(x)p(x)dx (6.34a)
X
(cid:90)
= [ag(x)+bh(x)]p(x)dx (6.34b)
(cid:90) (cid:90)
= a g(x)p(x)dx+b h(x)p(x)dx (6.34c)
= aE [g(x)]+bE [h(x)]. (6.34d)
X X
♢
Fortworandomvariables,wemaywishtocharacterizetheircorrespon-
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
190 ProbabilityandDistributions
dence to each other. The covariance intuitively represents the notion of
howdependentrandomvariablesaretooneanother.
covariance Definition 6.5 (Covariance (Univariate)). The covariance between two
univariate random variables X,Y R is given by the expected product
∈
oftheirdeviationsfromtheirrespectivemeans,i.e.,
Cov [x,y] := E (cid:2) (x E [x])(y E [y])(cid:3) . (6.35)
X,Y X,Y X Y
− −
Terminology:The
Remark. When the random variable associated with the expectation or
covarianceof
multivariaterandom covarianceisclearbyitsarguments,thesubscriptisoftensuppressed(for
variablesCov[x,y] example,E [x]isoftenwrittenasE[x]).
X
issometimes ♢
referredtoas By using the linearity of expectations, the expression in Definition 6.5
cross-covariance, can be rewritten as the expected value of the product minus the product
withcovariance oftheexpectedvalues,i.e.,
referringto
Cov[x,x]. Cov[x,y] = E[xy] E[x]E[y]. (6.36)
−
variance ThecovarianceofavariablewithitselfCov[x,x]iscalledthevarianceand
standarddeviation isdenotedbyV X[x].Thesquarerootofthevarianceiscalledthestandard
deviation and is often denoted by σ(x). The notion of covariance can be
generalizedtomultivariaterandomvariables.
Definition6.6(Covariance(Multivariate)). Ifweconsidertwomultivari-
ate random variables X and Y with states x RD and y RE respec-
∈ ∈
covariance tively,thecovariancebetweenX andY isdefinedas
Cov[x,y] = E[xy⊤] E[x]E[y]⊤ = Cov[y,x]⊤ RD×E. (6.37)
− ∈
Definition 6.6 can be applied with the same multivariate random vari-
able in both arguments, which results in a useful concept that intuitively
captures the “spread” of a random variable. For a multivariate random
variable, the variance describes the relation between individual dimen-
sionsoftherandomvariable.
variance Definition 6.7 (Variance). The variance of a random variable X with
statesx RD andameanvectorµ RD isdefinedas
∈ ∈
V [x] = Cov [x,x] (6.38a)
X X
= E [(x µ)(x µ)⊤] = E [xx⊤] E [x]E [x]⊤ (6.38b)
X X X X
− − −
 
Cov[x ,x ] Cov[x ,x ] ... Cov[x ,x ]
1 1 1 2 1 D
Cov[x 2,x 1] Cov[x 2,x 2] ... Cov[x 2,x D]
=  

. .
.
. .
.
... . .
.
 

. (6.38c)
Cov[x ,x ] ... ... Cov[x ,x ]
D 1 D D
covariancematrix TheD D matrixin(6.38c)iscalledthecovariancematrixofthemul-
×
tivariaterandomvariableX.Thecovariancematrixissymmetricandpos-
itivesemidefiniteandtellsussomethingaboutthespreadofthedata.On
marginal itsdiagonal,thecovariancematrixcontainsthevariancesofthemarginals
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.4 SummaryStatisticsandIndependence 191
Figure6.5
6 6 Two-dimensional
datasetswith
4 4
identicalmeansand
variancesalong
2 2
eachaxis(colored
0 0 lines)butwith
different
−2 −2 covariances.
5 0 5 5 0 5
− x − x
(a)xandyarenegativelycorrelated. (b)xandyarepositivelycorrelated.
(cid:90)
p(x ) = p(x ,...,x )dx , (6.39)
i 1 D \i
where “ i” denotes “all variables but i”. The off-diagonal entries are the
\
cross-covariancetermsCov[x i,x j]fori,j = 1,...,D, i = j. cross-covariance
̸
Remark. In this book, we generally assume that covariance matrices are
positive definite to enable better intuition. We therefore do not discuss
cornercasesthatresultinpositivesemidefinite(low-rank)covariancema-
trices.
♢
When we want to compare the covariances between different pairs of
random variables, it turns out that the variance of each random variable
affects the value of the covariance. The normalized version of covariance
iscalledthecorrelation.
Definition 6.8 (Correlation). The correlation between two random vari- correlation
ablesX,Y isgivenby
Cov[x,y]
corr[x,y] = [ 1,1]. (6.40)
(cid:112)V[x]V[y]
∈ −
Thecorrelationmatrixisthecovariancematrixofstandardizedrandom
variables, x/σ(x). In other words, each random variable is divided by its
standard deviation (the square root of the variance) in the correlation
matrix.
The covariance (and correlation) indicate how two random variables
arerelated;seeFigure6.5.Positivecorrelationcorr[x,y]meansthatwhen
xgrows,theny isalsoexpectedtogrow.Negativecorrelationmeansthat
asxincreases,theny decreases.
6.4.2 Empirical Means and Covariances
The definitions in Section 6.4.1 are often also called the population mean populationmean
and covariance, as itrefers to the truestatistics for the population.In ma- andcovariance
chinelearning,weneedtolearnfromempiricalobservationsofdata.Con-
sider a random variable X. There are two conceptual steps to go from
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
y y
192 ProbabilityandDistributions
populationstatisticstotherealizationofempiricalstatistics.First,weuse
thefactthatwehaveafinitedataset(ofsizeN)toconstructanempirical
statisticthatisafunctionofafinitenumberofidenticalrandomvariables,
X ,...,X . Second, we observe the data, that is, we look at the realiza-
1 N
tion x ,...,x of each of the random variables and apply the empirical
1 N
statistic.
Specifically,forthemean(Definition6.4),givenaparticulardatasetwe
empiricalmean canobtainanestimateofthemean,whichiscalledtheempiricalmeanor
samplemean samplemean.Thesameholdsfortheempiricalcovariance.
empiricalmean Definition6.9(EmpiricalMeanandCovariance). Theempiricalmeanvec-
tor is the arithmetic average of the observations for each variable, and it
isdefinedas
N
1 (cid:88)
x¯ := x , (6.41)
N n
n=1
wherex RD.
n
∈
empiricalcovariance Similartotheempiricalmean,theempiricalcovariancematrixisaD D
×
matrix
N
1 (cid:88)
Σ := (x x¯)(x x¯)⊤. (6.42)
N n − n −
n=1
Throughoutthe
To compute the statistics for a particular dataset, we would use the
book,weusethe
empirical realizations (observations) x ,...,x and use (6.41) and (6.42). Em-
1 N
covariance,whichis pirical covariance matrices are symmetric, positive semidefinite (see Sec-
abiasedestimate.
tion3.2.3).
Theunbiased
(sometimescalled
corrected)
covariancehasthe 6.4.3 Three Expressions for the Variance
factorN−1inthe
We now focus on a single random variable X and use the preceding em-
denominator
insteadofN. pirical formulas toderive three possible expressions forthe variance. The
Thederivationsare following derivation is the same for the population variance, except that
exercisesattheend weneedtotakecareofintegrals.Thestandarddefinitionofvariance,cor-
ofthischapter.
responding to the definition of covariance (Definition 6.5), is the expec-
tation of the squared deviation of a random variable X from its expected
valueµ,i.e.,
V [x] := E [(x µ)2]. (6.43)
X X
−
The expectation in (6.43) and the mean µ = E (x) are computed us-
X
ing (6.32), depending on whether X is a discrete or continuous random
variable.Thevarianceasexpressedin(6.43)isthemeanofanewrandom
variableZ := (X µ)2.
−
When estimating the variance in (6.43) empirically, we need to resort
to a two-pass algorithm: one pass through the data to calculate the mean
µusing(6.41),andthenasecondpassusingthisestimateµˆ calculatethe
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.4 SummaryStatisticsandIndependence 193
variance. It turns out that we can avoid two passes by rearranging the
terms. The formula in (6.43) can be converted to the so-called raw-score raw-scoreformula
formulaforvariance: forvariance
V [x] = E [x2] (E [x])2 . (6.44)
X X X
−
The expression in (6.44) can be remembered as “the mean of the square
minusthesquareofthemean”.Itcanbecalculatedempiricallyinonepass
through data since we can accumulate x (to calculate the mean) and x2
i i
simultaneously, where x i is the ith observation. Unfortunately, if imple- Ifthetwoterms
mentedinthisway,itcanbenumericallyunstable.Theraw-scoreversion in(6.44)arehuge
andapproximately
ofthevariancecanbeusefulinmachinelearning,e.g.,whenderivingthe
equal,wemay
bias–variancedecomposition(Bishop,2006).
sufferfroman
Athirdwaytounderstandthevarianceisthatitisasumofpairwisedif- unnecessarylossof
ferences between all pairs of observations. Consider a sample x ,...,x numericalprecision
1 N
ofrealizationsofrandomvariableX,andwecomputethesquareddiffer- infloating-point
arithmetic.
ence between pairs of x and x . By expanding the square, we can show
i j
that the sum of N2 pairwise differences is the empirical variance of the
observations:
 
(cid:32) (cid:33)2
N N N
1 (cid:88) 1 (cid:88) 1 (cid:88)
N2 (x i −x j)2 = 2 N x2 i − N x i  . (6.45)
i,j=1 i=1 i=1
We see that (6.45) is twice the raw-score expression (6.44). This means
thatwecanexpressthesumofpairwisedistances(ofwhichthereareN2
ofthem)asasumofdeviationsfromthemean(ofwhichthereareN).Ge-
ometrically, this means that there is an equivalence between the pairwise
distances and the distances from the center of the set of points. From a
computational perspective, this means that by computing the mean (N
terms in the summation), and then computing the variance (again N
terms in the summation), we can obtain an expression (left-hand side
of(6.45))thathasN2 terms.
6.4.4 Sums and Transformations of Random Variables
We may want to model a phenomenon that cannot be well explained by
textbook distributions (we introduce some in Sections 6.5 and 6.6), and
hence may perform simple manipulations of random variables (such as
addingtworandomvariables).
ConsidertworandomvariablesX,Y withstatesx,y RD.Then:
∈
E[x+y] = E[x]+E[y] (6.46)
E[x y] = E[x] E[y] (6.47)
− −
V[x+y] = V[x]+V[y]+Cov[x,y]+Cov[y,x] (6.48)
V[x y] = V[x]+V[y] Cov[x,y] Cov[y,x]. (6.49)
− − −
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
194 ProbabilityandDistributions
Mean and (co)variance exhibit some useful properties when it comes
toaffinetransformationofrandomvariables.Considerarandomvariable
X with mean µ and covariance matrix Σ and a (deterministic) affine
transformation y = Ax + b of x. Then y is itself a random variable
whosemeanvectorandcovariancematrixaregivenby
E [y] = E [Ax+b] = AE [x]+b = Aµ+b, (6.50)
Y X X
V [y] = V [Ax+b] = V [Ax] = AV [x]A⊤ = AΣA⊤, (6.51)
Y X X X
Thiscanbeshown respectively.Furthermore,
directlybyusingthe
Cov[x,y] = E[x(Ax+b)⊤] E[x]E[Ax+b]⊤ (6.52a)
definitionofthe
−
meanand = E[x]b⊤+E[xx⊤]A⊤ µb⊤ µµ⊤A⊤ (6.52b)
covariance. − −
= µb⊤ µb⊤+(cid:0)E[xx⊤] µµ⊤(cid:1) A⊤ (6.52c)
− −
(6. =38b) ΣA⊤, (6.52d)
whereΣ = E[xx⊤] µµ⊤ isthecovarianceofX.
−
6.4.5 Statistical Independence
statistical Definition 6.10 (Independence). Two random variables X,Y are statis-
independence ticallyindependentifandonlyif
p(x,y) = p(x)p(y). (6.53)
Intuitively,tworandomvariablesX andY areindependentifthevalue
ofy (onceknown)doesnotaddanyadditionalinformationaboutx(and
viceversa).IfX,Y are(statistically)independent,then
p(y x) = p(y)
|
p(x y) = p(x)
V | [x+y] = V [x]+V [y]
X,Y X Y
Cov [x,y] = 0
X,Y
The last point may not hold in converse, i.e., two random variables can
havecovariancezerobutarenotstatisticallyindependent.Tounderstand
why, recall that covariance measures only linear dependence. Therefore,
random variables that are nonlinearly dependent could have covariance
zero.
Example 6.5
Consider a random variable X with zero mean (E [x] = 0) and also
X
E [x3] = 0. Let y = x2 (hence, Y is dependent on X) and consider the
X
covariance(6.36)betweenX andY.Butthisgives
Cov[x,y] = E[xy] E[x]E[y] = E[x3] = 0. (6.54)
−
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.4 SummaryStatisticsandIndependence 195
In machine learning, we often consider problems that can be mod-
eled as independent and identically distributed (i.i.d.) random variables, independentand
X ,...,X . For more than two random variables, the word “indepen- identically
1 N
distributed
dent” (Definition 6.10) usually refers to mutually independent random
i.i.d.
variables, where all subsets are independent (see Pollard (2002, chap-
ter 4) and Jacod and Protter (2004, chapter 3)). The phrase “identically
distributed”meansthatalltherandomvariablesarefromthesamedistri-
bution.
Another concept that is important in machine learning is conditional
independence.
Definition 6.11 (Conditional Independence). Two random variables X
andY areconditionallyindependentgivenZ ifandonlyif conditionally
independent
p(x,y z) = p(x z)p(y z) forall z , (6.55)
| | | ∈ Z
where isthesetofstatesofrandomvariableZ.WewriteX Y Z to
Z ⊥⊥ |
denotethatX isconditionallyindependentofY givenZ.
Definition 6.11 requires that the relation in (6.55) must hold true for
everyvalueofz.Theinterpretationof(6.55)canbeunderstoodas“given
knowledgeaboutz,thedistributionofxandy factorizes”.Independence
canbecastasaspecialcaseofconditionalindependenceifwewriteX
⊥⊥
Y . By using the product rule of probability (6.22), we can expand the
|∅
left-handsideof(6.55)toobtain
p(x,y z) = p(x y,z)p(y z). (6.56)
| | |
Bycomparingtheright-handsideof(6.55)with(6.56),weseethatp(y z)
|
appearsinbothofthemsothat
p(x y,z) = p(x z). (6.57)
| |
Equation(6.57)providesanalternativedefinitionofconditionalindepen-
dence, i.e., X Y Z. This alternative presentation provides the inter-
⊥⊥ |
pretation“giventhatweknowz,knowledgeabouty doesnotchangeour
knowledgeofx”.
6.4.6 Inner Products of Random Variables
RecallthedefinitionofinnerproductsfromSection3.2.Wecandefinean Innerproducts
innerproductbetweenrandomvariables,whichwebrieflydescribeinthis between
section.IfwehavetwouncorrelatedrandomvariablesX,Y,then multivariaterandom
variablescanbe
V[x+y] = V[x]+V[y]. (6.58) treatedinasimilar
fashion
Since variances are measured in squared units, this looks very much like
thePythagoreantheoremforrighttrianglesc2 = a2+b2.
In the following, we see whether we can find a geometric interpreta-
tion of the variance relation of uncorrelated random variables in (6.58).
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
196 ProbabilityandDistributions
Figure6.6
Geometryof
randomvariables.If
randomvariablesX
andY are
uncorrelated,they
areorthogonal
vectorsina
corresponding
vectorspace,and
+ v
ar[y]
thePythagorean
(cid:112) v
ar[x]
theoremapplies. + y] = c a (cid:112) var[x]
(cid:112) v
ar[x
b
(cid:112)
var[y]
Random variables can be considered vectors in a vector space, and we
can define inner products to obtain geometric properties of random vari-
ables(Eaton,2007).Ifwedefine
X,Y := Cov[x,y] (6.59)
⟨ ⟩
forzeromeanrandomvariablesX andY,weobtainaninnerproduct.We
Cov[x,x]=0 ⇐⇒ seethatthecovarianceissymmetric,positivedefinite,andlinearineither
x=0 argument.Thelengthofarandomvariableis
Cov[αx+z,y]=
(cid:113) (cid:113)
αCov[x,y]+ X = Cov[x,x] = V[x] = σ[x], (6.60)
Cov[z,y]forα∈R. ∥ ∥
i.e., its standard deviation. The “longer” the random variable, the more
uncertainitis;andarandomvariablewithlength0isdeterministic.
Ifwelookattheangleθ betweentworandomvariablesX,Y,weget
X,Y Cov[x,y]
cosθ = ⟨ ⟩ = , (6.61)
X Y
(cid:112)V[x]V[y]
∥ ∥ ∥ ∥
which is the correlation (Definition 6.8) between the two random vari-
ables. This means that we can think of correlation as the cosine of the
angle between two random variables when we consider them geometri-
cally. We know from Definition 3.7 that X Y X,Y = 0. In our
⊥ ⇐⇒ ⟨ ⟩
case,thismeansthatX andY areorthogonalifandonlyifCov[x,y] = 0,
i.e.,theyareuncorrelated.Figure6.6illustratesthisrelationship.
Remark. While it is tempting to use the Euclidean distance (constructed
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.5 GaussianDistribution 197
Figure6.7
Gaussian
distributionoftwo
randomvariablesx1
0.20 andx2.
0.15)
2
x
0.10,
1
x
(
0.05p
0.00
7.5
5.0
2.5
−1 0 2.0 5.0 x 2
x 1 5−.0
1
−
from the preceding definition of inner products) to compare probability
distributions, it is unfortunately not the best way to obtain distances be-
tween distributions. Recall that the probability mass (or density) is posi-
tive and needs to add up to 1. These constraints mean that distributions
live on something called a statistical manifold. The study of this space of
probability distributions is called information geometry. Computing dis-
tances between distributions are often done using Kullback-Leibler diver-
gence,whichisageneralizationofdistancesthataccountforpropertiesof
thestatisticalmanifold.JustliketheEuclideandistanceisaspecialcaseof
ametric(Section3.3),theKullback-Leiblerdivergenceisaspecialcaseof
two more general classes of divergences called Bregman divergences and
f-divergences.Thestudyofdivergencesisbeyondthescopeofthisbook,
and we refer for more details to the recent book by Amari (2016), one of
thefoundersofthefieldofinformationgeometry.
♢
6.5 Gaussian Distribution
TheGaussiandistributionisthemostwell-studiedprobabilitydistribution
forcontinuous-valuedrandomvariables.Itisalsoreferredtoasthenormal normaldistribution
distribution.Itsimportanceoriginatesfromthefactthatithasmanycom- TheGaussian
putationallyconvenientproperties,whichwewillbediscussinginthefol- distributionarises
naturallywhenwe
lowing. In particular, we will use it to define the likelihood and prior for
considersumsof
linear regression (Chapter 9), and consider a mixture of Gaussians for
independentand
densityestimation(Chapter11). identically
There are many other areas of machine learning that also benefit from distributedrandom
usingaGaussiandistribution,forexampleGaussianprocesses,variational variables.Thisis
knownasthe
inference, and reinforcement learning. It is also widely used in other ap-
centrallimit
plicationareassuchassignalprocessing(e.g.,Kalmanfilter),control(e.g., theorem(Grinstead
linearquadraticregulator),andstatistics(e.g.,hypothesistesting). andSnell,1997).
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
198 ProbabilityandDistributions
Figure6.8
8
Gaussian p(x) Mean
distributions 0.20 Mean 6 Sample
overlaidwith100 0.15 Sample 4
samples.(a)One- 2σ
2
dimensionalcase; 0.10
(b)two-dimensional 0
0.05
case.
2
−
0.00
4
−
5.0 2.5 0.0 2.5 5.0 7.5 1 0 1
− − x − x1
(a) Univariate (one-dimensional) Gaussian; (b) Multivariate (two-dimensional) Gaus-
The red cross shows the mean and the red sian, viewed from top. The red cross shows
lineshowstheextentofthevariance. themeanandthecoloredlinesshowthecon-
tourlinesofthedensity.
For a univariate random variable, the Gaussian distribution has a den-
sitythatisgivenby
1 (cid:18) (x µ)2(cid:19)
p(x µ,σ2) = exp − . (6.62)
| √2πσ2 − 2σ2
multivariate The multivariate Gaussian distribution is fully characterized by a mean
Gaussian vectorµandacovariancematrix Σanddefinedas
distribution
meanvector p(x µ,Σ) = (2π)−D 2 Σ −1 2 exp(cid:0) 1(x µ)⊤Σ−1(x µ)(cid:1) , (6.63)
covariancematrix | | | − 2 − −
Alsoknownasa where x RD. We write p(x) = (cid:0) x µ, Σ(cid:1) or X (cid:0) µ, Σ(cid:1) . Fig-
∈ N | ∼ N
multivariatenormal ure 6.7 shows a bivariate Gaussian (mesh), with the corresponding con-
distribution.
tourplot.Figure6.8showsaunivariateGaussianandabivariateGaussian
with corresponding samples. The special case of the Gaussian with zero
meanandidentitycovariance,thatis,µ = 0andΣ = I,isreferredtoas
standardnormal thestandardnormaldistribution.
distribution Gaussians are widely used in statistical estimation and machine learn-
ingastheyhaveclosed-formexpressionsformarginalandconditionaldis-
tributions.InChapter9,weusetheseclosed-formexpressionsextensively
for linear regression. A major advantage of modeling with Gaussian ran-
domvariablesisthatvariabletransformations(Section6.7)areoftennot
needed. Since the Gaussian distribution is fully specified by its mean and
covariance, we often can obtain the transformed distribution by applying
thetransformationtothemeanandcovarianceoftherandomvariable.
6.5.1 Marginals and Conditionals of Gaussians are Gaussians
Inthefollowing,wepresentmarginalizationandconditioninginthegen-
eralcaseofmultivariaterandomvariables.Ifthisisconfusingatfirstread-
ing,thereaderisadvisedtoconsidertwounivariaterandomvariablesin-
stead. Let X and Y be two multivariate random variables, that may have
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2x
6.5 GaussianDistribution 199
different dimensions. To consider the effect of applying the sum rule of
probability and the effect of conditioning, we explicitly write the Gaus-
siandistributionintermsoftheconcatenatedstates[x⊤,y⊤],
(cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)
µ Σ Σ
p(x,y) = x , xx xy . (6.64)
N µ y Σ yx Σ yy
where Σ = Cov[x,x] and Σ = Cov[y,y] are the marginal covari-
xx yy
ance matrices of x and y, respectively, and Σ = Cov[x,y] is the cross-
xy
covariancematrixbetweenxandy.
Theconditionaldistributionp(x y)isalsoGaussian(illustratedinFig-
|
ure6.9(c))andgivenby(derivedinSection2.3ofBishop,2006)
(cid:0) (cid:1)
p(x y) = µ , Σ (6.65)
| N x|y x|y
µ = µ +Σ Σ−1(y µ ) (6.66)
x|y x xy yy − y
Σ = Σ Σ Σ−1Σ . (6.67)
x|y xx − xy yy yx
Note that in the computation of the mean in (6.66), the y-value is an
observationandnolongerrandom.
Remark. The conditional Gaussian distribution shows up in many places,
whereweareinterestedinposteriordistributions:
The Kalman filter (Kalman, 1960), one of the most central algorithms
for state estimation in signal processing, does nothing but computing
Gaussian conditionals of joint distributions (Deisenroth and Ohlsson,
2011;Sa¨rkka¨,2013).
Gaussianprocesses(RasmussenandWilliams,2006),whichareaprac-
ticalimplementationofadistributionoverfunctions.InaGaussianpro-
cess,wemakeassumptionsofjointGaussianityofrandomvariables.By
(Gaussian) conditioning on observed data, we can determine a poste-
riordistributionoverfunctions.
Latent linear Gaussian models (Roweis and Ghahramani, 1999; Mur-
phy, 2012), which include probabilistic principal component analysis
(PPCA) (Tipping and Bishop, 1999). We will look at PPCA in more de-
tailinSection10.7.
♢
The marginal distribution p(x) of a joint Gaussian distribution p(x,y)
(see (6.64)) is itself Gaussian and computed by applying the sum rule
(6.20)andgivenby
(cid:90)
(cid:0) (cid:1)
p(x) = p(x,y)dy = x µ , Σ . (6.68)
N | x xx
Thecorrespondingresultholdsforp(y),whichisobtainedbymarginaliz-
ingwithrespecttox.Intuitively,lookingatthejointdistributionin(6.64),
we ignore (i.e., integrate out) everything we are not interested in. This is
illustratedinFigure6.9(b).
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
200 ProbabilityandDistributions
Example 6.6
Figure6.9
(a)Bivariate 8
Gaussian;
6
(b)marginalofa
jointGaussian 4
distributionis
2
Gaussian;(c)the
conditional 0 x2= −1
distributionofa
2
Gaussianisalso −
Gaussian. 4
−
1 0 1
− x1
(a)BivariateGaussian.
p(x1) 1.2 p(x1|x2= −1)
0.6 Mean 1.0 Mean
2σ 2σ
0.8
0.4
0.6
0.4
0.2
0.2
0.0 0.0
1.5 1.0 0.5 0.0 0.5 1.0 1.5 1.5 1.0 0.5 0.0 0.5 1.0 1.5
− − − x1 − − − x1
(b)Marginaldistribution. (c)Conditionaldistribution.
ConsiderthebivariateGaussiandistribution(illustratedinFigure6.9):
(cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)
0 0.3 1
p(x ,x ) = , − . (6.69)
1 2 N 2 1 5
−
We can compute the parameters of the univariate Gaussian, conditioned
on x = 1, by applying (6.66) and (6.67) to obtain the mean and vari-
2
−
ancerespectively.Numerically,thisis
µ = 0+( 1) 0.2 ( 1 2) = 0.6 (6.70)
x1|x2=−1
− · · − −
and
σ2 = 0.3 ( 1) 0.2 ( 1) = 0.1. (6.71)
x1|x2=−1 − − · · −
Therefore,theconditionalGaussianisgivenby
(cid:0) (cid:1)
p(x x = 1) = 0.6, 0.1 . (6.72)
1 2
| − N
The marginal distribution p(x ), in contrast, can be obtained by apply-
1
ing(6.68),whichisessentiallyusingthemeanandvarianceoftherandom
variablex ,givingus
1
(cid:0) (cid:1)
p(x ) = 0, 0.3 . (6.73)
1
N
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2x
6.5 GaussianDistribution 201
6.5.2 Product of Gaussian Densities
For linear regression (Chapter 9), we need to compute a Gaussian likeli-
hood.Furthermore,wemaywishtoassumeaGaussianprior(Section9.3).
WeapplyBayes’Theoremtocomputetheposterior,whichresultsinamul-
tiplicationofthelikelihoodandtheprior,thatis,themultiplicationoftwo
(cid:0) (cid:1) (cid:0) (cid:1)
Gaussiandensities.TheproductoftwoGaussians x a, A x b, B Thederivationisan
isaGaussiandistributionscaledbyac R,giveN nbyc| (cid:0) x N c, C(cid:1)| with exerciseattheend
∈ N | ofthischapter.
C = (A−1+B−1)−1 (6.74)
c = C(A−1a+B−1b) (6.75)
c = (2π)−D 2 A+B −1 2 exp(cid:0) 1(a b)⊤(A+B)−1(a b)(cid:1) . (6.76)
| | − 2 − −
The scaling constant c itself can be written in the form of a Gaussian
density either in a or in b with an “inflated” covariance matrix A + B,
(cid:0) (cid:1) (cid:0) (cid:1)
i.e.,c = a b, A+B = b a, A+B .
N | N |
(cid:0) (cid:1)
Remark. For notation convenience, we will sometimes use x m, S
N |
to describe the functional form of a Gaussian density even if x is not a
random variable. We have just done this in the preceding demonstration
whenwewrote
(cid:0) (cid:1) (cid:0) (cid:1)
c = a b, A+B = b a, A+B . (6.77)
N | N |
Here,neitheranorbarerandomvariables.However,writingcinthisway
ismorecompactthan(6.76).
♢
6.5.3 Sums and Linear Transformations
If X,Y are independent Gaussian random variables (i.e., the joint distri-
(cid:0) (cid:1)
bution is given as p(x,y) = p(x)p(y)) with p(x) = x µ , Σ and
(cid:0) (cid:1) N | x x
p(y) = y µ , Σ ,thenx+y isalsoGaussiandistributedandgiven
N | y y
by
(cid:0) (cid:1)
p(x+y) = µ +µ , Σ +Σ . (6.78)
N x y x y
Knowing that p(x+y) is Gaussian, the mean and covariance matrix can
bedeterminedimmediatelyusingtheresultsfrom(6.46)through(6.49).
This property will be important when we consider i.i.d. Gaussian noise
acting on random variables, as is the case for linear regression (Chap-
ter9).
Example 6.7
Sinceexpectationsarelinearoperations,wecanobtaintheweightedsum
ofindependentGaussianrandomvariables
p(ax+by) = (cid:0) aµ +bµ , a2Σ +b2Σ (cid:1) . (6.79)
N x y x y
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
202 ProbabilityandDistributions
Remark. A case that will be useful in Chapter 11 is the weighted sum of
Gaussian densities. This is different from the weighted sum of Gaussian
randomvariables.
♢
In Theorem 6.12, the random variable x is from a density that is a
mixtureoftwodensitiesp (x)andp (x),weightedbyα.Thetheoremcan
1 2
begeneralizedtothemultivariaterandomvariablecase,sincelinearityof
expectations holds also for multivariate random variables. However, the
ideaofasquaredrandomvariableneedstobereplacedbyxx⊤.
Theorem 6.12. ConsideramixtureoftwounivariateGaussiandensities
p(x) = αp (x)+(1 α)p (x), (6.80)
1 2
−
where the scalar 0 < α < 1 is the mixture weight, and p (x) and p (x) are
1 2
univariate Gaussian densities (Equation (6.62)) with different parameters,
i.e.,(µ ,σ2) = (µ ,σ2).
1 1 ̸ 2 2
Then the mean of the mixture density p(x) is given by the weighted sum
ofthemeansofeachrandomvariable:
E[x] = αµ +(1 α)µ . (6.81)
1 2
−
Thevarianceofthemixturedensityp(x)isgivenby
(cid:16) (cid:17)
V[x] = (cid:2) ασ2+(1 α)σ2(cid:3) + (cid:2) αµ2+(1 α)µ2(cid:3) [αµ +(1 α)µ ]2 .
1 − 2 1 − 2 − 1 − 2
(6.82)
Proof The mean of the mixture density p(x) is given by the weighted
sumofthemeansofeachrandomvariable.Weapplythedefinitionofthe
mean(Definition6.4),andpluginourmixture(6.80),whichyields
(cid:90) ∞
E[x] = xp(x)dx (6.83a)
−∞
(cid:90) ∞
= (αxp (x)+(1 α)xp (x))dx (6.83b)
1 2
−
−∞
(cid:90) ∞ (cid:90) ∞
= α xp (x)dx+(1 α) xp (x)dx (6.83c)
1 2
−
−∞ −∞
= αµ +(1 α)µ . (6.83d)
1 2
−
To compute the variance, we can use the raw-score version of the vari-
ance from (6.44), which requires an expression of the expectation of the
squaredrandomvariable.Hereweusethedefinitionofanexpectationof
afunction(thesquare)ofarandomvariable(Definition6.3),
(cid:90) ∞
E[x2] = x2p(x)dx (6.84a)
−∞
(cid:90) ∞
= (cid:0) αx2p (x)+(1 α)x2p (x)(cid:1) dx (6.84b)
1 2
−
−∞
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.5 GaussianDistribution 203
(cid:90) ∞ (cid:90) ∞
= α x2p (x)dx+(1 α) x2p (x)dx (6.84c)
1 2
−
−∞ −∞
= α(µ2+σ2)+(1 α)(µ2+σ2), (6.84d)
1 1 − 2 2
where in the last equality, we again used the raw-score version of the
variance (6.44) giving σ2 = E[x2] µ2. This is rearranged such that the
−
expectationofasquaredrandomvariableisthesumofthesquaredmean
andthevariance.
Therefore,thevarianceisgivenbysubtracting(6.83d)from(6.84d),
V[x] = E[x2] (E[x])2 (6.85a)
−
= α(µ2+σ2)+(1 α)(µ2+σ2) (αµ +(1 α)µ )2 (6.85b)
1 1 − 2 2 − 1 − 2
= (cid:2) ασ2+(1 α)σ2(cid:3)
1 − 2
(cid:16) (cid:17)
+ (cid:2) αµ2+(1 α)µ2(cid:3) [αµ +(1 α)µ ]2 . (6.85c)
1 − 2 − 1 − 2
Remark. The preceding derivation holds for any density, but since the
Gaussian is fully determined by the mean and variance, the mixture den-
sitycanbedeterminedinclosedform.
♢
For a mixture density, the individual components can be considered
to be conditional distributions (conditioned on the component identity).
Equation (6.85c) is an example of the conditional variance formula, also
knownasthelawoftotalvariance,whichgenerallystatesthatfortworan- lawoftotalvariance
domvariablesX andY itholdsthatV [x] = E [V [x y]]+V [E [x y]],
X Y X Y X
| |
i.e.,the(total)varianceofX istheexpectedconditionalvarianceplusthe
varianceofaconditionalmean.
We consider in Example 6.17 a bivariate standard Gaussian random
variableX andperformedalineartransformationAxonit.Theoutcome
isaGaussianrandomvariablewithmeanzeroandcovarianceAA⊤.Ob-
serve that adding a constant vector will change the mean of the distribu-
tion, without affecting its variance, that is, the random variable x+µ is
Gaussian with mean µ and identity covariance. Hence, any linear/affine
transformationofaGaussianrandomvariableisGaussiandistributed. Anylinear/affine
(cid:0) (cid:1)
Consider a Gaussian distributed random variable X µ, Σ . For transformationofa
a given matrix A of appropriate shape, let Y be a rando∼ mN variable such Gaussianrandom
variableisalso
that y = Ax is a transformed version of x. We can compute the mean of
Gaussian
y byexploitingthattheexpectationisalinearoperator(6.50)asfollows: distributed.
E[y] = E[Ax] = AE[x] = Aµ. (6.86)
Similarlythevarianceofy canbefoundbyusing(6.51):
V[y] = V[Ax] = AV[x]A⊤ = AΣA⊤. (6.87)
Thismeansthattherandomvariabley isdistributedaccordingto
p(y) =
(cid:0)
y Aµ,
AΣA⊤(cid:1)
. (6.88)
N |
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
204 ProbabilityandDistributions
Let us now consider the reverse transformation: when we know that a
random variable has a mean that is a linear transformation of another
randomvariable.ForagivenfullrankmatrixA RM×N,whereM ⩾ N,
lety RM beaGaussianrandomvariablewith∈ meanAx,i.e.,
∈
(cid:0) (cid:1)
p(y) = y Ax, Σ . (6.89)
N |
What is the corresponding probability distribution p(x)? If A is invert-
ible, then we can write x = A−1y and apply the transformation in the
previous paragraph. However, in general A is not invertible, and we use
an approach similar to that of the pseudo-inverse (3.57). That is, we pre-
multiply both sides with A⊤ and then invert A⊤A, which is symmetric
andpositivedefinite,givingustherelation
y = Ax (A⊤A)−1A⊤y = x. (6.90)
⇐⇒
Hence,xisalineartransformationofy,andweobtain
p(x) = (cid:0) x (A⊤A)−1A⊤y, (A⊤A)−1A⊤ΣA(A⊤A)−1(cid:1) . (6.91)
N |
6.5.4 Sampling from Multivariate Gaussian Distributions
Wewillnotexplainthesubtletiesofrandomsamplingonacomputer,and
the interested reader is referred to Gentle (2004). In the case of a mul-
tivariate Gaussian, this process consists of three stages: first, we need a
source of pseudo-random numbers that provide a uniform sample in the
interval [0,1]; second, we use a non-linear transformation such as the
Box-Mu¨llertransform(Devroye,1986)toobtainasamplefromaunivari-
ate Gaussian; and third, we collate a vector of these samples to obtain a
(cid:0) (cid:1)
samplefromamultivariatestandardnormal 0, I .
N
For a general multivariate Gaussian, that is, where the mean is non
zero and the covariance is not the identity matrix, we use the proper-
ties of linear transformations of a Gaussian random variable. Assume we
are interested in generating samples x ,i = 1,...,n, from a multivariate
i
Tocomputethe Gaussian distribution with mean µ and covariance matrix Σ. We would
Cholesky like to construct the sample from a sampler that provides samples from
factorizationofa themultivariatestandardnormal (cid:0) 0, I(cid:1) .
matrix,itisrequired N (cid:0) (cid:1)
To obtain samples from a multivariate normal µ, Σ , we can use
thatthematrixis
N
symmetricand the properties of a linear transformation of a Gaussian random variable:
positivedefinite If x (cid:0) 0, I(cid:1) , then y = Ax+µ, where AA⊤ = Σ is Gaussian dis-
(Section3.2.3). ∼ N
tributedwithmeanµandcovariancematrixΣ.Oneconvenientchoiceof
Covariancematrices
A is to use the Cholesky decomposition (Section 4.3) of the covariance
possessthis
property. matrix Σ = AA⊤. The Cholesky decomposition has the benefit that A is
triangular,leadingtoefficientcomputation.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.6 ConjugacyandtheExponentialFamily 205
6.6 Conjugacy and the Exponential Family
Many of the probability distributions “with names” that we find in statis-
tics textbooks were discovered to model particular types of phenomena.
For example, we have seen the Gaussian distribution in Section 6.5. The
distributions are also related to each other in complex ways (Leemis and
McQueston,2008).Forabeginnerinthefield,itcanbeoverwhelmingto
figure out which distribution to use. In addition, many of these distribu-
tionswerediscoveredatatimethatstatisticsandcomputationweredone “Computers”usedto
by pencil and paper. It is natural to ask what are meaningful concepts beajobdescription.
in the computing age (Efron and Hastie, 2016). In the previous section,
we saw that many of the operations required for inference can be conve-
niently calculated when the distribution is Gaussian. It is worth recalling
at this point the desiderata for manipulating probability distributions in
themachinelearningcontext:
1. Thereissome“closureproperty”whenapplyingtherulesofprobability,
e.g., Bayes’ theorem. By closure, we mean that applying a particular
operationreturnsanobjectofthesametype.
2. As we collect more data, we do not need more parameters to describe
thedistribution.
3. Since we are interested in learning from data, we want parameter es-
timationtobehavenicely.
It turns out that the class of distributions called the exponential family exponentialfamily
provides the right balance of generality while retaining favorable compu-
tationandinferenceproperties.Beforeweintroducetheexponentialfam-
ily, let us see three more members of “named” probability distributions,
the Bernoulli (Example 6.8), Binomial (Example 6.9), and Beta (Exam-
ple6.10)distributions.
Example 6.8
The Bernoulli distribution is a distribution for a single binary random Bernoulli
variableX withstatex 0,1 .Itisgovernedbyasinglecontinuouspa- distribution
∈ { }
rameterµ [0,1]thatrepresentstheprobabilityofX = 1.TheBernoulli
∈
distributionBer(µ)isdefinedas
p(x µ) = µx(1 µ)1−x, x 0,1 , (6.92)
| − ∈ { }
E[x] = µ, (6.93)
V[x] = µ(1 µ), (6.94)
−
where E[x] and V[x] are the mean and variance of the binary random
variableX.
An example where the Bernoulli distribution can be used is when we
areinterestedinmodelingtheprobabilityof“heads”whenflippingacoin.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
206 ProbabilityandDistributions
Figure6.10
Examplesofthe µ=0.1
Binomial 0.3 µ=0.4
distributionfor
µ=0.75
µ∈{0.1,0.4,0.75}
andN =15.
0.2
0.1
0.0
0.0 2.5 5.0 7.5 10.0 12.5 15.0
Numbermofobservationsx=1inN =15experiments
Remark. The rewriting above of the Bernoulli distribution, where we use
Booleanvariablesasnumerical0or1andexpressthemintheexponents,
is a trick that is often used in machine learning textbooks. Another oc-
curenceofthisiswhenexpressingtheMultinomialdistribution.
♢
Example 6.9 (Binomial Distribution)
Binomial The BinomialdistributionisageneralizationoftheBernoullidistribution
distribution to a distribution over integers (illustrated in Figure 6.10). In particular,
the Binomial can be used to describe the probability of observing m oc-
currences of X = 1 in a set of N samples from a Bernoulli distribution
where p(X = 1) = µ [0,1]. The Binomial distribution Bin(N,µ) is
∈
definedas
(cid:32) (cid:33)
N
p(m N,µ) = µm(1 µ)N−m, (6.95)
| m −
E[m] = Nµ, (6.96)
V[m] = Nµ(1 µ), (6.97)
−
whereE[m]andV[m]arethemeanandvarianceofm,respectively.
AnexamplewheretheBinomialcouldbeusedisifwewanttodescribe
the probability of observing m “heads” in N coin-flip experiments if the
probabilityforobservingheadinasingleexperimentisµ.
Example 6.10 (Beta Distribution)
We may wish to model a continuous random variable on a finite interval.
Betadistribution The Beta distribution is a distribution over a continuous random variable
µ [0,1],whichisoftenusedtorepresenttheprobabilityforsomebinary
∈
event(e.g.,theparametergoverningtheBernoullidistribution).TheBeta
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
)m(p
6.6 ConjugacyandtheExponentialFamily 207
distribution Beta(α,β) (illustrated in Figure 6.11) itself is governed by
twoparametersα > 0, β > 0andisdefinedas
Γ(α+β)
p(µ α,β) = µα−1(1 µ)β−1 (6.98)
| Γ(α)Γ(β) −
α αβ
E[µ] = , V[µ] = (6.99)
α+β (α+β)2(α+β+1)
whereΓ( )istheGammafunctiondefinedas
·
(cid:90) ∞
Γ(t) := xt−1exp( x)dx, t > 0. (6.100)
−
0
Γ(t+1) = tΓ(t). (6.101)
Note that the fraction of Gamma functions in (6.98) normalizes the Beta
distribution.
10 Figure6.11
α=0.5=β Examplesofthe
8 α=1=β Betadistributionfor
α=2,β=0.3 differentvaluesofα
andβ.
6 α=4,β=10
α=5,β=1
4
2
0
0.0 0.2 0.4 0.6 0.8 1.0
µ
Intuitively,αmovesprobabilitymasstoward1,whereasβ movesprob-
abilitymasstoward0.Therearesomespecialcases(Murphy,2012):
Forα = 1 = β,weobtaintheuniformdistribution [0,1].
U
Forα,β < 1,wegetabimodaldistributionwithspikesat0and1.
Forα,β > 1,thedistributionisunimodal.
For α,β > 1 and α = β, the distribution is unimodal, symmetric, and
centeredintheinterval[0,1],i.e.,themode/meanisat 1.
2
Remark. There is a whole zoo of distributions with names, and they are
related in different ways to each other (Leemis and McQueston, 2008).
It is worth keeping in mind that each named distribution is created for a
particular reason, but may have other applications. Knowing the reason
behind the creation of a particular distribution often allows insight into
how to best use it. We introduced the preceding three distributions to be
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
)β,αµ(p
|
208 ProbabilityandDistributions
able to illustrate the concepts of conjugacy (Section 6.6.1) and exponen-
tialfamilies(Section6.6.3).
♢
6.6.1 Conjugacy
According to Bayes’ theorem (6.23), the posterior is proportional to the
product of the prior and the likelihood. The specification of the prior can
be tricky for two reasons: First, the prior should encapsulate our knowl-
edge about the problem before we see any data. This is often difficult to
describe.Second,itisoftennotpossibletocomputetheposteriordistribu-
tionanalytically.However,therearesomepriorsthatarecomputationally
conjugateprior convenient:conjugatepriors.
conjugate Definition 6.13 (Conjugate Prior). A prior is conjugate for the likelihood
functioniftheposteriorisofthesameform/typeastheprior.
Conjugacy is particularly convenient because we can algebraically cal-
culate our posterior distribution by updating the parameters of the prior
distribution.
Remark. Whenconsideringthegeometryofprobabilitydistributions,con-
jugatepriorsretainthesamedistancestructureasthelikelihood(Agarwal
andDaum´eIII,2010).
♢
Tointroduceaconcreteexampleofconjugatepriors,wedescribeinEx-
ample 6.11 the Binomial distribution (defined on discrete random vari-
ables) and the Beta distribution (defined on continuous random vari-
ables).
Example 6.11 (Beta-Binomial Conjugacy)
ConsideraBinomialrandomvariablex Bin(N,µ)where
∼
(cid:32) (cid:33)
N
p(x N,µ) = µx(1 µ)N−x, x = 0,1,...,N , (6.102)
| x −
is the probability of finding x times the outcome “heads” in N coin flips,
where µ is the probability of a “head”. We place a Beta prior on the pa-
rameterµ,thatis,µ Beta(α,β),where
∼
Γ(α+β)
p(µ α,β) = µα−1(1 µ)β−1. (6.103)
| Γ(α)Γ(β) −
Ifwenowobservesomeoutcomex = h,thatis,weseehheadsinN coin
flips,wecomputetheposteriordistributiononµas
p(µ x = h,N,α,β) p(x N,µ)p(µ α,β) (6.104a)
| ∝ | |
µh(1 µ)(N−h)µα−1(1 µ)β−1 (6.104b)
∝ − −
= µh+α−1(1 µ)(N−h)+β−1 (6.104c)
−
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.6 ConjugacyandtheExponentialFamily 209
Likelihood Conjugateprior Posterior Table6.2 Examples
Bernoulli Beta Beta ofconjugatepriors
Binomial Beta Beta forcommon
Gaussian Gaussian/inverseGamma Gaussian/inverseGamma likelihoodfunctions.
Gaussian Gaussian/inverseWishart Gaussian/inverseWishart
Multinomial Dirichlet Dirichlet
Beta(h+α,N h+β), (6.104d)
∝ −
i.e., the posterior distribution is a Beta distribution as the prior, i.e., the
Beta prior is conjugate for the parameter µ in the Binomial likelihood
function.
In the following example, we will derive a result that is similar to the
Beta-Binomialconjugacyresult.HerewewillshowthattheBetadistribu-
tionisaconjugatepriorfortheBernoullidistribution.
Example 6.12 (Beta-Bernoulli Conjugacy)
Let x 0,1 be distributed according to the Bernoulli distribution with
∈ { }
parameter θ [0,1], that is, p(x = 1 θ) = θ. This can also be expressed
∈ |
as p(x θ) = θx(1 θ)1−x. Let θ be distributed according to a Beta distri-
| −
butionwithparametersα,β,thatis,p(θ α,β) θα−1(1 θ)β−1.
| ∝ −
MultiplyingtheBetaandtheBernoullidistributions,weget
p(θ x,α,β) = p(x θ)p(θ α,β) (6.105a)
| | |
θx(1 θ)1−xθα−1(1 θ)β−1 (6.105b)
∝ − −
= θα+x−1(1 θ)β+(1−x)−1 (6.105c)
−
p(θ α+x,β+(1 x)). (6.105d)
∝ | −
ThelastlineistheBetadistributionwithparameters(α+x,β+(1 x)).
−
Table6.2listsexamplesforconjugatepriorsfortheparametersofsome
standard likelihoods used in probabilistic modeling. Distributions such as TheGammaprioris
Multinomial,inverseGamma,inverseWishart,andDirichletcanbefound conjugateforthe
precision(inverse
inanystatisticaltext,andaredescribedinBishop(2006),forexample.
variance)inthe
TheBetadistributionistheconjugatepriorfortheparameterµinboth
univariateGaussian
theBinomialandtheBernoullilikelihood.ForaGaussianlikelihoodfunc- likelihood,andthe
tion, we can place a conjugate Gaussian prior on the mean. The reason Wishartprioris
why the Gaussian likelihood appears twice in the table is that we need conjugateforthe
precisionmatrix
to distinguish the univariate from the multivariate case. In the univariate
(inversecovariance
(scalar) case, the inverse Gamma is the conjugate prior for the variance. matrix)inthe
In the multivariate case, we use a conjugate inverse Wishart distribution multivariate
asaprioronthecovariancematrix.TheDirichletdistributionistheconju- Gaussianlikelihood.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
210 ProbabilityandDistributions
gate prior for the multinomial likelihood function. For further details, we
refertoBishop(2006).
6.6.2 Sufficient Statistics
Recall that a statistic of a random variable is a deterministic function of
that random variable. For example, if x = [x ,...,x ]⊤ is a vector of
1 N
(cid:0) (cid:1)
univariate Gaussian random variables, that is, x µ, σ2 , then the
n
∼ N
sample mean µˆ = 1(x + +x ) is a statistic. Sir Ronald Fisher dis-
N 1 ··· N
sufficientstatistics covered the notion of sufficient statistics: the idea that there are statistics
that will contain all available information that can be inferred from data
correspondingtothedistributionunderconsideration.Inotherwords,suf-
ficientstatisticscarryalltheinformationneededtomakeinferenceabout
the population, that is, they are the statistics that are sufficient to repre-
sentthedistribution.
Forasetofdistributionsparametrizedbyθ,letX bearandomvariable
withdistributionp(x θ )givenanunknownθ .Avectorϕ(x)ofstatistics
0 0
|
is called sufficient statistics for θ if they contain all possible informa-
0
tionaboutθ .Tobemoreformalabout“containallpossibleinformation”,
0
this means that the probability of x given θ can be factored into a part
that does not depend on θ, and a part that depends on θ only via ϕ(x).
The Fisher-Neyman factorization theorem formalizes this notion, which
westateinTheorem6.14withoutproof.
Theorem 6.14 (Fisher-Neyman). [Theorem 6.5 in Lehmann and Casella
Fisher-Neyman (1998)] Let X have probability density function p(x θ). Then the statistics
|
theorem ϕ(x)aresufficientforθ ifandonlyifp(x θ)canbewrittenintheform
|
p(x θ) = h(x)g (ϕ(x)), (6.106)
θ
|
where h(x) is a distribution independent of θ and g captures all the depen-
θ
denceonθ viasufficientstatisticsϕ(x).
Ifp(x θ)doesnotdependonθ,thenϕ(x)istriviallyasufficientstatistic
|
for any function ϕ. The more interesting case is that p(x θ) is dependent
|
only on ϕ(x) and not x itself. In this case, ϕ(x) is a sufficient statistic for
θ.
In machine learning, we consider a finite number of samples from a
distribution. One could imagine that for simple distributions (such as the
Bernoulli in Example 6.8) we only need a small number of samples to
estimate the parameters of the distributions. We could also consider the
opposite problem: If we have a set of data (a sample from an unknown
distribution), which distribution gives the best fit? A natural question to
ask is, as we observe more data, do we need more parameters θ to de-
scribe the distribution? It turns out that the answer is yes in general, and
thisisstudiedinnon-parametricstatistics(Wasserman,2007).Aconverse
questionistoconsiderwhichclassofdistributionshavefinite-dimensional
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.6 ConjugacyandtheExponentialFamily 211
sufficient statistics, that is the number of parameters needed to describe
them does not increase arbitrarily. The answer is exponential family dis-
tributions,describedinthefollowingsection.
6.6.3 Exponential Family
There are three possible levels of abstraction we can have when con-
sidering distributions (of discrete or continuous random variables). At
level one (the most concrete end of the spectrum), we have a particu-
lar named distribution with fixed parameters, for example a univariate
(cid:0) (cid:1)
Gaussian 0, 1 withzeromeanandunitvariance.Inmachinelearning,
N
we often use the second level of abstraction, that is, we fix the paramet-
ricform(theunivariateGaussian)andinfertheparametersfromdata.For
(cid:0) (cid:1)
example,weassumeaunivariateGaussian µ, σ2 withunknownmean
N
µ and unknown variance σ2, and use a maximum likelihood fit to deter-
mine the best parameters (µ,σ2). We will see an example of this when
considering linear regression in Chapter 9. A third level of abstraction is
toconsiderfamiliesofdistributions,andinthisbook,weconsidertheex-
ponential family. The univariate Gaussian is an example of a member of
theexponentialfamily.Manyofthewidelyusedstatisticalmodels,includ-
ing all the “named” models in Table 6.2, are members of the exponential
family.Theycanallbeunifiedintooneconcept(Brown,1986).
Remark. A brief historical anecdote: Like many concepts in mathemat-
ics and science, exponential families were independently discovered at
the same time by different researchers. In the years 1935–1936, Edwin
Pitman in Tasmania, Georges Darmois in Paris, and Bernard Koopman in
NewYorkindependentlyshowedthattheexponentialfamiliesaretheonly
families that enjoy finite-dimensional sufficient statistics under repeated
independentsampling(LehmannandCasella,1998).
♢
An exponential family is a family of probability distributions, parame- exponentialfamily
terizedbyθ RD,oftheform
∈
p(x θ) = h(x)exp( θ,ϕ(x) A(θ)) , (6.107)
| ⟨ ⟩−
whereϕ(x)isthevectorofsufficientstatistics.Ingeneral,anyinnerprod-
uct(Section3.2)canbeusedin(6.107),andforconcretenesswewilluse
thestandarddotproducthere( θ,ϕ(x) = θ⊤ϕ(x)).Notethattheform
⟨ ⟩
oftheexponentialfamilyisessentiallyaparticularexpressionofg (ϕ(x))
θ
intheFisher-Neymantheorem(Theorem6.14).
The factor h(x) can be absorbed into the dot product term by adding
another entry (logh(x)) to the vector of sufficient statistics ϕ(x), and
constraining the corresponding parameter θ = 1. The term A(θ) is the
0
normalizationconstantthatensuresthatthedistributionsumsuporinte-
grates to one and is called the log-partition function. A good intuitive no- log-partition
tion of exponential families can be obtained by ignoring these two terms function
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
212 ProbabilityandDistributions
andconsideringexponentialfamiliesasdistributionsoftheform
p(x θ)
exp(cid:0) θ⊤ϕ(x)(cid:1)
. (6.108)
| ∝
naturalparameters For this form of parametrization, the parameters θ are called the natural
parameters. At first glance, it seems that exponential families are a mun-
danetransformationbyaddingtheexponentialfunctiontotheresultofa
dot product. However, there are many implications that allow for conve-
nient modeling and efficient computation based on the fact that we can
captureinformationaboutdatainϕ(x).
Example 6.13 (Gaussian as Exponential Family)
(cid:20) (cid:21)
(cid:0) (cid:1) x
ConsidertheunivariateGaussiandistribution µ, σ2 .Letϕ(x) = .
N x2
Thenbyusingthedefinitionoftheexponentialfamily,
p(x θ) exp(θ x+θ x2). (6.109)
1 2
| ∝
Setting
(cid:20) µ 1 (cid:21)⊤
θ = , (6.110)
σ2 −2σ2
andsubstitutinginto(6.109),weobtain
(cid:18)µx x2 (cid:19) (cid:18) 1 (cid:19)
p(x θ) exp exp (x µ)2 . (6.111)
| ∝ σ2 − 2σ2 ∝ −2σ2 −
Therefore, the univariate Gaussian distribution is a member of the expo-
(cid:20) (cid:21)
x
nential family with sufficient statistic ϕ(x) = , and natural parame-
x2
tersgivenbyθ in(6.110).
Example 6.14 (Bernoulli as Exponential Family)
RecalltheBernoullidistributionfromExample6.8
p(x µ) = µx(1 µ)1−x, x 0,1 . (6.112)
| − ∈ { }
Thiscanbewritteninexponentialfamilyform
p(x µ) = exp(cid:2) log(cid:0) µx(1 µ)1−x(cid:1)(cid:3) (6.113a)
| −
= exp[xlogµ+(1 x)log(1 µ)] (6.113b)
− −
= exp[xlogµ xlog(1 µ)+log(1 µ)] (6.113c)
− − −
(cid:104) (cid:105)
= exp xlog µ +log(1 µ) . (6.113d)
1−µ −
The last line (6.113d) can be identified as being in exponential family
form(6.107)byobservingthat
h(x) = 1 (6.114)
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.6 ConjugacyandtheExponentialFamily 213
θ = log µ (6.115)
1−µ
ϕ(x) = x (6.116)
A(θ) = log(1 µ) = log(1+exp(θ)). (6.117)
− −
Therelationshipbetweenθ andµisinvertiblesothat
1
µ = . (6.118)
1+exp( θ)
−
Therelation(6.118)isusedtoobtaintherightequalityof(6.117).
Remark. TherelationshipbetweentheoriginalBernoulliparameterµand
the natural parameter θ is known as the sigmoid or logistic function. Ob- sigmoid
serve that µ (0,1) but θ R, and therefore the sigmoid function
∈ ∈
squeezes a real value into the range (0,1). This property is useful in ma-
chinelearning,forexampleitisusedinlogisticregression(Bishop,2006,
section4.3.2),aswellasasanonlinearactivationfunctionsinneuralnet-
works(Goodfellowetal.,2016,chapter6).
♢
Itisoftennotobvioushowtofindtheparametricformoftheconjugate
distribution of a particular distribution (for example, those in Table 6.2).
Exponential families provide a convenient way to find conjugate pairs of
distributions. Consider the random variable X is a member of the expo-
nentialfamily(6.107):
p(x θ) = h(x)exp( θ,ϕ(x) A(θ)) . (6.119)
| ⟨ ⟩−
Every member of the exponential family has a conjugate prior (Brown,
1986)
(cid:18)(cid:28)(cid:20) (cid:21) (cid:20) (cid:21)(cid:29) (cid:19)
γ θ
p(θ γ) = h (θ)exp 1 , A (γ) , (6.120)
| c γ 2 A(θ) − c
−
(cid:20) (cid:21)
γ
where γ = 1 has dimension dim(θ) + 1. The sufficient statistics of
γ
2
(cid:20) (cid:21)
θ
the conjugate prior are . By using the knowledge of the general
A(θ)
−
formofconjugatepriorsforexponentialfamilies,wecanderivefunctional
formsofconjugatepriorscorrespondingtoparticulardistributions.
Example 6.15
RecalltheexponentialfamilyformoftheBernoullidistribution(6.113d)
(cid:20) µ (cid:21)
p(x µ) = exp xlog +log(1 µ) . (6.121)
| 1 µ −
−
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
214 ProbabilityandDistributions
Thecanonicalconjugatepriorhastheform
µ (cid:20) µ (cid:21)
p(µ α,β) = exp αlog +(β+α)log(1 µ) A (γ) ,
| 1 µ 1 µ − − c
− −
(6.122)
where we defined γ := [α,β + α]⊤ and h (µ) := µ/(1 µ). Equa-
c
−
tion(6.122)thensimplifiesto
p(µ α,β) = exp[(α 1)logµ+(β 1)log(1 µ) A (α,β)] .
c
| − − − −
(6.123)
Puttingthisinnon-exponentialfamilyformyields
p(µ α,β) µα−1(1 µ)β−1, (6.124)
| ∝ −
which we identify as the Beta distribution (6.98). In example 6.12, we
assumed that the Beta distribution is the conjugate prior of the Bernoulli
distribution and showed that it was indeed the conjugate prior. In this
example, we derived the form of the Beta distribution by looking at the
canonicalconjugateprioroftheBernoullidistributioninexponentialfam-
ilyform.
As mentioned in the previous section, the main motivation for expo-
nential families is that they have finite-dimensional sufficient statistics.
Additionally,conjugatedistributionsareeasytowritedown,andthecon-
jugate distributions also come from an exponential family. From an infer-
ence perspective, maximum likelihood estimation behaves nicely because
empiricalestimatesofsufficientstatisticsareoptimalestimatesofthepop-
ulation values of sufficient statistics (recall the mean and covariance of a
Gaussian). From an optimization perspective, the log-likelihood function
is concave, allowing for efficient optimization approaches to be applied
(Chapter7).
6.7 Change of Variables/Inverse Transform
It may seem that there are very many known distributions, but in reality
the set of distributions for which we have names is quite limited. There-
fore, it is often useful to understand how transformed random variables
are distributed. For example, assuming that X is a random variable dis-
(cid:0) (cid:1)
tributed according to the univariate normal distribution 0, 1 , what is
N
the distribution of X2? Another example, which is quite common in ma-
chine learning, is, given that X and X are univariate standard normal,
1 2
whatisthedistributionof 1(X +X )?
2 1 2
One option to work out the distribution of 1(X +X ) is to calculate
2 1 2
the meanand varianceof X andX and thencombine them.As we saw
1 2
inSection6.4.4,wecancalculatethemeanandvarianceofresultingran-
dom variables when we consider affine transformations of random vari-
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.7 ChangeofVariables/InverseTransform 215
ables. However, we may not be able to obtain the functional form of the
distribution under transformations. Furthermore, we may be interested
in nonlinear transformations of random variables for which closed-form
expressionsarenotreadilyavailable.
Remark(Notation). Inthissection,wewillbeexplicitaboutrandomvari-
ables and the values they take. Hence, recall that we use capital letters
X,Y to denote random variables and small letters x,y to denote the val-
uesinthetargetspace thattherandomvariablestake.Wewillexplicitly
T
write pmfs of discrete random variables X as P(X = x). For continuous
randomvariablesX (Section6.2.2),thepdfiswrittenasf(x)andthecdf
iswrittenasF (x).
X
♢
We will look at two approaches for obtaining distributions of transfor-
mations of random variables: a direct approach using the definition of a
cumulative distribution function and a change-of-variable approach that
usesthechainruleofcalculus(Section5.2.2).Thechange-of-variableap- Momentgenerating
proach is widely used because it provides a “recipe” for attempting to functionscanalso
beusedtostudy
compute the resulting distribution due to a transformation. We will ex-
transformationsof
plainthetechniquesforunivariaterandomvariables,andwillonlybriefly
random
providetheresultsforthegeneralcaseofmultivariaterandomvariables. variables(Casella
Transformations of discrete random variables can be understood di- andBerger,2002,
rectly.SupposethatthereisadiscreterandomvariableX withpmfP(X = chapter2).
x) (Section 6.2.1), and an invertible function U(x). Consider the trans-
formedrandomvariableY := U(X),withpmfP(Y = y).Then
P(Y = y) = P(U(X) = y) transformationof interest (6.125a)
= P(X = U−1(y)) inverse (6.125b)
where we can observe that x = U−1(y). Therefore, for discrete random
variables,transformationsdirectlychangetheindividualevents(withthe
probabilitiesappropriatelytransformed).
6.7.1 Distribution Function Technique
Thedistributionfunctiontechniquegoesbacktofirstprinciples,anduses
thedefinitionofacdfF (x) = P(X ⩽ x)andthefactthatitsdifferential
X
is the pdf f(x) (Wasserman, 2004, chapter 2). For a random variable X
andafunctionU,wefindthepdfoftherandomvariableY := U(X)by
1. Findingthecdf:
F (y) = P(Y ⩽ y) (6.126)
Y
2. DifferentiatingthecdfF (y)togetthepdff(y).
Y
d
f(y) = F (y). (6.127)
dy Y
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
216 ProbabilityandDistributions
Wealsoneedtokeepinmindthatthedomainoftherandomvariablemay
havechangedduetothetransformationbyU.
Example 6.16
Let X be a continuous random variable with probability density function
on0 ⩽ x ⩽ 1
f(x) = 3x2. (6.128)
WeareinterestedinfindingthepdfofY = X2.
Thefunctionf isanincreasingfunctionofx,andthereforetheresulting
valueofy liesintheinterval[0,1].Weobtain
F (y) = P(Y ⩽ y) definitionof cdf (6.129a)
Y
= P(X2 ⩽ y) transformationof interest (6.129b)
= P(X ⩽ y21 ) inverse (6.129c)
= F X(y21 ) definitionof cdf (6.129d)
(cid:90) y21
= 3t2dt cdf asadefiniteintegral (6.129e)
0
1
=
(cid:2) t3(cid:3)t=y2
resultof integration (6.129f)
t=0
= y23 , 0 ⩽ y ⩽ 1. (6.129g)
Therefore,thecdfofY is
F Y(y) = y23 (6.130)
for0 ⩽ y ⩽ 1.Toobtainthepdf,wedifferentiatethecdf
d 3
f(y) = dyF Y(y) = 2y1 2 (6.131)
for0 ⩽ y ⩽ 1.
InExample6.16,weconsideredastrictlymonotonicallyincreasingfunc-
Functionsthathave tion f(x) = 3x2. This means that we could compute an inverse function.
inversesarecalled In general, we require that the function of interest y = U(x) has an in-
bijectivefunctions verse x = U−1(y). A useful result can be obtained by considering the cu-
(Section2.7).
mulative distribution function F (x) of a random variable X, and using
X
itasthetransformationU(x).Thisleadstothefollowingtheorem.
Theorem6.15. [Theorem2.1.10inCasellaandBerger(2002)]LetX bea
continuous random variable with a strictly monotonic cumulative distribu-
tionfunctionF (x).ThentherandomvariableY definedas
X
Y := F (X) (6.132)
X
hasauniformdistribution.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.7 ChangeofVariables/InverseTransform 217
Theorem 6.15 is known as the probability integral transform, and it is probabilityintegral
usedtoderivealgorithmsforsamplingfromdistributionsbytransforming transform
the result of sampling from a uniform random variable (Bishop, 2006).
Thealgorithmworksbyfirstgeneratingasamplefromauniformdistribu-
tion, then transforming it by the inverse cdf (assuming this is available)
to obtain a sample from the desired distribution. The probability integral
transformisalsousedforhypothesistestingwhetherasamplecomesfrom
aparticulardistribution(LehmannandRomano,2005).Theideathatthe
output of a cdf gives a uniform distribution also forms the basis of copu-
las(Nelsen,2006).
6.7.2 Change of Variables
The distribution function technique in Section 6.7.1 is derived from first
principles, based on the definitions of cdfs and using properties of in-
verses,differentiation,andintegration.Thisargumentfromfirstprinciples
reliesontwofacts:
1. WecantransformthecdfofY intoanexpressionthatisacdfofX.
2. Wecandifferentiatethecdftoobtainthepdf.
Letusbreakdownthereasoningstepbystep,withthegoalofunderstand-
ingthemoregeneralchange-of-variablesapproachinTheorem6.16. Changeofvariables
inprobabilityrelies
Remark. The name “change of variables” comes from the idea of chang-
onthe
ing the variable of integration when faced with a difficult integral. For
change-of-variables
univariatefunctions,weusethesubstitutionruleofintegration, methodin
calculus(Tandra,
(cid:90) (cid:90)
f(g(x))g′(x)dx = f(u)du, where u = g(x). (6.133) 2014).
Thederivationofthisruleisbasedonthechainruleofcalculus(5.32)and
byapplyingtwicethefundamentaltheoremofcalculus.Thefundamental
theoremofcalculusformalizesthefactthatintegrationanddifferentiation
are somehow “inverses” of each other. An intuitive understanding of the
rule can be obtained by thinking (loosely) about small changes (differen-
tials)totheequationu = g(x),thatisbyconsidering∆u = g′(x)∆xasa
differentialofu = g(x).Bysubstitutingu = g(x),theargumentinsidethe
integralontheright-handsideof(6.133)becomesf(g(x)).Bypretending
that the term du can be approximated by du ∆u = g′(x)∆x, and that
≈
dx ∆x,weobtain(6.133).
≈ ♢
Consider a univariate random variable X, and an invertible function
U, which gives us another random variable Y = U(X). We assume that
random variable X has states x [a,b]. By the definition of the cdf, we
∈
have
F (y) = P(Y ⩽ y). (6.134)
Y
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
218 ProbabilityandDistributions
WeareinterestedinafunctionU oftherandomvariable
P(Y ⩽ y) = P(U(X) ⩽ y), (6.135)
where we assume that the function U is invertible. An invertible function
onanintervaliseitherstrictlyincreasingorstrictlydecreasing.Inthecase
thatU isstrictlyincreasing,thenitsinverseU−1 isalsostrictlyincreasing.
ByapplyingtheinverseU−1 totheargumentsofP(U(X) ⩽ y),weobtain
P(U(X) ⩽ y) = P(U−1(U(X)) ⩽ U−1(y)) = P(X ⩽ U−1(y)).
(6.136)
Theright-mosttermin(6.136)isanexpressionofthecdfofX.Recallthe
definitionofthecdfintermsofthepdf
(cid:90) U−1(y)
P(X ⩽ U−1(y)) = f(x)dx. (6.137)
a
NowwehaveanexpressionofthecdfofY intermsofx:
(cid:90) U−1(y)
F (y) = f(x)dx. (6.138)
Y
a
Toobtainthepdf,wedifferentiate(6.138)withrespecttoy:
d d (cid:90) U−1(y)
f(y) = F (y) = f(x)dx. (6.139)
dy y dy
a
Note that the integral on the right-hand side is with respect to x, but we
need an integral with respect to y because we are differentiating with
respecttoy.Inparticular,weuse(6.133)togetthesubstitution
(cid:90) (cid:90)
f(U−1(y))U−1′ (y)dy = f(x)dx where x = U−1(y). (6.140)
Using(6.140)ontheright-handsideof(6.139)givesus
d (cid:90) U−1(y)
f(y) = f
(U−1(y))U−1′
(y)dy. (6.141)
dy x
a
We then recall that differentiation is a linear operator and we use the
subscriptxtoremindourselvesthatf (U−1(y))isafunctionofxandnot
x
y.Invokingthefundamentaltheoremofcalculusagaingivesus
(cid:18) (cid:19)
d
f(y) = f (U−1(y)) U−1(y) . (6.142)
x · dy
RecallthatweassumedthatU isastrictlyincreasingfunction.Fordecreas-
ing functions, it turns out that we have a negative sign when we follow
thesamederivation.Weintroducetheabsolutevalueofthedifferentialto
havethesameexpressionforbothincreasinganddecreasingU:
(cid:12) (cid:12)
(cid:12) d (cid:12)
f(y) = f x(U−1(y)) ·(cid:12) (cid:12)dyU−1(y)(cid:12)
(cid:12)
. (6.143)
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.7 ChangeofVariables/InverseTransform 219
(cid:12) (cid:12)
This is called the change-of-variable technique. The term (cid:12) (cid:12)dd yU−1(y)(cid:12) (cid:12) in change-of-variable
(6.143) measures how much a unit volume changes when applying U technique
(seealsothedefinitionoftheJacobianinSection5.3).
Remark. Incomparisontothediscretecasein(6.125b),wehaveanaddi-
(cid:12) (cid:12)
tionalfactor(cid:12) d U−1(y)(cid:12).Thecontinuouscaserequiresmorecarebecause
(cid:12)dy (cid:12)
P(Y = y) = 0 for all y. The probability density function f(y) does not
haveadescriptionasaprobabilityofaneventinvolvingy.
♢
Sofarinthissection,wehavebeenstudyingunivariatechangeofvari-
ables. The case for multivariate random variables is analogous, but com-
plicated by fact that the absolute value cannot be used for multivariate
functions. Instead, we use the determinant of the Jacobian matrix. Recall
from (5.58) that the Jacobian is a matrix of partial derivatives, and that
the existence of a nonzero determinant shows that we can invert the Ja-
cobian. Recall the discussion in Section 4.1 that the determinant arises
because our differentials (cubes of volume) are transformed into paral-
lelepipeds by the Jacobian. Let us summarize preceding the discussion in
thefollowing theorem,whichgives usa recipeformultivariate changeof
variables.
Theorem6.16. [Theorem17.2inBillingsley(1995)]Letf(x)bethevalue
oftheprobabilitydensityofthemultivariatecontinuousrandomvariableX.
If the vector-valued function y = U(x) is differentiable and invertible for
all values within the domain of x, then for corresponding values of y, the
probabilitydensityofY = U(X)isgivenby
(cid:12) (cid:12) (cid:18) ∂ (cid:19)(cid:12) (cid:12)
f(y) = f x(U−1(y)) ·(cid:12) (cid:12)det ∂yU−1(y) (cid:12) (cid:12). (6.144)
Thetheoremlooksintimidatingatfirstglance,butthekeypointisthat
a change of variable of a multivariate random variable follows the pro-
cedure of the univariate change of variable. First we need to work out
the inverse transform, and substitute that into the density of x. Then we
calculate the determinant of the Jacobian and multiply the result. The
followingexampleillustratesthecaseofabivariaterandomvariable.
Example 6.17
(cid:20) (cid:21)
x
ConsiderabivariaterandomvariableX withstatesx = 1 andproba-
x
2
bilitydensityfunction
(cid:32) (cid:33)
(cid:18)(cid:20) (cid:21)(cid:19) (cid:20) (cid:21)⊤(cid:20) (cid:21)
x 1 1 x x
f 1 = exp 1 1 . (6.145)
x
2
2π −2 x
2
x
2
Weusethechange-of-variabletechniquefromTheorem6.16toderivethe
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
220 ProbabilityandDistributions
effect of a linear transformation (Section 2.7) of the random variable.
ConsideramatrixA R2×2 definedas
∈
(cid:20) (cid:21)
a b
A = . (6.146)
c d
We are interested in finding the probability density function of the trans-
formedbivariaterandomvariableY withstatesy = Ax.
Recallthatforchangeofvariableswerequiretheinversetransformation
of x as a function of y. Since we consider linear transformations, the
inverse transformation is given by the matrix inverse (see Section 2.2.2).
For2 2matrices,wecanexplicitlywriteouttheformula,givenby
×
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21)
x y 1 d b y
1 = A−1 1 = − 1 . (6.147)
x y ad bc c a y
2 2 2
− −
Observe that ad bc is the determinant (Section 4.1) of A. The corre-
−
spondingprobabilitydensityfunctionisgivenby
1 (cid:16) (cid:17)
f(x) = f(A−1y) = exp 1y⊤A−⊤A−1y . (6.148)
2π −2
Thepartialderivativeofamatrixtimesavectorwithrespecttothevector
isthematrixitself(Section5.5),andtherefore
∂
A−1y = A−1. (6.149)
∂y
Recall from Section 4.1 that the determinant of the inverse is the inverse
ofthedeterminantsothatthedeterminantoftheJacobianmatrixis
(cid:18) ∂ (cid:19) 1
det A−1y = . (6.150)
∂y ad bc
−
We are now able to apply the change-of-variable formula from Theo-
rem6.16bymultiplying(6.148)with(6.150),whichyields
f(y) =
f(x)(cid:12) (cid:12) (cid:12)det(cid:18) ∂ A−1y(cid:19)(cid:12) (cid:12)
(cid:12) (6.151a)
(cid:12) ∂y (cid:12)
1 (cid:16) (cid:17)
= exp 1y⊤A−⊤A−1y ad bc −1. (6.151b)
2π −2 | − |
While Example 6.17 is based on a bivariate random variable, which al-
lowsustoeasilycomputethematrixinverse,theprecedingrelationholds
forhigherdimensions.
Remark. WesawinSection6.5thatthedensityf(x)in(6.148)isactually
thestandardGaussiandistribution,andthetransformeddensityf(y)isa
bivariateGaussianwithcovarianceΣ = AA⊤.
♢
We will use the ideas in this chapter to describe probabilistic modeling
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
6.8 FurtherReading 221
inSection8.4,aswellasintroduceagraphicallanguageinSection8.5.We
will see direct machine learning applications of these ideas in Chapters 9
and11.
6.8 Further Reading
This chapter is rather terse at times. Grinstead and Snell (1997) and
Walpole et al. (2011) provide more relaxed presentations that are suit-
able for self-study. Readers interested in more philosophical aspects of
probability should consider Hacking (2001), whereas an approach that
is more related to software engineering is presented by Downey (2014).
An overview of exponential families can be found in Barndorff-Nielsen
(2014). We will see more about how to use probability distributions to
model machine learning tasks in Chapter 8. Ironically, the recent surge
in interest in neural networks has resulted in a broader appreciation of
probabilisticmodels.Forexample,theideaofnormalizingflows(Jimenez
RezendeandMohamed,2015)reliesonchangeofvariablesfortransform-
ingrandomvariables.Anoverviewofmethodsforvariationalinferenceas
applied to neural networks is described in chapters 16 to 20 of the book
byGoodfellowetal.(2016).
Wesidesteppedalargepartofthedifficultyincontinuousrandomvari-
ablesbyavoidingmeasuretheoreticquestions(Billingsley,1995;Pollard,
2002),andbyassumingwithoutconstructionthatwehaverealnumbers,
andwaysofdefiningsetsonrealnumbersaswellastheirappropriatefre-
quencyofoccurrence.Thesedetailsdomatter,forexample,inthespecifi-
cation of conditional probability p(y x) for continuous random variables
|
x,y (Proschan and Presnell, 1998). The lazy notation hides the fact that
we want to specify that X = x (which is a set of measure zero). Fur-
thermore, we are interested in the probability density function of y. A
more precise notation would have to say E [f(y) σ(x)], where we take
y
|
theexpectationoveryofatestfunctionf conditionedontheσ-algebraof
x. A more technical audience interested in the details of probability the-
ory have many options (Jaynes, 2003; MacKay, 2003; Jacod and Protter,
2004; Grimmett and Welsh, 2014), including some very technical discus-
sions(Shiryayev,1984;LehmannandCasella,1998;Dudley,2002;Bickel
andDoksum,2006;C¸inlar,2011).Analternativewaytoapproachproba-
bility is to start with the concept of expectation, and “work backward” to
derive the necessary properties of a probability space (Whittle, 2000). As
machine learning allows us to model more intricate distributions on ever
more complex types of data, a developer of probabilistic machine learn-
ing models would have to understand these more technical aspects. Ma-
chinelearningtextswithaprobabilisticmodelingfocusincludethebooks
byMacKay(2003);Bishop(2006);RasmussenandWilliams(2006);Bar-
ber(2012);Murphy(2012).
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
222 ProbabilityandDistributions
Exercises
6.1 Considerthefollowingbivariatedistributionp(x,y)oftwodiscreterandom
variablesX andY.
y 1 0.01 0.02 0.03 0.1 0.1
Y y 2 0.05 0.1 0.05 0.07 0.2
y 3 0.1 0.05 0.03 0.05 0.04
x x x x x
1 2 3 4 5
X
Compute:
a. Themarginaldistributionsp(x)andp(y).
b. Theconditionaldistributionsp(x|Y =y )andp(y|X =x ).
1 3
6.2 ConsideramixtureoftwoGaussiandistributions(illustratedinFigure6.4),
(cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)
10 1 0 0 8.4 2.0
0.4N , +0.6N , .
2 0 1 0 2.0 1.7
a. Computethemarginaldistributionsforeachdimension.
b. Computethemean,modeandmedianforeachmarginaldistribution.
c. Computethemeanandmodeforthetwo-dimensionaldistribution.
6.3 You have written a computer program that sometimes compiles and some-
timesnot(codedoesnotchange).Youdecidetomodeltheapparentstochas-
ticity(successvs.nosuccess)xofthecompilerusingaBernoullidistribution
withparameterµ:
p(x|µ)=µx(1−µ)1−x, x∈{0,1}.
ChooseaconjugatepriorfortheBernoullilikelihoodandcomputethepos-
teriordistributionp(µ|x ,...,x ).
1 N
6.4 Therearetwobags.Thefirstbagcontainsfourmangosandtwoapples;the
secondbagcontainsfourmangosandfourapples.
We also have a biased coin, which shows “heads” with probability 0.6 and
“tails” with probability 0.4. If the coin shows “heads”. we pick a fruit at
randomfrombag1;otherwisewepickafruitatrandomfrombag2.
Yourfriendflipsthecoin(youcannotseetheresult),picksafruitatrandom
fromthecorrespondingbag,andpresentsyouamango.
Whatistheprobabilitythatthemangowaspickedfrombag2?
Hint:UseBayes’theorem.
6.5 Considerthetime-seriesmodel
(cid:0) (cid:1)
x
t+1
=Axt+w, w∼N 0, Q
(cid:0) (cid:1)
y
t
=Cxt+v, v∼N 0, R ,
wherew,varei.i.d.Gaussiannoisevariables.Further,assumethatp(x )=
0
(cid:0) (cid:1)
N µ , Σ .
0 0
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Exercises 223
a. Whatistheformofp(x ,x ,...,x )?Justifyyouranswer(youdonot
0 1 T
havetoexplicitlycomputethejointdistribution).
(cid:0) (cid:1)
b. Assumethatp(xt|y 1,...,y t)=N µ t, Σt .
1. Computep(x |y ,...,y ).
t+1 1 t
2. Computep(x ,y |y ,...,y ).
t+1 t+1 1 t
3. Attimet+1,weobservethevaluey =yˆ.Computetheconditional
t+1
distributionp(x |y ,...,y ).
t+1 1 t+1
6.6 Provetherelationshipin(6.44),whichrelatesthestandarddefinitionofthe
variancetotheraw-scoreexpressionforthevariance.
6.7 Prove the relationship in (6.45), which relates the pairwise difference be-
tweenexamplesinadatasetwiththeraw-scoreexpressionforthevariance.
6.8 Express the Bernoulli distribution in the natural parameter form of the ex-
ponentialfamily,see(6.107).
6.9 ExpresstheBinomialdistributionasanexponentialfamilydistribution.Also
expresstheBetadistributionisanexponentialfamilydistribution.Showthat
the product of the Beta and the Binomial distribution is also a member of
theexponentialfamily.
6.10 DerivetherelationshipinSection6.5.2intwoways:
a. Bycompletingthesquare
b. ByexpressingtheGaussianinitsexponentialfamilyform
(cid:0) (cid:1) (cid:0) (cid:1)
The product of two Gaussians N x|a, A N x|b, B is an unnormalized
(cid:0) (cid:1)
GaussiandistributioncN x|c, C with
C =(A−1+B−1)−1
c=C(A−1a+B−1b)
c=(2π)−D 2 |A+B|− 21 exp(cid:0) − 1(a−b)⊤(A+B)−1(a−b)(cid:1) .
2
Notethatthenormalizingconstantcitselfcanbeconsidereda(normalized)
Gaussiandistributioneitherinaorinbwithan“inflated”covariancematrix
(cid:0) (cid:1) (cid:0) (cid:1)
A+B,i.e.,c=N a|b, A+B =N b|a, A+B .
6.11 IteratedExpectations.
Considertworandomvariablesx,ywithjointdistributionp(x,y).Showthat
E [x]=E (cid:2)E [x|y](cid:3) .
X Y X
Here,E [x|y]denotestheexpectedvalueofxundertheconditionaldistri-
X
butionp(x|y).
6.12 ManipulationofGaussianRandomVariables.
Consider a Gaussian random variable x ∼ N(cid:0) x|µ x, Σx(cid:1) , where x ∈ RD.
Furthermore,wehave
y=Ax+b+w,
where y ∈ RE, A ∈ RE×D, b ∈ RE, and w ∼ N(cid:0) w|0, Q(cid:1) is indepen-
dent Gaussian noise. “Independent” implies that x and w are independent
randomvariablesandthatQisdiagonal.
a. Writedownthelikelihoodp(y|x).
(cid:82)
b. Thedistributionp(y)= p(y|x)p(x)dxisGaussian.Computethemean
µ
y
andthecovarianceΣy.Deriveyourresultindetail.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
224 ProbabilityandDistributions
c. Therandomvariabley isbeingtransformedaccordingtothemeasure-
mentmapping
z=Cy+v,
wherez ∈ RF,C ∈ RF×E,andv ∼ N(cid:0) v|0, R(cid:1) isindependentGaus-
sian(measurement)noise.
Writedownp(z|y).
Computep(z),i.e.,themeanµ
z
andthecovarianceΣz.Deriveyour
resultindetail.
d. Now,avalueyˆ ismeasured.Computetheposteriordistributionp(x|yˆ).
Hint for solution: This posterior is also Gaussian, i.e., we need to de-
termine only its mean and covariance matrix. Start by explicitly com-
puting the joint Gaussian p(x,y). This also requires us to compute the
cross-covariances Covx,y[x,y] and Covy,x[y,x]. Then apply the rules
forGaussianconditioning.
6.13 ProbabilityIntegralTransformation
GivenacontinuousrandomvariableX,withcdfF (x),showthattheran-
X
domvariableY :=F (X)isuniformlydistributed(Theorem6.15).
X
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
7
Continuous Optimization
Since machine learning algorithms are implemented on a computer, the
mathematicalformulationsareexpressedasnumericaloptimizationmeth-
ods. This chapter describes the basic numerical methods for training ma-
chine learning models. Training a machine learning model often boils
down to finding a good set of parameters. The notion of “good” is de-
termined by the objective function or the probabilistic model, which we
will see examples of in the second part of this book. Given an objective
function,findingthebestvalueisdoneusingoptimizationalgorithms. Sinceweconsider
Thischaptercoverstwomainbranchesofcontinuousoptimization(Fig- dataandmodelsin
RD,the
ure 7.1): unconstrained and constrained optimization. We will assume in
optimization
this chapter that our objective function is differentiable (see Chapter 5),
problemsweface
hencewehaveaccesstoagradientateachlocationinthespacetohelpus arecontinuous
find the optimum value. By convention, most objective functions in ma- optimization
chine learning are intended to be minimized, that is, the best value is the problems,as
opposedto
minimum value. Intuitively finding the best value is like finding the val-
combinatorial
leysoftheobjectivefunction,andthegradientspointusuphill.Theideais optimization
tomovedownhill(oppositetothegradient)andhopetofindthedeepest problemsfor
point. For unconstrained optimization, this is the only concept we need, discretevariables.
but there are several design choices, which we discuss in Section 7.1. For
constrained optimization, we need to introduce other concepts to man-
age the constraints (Section 7.2). We will also introduce a special class
ofproblems(convexoptimizationproblemsinSection7.3)wherewecan
makestatementsaboutreachingtheglobaloptimum.
ConsiderthefunctioninFigure7.2.Thefunctionhasaglobalminimum globalminimum
around x = 4.5, with a function value of approximately 47. Since
− −
the function is “smooth,” the gradients can be used to help find the min-
imum by indicating whether we should take a step to the right or left.
Thisassumesthatweareinthecorrectbowl,asthereexistsanotherlocal localminimum
minimum around x = 0.7. Recall that we can solve for all the stationary
pointsofafunctionbycalculatingitsderivativeandsettingittozero.For Stationarypoints
aretherealrootsof
ℓ(x) = x4+7x3+5x2 17x+3, (7.1)
thederivative,that
−
is,pointsthathave
weobtainthecorrespondinggradientas
zerogradient.
dℓ(x)
= 4x3+21x2+10x 17. (7.2)
dx −
225
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
226 ContinuousOptimization
Figure7.1 Amind Continuous
mapoftheconcepts optimization Stepsize
relatedto
optimization,as
presentedinthis
chapter.Thereare Unconstrained
optimization Gradientdescent Momentum
twomainideas:
gradientdescent
andconvex
optimization.
Stochastic
gradient
Constrained Chapter10 descent
optimization Dimensionreduc.
Lagrange Chapter11
multipliers Densityestimation
Convex
Convexoptimization Linear
&duality programming
Convexconjugate Quadratic Chapter12
programming Classification
Sincethisisacubicequation,ithasingeneralthreesolutionswhensetto
zero. In the example, two of them are minimums and one is a maximum
(around x = 1.4). To check whether a stationary point is a minimum
−
or maximum, we need to take the derivative a second time and check
whether the second derivative is positive or negative at the stationary
point.Inourcase,thesecondderivativeis
d2ℓ(x)
= 12x2+42x+10. (7.3)
dx2
By substituting our visually estimated values of x = 4.5, 1.4,0.7, we
− −(cid:16) (cid:17)
willobservethatasexpectedthemiddlepointisamaximum
d2ℓ(x)
< 0
dx2
andtheothertwostationarypointsareminimums.
Note that we have avoided analytically solving for values of x in the
previous discussion, although for low-order polynomials such as the pre-
ceding we could do so. In general, we are unable to find analytic solu-
tions,andhenceweneedtostartatsomevalue,sayx = 6,andfollow
0
−
the negative gradient. The negative gradient indicates that we should go
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
7.1 OptimizationUsingGradientDescent 227
Figure7.2 Example
objectivefunction.
60
Negativegradients
areindicatedby
40 x4+7x3+5x2 17x+3 arrows,andthe
− globalminimumis
indicatedbythe
20 dashedblueline.
0
20
−
40
−
60
− 6 5 4 3 2 1 0 1 2
− − − − − −
Valueofparameter
right, but not how far (this is called the step-size). Furthermore, if we Accordingtothe
had started at the right side (e.g., x = 0) the negative gradient would Abel–Ruffini
0
theorem,thereisin
haveledustothewrongminimum.Figure7.2illustratesthefactthatfor
generalnoalgebraic
x > 1,thenegativegradientpointstowardtheminimumontherightof
− solutionfor
thefigure,whichhasalargerobjectivevalue. polynomialsof
In Section 7.3, we will learn about a class of functions, called convex degree5ormore
(Abel,1826).
functions,thatdonotexhibitthistrickydependencyonthestartingpoint
of the optimization algorithm. For convex functions, all local minimums
are global minimum. It turns out that many machine learning objective Forconvexfunctions
functions are designed such that they are convex, and we will see an ex- alllocalminimaare
globalminimum.
ampleinChapter12.
Thediscussioninthischaptersofarwasaboutaone-dimensionalfunc-
tion, where we are able to visualize the ideas of gradients, descent direc-
tions,andoptimalvalues.Intherestofthischapterwedevelopthesame
ideas in high dimensions. Unfortunately, we can only visualize the con-
cepts in one dimension, but some concepts do not generalize directly to
higherdimensions,thereforesomecareneedstobetakenwhenreading.
7.1 Optimization Using Gradient Descent
Wenowconsidertheproblemofsolvingfortheminimumofareal-valued
function
minf(x), (7.4)
x
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
evitcejbO
228 ContinuousOptimization
where f : Rd R is an objective function that captures the machine
→
learningproblemathand.Weassumethatourfunctionf isdifferentiable,
andweareunabletoanalyticallyfindasolutioninclosedform.
Gradient descent is a first-order optimization algorithm. To find a local
minimum of a function using gradient descent, one takes steps propor-
tional to the negative of the gradient of the function at the current point.
Weusethe Recall from Section 5.1 that the gradient points in the direction of the
conventionofrow steepest ascent. Another useful intuition is to consider the set of lines
vectorsfor wherethefunctionisatacertainvalue(f(x) = cforsomevaluec R),
gradients. ∈
which are known as the contour lines. The gradient points in a direction
thatisorthogonaltothecontourlinesofthefunctionwewishtooptimize.
Let us consider multivariate functions. Imagine a surface (described by
the function f(x)) with a ball starting at a particular location x . When
0
the ball is released, it will move downhill in the direction of steepest de-
scent.Gradientdescentexploitsthefactthatf(x )decreasesfastestifone
0
movesfromx inthedirectionofthenegativegradient (( f)(x ))⊤ of
0 0
− ∇
f at x . We assume in this book that the functions are differentiable, and
0
referthereadertomoregeneralsettingsinSection7.4.Then,if
x = x γ(( f)(x ))⊤ (7.5)
1 0 0
− ∇
for a small step-size γ ⩾ 0, then f(x ) ⩽ f(x ). Note that we use the
1 0
transpose for the gradient since otherwise the dimensions will not work
out.
This observation allows us to define a simple gradient descent algo-
rithm: If we want to find a local optimum f(x ) of a function f : Rn
∗
R, x f(x),westartwithaninitialguessx oftheparameterswewi→ sh
0
(cid:55)→
tooptimizeandtheniterateaccordingto
x = x γ (( f)(x ))⊤. (7.6)
i+1 i i i
− ∇
For suitable step-size γ , the sequence f(x ) ⩾ f(x ) ⩾ ... converges to
i 0 1
alocalminimum.
Example 7.1
Consideraquadraticfunctionintwodimensions
(cid:18)(cid:20) (cid:21)(cid:19) (cid:20) (cid:21)⊤(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)⊤(cid:20) (cid:21)
x 1 x 2 1 x 5 x
f 1 = 1 1 1 (7.7)
x 2 2 x 2 1 20 x 2 − 3 x 2
withgradient
(cid:18)(cid:20) (cid:21)(cid:19) (cid:20) (cid:21)⊤(cid:20) (cid:21) (cid:20) (cid:21)⊤
x x 2 1 5
f 1 = 1 . (7.8)
∇ x 2 x 2 1 20 − 3
Starting at the initial location x = [ 3, 1]⊤, we iteratively apply (7.6)
0
− −
to obtain a sequence of estimates that converge to the minimum value
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
7.1 OptimizationUsingGradientDescent 229
2 90 Figure7.3 Gradient
50.0 40.0
descentona
75 two-dimensional
quadraticsurface
1
60 (shownasa
0.0
heatmap).See
45 Example7.1fora
0 description.
30
15
1
− 10.0
0
30.0
20.0
2
80.7 00.0 60.0
50.0 40.0 15
− 4 2 0 2 4 −
− − x
1
(illustrated in Figure 7.3). We can see (both from the figure and by plug-
gingx into(7.8)withγ = 0.085)thatthenegativegradientatx points
0 0
north and east, leading to x = [ 1.98,1.21]⊤. Repeating that argument
1
−
givesusx = [ 1.32, 0.42]⊤,andsoon.
2
− −
Remark. Gradient descent can be relatively slow close to the minimum:
Its asymptotic rate of convergence is inferior to many other methods. Us-
ingtheballrollingdownthehillanalogy,whenthesurfaceisalong,thin
valley, the problem is poorly conditioned (Trefethen and Bau III, 1997).
For poorly conditioned convex problems, gradient descent increasingly
“zigzags” as the gradients point nearly orthogonally to the shortest di-
rectiontoaminimumpoint;seeFigure7.3.
♢
7.1.1 Step-size
As mentioned earlier, choosing a good step-size is important in gradient
descent. If the step-size is too small, gradient descent can be slow. If the Thestep-sizeisalso
step-size is chosen too large, gradient descent can overshoot, fail to con- calledthelearning
rate.
verge, or even diverge. We will discuss the use of momentum in the next
section. It is a method that smoothes out erratic behavior of gradient up-
datesanddampensoscillations.
Adaptive gradient methods rescale the step-size at each iteration, de-
pending on local properties of the function. There are two simple heuris-
tics(Toussaint,2012):
When the function value increases after a gradient step, the step-size
wastoolarge.Undothestepanddecreasethestep-size.
Whenthefunctionvaluedecreasesthestepcouldhavebeenlarger.Try
toincreasethestep-size.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
x
2
230 ContinuousOptimization
Although the “undo” step seems to be a waste of resources, using this
heuristicguaranteesmonotonicconvergence.
Example 7.2 (Solving a Linear Equation System)
WhenwesolvelinearequationsoftheformAx = b,inpracticewesolve
Ax b = 0approximatelybyfindingx thatminimizesthesquarederror
∗
−
Ax b 2 = (Ax b)⊤(Ax b) (7.9)
∥ − ∥ − −
ifweusetheEuclideannorm.Thegradientof(7.9)withrespecttoxis
= 2(Ax b)⊤A. (7.10)
x
∇ −
We can use this gradient directly in a gradient descent algorithm. How-
ever, for this particular special case, it turns out that there is an analytic
solution, which can be found by setting the gradient to zero. We will see
moreonsolvingsquarederrorproblemsinChapter9.
Remark. WhenappliedtothesolutionoflinearsystemsofequationsAx =
b,gradientdescentmayconvergeslowly.Thespeedofconvergenceofgra-
conditionnumber dient descent is dependent on the condition number κ = σ(A)max, which
σ(A)min
is the ratio of the maximum to the minimum singular value (Section 4.5)
of A. The condition number essentially measures the ratio of the most
curved direction versus the least curved direction, which corresponds to
ourimagerythatpoorlyconditionedproblemsarelong,thinvalleys:They
are very curved in one direction, but very flat in the other. Instead of di-
rectlysolvingAx = b,onecouldinsteadsolveP−1(Ax b) = 0,where
preconditioner P iscalledthepreconditioner.ThegoalistodesignP−1− suchthatP−1A
has a better condition number, but at the same time P−1 is easy to com-
pute. For further information on gradient descent, preconditioning, and
convergencewerefertoBoydandVandenberghe(2004,chapter9).
♢
7.1.2 Gradient Descent With Momentum
As illustrated in Figure 7.3, the convergence of gradient descent may be
very slow if the curvature of the optimization surface is such that there
areregionsthatarepoorlyscaled.Thecurvatureissuchthatthegradient
descent steps hops between the walls of the valley and approaches the
optimum in small steps. The proposed tweak to improve convergence is
Goh(2017)wrote togivegradientdescentsomememory.
anintuitiveblog Gradientdescentwithmomentum(Rumelhartetal.,1986)isamethod
postongradient
that introduces an additional term to remember what happened in the
descentwith
previous iteration. This memory dampens oscillations and smoothes out
momentum.
the gradient updates. Continuing the ball analogy, the momentum term
emulates the phenomenon of a heavy ball that is reluctant to change di-
rections.Theideaistohaveagradientupdatewithmemorytoimplement
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
7.1 OptimizationUsingGradientDescent 231
a moving average. The momentum-based method remembers the update
∆x ateachiterationianddeterminesthenextupdateasalinearcombi-
i
nationofthecurrentandpreviousgradients
x = x γ (( f)(x ))⊤+α∆x (7.11)
i+1 i i i i
− ∇
∆x = x x = α∆x γ (( f)(x ))⊤, (7.12)
i i i−1 i−1 i−1 i−1
− − ∇
where α [0,1]. Sometimes we will only know the gradient approxi-
∈
mately. In such cases, the momentum term is useful since it averages out
different noisy estimates of the gradient. One particularly useful way to
obtain an approximate gradient is by using a stochastic approximation,
whichwediscussnext.
7.1.3 Stochastic Gradient Descent
Computingthegradientcanbeverytimeconsuming.However,oftenitis
possible to find a “cheap” approximation of the gradient. Approximating
thegradientisstillusefulaslongasitpointsinroughlythesamedirection
asthetruegradient. stochasticgradient
Stochastic gradient descent (often shortened as SGD) is a stochastic ap- descent
proximation of the gradient descent method for minimizing an objective
function that is written as a sum of differentiable functions. The word
stochastic here refers to the fact that we acknowledge that we do not
know the gradient precisely, but instead only know a noisy approxima-
tion to it. By constraining the probability distribution of the approximate
gradients,wecanstilltheoreticallyguaranteethatSGDwillconverge.
Inmachinelearning,givenn = 1,...,N datapoints,weoftenconsider
objective functions that are the sum of the losses L incurred by each
n
examplen.Inmathematicalnotation,wehavetheform
N
(cid:88)
L(θ) = L (θ), (7.13)
n
n=1
whereθisthevectorofparametersofinterest,i.e.,wewanttofindθthat
minimizesL.Anexamplefromregression(Chapter9)isthenegativelog-
likelihood, which is expressed as a sum over log-likelihoods of individual
examplessothat
N
(cid:88)
L(θ) = logp(y x ,θ), (7.14)
n n
− |
n=1
wherex RD arethetraininginputs,y arethetrainingtargets,andθ
n n
∈
aretheparametersoftheregressionmodel.
Standard gradient descent, as introduced previously, is a “batch” opti-
mizationmethod,i.e.,optimizationisperformedusingthefulltrainingset
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
232 ContinuousOptimization
byupdatingthevectorofparametersaccordingto
N
(cid:88)
θ = θ γ ( L(θ ))⊤ = θ γ ( L (θ ))⊤ (7.15)
i+1 i i i i i n i
− ∇ − ∇
n=1
forasuitablestep-sizeparameterγ .Evaluatingthesumgradientmayre-
i
quire expensive evaluations of the gradients from all individual functions
L . When the training set is enormous and/or no simple formulas exist,
n
evaluatingthesumsofgradientsbecomesveryexpensive.
Considertheterm(cid:80)N
( L (θ ))in(7.15).Wecanreducetheamount
n=1 ∇ n i
of computation by taking a sum over a smaller set of L . In contrast to
n
batchgradientdescent,whichusesallL forn = 1,...,N,werandomly
n
choose a subset of L for mini-batch gradient descent. In the extreme
n
case, we randomly select only a single L to estimate the gradient. The
n
key insight about why taking a subset of data is sensible is to realize that
for gradient descent to converge, we only require that the gradient is an
unbiased estimate of the true gradient. In fact the term
(cid:80)N
( L (θ ))
n=1 ∇ n i
in(7.15)isanempiricalestimateoftheexpectedvalue(Section6.4.1)of
the gradient. Therefore, any other unbiased empirical estimate of the ex-
pectedvalue,forexampleusinganysubsampleofthedata,wouldsuffice
forconvergenceofgradientdescent.
Remark. Whenthelearningratedecreasesatanappropriaterate,andsub-
ject to relatively mild assumptions, stochastic gradient descent converges
almostsurelytolocalminimum(Bottou,1998).
♢
Why should oneconsider using an approximate gradient?A major rea-
son is practical implementation constraints, such as the size of central
processing unit (CPU)/graphics processing unit (GPU) memory or limits
oncomputationaltime.Wecanthinkofthesizeofthesubsetusedtoesti-
matethegradientinthesamewaythatwethoughtofthesizeofasample
when estimating empirical means (Section 6.4.1). Large mini-batch sizes
will provide accurate estimates of the gradient, reducing the variance in
theparameterupdate.Furthermore,largemini-batchestakeadvantageof
highly optimized matrix operations in vectorized implementations of the
cost and gradient. The reduction in variance leads to more stable conver-
gence,buteachgradientcalculationwillbemoreexpensive.
In contrast, small mini-batches are quick to estimate. If we keep the
mini-batch size small, the noise in our gradient estimate will allow us to
get out of some bad local optima, which we may otherwise get stuck in.
In machine learning, optimization methods are used for training by min-
imizing an objective function on the training data, but the overall goal
is to improve generalization performance (Chapter 8). Since the goal in
machinelearningdoesnotnecessarilyneedapreciseestimateofthemin-
imum of the objective function, approximate gradients using mini-batch
approaches have been widely used. Stochastic gradient descent is very
effective in large-scale machine learning problems (Bottou et al., 2018),
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
7.2 ConstrainedOptimizationandLagrangeMultipliers 233
3 Figure7.4
Illustrationof
constrained
optimization.The
2
unconstrained
problem(indicated
bythecontour
1 lines)hasa
minimumonthe
rightside(indicated
0 bythecircle).The
boxconstraints
(−1⩽x⩽1and
−1⩽y⩽1)require
1
− thattheoptimal
solutioniswithin
thebox,resultingin
2 anoptimalvalue
−
indicatedbythe
star.
3
− 3 2 1 0 1 2 3
− − − x1
such as training deep neural networks on millions ofimages (Dean et al.,
2012),topicmodels(Hoffmanetal.,2013),reinforcementlearning(Mnih
etal.,2015),ortrainingoflarge-scaleGaussianprocessmodels(Hensman
etal.,2013;Galetal.,2014).
7.2 Constrained Optimization and Lagrange Multipliers
Intheprevioussection,weconsideredtheproblemofsolvingforthemin-
imumofafunction
minf(x), (7.16)
x
wheref : RD R.
→
In this section, we have additional constraints. That is, for real-valued
functions g : RD R for i = 1,...,m, we consider the constrained
i
→
optimizationproblem(seeFigure7.4foranillustration)
min f(x) (7.17)
x
subjectto g (x) ⩽ 0 forall i = 1,...,m.
i
It is worth pointing out that the functions f and g could be non-convex
i
ingeneral,andwewillconsidertheconvexcaseinthenextsection.
One obvious, but not very practical, way of converting the constrained
problem(7.17)intoanunconstrainedoneistouseanindicatorfunction
m
(cid:88)
J(x) = f(x)+ 1(g (x)), (7.18)
i
i=1
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
2x
234 ContinuousOptimization
where1(z)isaninfinitestepfunction
(cid:40)
0 ifz ⩽ 0
1(z) = . (7.19)
otherwise
∞
This gives infinite penalty if the constraint is not satisfied, and hence
would provide the same solution. However, this infinite step function is
equally difficult to optimize. We can overcome this difficulty by introduc-
Lagrangemultiplier ingLagrangemultipliers.TheideaofLagrangemultipliersistoreplacethe
stepfunctionwithalinearfunction.
Lagrangian We associate to problem (7.17) the Lagrangian by introducing the La-
grange multipliers λ ⩾ 0 corresponding to each inequality constraint re-
i
spectively(BoydandVandenberghe,2004,chapter4)sothat
m
(cid:88)
L(x,λ) = f(x)+ λ g (x) (7.20a)
i i
i=1
= f(x)+λ⊤g(x), (7.20b)
where in the last line we have concatenated all constraints g (x) into a
i
vectorg(x),andalltheLagrangemultipliersintoavectorλ Rm.
∈
We now introduce the idea of Lagrangian duality. In general, duality
in optimization is the idea of converting an optimization problem in one
set of variables x (called the primal variables), into another optimization
problem in a different set of variables λ (called the dual variables). We
introduce two different approaches to duality: In this section, we discuss
Lagrangianduality;inSection7.3.3,wediscussLegendre-Fenchelduality.
Definition 7.1. Theproblemin(7.17)
min f(x) (7.21)
x
subjectto g (x) ⩽ 0 forall i = 1,...,m
i
primalproblem is known as the primal problem, corresponding to the primal variables x.
Lagrangiandual TheassociatedLagrangiandualproblemisgivenby
problem
max D(λ)
λ∈Rm (7.22)
subjectto λ ⩾ 0,
whereλarethedualvariablesandD(λ) = min x∈RdL(x,λ).
Remark. InthediscussionofDefinition7.1,weusetwoconceptsthatare
alsoofindependentinterest(BoydandVandenberghe,2004).
minimaxinequality First is the minimax inequality, which says that for any function with
twoargumentsφ(x,y),themaximinislessthantheminimax,i.e.,
maxminφ(x,y) ⩽ minmaxφ(x,y). (7.23)
y x x y
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
7.2 ConstrainedOptimizationandLagrangeMultipliers 235
Thisinequalitycanbeprovedbyconsideringtheinequality
Forallx,y minφ(x,y) ⩽ maxφ(x,y). (7.24)
x y
Notethattakingthemaximumoveryoftheleft-handsideof(7.24)main-
tains the inequality since the inequality is true for all y. Similarly, we can
taketheminimumoverxoftheright-handsideof(7.24)toobtain(7.23).
The second concept is weak duality, which uses (7.23) to show that weakduality
primal values are always greater than or equal to dual values. This is de-
scribedinmoredetailin(7.27).
♢
Recall that the difference between J(x) in (7.18) and the Lagrangian
in (7.20b) is that we have relaxed the indicator function to a linear func-
tion. Therefore, when λ ⩾ 0, the Lagrangian L(x,λ) is a lower bound of
J(x).Hence,themaximumofL(x,λ)withrespecttoλis
J(x) = maxL(x,λ). (7.25)
λ⩾0
RecallthattheoriginalproblemwasminimizingJ(x),
min maxL(x,λ). (7.26)
x∈Rd λ⩾0
By the minimax inequality (7.23), it follows that swapping the order of
theminimumandmaximumresultsinasmallervalue,i.e.,
min maxL(x,λ) ⩾ max min L(x,λ). (7.27)
x∈Rd λ⩾0 λ⩾0 x∈Rd
This is also known as weak duality. Note that the inner part of the right- weakduality
handsideisthedualobjectivefunctionD(λ)andthedefinitionfollows.
Incontrasttotheoriginaloptimizationproblem,whichhasconstraints,
min x∈RdL(x,λ) is an unconstrained optimization problem for a given
value of λ. If solving min x∈RdL(x,λ) is easy, then the overall problem is
easy to solve. We can see this by observing from (7.20b) that L(x,λ) is
affine with respect to λ. Therefore min x∈RdL(x,λ) is a pointwise min-
imum of affine functions of λ, and hence D(λ) is concave even though
f( ) and g ( ) may be nonconvex. The outer problem, maximization over
i
· ·
λ,isthemaximumofaconcavefunctionandcanbeefficientlycomputed.
Assuming f( ) and g ( ) are differentiable, we find the Lagrange dual
i
· ·
problem by differentiating the Lagrangian with respect to x, setting the
differentialtozero,andsolvingfortheoptimalvalue.Wewilldiscusstwo
concrete examples in Sections 7.3.1 and 7.3.2, where f( ) and g ( ) are
i
· ·
convex.
Remark (Equality Constraints). Consider (7.17) with additional equality
constraints
min f(x)
x
subjectto g (x) ⩽ 0 forall i = 1,...,m (7.28)
i
h (x) = 0 forall j = 1,...,n.
j
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
236 ContinuousOptimization
We can model equality constraints by replacing them with two inequality
constraints.Thatisforeachequalityconstrainth (x) = 0weequivalently
j
replace it by two constraints h (x) ⩽ 0 and h (x) ⩾ 0. It turns out that
j j
theresultingLagrangemultipliersarethenunconstrained.
Therefore, we constrain the Lagrange multipliers corresponding to the
inequality constraints in (7.28) to be non-negative, and leave the La-
grangemultiplierscorrespondingtotheequalityconstraintsunconstrained.
♢
7.3 Convex Optimization
We focus our attention of a particularly useful class of optimization prob-
lems, where we can guarantee global optimality. When f( ) is a convex
·
function,andwhentheconstraintsinvolvingg( )andh( )areconvexsets,
· ·
convexoptimization thisiscalledaconvexoptimizationproblem.Inthissetting,wehavestrong
problem duality: The optimal solution of the dual problem is the same as the opti-
strongduality malsolutionoftheprimalproblem.Thedistinctionbetweenconvexfunc-
tionsandconvexsetsareoftennotstrictlypresentedinmachinelearning
literature,butonecanofteninfertheimpliedmeaningfromcontext.
convexset Definition7.2. Aset isaconvexsetifforanyx,y andforanyscalar
θ with0 ⩽ θ ⩽ 1,wehC ave ∈ C
θx+(1 θ)y . (7.29)
− ∈ C
Figure7.5 Example
ofaconvexset. Convex sets are sets such that a straight line connecting any two ele-
ments of the set lie inside the set. Figures 7.5 and 7.6 illustrate convex
andnonconvexsets,respectively.
Convex functions are functions such that a straight line between any
twopointsofthefunctionlieabovethefunction.Figure7.2showsanon-
convexfunction,andFigure7.3showsaconvexfunction.Anotherconvex
functionisshowninFigure7.7.
Figure7.6 Example
ofanonconvexset. Definition7.3. Letfunctionf : RD Rbeafunctionwhosedomainisa
→
convexset.Thefunctionf isaconvexfunctionifforallx,y inthedomain
off,andforanyscalarθ with0 ⩽ θ ⩽ 1,wehave
f(θx+(1 θ)y) ⩽ θf(x)+(1 θ)f(y). (7.30)
− −
Remark. Aconcavefunctionisthenegativeofaconvexfunction.
♢
Theconstraintsinvolvingg( )andh( )in(7.28)truncatefunctionsata
convexfunction
· ·
scalar value, resulting in sets. Another relation between convex functions
concavefunction
and convex sets is to consider the set obtained by “filling in” a convex
function.Aconvexfunctionisabowl-likeobject,andweimaginepouring
epigraph water into it to fill it up. This resulting filled-in set, called the epigraph of
theconvexfunction,isaconvexset.
If a function f : Rn R is differentiable, we can specify convexity in
→
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
7.3 ConvexOptimization 237
Figure7.7 Example
ofaconvex
40 function.
y=3x2 5x+2
30
−
20
10
0
3 2 1 0 1 2 3
− − −
x
terms of its gradient f(x) (Section 5.2). A function f(x) is convex if
x
∇
andonlyifforanytwopointsx,y itholdsthat
f(y) ⩾ f(x)+ f(x)⊤(y x). (7.31)
x
∇ −
Ifwefurtherknowthatafunctionf(x)istwicedifferentiable,thatis,the
Hessian(5.147)existsforallvaluesinthedomainofx,thenthefunction
f(x) is convex if and only if 2f(x) is positive semidefinite (Boyd and
∇x
Vandenberghe,2004).
Example 7.3
The negative entropy f(x) = xlog x is convex for x > 0. A visualization
2
ofthefunctionisshowninFigure7.8,andwecanseethatthefunctionis
convex.Toillustratethepreviousdefinitionsofconvexity,letuscheckthe
calculationsfortwopointsx = 2andx = 4.Notethattoproveconvexity
off(x)wewouldneedtocheckforallpointsx R.
∈
RecallDefinition7.3.Considerapointmidwaybetweenthetwopoints
(thatisθ = 0.5);thentheleft-handsideisf(0.5 2+0.5 4) = 3log 3
· · 2 ≈
4.75.Theright-handsideis0.5(2log 2)+0.5(4log 4) = 1+4 = 5.And
2 2
thereforethedefinitionissatisfied.
Sincef(x)isdifferentiable,wecanalternativelyuse(7.31).Calculating
thederivativeoff(x),weobtain
1 1
(xlog x) = 1 log x+x = log x+ . (7.32)
∇x 2 · 2 · xlog 2 2 log 2
e e
Using the same two test points x = 2 and x = 4, the left-hand side of
(7.31)isgivenbyf(4) = 8.Theright-handsideis
f(x)+ ⊤(y x) = f(2)+ f(2) (4 2) (7.33a)
∇x − ∇ · −
1
= 2+(1+ ) 2 6.9. (7.33b)
log 2 · ≈
e
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
y
238 ContinuousOptimization
Figure7.8 The
negativeentropy xlog x
2
function(whichis 10
tangent at x=2
convex)andits
tangentatx=2.
5
0
0 1 2 3 4 5
x
We can check that a function or set is convex from first principles by
recallingthedefinitions.Inpractice,weoftenrelyonoperationsthatpre-
serve convexity to check that a particular function or set is convex. Al-
though the details are vastly different, this is again the idea of closure
thatweintroducedinChapter2forvectorspaces.
Example 7.4
A nonnegative weighted sum of convex functions is convex. Observe that
if f is a convex function, and α ⩾ 0 is a nonnegative scalar, then the
functionαf isconvex.Wecanseethisbymultiplyingαtobothsidesofthe
equation in Definition 7.3, and recalling that multiplying a nonnegative
numberdoesnotchangetheinequality.
Iff andf areconvexfunctions,thenwehavebythedefinition
1 2
f (θx+(1 θ)y) ⩽ θf (x)+(1 θ)f (y) (7.34)
1 1 1
− −
f (θx+(1 θ)y) ⩽ θf (x)+(1 θ)f (y). (7.35)
2 2 2
− −
Summingupbothsidesgivesus
f (θx+(1 θ)y)+f (θx+(1 θ)y)
1 2
− −
⩽ θf (x)+(1 θ)f (y)+θf (x)+(1 θ)f (y), (7.36)
1 1 2 2
− −
wheretheright-handsidecanberearrangedto
θ(f (x)+f (x))+(1 θ)(f (y)+f (y)), (7.37)
1 2 1 2
−
completingtheproofthatthesumofconvexfunctionsisconvex.
Combining the preceding two facts, we see that αf (x) + βf (x) is
1 2
convex for α,β ⩾ 0. This closure property can be extended using a sim-
ilar argument for nonnegative weighted sums of more than two convex
functions.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
)x(f
7.3 ConvexOptimization 239
Remark. The inequality in (7.30) is sometimes called Jensen’s inequality. Jensen’sinequality
Infact,awholeclassofinequalitiesfortakingnonnegativeweightedsums
ofconvexfunctionsareallcalledJensen’sinequality.
♢
Insummary,aconstrainedoptimizationproblemiscalledaconvexopti- convexoptimization
mizationproblemif problem
minf(x)
x
subjecttog (x) ⩽ 0 forall i = 1,...,m (7.38)
i
h (x) = 0 forall j = 1,...,n,
j
whereallfunctionsf(x)andg (x)areconvexfunctions,andallh (x) =
i j
0areconvexsets.Inthefollowing,wewilldescribetwoclassesofconvex
optimizationproblemsthatarewidelyusedandwellunderstood.
7.3.1 Linear Programming
Considerthespecialcasewhenalltheprecedingfunctionsarelinear,i.e.,
min c⊤x (7.39)
x∈Rd
subjectto Ax ⩽ b,
whereA Rm×d andb Rm.Thisisknownasalinearprogram.Ithasd linearprogram
∈ ∈
variablesandmlinearconstraints.TheLagrangianisgivenby Linearprogramsare
oneofthemost
L(x,λ) = c⊤x+λ⊤(Ax b), (7.40) widelyused
−
approachesin
where λ Rm is the vector of non-negative Lagrange multipliers. Rear- industry.
∈
rangingthetermscorrespondingtoxyields
L(x,λ) = (c+A⊤λ)⊤x λ⊤b. (7.41)
−
Taking the derivative of L(x,λ) with respect to x and setting it to zero
givesus
c+A⊤λ = 0. (7.42)
Therefore, the dual Lagrangian is D(λ) = λ⊤b. Recall we would like
−
to maximize D(λ). In addition to the constraint due to the derivative of
L(x,λ) being zero, we also have the fact that λ ⩾ 0, resulting in the
followingdualoptimizationproblem Itisconventionto
minimizetheprimal
max b⊤λ (7.43) andmaximizethe
λ∈Rm −
dual.
subjectto c+A⊤λ = 0
λ ⩾ 0.
This is also a linear program, but with m variables. We have the choice
of solving the primal (7.39) or the dual (7.43) program depending on
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
240 ContinuousOptimization
whethermordislarger.Recallthatdisthenumberofvariablesandmis
thenumberofconstraintsintheprimallinearprogram.
Example 7.5 (Linear Program)
Considerthelinearprogram
(cid:20) (cid:21)⊤(cid:20) (cid:21)
5 x
min 1
x∈R2 − 3 x 2
   
2 2 33
(7.44)
 2 4(cid:20) (cid:21)  8 
subjectto   2 − 1   x 1 ⩽   5  
 − 0 1  x 2   1 
− −
0 1 8
withtwovariables.ThisprogramisalsoshowninFigure7.9.Theobjective
function is linear, resulting in linear contour lines. The constraint set in
standardformistranslatedintothelegend.Theoptimalvaluemustliein
theshaded(feasible)region,andisindicatedbythestar.
Figure7.9
Illustrationofa
10
2 4x x2 2≤ ≥3 23 x1− −2x 81
linearprogram.The x2≤2x1−5
unconstrained x2≥1
problem(indicated x2≤8
bythecontour 8
lines)hasa
minimumonthe
rightside.The
6
optimalvaluegiven
theconstraintsare
shownbythestar.
4
2
0
0 2 4 6 8 10 12 14 16
x1
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2x
7.3 ConvexOptimization 241
7.3.2 Quadratic Programming
Considerthecaseofaconvexquadraticobjectivefunction,wherethecon-
straintsareaffine,i.e.,
1
min x⊤Qx+c⊤x (7.45)
x∈Rd 2
subjectto Ax ⩽ b,
whereA Rm×d,b Rm,andc Rd.ThesquaresymmetricmatrixQ
Rd×d is p∈ ositive defi∈ nite, and the∈ refore the objective function is convex∈ .
Thisis knownasaquadraticprogram.Observe thatithasdvariablesand
mlinearconstraints.
Example 7.6 (Quadratic Program)
Considerthequadraticprogram
(cid:20) (cid:21)⊤(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)⊤(cid:20) (cid:21)
1 x 2 1 x 5 x
min 1 1 + 1 (7.46)
x∈R2 2 x 2 1 4 x 2 3 x 2
   
1 0 1
(cid:20) (cid:21)
subjectto  −1 0   x 1 ⩽  1  (7.47)
 0 1  x 2 1
0 1 1
−
of two variables. The program is also illustrated in Figure 7.4. The objec-
tive function is quadratic with a positive semidefinite matrix Q, resulting
inellipticalcontourlines.Theoptimalvaluemustlieintheshaded(feasi-
ble)region,andisindicatedbythestar.
TheLagrangianisgivenby
1
L(x,λ) = x⊤Qx+c⊤x+λ⊤(Ax b) (7.48a)
2 −
1
= x⊤Qx+(c+A⊤λ)⊤x λ⊤b, (7.48b)
2 −
whereagainwehaverearrangedtheterms.TakingthederivativeofL(x,λ)
withrespecttoxandsettingittozerogives
Qx+(c+A⊤λ) = 0. (7.49)
SinceQispositivedefiniteandthereforeinvertible,weget
x = Q−1(c+A⊤λ). (7.50)
−
Substituting (7.50) into the primal Lagrangian L(x,λ), we get the dual
Lagrangian
1
D(λ) = (c+A⊤λ)⊤Q−1(c+A⊤λ) λ⊤b. (7.51)
−2 −
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
242 ContinuousOptimization
Therefore,thedualoptimizationproblemisgivenby
1
max (c+A⊤λ)⊤Q−1(c+A⊤λ) λ⊤b
λ∈Rm − 2 − (7.52)
subjectto λ ⩾ 0.
Wewillseeanapplicationofquadraticprogramminginmachinelearning
inChapter12.
7.3.3 Legendre–Fenchel Transform and Convex Conjugate
Let us revisit the idea of duality from Section 7.2, without considering
constraints. One useful fact about a convex set is that it can be equiva-
lently described by its supporting hyperplanes. A hyperplane is called a
supporting supporting hyperplane of a convex set if it intersects the convex set, and
hyperplane theconvexsetiscontainedonjustonesideofit.Recallthatwecanfillup
aconvexfunctiontoobtaintheepigraph,whichisaconvexset.Therefore,
wecanalsodescribeconvexfunctionsintermsoftheirsupportinghyper-
planes.Furthermore,observethatthesupportinghyperplanejusttouches
theconvexfunction,andisinfactthetangenttothefunctionatthatpoint.
And recall that the tangent of a function f(x) at a given point x is the
0
(cid:12)
evaluation of the gradient of that function at that point df(x)(cid:12) . In
dx (cid:12)
x=x0
summary,becauseconvexsetscanbeequivalentlydescribedbytheirsup-
porting hyperplanes, convex functions can be equivalently described by a
Legendretransform functionoftheirgradient.TheLegendretransformformalizesthisconcept.
Physicsstudentsare
oftenintroducedto We begin with the most general definition, which unfortunately has a
theLegendre
counter-intuitiveform,andlookatspecialcasestorelatethedefinitionto
transformas
the intuition described in the preceding paragraph. The Legendre-Fenchel
relatingthe
Lagrangianandthe transform is a transformation (in the sense of a Fourier transform) from
Hamiltonianin a convex differentiable function f(x) to a function that depends on the
classicalmechanics. tangentss(x) = f(x).Itisworthstressingthatthisisatransformation
x
Legendre-Fenchel ∇
ofthefunctionf( )andnotthevariablexorthefunctionevaluatedatx.
transform ·
TheLegendre-Fencheltransformisalsoknownastheconvexconjugate(for
convexconjugate
reasons we will see soon) and is closely related to duality (Hiriart-Urruty
andLemar´echal,2001,chapter5).
convexconjugate Definition 7.4. The convex conjugate of a function f : RD R is a
→
functionf∗ definedby
f∗(s) = sup ( s,x f(x)) . (7.53)
x∈RD ⟨ ⟩−
Note that the preceding convex conjugate definition does not need the
functionf tobeconvexnordifferentiable.InDefinition7.4,wehaveused
a general inner product (Section 3.2) but in the rest of this section we
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
7.3 ConvexOptimization 243
willconsiderthestandarddotproductbetweenfinite-dimensionalvectors
( s,x = s⊤x)toavoidtoomanytechnicaldetails.
⟨ ⟩
To understand Definition 7.4 in a geometric fashion, consider a nice Thisderivationis
simple one-dimensional convex and differentiable function, for example easiestto
f(x) = x2.Note thatsince weare lookingat aone-dimensional problem, understandby
drawingthe
hyperplanesreducetoaline.Consideraliney = sx+c.Recallthatweare
reasoningasit
able to describe convex functions by their supporting hyperplanes, so let progresses.
us try to describe this function f(x) by its supporting lines. Fix the gradi-
entofthelines Randforeachpoint(x ,f(x ))onthegraphoff,find
0 0
∈
theminimumvalueofcsuchthatthelinestillintersects(x ,f(x )).Note
0 0
that the minimum value of c is the place where a line with slope s “just
touches” the function f(x) = x2. The line passing through (x ,f(x ))
0 0
withgradientsisgivenby
y f(x ) = s(x x ). (7.54)
0 0
− −
The y-intercept of this line is sx +f(x ). The minimum of c for which
0 0
−
y = sx+cintersectswiththegraphoff istherefore
inf sx +f(x ). (7.55)
0 0
x0 −
The preceding convex conjugate is by convention defined to be the nega-
tive of this. The reasoning in this paragraph did not rely on the fact that
wechoseaone-dimensionalconvexanddifferentiablefunction,andholds
forf : RD R,whicharenonconvexandnon-differentiable.
→ Theclassical
Remark. Convexdifferentiablefunctionssuchastheexamplef(x) = x2is Legendretransform
anicespecialcase,wherethereisnoneedforthesupremum,andthereis isdefinedonconvex
differentiable
a one-to-one correspondence between a function and its Legendre trans-
functionsinRD.
form. Let us derive this from first principles. For a convex differentiable
function,weknowthatatx thetangenttouchesf(x )sothat
0 0
f(x ) = sx +c. (7.56)
0 0
Recall that we want to describe the convex function f(x) in terms of its
gradient f(x), and that s = f(x ). We rearrange to get an expres-
x x 0
∇ ∇
sionfor ctoobtain
−
c = sx f(x ). (7.57)
0 0
− −
Note that c changes with x and therefore with s, which is why we can
0
−
thinkofitasafunctionofs,whichwecall
f∗(s) := sx f(x ). (7.58)
0 0
−
Comparing(7.58)withDefinition7.4,weseethat(7.58)isaspecialcase
(withoutthesupremum).
♢
The conjugate function has nice properties; for example, for convex
functions,applyingtheLegendretransformagaingetsusbacktotheorig-
inalfunction.Inthesamewaythattheslopeoff(x)iss,theslopeoff∗(s)
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
244 ContinuousOptimization
isx.Thefollowingtwoexamplesshowcommonusesofconvexconjugates
inmachinelearning.
Example 7.7 (Convex Conjugates)
To illustrate the application of convex conjugates, consider the quadratic
function
λ
f(y) = y⊤K−1y (7.59)
2
based on a positive definite matrix K Rn×n. We denote the primal
variabletobey Rn andthedualvariab∈ letobeα Rn.
∈ ∈
ApplyingDefinition7.4,weobtainthefunction
λ
f∗(α) = sup y,α y⊤K−1y. (7.60)
y∈Rn⟨ ⟩− 2
Since the function is differentiable, we can find the maximum by taking
thederivativeandwithrespecttoy settingittozero.
∂(cid:2)
y,α
λy⊤K−1y(cid:3)
⟨ ⟩− 2 = (α λK−1y)⊤ (7.61)
∂y −
and hence when the gradient is zero we have y = 1Kα. Substituting
λ
into(7.60)yields
1 λ (cid:18) 1 (cid:19)⊤ (cid:18) 1 (cid:19) 1
f∗(α) = α⊤Kα Kα K−1 Kα = α⊤Kα.
λ − 2 λ λ 2λ
(7.62)
Example 7.8
Inmachinelearning,weoftenusesumsoffunctions;forexample,theob-
jectivefunctionofthetrainingsetincludesasumofthelossesforeachex-
ampleinthetrainingset.Inthefollowing,wederivetheconvexconjugate
of a sum of losses ℓ(t), where ℓ : R R. This also illustrates the appli-
cation of the convex conjugate to
the→
vector case. Let (t) =
(cid:80)n
ℓ (t ).
L i=1 i i
Then,
n
(cid:88)
∗(z) = sup z,t ℓ (t ) (7.63a)
i i
L t∈Rn⟨ ⟩−
i=1
n
(cid:88)
= sup z t ℓ (t ) definitionof dotproduct (7.63b)
i i i i
t∈Rn −
i=1
n
(cid:88)
= sup z t ℓ (t ) (7.63c)
i i i i
t∈Rn −
i=1
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
7.3 ConvexOptimization 245
n
(cid:88)
= ℓ∗(z ). definitionof conjugate (7.63d)
i i
i=1
RecallthatinSection7.2wederivedadualoptimizationproblemusing
Lagrange multipliers. Furthermore, for convex optimization problems we
have strong duality, that is the solutions of the primal and dual problem
match. The Legendre-Fenchel transform described here also can be used
to derive a dual optimization problem. Furthermore, when the function
is convex and differentiable, the supremum is unique. To further investi-
gate the relation between these two approaches, let us consider a linear
equalityconstrainedconvexoptimizationproblem.
Example 7.9
Letf(y)andg(x)beconvexfunctions,andAarealmatrixofappropriate
dimensionssuchthatAx = y.Then
minf(Ax)+g(x) = min f(y)+g(x). (7.64)
x Ax=y
ByintroducingtheLagrangemultiplierufortheconstraintsAx = y,
min f(y)+g(x) = minmaxf(y)+g(x)+(Ax y)⊤u (7.65a)
Ax=y x,y u −
= maxminf(y)+g(x)+(Ax y)⊤u, (7.65b)
u x,y −
where the last step of swapping max and min is due to the fact that f(y)
and g(x) are convex functions. By splitting up the dot product term and
collectingxandy,
maxminf(y)+g(x)+(Ax y)⊤u (7.66a)
u x,y −
(cid:20) (cid:21)
(cid:104) (cid:105)
= max min y⊤u+f(y) + min(Ax)⊤u+g(x) (7.66b)
u y − x
(cid:20) (cid:21)
(cid:104) (cid:105)
= max min y⊤u+f(y) + minx⊤A⊤u+g(x) (7.66c)
u y − x
Recall the convex conjugate (Definition 7.4) and the fact that dot prod- Forgeneralinner
uctsaresymmetric,
products,A⊤is
replacedbythe
(cid:20) (cid:21) (cid:104) (cid:105) adjointA∗.
max min y⊤u+f(y) + minx⊤A⊤u+g(x) (7.67a)
u y − x
= max f∗(u) g∗( A⊤u). (7.67b)
u − − −
Therefore,wehaveshownthat
minf(Ax)+g(x) = max f∗(u) g∗( A⊤u). (7.68)
x u − − −
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
246 ContinuousOptimization
The Legendre-Fenchel conjugate turns out to be quite useful for ma-
chine learning problems that can be expressed as convex optimization
problems.Inparticular,forconvexlossfunctionsthatapplyindependently
to each example, the conjugate loss is a convenient way to derive a dual
problem.
7.4 Further Reading
Continuous optimization is an active area of research, and we do not try
toprovideacomprehensiveaccountofrecentadvances.
From a gradient descent perspective, there are two major weaknesses
which each have their own set of literature. The first challenge is the fact
that gradient descent is a first-order algorithm, and does not use infor-
mation about the curvature of the surface. When there are long valleys,
the gradient points perpendicularly to the direction of interest. The idea
of momentum can be generalized to a general class of acceleration meth-
ods (Nesterov,2018). Conjugategradient methodsavoid theissues faced
bygradientdescentbytakingpreviousdirectionsintoaccount(Shewchuk,
1994).Second-ordermethodssuchasNewtonmethodsusetheHessianto
provide information about the curvature. Many of the choices for choos-
ingstep-sizesandideaslikemomentumarisebyconsideringthecurvature
of the objective function (Goh, 2017; Bottou et al., 2018). Quasi-Newton
methodssuchasL-BFGStrytousecheapercomputationalmethodstoap-
proximate the Hessian (Nocedal and Wright, 2006). Recently there has
been interest in other metrics for computing descent directions, result-
ing in approaches such as mirror descent (Beck and Teboulle, 2003) and
naturalgradient(Toussaint,2012).
The second challenge is to handle non-differentiable functions. Gradi-
ent methods are not well defined when there are kinks in the function.
In these cases, subgradient methods can be used (Shor, 1985). For fur-
ther information and algorithms for optimizing non-differentiable func-
tions, we refer to the book by Bertsekas (1999). There is a vast amount
of literature on different approaches for numerically solving continuous
optimizationproblems,includingalgorithmsforconstrainedoptimization
problems. Good starting points to appreciate this literature are the books
byLuenberger(1969)andBonnansetal.(2006).Arecentsurveyofcon-
HugoGon¸calves’ tinuousoptimizationisprovidedbyBubeck(2015).
blogisalsoagood Modern applications of machine learning often mean that the size of
resourceforan
datasets prohibit the use of batch gradient descent, and hence stochastic
easierintroduction
gradientdescentisthecurrentworkhorseoflarge-scalemachinelearning
toLegendre–Fenchel
transforms: methods. Recent surveys of the literature include Hazan (2015) and Bot-
https://tinyurl. touetal.(2018).
com/ydaal7hj For duality and convex optimization, the book by Boyd and Vanden-
berghe (2004) includes lectures and slides online. A more mathematical
treatment is provided by Bertsekas (2009), and recent book by one of
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Exercises 247
the key researchers in the area of optimization is Nesterov (2018). Con-
vexoptimizationisbaseduponconvexanalysis,andthereaderinterested
in more foundational results about convex functions is referred to Rock-
afellar (1970), Hiriart-Urruty and Lemar´echal (2001), and Borwein and
Lewis(2006).Legendre–Fencheltransformsarealsocoveredintheafore-
mentioned books on convex analysis, but a more beginner-friendly pre-
sentation is available at Zia et al. (2009). The role of Legendre–Fenchel
transforms in the analysis of convex optimization algorithms is surveyed
inPolyak(2016).
Exercises
7.1 Considertheunivariatefunction
f(x)=x3+6x2−3x−5.
Find its stationary points and indicate whether they are maximum, mini-
mum,orsaddlepoints.
7.2 Considertheupdateequationforstochasticgradientdescent(Equation(7.15)).
Writedowntheupdatewhenweuseamini-batchsizeofone.
7.3 Considerwhetherthefollowingstatementsaretrueorfalse:
a. Theintersectionofanytwoconvexsetsisconvex.
b. Theunionofanytwoconvexsetsisconvex.
c. ThedifferenceofaconvexsetAfromanotherconvexsetB isconvex.
7.4 Considerwhetherthefollowingstatementsaretrueorfalse:
a. Thesumofanytwoconvexfunctionsisconvex.
b. Thedifferenceofanytwoconvexfunctionsisconvex.
c. Theproductofanytwoconvexfunctionsisconvex.
d. Themaximumofanytwoconvexfunctionsisconvex.
7.5 Expressthefollowingoptimizationproblemasastandardlinearprogramin
matrixnotation
max p⊤x+ξ
x∈R2,ξ∈R
subjecttotheconstraintsthatξ⩾0,x ⩽0andx ⩽3.
0 1
7.6 ConsiderthelinearprogramillustratedinFigure7.9,
(cid:20) (cid:21)⊤(cid:20) (cid:21)
5 x
min − 1
x∈R2 3 x 2
   
2 2 33
 2 −4(cid:20) (cid:21)  8 
subjectto  −2 1   x 1 ⩽  5  
  0 −1  x 2  −1 
0 1 8
DerivetheduallinearprogramusingLagrangeduality.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
248 ContinuousOptimization
7.7 ConsiderthequadraticprogramillustratedinFigure7.4,
(cid:20) (cid:21)⊤(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)⊤(cid:20) (cid:21)
min 1 x 1 2 1 x 1 + 5 x 1
x∈R22 x
2
1 4 x
2
3 x
2
   
1 0 1
(cid:20) (cid:21)
subjectto  −1 0   x 1 ⩽ 1 
 0 1  x 2 1
0 −1 1
DerivethedualquadraticprogramusingLagrangeduality.
7.8 Considerthefollowingconvexoptimizationproblem
1
min w⊤w
w∈RD 2
subjectto w⊤x⩾1.
DerivetheLagrangiandualbyintroducingtheLagrangemultiplierλ.
7.9 Considerthenegativeentropyofx∈RD,
D
(cid:88)
f(x)= x logx .
d d
d=1
Derive the convex conjugate function f∗(s), by assuming the standard dot
product.
Hint:Takethegradientofanappropriatefunctionandsetthegradienttozero.
7.10 Considerthefunction
1
f(x)= x⊤Ax+b⊤x+c,
2
whereAisstrictlypositivedefinite,whichmeansthatitisinvertible.Derive
theconvexconjugateoff(x).
Hint:Takethegradientofanappropriatefunctionandsetthegradienttozero.
7.11 The hinge loss (which is the loss used by the support vector machine) is
givenby
L(α)=max{0,1−α},
If we are interested in applying gradient methods such as L-BFGS, and do
not want to resort to subgradient methods, we need to smooth the kink in
thehingeloss.ComputetheconvexconjugateofthehingelossL∗(β)where
β isthedualvariable.Addaℓ proximalterm,andcomputetheconjugate
2
oftheresultingfunction
γ
L∗(β)+ β2,
2
whereγ isagivenhyperparameter.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Part II
Central Machine Learning Problems
249
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.

8
When Models Meet Data
In the first part of the book, we introduced the mathematics that form
the foundations of many machine learning methods. The hope is that a
reader would be able to learn the rudimentary forms of the language of
mathematics from the first part, which we will now use to describe and
discuss machine learning. The second part of the book introduces four
pillarsofmachinelearning:
Regression(Chapter9)
Dimensionalityreduction(Chapter10)
Densityestimation(Chapter11)
Classification(Chapter12)
Themainaimofthispartofthebookistoillustratehowthemathematical
concepts introduced in the first part of the book can be used to design
machine learning algorithms that can be used to solve tasks within the
remitofthefourpillars.Wedonotintendtointroduceadvancedmachine
learning concepts, but instead to provide a set of practical methods that
allow the reader to apply the knowledge they gained from the first part
of the book. It also provides a gateway to the wider machine learning
literatureforreadersalreadyfamiliarwiththemathematics.
8.1 Data, Models, and Learning
It is worth at this point, to pause and consider the problem that a ma-
chine learning algorithm is designed to solve. As discussed in Chapter 1,
there are three major components of a machine learning system: data,
models,andlearning.Themainquestionofmachinelearningis“Whatdo
wemeanbygoodmodels?”.Thewordmodelhasmanysubtleties,andwe model
will revisit it multiple times in this chapter. It is also not entirely obvious
how to objectively define the word “good”. One of the guiding principles
of machine learning is that good models should perform well on unseen
data. This requires us to define some performance metrics, such as accu-
racyordistancefromgroundtruth,aswellasfiguringoutwaystodowell
undertheseperformancemetrics.Thischaptercoversafewnecessarybits
and pieces of mathematical and statistical language that are commonly
251
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
252 WhenModelsMeetData
Table8.1 Example Name Gender Degree Postcode Age Annualsalary
datafroma Aditya M MSc W21BG 36 89563
fictitioushuman Bob M PhD EC1A1BA 47 123543
resourcedatabase Chlo´e F BEcon SW1A1BH 26 23989
thatisnotina Daisuke M BSc SE207AT 68 138769
numericalformat. Elisabeth F MBA SE10AA 33 113888
used to talk about machine learning models. By doing so, we briefly out-
linethecurrentbestpracticesfortrainingamodelsuchthattheresulting
predictordoeswellondatathatwehavenotyetseen.
As mentioned in Chapter 1, there are two different senses in which we
usethephrase“machinelearningalgorithm”:trainingandprediction.We
will describe these ideas in this chapter, as well as the idea of selecting
among different models. We will introduce the framework of empirical
risk minimization in Section 8.2, the principle of maximum likelihood in
Section8.3,andtheideaofprobabilisticmodelsinSection8.4.Webriefly
outline a graphical language for specifying probabilistic models in Sec-
tion8.5andfinallydiscussmodelselectioninSection8.6.Therestofthis
section expands upon the three main components of machine learning:
data,modelsandlearning.
8.1.1 Data as Vectors
Weassumethatourdatacanbereadbyacomputer,andrepresentedade-
quatelyinanumericalformat.Dataisassumedtobetabular(Figure8.1),
where we think of each row of the table as representing a particular in-
Dataisassumedto stance or example, and each column to be a particular feature. In recent
beinatidy years,machinelearninghasbeenappliedtomanytypesofdatathatdonot
format(Wickham,
obviouslycomeinthetabularnumericalformat,forexamplegenomicse-
2014;Codd,1990).
quences,textandimagecontentsofawebpage,andsocialmediagraphs.
We do not discuss the important and challenging aspects of identifying
goodfeatures.Manyoftheseaspectsdependondomainexpertiseandre-
quire careful engineering, and, in recent years, they have been put under
theumbrellaofdatascience(Stray,2016;AdhikariandDeNero,2018).
Even when we have data in tabular format, there are still choices to be
madetoobtainanumericalrepresentation.Forexample,inTable8.1,the
gender column (a categorical variable) may be converted into numbers 0
representing “Male” and 1 representing “Female”. Alternatively, the gen-
der could be represented by numbers 1,+1, respectively (as shown in
−
Table 8.2). Furthermore, it is often important to use domain knowledge
when constructing the representation, such as knowing that university
degrees progress from bachelor’s to master’s to PhD or realizing that the
postcode provided is not just a string of characters but actually encodes
an area in London. In Table 8.2, we converted the data from Table 8.1
toanumericalformat,andeachpostcodeisrepresentedastwonumbers,
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.1 Data,Models,andLearning 253
GenderID Degree Latitude Longitude Age AnnualSalary Table8.2 Example
(indegrees) (indegrees) (inthousands) datafroma
-1 2 51.5073 0.1290 36 89.563 fictitioushuman
-1 3 51.5074 0.1275 47 123.543 resourcedatabase
+1 1 51.5071 0.1278 26 23.989 (seeTable8.1),
-1 1 51.5075 0.1281 68 138.769 convertedtoa
+1 2 51.5074 0.1278 33 113.888 numericalformat.
a latitude and longitude. Even numerical data that could potentially be
directly read into a machine learning algorithm should be carefully con-
sideredforunits,scaling,andconstraints.Withoutadditionalinformation,
one should shift and scale all columns of the dataset such that they have
an empirical mean of 0 and an empirical variance of 1. For the purposes
of this book, we assume that a domain expert already converted data ap-
propriately,i.e.,eachinputx isaD-dimensionalvectorofrealnumbers,
n
whicharecalledfeatures,attributes,orcovariates.Weconsideradatasetto feature
be of the form as illustrated by Table 8.2. Observe that we have dropped attribute
theNamecolumnofTable8.1inthenewnumericalrepresentation.There covariate
aretwomainreasonswhythisisdesirable:(1)wedonotexpecttheiden-
tifier (the Name) to be informative for a machine learning task; and (2)
we may wish to anonymize the data to help protect the privacy of the
employees.
In this part of the book, we will use N to denote the number of exam-
ples in a dataset and index the examples with lowercase n = 1,...,N.
We assume that we are given a set of numerical data, represented as an
array of vectors (Table 8.2). Each row is a particular individual x , often
n
referredtoasanexampleordatapointinmachinelearning.Thesubscript example
n refers to the fact that this is the nth example out of a total of N exam- datapoint
plesinthedataset.Eachcolumnrepresentsaparticularfeatureofinterest
abouttheexample,andweindexthefeaturesasd = 1,...,D.Recallthat
dataisrepresentedasvectors,whichmeansthateachexample(eachdata
point) is a D-dimensional vector. The orientation of the table originates
from the database community, but for some machine learning algorithms
(e.g., in Chapter 10) it is more convenient to represent examples as col-
umnvectors.
Letusconsidertheproblemofpredictingannualsalaryfromage,based
on the data in Table 8.2. This is called a supervised learning problem
where we have a label y n (the salary) associated with each example x n label
(the age). The label y has various other names, including target, re-
n
sponse variable, and annotation. A dataset is written as a set of example-
label pairs (x ,y ),...,(x ,y ),...,(x ,y ) . The table of examples
1 1 n n N N
x ,...,x { is often concatenated, and writt} en as X RN×D. Fig-
1 N
{ } ∈
ure 8.1 illustrates the dataset consisting of the two rightmost columns
ofTable8.2,wherex = ageandy = salary.
Weusetheconceptsintroducedinthefirstpartofthebooktoformalize
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
254 WhenModelsMeetData
Figure8.1 Toydata
forlinearregression. 150
Trainingdatain
(xn,yn)pairsfrom 125
therightmosttwo ?
columnsof
100
Table8.2.Weare
interestedinthe
75
salaryofaperson
agedsixty(x=60)
50
illustratedasa
verticaldashedred
25
line,whichisnot
partofthetraining
0
data.
0 10 20 30 40 50 60 70 80
x
the machine learning problems such as that in the previous paragraph.
Representing data as vectors x allows us to use concepts from linear al-
n
gebra (introduced in Chapter 2). In many machine learning algorithms,
weneedtoadditionallybeabletocomparetwovectors.Aswewillseein
Chapters 9 and 12, computing the similarity or distance between two ex-
amplesallowsustoformalizetheintuitionthatexampleswithsimilarfea-
tures should have similar labels. The comparison of two vectors requires
that we construct a geometry (explained in Chapter 3) and allows us to
optimizetheresultinglearningproblemusingtechniquesfromChapter7.
Sincewehavevectorrepresentationsofdata,wecanmanipulatedatato
find potentially better representations of it. We will discuss finding good
representations in two ways: finding lower-dimensional approximations
of the original feature vector, and using nonlinear higher-dimensional
combinations of the original feature vector. In Chapter 10, we will see an
example of finding a low-dimensional approximation of the original data
spacebyfindingtheprincipalcomponents.Findingprincipalcomponents
iscloselyrelatedtoconceptsofeigenvalueandsingularvaluedecomposi-
tionasintroducedinChapter4.Forthehigh-dimensionalrepresentation,
featuremap we will see an explicit feature map ϕ( ) that allows us to represent in-
·
puts x using a higher-dimensional representation ϕ(x ). The main mo-
n n
tivation for higher-dimensional representations is that we can construct
newfeaturesasnon-linearcombinationsoftheoriginalfeatures,whichin
turn may make the learning problem easier. We will discuss the feature
kernel map in Section 9.2 and show how this feature map leads to a kernel in
Section 12.4. In recent years, deep learning methods (Goodfellow et al.,
2016)haveshownpromiseinusingthedataitselftolearnnewgoodfea-
tures and have been very successful in areas, such as computer vision,
speech recognition, and natural language processing. We will not cover
neural networks in this part of the book, but the reader is referred to
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
y
8.1 Data,Models,andLearning 255
Figure8.2 Example
150 function(blacksolid
diagonalline)and
125 itspredictionat
x=60,i.e.,
f(60)=100.
100
75
50
25
0
0 10 20 30 40 50 60 70 80
x
Section 5.6 for the mathematical description of backpropagation, a key
conceptfortrainingneuralnetworks.
8.1.2 Models as Functions
Oncewehavedatainanappropriatevectorrepresentation,wecangetto
the business of constructing a predictive function (known as a predictor). predictor
InChapter1,wedidnotyethavethelanguagetobepreciseaboutmodels.
Using the concepts from the first part of the book, we can now introduce
what “model” means. We present two major approaches in this book: a
predictor as a function, and a predictor as a probabilistic model. We de-
scribetheformerhereandthelatterinthenextsubsection.
A predictor is a function that, when given a particular input example
(in our case, a vector of features), produces an output. For now, consider
theoutputtobeasinglenumber,i.e.,areal-valuedscalaroutput.Thiscan
bewrittenas
f : RD R, (8.1)
→
wheretheinputvectorxisD-dimensional(hasDfeatures),andthefunc-
tion f then applied to it (written as f(x)) returns a real number. Fig-
ure 8.2 illustrates a possible function that can be used to compute the
valueofthepredictionforinputvaluesx.
Inthisbook,wedonotconsiderthegeneralcaseofallfunctions,which
would involve the need for functional analysis. Instead, we consider the
specialcaseoflinearfunctions
f(x) = θ⊤x+θ (8.2)
0
for unknown θ and θ . This restriction means that the contents of Chap-
0
ters 2 and 3 suffice for precisely stating the notion of a predictor for
thenon-probabilistic(incontrasttotheprobabilisticviewdescribednext)
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
y
256 WhenModelsMeetData
Figure8.3 Example
function(blacksolid 150
diagonalline)and
itspredictive 125
uncertaintyat
x=60(drawnasa
100
Gaussian).
75
50
25
0
0 10 20 30 40 50 60 70 80
x
viewofmachinelearning.Linearfunctionsstrikeagoodbalancebetween
thegeneralityoftheproblemsthatcanbesolvedandtheamountofback-
groundmathematicsthatisneeded.
8.1.3 Models as Probability Distributions
We often consider data to be noisy observations of some true underlying
effect, and hope that by applying machine learning we can identify the
signal from the noise. This requires us to have a language for quantify-
ing the effect of noise. We often would also like to have predictors that
express some sortof uncertainty, e.g., to quantify theconfidence we have
about the value of the prediction for a particular test data point. As we
have seen in Chapter 6, probability theory provides a language for quan-
tifying uncertainty. Figure 8.3 illustrates the predictive uncertainty of the
functionasaGaussiandistribution.
Instead of considering a predictor as a single function, we could con-
siderpredictorstobeprobabilisticmodels,i.e.,modelsdescribingthedis-
tribution of possible functions. We limit ourselves in this book to the spe-
cialcaseofdistributionswithfinite-dimensionalparameters,whichallows
us to describe probabilistic models without needing stochastic processes
and random measures. For this special case, we can think about prob-
abilistic models as multivariate probability distributions, which already
allowforarichclassofmodels.
We will introduce how to use concepts from probability (Chapter 6) to
definemachinelearningmodelsinSection8.4,andintroduceagraphical
language for describing probabilistic models in a compact way in Sec-
tion8.5.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
y
8.1 Data,Models,andLearning 257
8.1.4 Learning is Finding Parameters
The goal of learning is to find a model and its corresponding parame-
ters such that the resulting predictor will perform well on unseen data.
There are conceptually three distinct algorithmic phases when discussing
machinelearningalgorithms:
1. Predictionorinference
2. Trainingorparameterestimation
3. Hyperparametertuningormodelselection
Thepredictionphaseiswhenweuseatrainedpredictoronpreviouslyun-
seentestdata.Inotherwords,theparametersandmodelchoiceisalready
fixed and the predictor is applied to new vectors representing new input
datapoints.AsoutlinedinChapter1andtheprevioussubsection,wewill
consider two schools of machine learning in this book, corresponding to
whether the predictor is a function or a probabilistic model. When we
have a probabilistic model (discussed further in Section 8.4) the predic-
tionphaseiscalledinference.
Remark. Unfortunately, there is no agreed upon naming for the different
algorithmicphases.Theword“inference”issometimesalsousedtomean
parameterestimationofaprobabilisticmodel,andlessoftenmaybealso
usedtomeanpredictionfornon-probabilisticmodels.
♢
The training or parameter estimation phase is when weadjust our pre-
dictive model based on training data. We would like to find good predic-
tors given training data, and there are two main strategies for doing so:
finding the best predictor based on some measure of quality (sometimes
called finding a point estimate), or using Bayesian inference. Finding a
point estimate can be applied to both types of predictors, but Bayesian
inferencerequiresprobabilisticmodels.
Forthenon-probabilisticmodel,wefollowtheprincipleofempiricalrisk empiricalrisk
minimization, which we describe in Section 8.2. Empirical risk minimiza- minimization
tion directly provides an optimization problem for finding good parame-
ters. With a statistical model, the principle of maximum likelihood is used maximumlikelihood
tofindagoodsetofparameters(Section8.3).Wecanadditionallymodel
the uncertainty of parameters using a probabilistic model, which we will
lookatinmoredetailinSection8.4.
We use numerical methods to find good parameters that “fit” the data,
and most training methods can be thought of as hill-climbing approaches
tofindthemaximumofanobjective,forexamplethemaximumofalikeli-
hood.Toapplyhill-climbingapproachesweusethegradientsdescribedin Theconventionin
Chapter5andimplementnumericaloptimizationapproachesfromChap- optimizationisto
minimizeobjectives.
ter7.
Hence,thereisoften
AsmentionedinChapter1,weareinterestedinlearningamodelbased
anextraminussign
on data such that it performs well on future data. It is not enough for inmachinelearning
objectives.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
258 WhenModelsMeetData
the model to only fit the training data well, the predictor needs to per-
form well on unseen data. We simulate the behavior of our predictor on
cross-validation future unseen data using cross-validation (Section 8.2.4). As we will see
in this chapter, to achieve the goal of performing well on unseen data,
we will need to balance between fitting well on training data and finding
“simple” explanations of the phenomenon. This trade-off is achieved us-
ing regularization (Section 8.2.3) or by adding a prior (Section 8.3.2). In
philosophy, this is considered to be neither induction nor deduction, but
abduction is called abduction. According to the Stanford Encyclopedia of Philosophy,
abduction is the process of inference to the best explanation (Douven,
Agoodmovietitleis 2017).
“AIabduction”. We often need to make high-level modeling decisions about the struc-
ture of the predictor, such as the number of components to use or the
class of probability distributions to consider. The choice of the number of
hyperparameter components is an example of a hyperparameter, and this choice can af-
fect the performance of the model significantly. The problem of choosing
modelselection among different models is called model selection, which we describe in
Section 8.6. For non-probabilistic models, model selection is often done
nested using nested cross-validation, which is described in Section 8.6.1. We also
cross-validation usemodelselectiontochoosehyperparametersofourmodel.
Remark. Thedistinctionbetweenparametersandhyperparametersissome-
what arbitrary, and is mostly driven by the distinction between what can
be numerically optimized versus what needs to use search techniques.
Another way to consider the distinction is to consider parameters as the
explicitparametersofaprobabilisticmodel,andtoconsiderhyperparam-
eters(higher-levelparameters)asparametersthatcontrolthedistribution
oftheseexplicitparameters.
♢
Inthefollowingsections,wewilllookatthreeflavorsofmachinelearn-
ing: empirical risk minimization (Section 8.2), the principle of maximum
likelihood(Section8.3),andprobabilisticmodeling(Section8.4).
8.2 Empirical Risk Minimization
After having all the mathematics under our belt, we are now in a posi-
tion to introduce what it means to learn. The “learning” part of machine
learningboilsdowntoestimatingparametersbasedontrainingdata.
In this section, we consider the case of a predictor that is a function,
and consider the case of probabilistic models in Section 8.3. We describe
theideaofempiricalriskminimization,whichwasoriginallypopularized
by the proposal of the support vector machine (described in Chapter 12).
However, its general principles are widely applicable and allow us to ask
thequestionofwhatislearningwithoutexplicitlyconstructingprobabilis-
tic models. There are four main design choices, which we will cover in
detailinthefollowingsubsections:
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.2 EmpiricalRiskMinimization 259
Section 8.2.1 Whatisthesetoffunctionsweallowthepredictortotake?
Section 8.2.2 How do we measure how well the predictor performs on
thetrainingdata?
Section 8.2.3 How do we construct predictors from only training data
thatperformswellonunseentestdata?
Section 8.2.4 Whatistheprocedureforsearchingoverthespaceofmod-
els?
8.2.1 Hypothesis Class of Functions
Assume we are given N examples x RD and corresponding scalar la-
n
belsy
R.Weconsiderthesupervised∈
learningsetting,whereweobtain
n
∈
pairs (x ,y ),...,(x ,y ). Given this data, we would like to estimate a
1 1 N N
predictorf( ,θ) : RD R,parametrizedbyθ.Wehopetobeabletofind
agoodparam· eterθ∗ s→ uchthatwefitthedatawell,thatis,
f(x ,θ∗) y forall n = 1,...,N . (8.3)
n n
≈
Inthissection,weusethenotationyˆ = f(x ,θ∗)torepresenttheoutput
n n
ofthepredictor.
Remark. For ease of presentation, we will describe empirical risk mini-
mization in terms of supervised learning (where we have labels). This
simplifies the definition of the hypothesis class and the loss function. It
is also common in machine learning to choose a parametrized class of
functions,forexampleaffinefunctions.
♢
Example 8.1
Weintroducetheproblemofordinaryleast-squaresregressiontoillustrate
empiricalriskminimization.Amorecomprehensiveaccountofregression
is given in Chapter 9. When the label y is real-valued, a popular choice
n
of function class for predictors is the set of affine functions. We choose a Affinefunctionsare
more compact notation for an affine function by concatenating an addi- oftenreferredtoas
linearfunctionsin
tional unit feature x(0) = 1 to x , i.e., x = [1,x(1),x(2),...,x(D)]⊤. The
n n n n n machinelearning.
parametervectoriscorrespondinglyθ = [θ ,θ ,θ ,...,θ ]⊤,allowingus
0 1 2 D
towritethepredictorasalinearfunction
f(x ,θ) = θ⊤x . (8.4)
n n
Thislinearpredictorisequivalenttotheaffinemodel
D
(cid:88)
f(x ,θ) = θ + θ x(d). (8.5)
n 0 d n
d=1
The predictor takes the vector of features representing a single example
x as input and produces a real-valued output, i.e., f : RD+1 R. The
n
→
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
260 WhenModelsMeetData
previous figures in this chapter had a straight line as a predictor, which
meansthatwehaveassumedanaffinefunction.
Instead of a linear function, we may wish to consider non-linear func-
tionsaspredictors.Recentadvancesinneuralnetworksallowforefficient
computationofmorecomplexnon-linearfunctionclasses.
Given the class of functions, we want to search for a good predictor.
Wenowmoveontothesecondingredientofempiricalriskminimization:
howtomeasurehowwellthepredictorfitsthetrainingdata.
8.2.2 Loss Function for Training
Considerthelabely foraparticularexample;andthecorrespondingpre-
n
diction yˆ that we make based on x . To define what it means to fit the
n n
lossfunction datawell,weneedtospecifyalossfunctionℓ(y n,yˆ n)thattakestheground
truthlabelandthepredictionasinputandproducesanon-negativenum-
ber (referred to as the loss) representing how much error we have made
Theexpression onthisparticularprediction.Ourgoalforfindingagoodparametervector
“error”isoftenused θ∗ istominimizetheaveragelossonthesetofN trainingexamples.
tomeanloss.
One assumption that is commonly made in machine learning is that
independentand the set of examples (x 1,y 1),...,(x N,y N) is independent and identically
identically distributed. The word independent (Section 6.4.5) means that two data
distributed points(x ,y )and(x ,y )donotstatisticallydependoneachother,mean-
i i j j
ing that the empirical mean is a good estimate of the population mean
(Section 6.4.1). This implies that we can use the empirical mean of the
trainingset lossonthetrainingdata.Foragiventrainingset (x 1,y 1),...,(x N,y N) ,
{ }
we introduce the notation of an example matrix X := [x ,...,x ]⊤
1 N
RN×D and a label vector y := [y ,...,y ]⊤ RN. Using this matri∈ x
1 N
∈
notationtheaveragelossisgivenby
N
1 (cid:88)
R (f,X,y) = ℓ(y ,yˆ ), (8.6)
emp N n n
n=1
empiricalrisk where yˆ n = f(x n,θ). Equation (8.6) is called the empirical risk and de-
pendsonthreearguments,thepredictorf andthedataX,y.Thisgeneral
empiricalrisk strategyforlearningiscalledempiricalriskminimization.
minimization
Example 8.2 (Least-Squares Loss)
Continuing the example of least-squares regression, we specify that we
measure the cost of making an error during training using the squared
lossℓ(y ,yˆ ) = (y yˆ )2.Wewishtominimizetheempiricalrisk(8.6),
n n n n
−
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.2 EmpiricalRiskMinimization 261
whichistheaverageofthelossesoverthedata
N
1 (cid:88)
min (y f(x ,θ))2, (8.7)
θ∈RD N n − n
n=1
wherewesubstitutedthe predictoryˆ = f(x ,θ).Byusingour choiceof
n n
alinearpredictorf(x ,θ) = θ⊤x ,weobtaintheoptimizationproblem
n n
N
1 (cid:88)
min (y θ⊤x )2. (8.8)
θ∈RD N n − n
n=1
Thisequationcanbeequivalentlyexpressedinmatrixform
1
min y Xθ 2 . (8.9)
θ∈RD N ∥ − ∥
Thisisknownastheleast-squaresproblem.Thereexistsaclosed-forman- least-squares
alytic solution for this by solving the normal equations, which we will problem
discussinSection9.2.
We are not interested in a predictor that only performs well on the
training data. Instead, we seek a predictor that performs well (has low
risk) on unseen test data. More formally, we are interested in finding a
predictorf (withparametersfixed)thatminimizestheexpectedrisk expectedrisk
R (f) = E [ℓ(y,f(x))], (8.10)
true x,y
where y is the label and f(x) is the prediction based on the example x.
ThenotationR (f)indicatesthatthisisthetrueriskifwehadaccessto
true
aninfiniteamountofdata.Theexpectationisoverthe(infinite)setofall Anotherphrase
possibledataandlabels.Therearetwopracticalquestionsthatarisefrom commonlyusedfor
expectedriskis
our desire to minimize expected risk, which we address in the following
“populationrisk”.
twosubsections:
Howshouldwechangeourtrainingproceduretogeneralizewell?
Howdoweestimateexpectedriskfrom(finite)data?
Remark. Many machine learning tasks are specified with an associated
performance measure, e.g., accuracy of prediction or root mean squared
error.Theperformancemeasurecouldbemorecomplex,becostsensitive,
and capture details about the particular application. In principle, the de-
signofthelossfunctionforempiricalriskminimizationshouldcorrespond
directly to the performance measure specified by the machine learning
task.Inpractice,thereisoftenamismatchbetweenthedesignoftheloss
function and the performance measure. This could be due to issues such
aseaseofimplementationorefficiencyofoptimization.
♢
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
262 WhenModelsMeetData
8.2.3 Regularization to Reduce Overfitting
This section describes an addition to empirical risk minimization that al-
lows it to generalize well (approximately minimizing expected risk). Re-
callthattheaimoftrainingamachinelearningpredictorissothatwecan
performwell onunseen data,i.e., thepredictor generalizeswell. Wesim-
ulate this unseen data by holding out a proportion of the whole dataset.
testset Thisholdoutsetisreferredtoasthetestset.Givenasufficientlyrichclass
Evenknowingonly offunctionsforthepredictorf,wecanessentiallymemorizethetraining
theperformanceof datatoobtainzeroempiricalrisk.Whilethisisgreattominimizetheloss
thepredictoronthe
(and therefore the risk) on the training data, we would not expect the
testsetleaks
information(Blum predictor to generalize well to unseen data. In practice, we have only a
andHardt,2015). finite set of data, and hence we split our data into a training and a test
set. The training set is used to fit the model, and the test set (not seen
by the machine learning algorithm during training) is used to evaluate
generalization performance. It is important for the user to not cycle back
to a new round of training after having observed the test set. We use the
subscripts and to denote the training and test sets, respectively.
train test
We will revisit this idea of using a finite dataset to evaluate expected risk
inSection8.2.4.
overfitting Itturnsoutthatempiricalriskminimizationcanleadtooverfitting,i.e.,
the predictor fits too closely to the training data and does not general-
ize well to new data (Mitchell, 1997). This general phenomenon of hav-
ing very small average loss on the training set but large average loss on
the test set tends to occur when we have little data and a complex hy-
pothesis class. For a particular predictor f (with parameters fixed), the
phenomenon of overfitting occurs when the risk estimate from the train-
ingdataR (f,X ,y )underestimatestheexpectedriskR (f).
emp train train true
Since we estimate the expected risk R (f) by using the empirical risk
true
on the test set R (f,X ,y ) if the test risk is much larger than
emp test test
the training risk, this is an indication of overfitting. We revisit the idea of
overfittinginSection8.3.3.
Therefore, we need to somehow bias the search for the minimizer of
empirical risk by introducing a penalty term, which makes it harder for
the optimizer to return an overly flexible predictor. In machine learning,
regularization the penalty term is referred to as regularization. Regularization is a way
to compromise between accurate solution of empirical risk minimization
andthesizeorcomplexityofthesolution.
Example 8.3 (Regularized Least Squares)
Regularization is an approach that discourages complex or extreme solu-
tions to an optimization problem. The simplest regularization strategy is
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.2 EmpiricalRiskMinimization 263
toreplacetheleast-squaresproblem
1
min y Xθ 2 . (8.11)
θ N ∥ − ∥
in the previous example with the “regularized” problem by adding a
penaltyterminvolvingonlyθ:
1
min y Xθ 2 +λ θ 2 . (8.12)
θ N ∥ − ∥ ∥ ∥
2
The additional term θ is called the regularizer, and the parameter regularizer
∥ ∥
λ is the regularization parameter. The regularization parameter trades regularization
off minimizing the loss on the training set and the magnitude of the pa- parameter
rameters θ. It often happens that the magnitude of the parameter values
becomesrelativelylargeifwerunintooverfitting(Bishop,2006).
Theregularizationtermissometimescalledthepenaltyterm,whichbi- penaltyterm
asesthevectorθ tobeclosertotheorigin.Theideaofregularizationalso
appearsinprobabilisticmodelsasthepriorprobabilityoftheparameters.
RecallfromSection6.6thatfortheposteriordistributiontobeofthesame
formasthepriordistribution,thepriorandthelikelihoodneedtobecon-
jugate.WewillrevisitthisideainSection8.3.2.WewillseeinChapter12
thattheideaoftheregularizerisequivalenttotheideaofalargemargin.
8.2.4 Cross-Validation to Assess the Generalization Performance
Wementionedintheprevioussectionthatwemeasurethegeneralization
error by estimating it by applying the predictor on test data. This data is
alsosometimesreferredtoasthevalidationset.Thevalidationsetisasub- validationset
setoftheavailabletrainingdatathatwekeepaside.Apracticalissuewith
this approach is that the amount of data is limited, and ideally we would
use as much of the data available to train the model. This would require
us to keep our validation set small, which then would lead to a noisy
V
estimate (with high variance) of the predictive performance. One solu-
tion to these contradictory objectives (large training set, large validation
set)istousecross-validation.K-foldcross-validationeffectivelypartitions cross-validation
the data into K chunks, K 1 of which form the training set , and
− R
the last chunk serves as the validation set (similar to the idea outlined
V
previously). Cross-validation iterates through (ideally) all combinations
of assignments of chunks to and ; see Figure 8.4. This procedure is
R V
repeated for all K choices for the validation set, and the performance of
themodelfromtheK runsisaveraged.
Wepartitionourdatasetintotwosets = ,suchthattheydonot
D R∪V
overlap ( = ), where is the validation set, and train our model
R∩V ∅ V
on . After training, we assess the performance of the predictor f on the
R
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
264 WhenModelsMeetData
Figure8.4 K-fold
cross-validation.
Training
Thedatasetis
dividedintoK=5
chunks,K−1of
whichserveasthe
trainingset(blue)
andoneasthe Validation
validationset
(orangehatch).
validation set (e.g., by computing root mean square error (RMSE) of
V
thetrainedmodelonthevalidationset).Moreprecisely,foreachpartition
k the training data (k) produces a predictor f(k), which is then applied
R
tovalidationset (k) tocomputetheempiricalriskR(f(k), (k)).Wecycle
V V
throughallpossiblepartitioningsofvalidationandtrainingsetsandcom-
pute the average generalization error of the predictor. Cross-validation
approximatestheexpectedgeneralizationerror
K
1 (cid:88)
E [R(f, )] R(f(k), (k)), (8.13)
V V ≈ K V
k=1
where R(f(k), (k)) is the risk (e.g., RMSE) on the validation set (k) for
V V
predictor f(k). The approximation has two sources: first, due to the finite
trainingset,whichresultsinnotthebestpossiblef(k);andsecond,dueto
the finite validation set, which results in an inaccurate estimation of the
risk R(f(k), (k)). A potential disadvantage of K-fold cross-validation is
V
the computational cost of training the model K times, which can be bur-
densome if the training cost is computationally expensive. In practice, it
isoftennotsufficienttolookatthedirectparametersalone.Forexample,
we need to explore multiple complexity parameters (e.g., multiple regu-
larizationparameters),whichmaynotbedirectparametersofthemodel.
Evaluatingthequalityofthemodel,dependingonthesehyperparameters,
mayresultinanumberoftrainingrunsthatisexponentialinthenumber
ofmodelparameters.Onecanusenestedcross-validation(Section8.6.1)
tosearchforgoodhyperparameters.
embarrassingly However,cross-validationisanembarrassinglyparallelproblem,i.e.,lit-
parallel tle effort is needed to separate the problem into a number of parallel
tasks.Givensufficientcomputingresources(e.g.,cloudcomputing,server
farms),cross-validationdoesnotrequirelongerthanasingleperformance
assessment.
Inthissection,wesawthatempiricalriskminimizationisbasedonthe
followingconcepts:thehypothesisclassoffunctions,thelossfunctionand
regularization.InSection8.3,wewillseetheeffectofusingaprobability
distributiontoreplacetheideaoflossfunctionsandregularization.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.3 ParameterEstimation 265
8.2.5 Further Reading
Due to the fact that the original development of empirical risk minimiza-
tion (Vapnik, 1998) was couched in heavily theoretical language, many
of the subsequent developments have been theoretical. The area of study
is called statistical learning theory (Vapnik, 1999; Evgeniou et al., 2000; statisticallearning
Hastieetal.,2001;vonLuxburgandScho¨lkopf,2011).Arecentmachine theory
learningtextbookthatbuildsonthetheoreticalfoundationsanddevelops
efficientlearningalgorithmsisShalev-ShwartzandBen-David(2014).
Theconceptofregularizationhasitsrootsinthesolutionofill-posedin-
verse problems (Neumaier, 1998). The approach presented here is called
Tikhonovregularization,andthereisacloselyrelatedconstrainedversion Tikhonov
called Ivanov regularization. Tikhonov regularization has deep relation- regularization
ships to the bias-variance trade-off and feature selection (Bu¨hlmann and
Van De Geer, 2011). An alternative to cross-validation is bootstrap and
jackknife(EfronandTibshirani,1993;DavidsonandHinkley,1997;Hall,
1992).
Thinking about empirical risk minimization (Section 8.2) as “probabil-
ity free” is incorrect. There is an underlying unknown probability distri-
bution p(x,y) that governs the data generation. However, the approach
of empirical risk minimization is agnostic to that choice of distribution.
This is in contrast to standard statistical approaches that explicitly re-
quire the knowledge of p(x,y). Furthermore, since the distribution is a
jointdistributiononbothexamplesxandlabelsy,thelabelscanbenon-
deterministic. In contrast to standard statistics we do not need to specify
thenoisedistributionforthelabelsy.
8.3 Parameter Estimation
In Section 8.2, we did not explicitly model our problem using probability
distributions. In this section, we will see how to use probability distribu-
tions to model our uncertainty due to the observation process and our
uncertainty in the parameters of our predictors. In Section 8.3.1, we in-
troducethelikelihood,whichisanalogoustotheconceptoflossfunctions
(Section8.2.2)inempiricalriskminimization.Theconceptofpriors(Sec-
tion8.3.2)isanalogoustotheconceptofregularization(Section8.2.3).
8.3.1 Maximum Likelihood Estimation
Theideabehindmaximumlikelihoodestimation(MLE)istodefineafunc- maximumlikelihood
tion of the parameters that enables us to find a model that fits the data estimation
well. The estimation problem is focused on the likelihood function, or likelihood
more precisely its negative logarithm. For data represented by a random
variable x and for a family of probability densities p(x θ) parametrized
|
byθ,thenegativelog-likelihoodisgivenby negative
log-likelihood
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
266 WhenModelsMeetData
(θ) = logp(x θ). (8.14)
x
L − |
The notation (θ) emphasizes the fact that the parameter θ is varying
x
L
andthedataxisfixed.Weveryoftendropthereferencetoxwhenwriting
the negative log-likelihood, as it is really a function of θ, and write it as
(θ) when the random variable representing the uncertainty in the data
L
isclearfromthecontext.
Let us interpret what the probability density p(x θ) is modeling for a
|
fixedvalueofθ.Itisadistributionthatmodelstheuncertaintyofthedata
foragivenparametersetting.Foragivendatasetx,thelikelihoodallows
ustoexpresspreferencesaboutdifferentsettingsoftheparametersθ,and
wecanchoosethesettingthatmore“likely”hasgeneratedthedata.
In a complementary view, if we consider the data to be fixed (because
ithasbeenobserved),andwevarytheparametersθ,whatdoes (θ)tell
L
us?Ittellsushowlikelyaparticularsettingofθ isfortheobservationsx.
Basedonthissecondview,themaximumlikelihoodestimatorgivesusthe
mostlikelyparameterθ forthesetofdata.
We consider the supervised learning setting, where we obtain pairs
(x ,y ),...,(x ,y ) with x RD and labels y R. We are inter-
1 1 N N n n
∈ ∈
ested in constructing a predictor that takes a feature vector x as input
n
and produces a prediction y (or something close to it), i.e., given a vec-
n
torx wewanttheprobabilitydistributionofthelabely .Inotherwords,
n n
we specify the conditional probability distribution of the labels given the
examplesfortheparticularparametersettingθ.
Example 8.4
The first example that is often used is to specify that the conditional
probability of the labels given the examples is a Gaussian distribution. In
other words, we assume that we can explain our observation uncertainty
by independent Gaussian noise (refer to Section 6.5) with zero mean,
(cid:0) (cid:1)
ε 0, σ2 . We further assume that the linear model x⊤θ is used for
n ∼ N n
prediction.ThismeanswespecifyaGaussianlikelihoodforeachexample
labelpair(x ,y ),
n n
p(y x ,θ) = (cid:0) y x⊤θ, σ2(cid:1) . (8.15)
n | n N n | n
An illustration of a Gaussian likelihood for a given parameter θ is shown
in Figure 8.3. We will see in Section 9.2 how to explicitly expand the
precedingexpressionoutintermsoftheGaussiandistribution.
independentand Weassumethatthesetofexamples(x 1,y 1),...,(x N,y N)areindependent
identically andidenticallydistributed(i.i.d.).Theword“independent”(Section6.4.5)
distributed impliesthatthelikelihoodinvolvingthewholedataset( = y ,...,y
1 N
Y { }
and = x ,...,x ) factorizes into a product of the likelihoods of
1 N
X { }
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.3 ParameterEstimation 267
eachindividualexample
N
(cid:89)
p( ,θ) = p(y x ,θ), (8.16)
n n
Y|X |
n=1
wherep(y x ,θ)isaparticulardistribution(whichwasGaussianinEx-
n n
|
ample8.4).Theexpression“identicallydistributed”meansthateachterm
in the product (8.16) is of the same distribution, and all of them share
the same parameters. It is often easier from an optimization viewpoint to
computefunctionsthatcanbedecomposedintosumsofsimplerfunctions.
Hence,inmachinelearningweoftenconsiderthenegativelog-likelihood Recalllog(ab)=
log(a)+log(b)
N
(cid:88)
(θ) = logp( ,θ) = logp(y x ,θ). (8.17)
n n
L − Y|X − |
n=1
Whileitistempingtointerpretthefactthatθ isontherightofthecondi-
tioninginp(y x ,θ)(8.15),andhenceshouldbeinterpretedasobserved
n n
|
andfixed,thisinterpretationisincorrect.Thenegativelog-likelihood (θ)
L
is a function of θ. Therefore, to find a good parameter vector θ that
explains the data (x ,y ),...,(x ,y ) well, minimize the negative log-
1 1 N N
likelihood (θ)withrespecttoθ.
L
Remark. The negative sign in (8.17) is a historical artifact that is due
to the convention that we want to maximize likelihood, but numerical
optimizationliteraturetendstostudyminimizationoffunctions.
♢
Example 8.5
Continuing on our example of Gaussian likelihoods (8.15), the negative
log-likelihoodcanberewrittenas
N N
(θ) = (cid:88) logp(y x ,θ) = (cid:88) log (cid:0) y x⊤θ, σ2(cid:1) (8.18a)
L − n | n − N n | n
n=1 n=1
=
(cid:88)N
log
1 exp(cid:18) (y
n
−x⊤ nθ)2(cid:19)
(8.18b)
− √2πσ2 − 2σ2
n=1
=
(cid:88)N logexp(cid:18) (y
n
−x⊤ nθ)2(cid:19) (cid:88)N
log
1
(8.18c)
− − 2σ2 − √2πσ2
n=1 n=1
N N
1 (cid:88) (cid:88) 1
= (y x⊤θ)2 log . (8.18d)
2σ2 n − n − √2πσ2
n=1 n=1
Asσisgiven,thesecondtermin(8.18d)isconstant,andminimizing (θ)
L
corresponds to solving the least-squares problem (compare with (8.8))
expressedinthefirstterm.
It turns out that for Gaussian likelihoods the resulting optimization
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
268 WhenModelsMeetData
Figure8.5 Forthe
givendata,the 150
maximumlikelihood
estimateofthe 125
parametersresults
intheblack
100
diagonalline.The
orangesquare
75
showsthevalueof
themaximum
50
likelihood
predictionat
25
x=60.
0
0 10 20 30 40 50 60 70 80
x
Figure8.6
Comparingthe 150 MLE
predictionswiththe MAP
maximumlikelihood 125
estimateandthe
MAPestimateat
100
x=60.Theprior
biasestheslopeto
75
belesssteepandthe
intercepttobe
50
closertozero.In
thisexample,the
25
biasthatmovesthe
interceptcloserto
0
zeroactually
0 10 20 30 40 50 60 70 80
increasestheslope.
x
problem corresponding to maximum likelihood estimation has a closed-
form solution. We will see more details on this in Chapter 9. Figure 8.5
shows a regression dataset and the function that is induced by the maxi-
mum-likelihood parameters. Maximum likelihood estimation may suffer
fromoverfitting(Section8.3.3),analogoustounregularizedempiricalrisk
minimization (Section 8.2.3). For other likelihood functions, i.e., if we
modelournoisewithnon-Gaussiandistributions,maximumlikelihoodes-
timation may not have a closed-form analytic solution. In this case, we
resorttonumericaloptimizationmethodsdiscussedinChapter7.
8.3.2 Maximum A Posteriori Estimation
Ifwehavepriorknowledgeaboutthedistributionoftheparametersθ,we
can multiply an additional term to the likelihood. This additional term is
apriorprobabilitydistributiononparametersp(θ).Foragivenprior,after
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
y
y
8.3 ParameterEstimation 269
observing some data x, how should we update the distribution of θ? In
otherwords,howshouldwerepresentthefactthatwehavemorespecific
knowledge of θ after observing data x? Bayes’ theorem, as discussed in
Section 6.3, gives us a principled tool to update our probability distribu-
tionsofrandomvariables.Itallowsustocomputeaposteriordistribution posterior
p(θ x) (the more specific knowledge) on the parameters θ from general
|
prior statements (prior distribution) p(θ) and the function p(x θ) that prior
|
linkstheparametersθ andtheobserveddatax(calledthelikelihood): likelihood
p(x θ)p(θ)
p(θ x) = | . (8.19)
| p(x)
Recall that we are interested in finding the parameter θ that maximizes
the posterior. Since the distribution p(x) does not depend on θ, we can
ignorethevalueofthedenominatorfortheoptimizationandobtain
p(θ x) p(x θ)p(θ). (8.20)
| ∝ |
The preceding proportion relation hides the density of the data p(x),
which may be difficult to estimate. Instead of estimating the minimum
of the negative log-likelihood, we now estimate the minimum of the neg-
ative log-posterior, which is referred to as maximum a posteriori estima- maximuma
tion (MAP estimation). An illustration of the effect of adding a zero-mean posteriori
estimation
GaussianpriorisshowninFigure8.6.
MAPestimation
Example 8.6
InadditiontotheassumptionofGaussianlikelihoodinthepreviousexam-
ple, we assume that the parameter vector is distributed as a multivariate
(cid:0) (cid:1)
Gaussian with zero mean, i.e., p(θ) = 0, Σ , where Σ is the covari-
N
ance matrix (Section 6.5). Note that the conjugate prior of a Gaussian
is also a Gaussian (Section 6.6.1), and therefore we expect the posterior
distribution to also be a Gaussian. We will see the details of maximum a
posterioriestimationinChapter9.
The idea of including prior knowledge about where good parameters
lie is widespread in machine learning. An alternative view, which we saw
in Section 8.2.3, is the idea of regularization, which introduces an addi-
tional term that biases the resulting parameters to be close to the origin.
Maximum a posteriori estimation can be considered to bridge the non-
probabilistic and probabilistic worlds as it explicitly acknowledges the
need for a prior distribution but it still only produces a point estimate
oftheparameters.
Remark. The maximum likelihood estimate θ possesses the following
ML
properties(LehmannandCasella,1998;EfronandHastie,2016):
Asymptotic consistency: The MLE converges to the true value in the
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
270 WhenModelsMeetData
Figure8.7 Model
fitting.Ina M
θ
parametrizedclass
M θofmodels,we
M
optimizethemodel θ∗
parametersθto M∗
M
minimizethe θ0
distancetothetrue
(unknown)model
M∗.
limit of infinitely many observations, plus a random error that is ap-
proximatelynormal.
The size of the samples necessary to achieve these properties can be
quitelarge.
The error’s variance decays in 1/N, where N is the number of data
points.
Especially, in the “small” data regime, maximum likelihood estimation
canleadtooverfitting.
♢
Theprincipleofmaximumlikelihoodestimation(andmaximumapos-
teriori estimation) uses probabilistic modeling to reason about the uncer-
taintyinthedataandmodelparameters.However,wehavenotyettaken
probabilisticmodelingtoitsfullextent.Inthissection,theresultingtrain-
ingprocedurestillproducesapointestimateofthepredictor,i.e.,training
returns one single set of parameter values that represent the best predic-
tor.InSection8.4,wewilltaketheviewthattheparametervaluesshould
also be treated as random variables, and instead of estimating “best” val-
ues of that distribution, we will use the full parameter distribution when
makingpredictions.
8.3.3 Model Fitting
Consider the setting where we are given a dataset, and we are interested
in fitting a parametrized model to the data. When we talk about “fit-
ting”, we typically mean optimizing/learning model parameters so that
they minimize some loss function, e.g., the negative log-likelihood. With
maximum likelihood (Section 8.3.1) and maximum a posteriori estima-
tion(Section8.3.2),wealreadydiscussedtwocommonlyusedalgorithms
formodelfitting.
TheparametrizationofthemodeldefinesamodelclassM withwhich
θ
wecanoperate.Forexample,inalinearregressionsetting,wemaydefine
the relationship between inputs x and (noise-free) observations y to be
y = ax+b,whereθ := a,b arethemodelparameters.Inthiscase,the
{ }
model parameters θ describe the family of affine functions, i.e., straight
lines with slope a, which are offset from 0 by b. Assume the data comes
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.3 ParameterEstimation 271
Figure8.8 Fitting
4 Trainingdata 4 Trainingdata 4 Trainingdata
MLE MLE MLE (bymaximum
2 2 2 likelihood)of
0 0 0 differentmodel
−2 −2 −2 classestoa
4 4 4 regressiondataset.
− − −
4 2 0 2 4 4 2 0 2 4 4 2 0 2 4
− − x − − x − − x
(a)Overfitting (b)Underfitting. (c)Fittingwell.
from a model M∗, which is unknown to us. For a given training dataset,
we optimize θ so that M is as close as possible to M∗, where the “close-
θ
ness” is defined by the objective function we optimize (e.g., squared loss
onthetrainingdata).Figure8.7illustratesasettingwherewehaveasmall
model class (indicated by the circle M ), and the data generation model
θ
M∗ lies outside the set of considered models. We begin our parameter
search at M . After the optimization, i.e., when we obtain the best pos-
θ0
sible parameters θ∗, we distinguish three different cases: (i) overfitting,
(ii) underfitting, and (iii) fitting well. We will give a high-level intuition
ofwhatthesethreeconceptsmean.
Roughly speaking, overfitting refers to the situation where the para- overfitting
metrized model class is too rich to model the dataset generated by M∗,
i.e.,M couldmodelmuchmorecomplicateddatasets.Forinstance,ifthe
θ
dataset was generated by a linear function, and we define M to be the
θ
class of seventh-order polynomials, we could model not only linear func-
tions, but also polynomials of degree two, three, etc. Models that over-
fit typically have a large number of parameters. An observation we often Onewaytodetect
makeisthattheoverlyflexiblemodelclassM usesallitsmodelingpower overfittingin
θ
practiceisto
to reduce the training error. If the training data is noisy, it will therefore
observethatthe
findsomeuseful signal inthenoiseitself.Thiswill causeenormousprob-
modelhaslow
lemswhenwepredictawayfromthetrainingdata.Figure8.8(a)givesan trainingriskbut
example of overfitting in the context of regression where the model pa- hightestriskduring
rametersarelearnedbymeansofmaximumlikelihood(seeSection8.3.1). crossvalidation
(Section8.2.4).
WewilldiscussoverfittinginregressionmoreinSection9.2.2.
When we run into underfitting, we encounter the opposite problem underfitting
where the model class M is not rich enough. For example, if our dataset
θ
was generated by a sinusoidal function, but θ only parametrizes straight
lines, the best optimization procedure will not get us close to the true
model.However,westilloptimizetheparametersandfindthebeststraight
line that models the dataset. Figure 8.8(b) shows an example of a model
thatunderfitsbecauseitisinsufficientlyflexible.Modelsthatunderfittyp-
icallyhavefewparameters.
The third case is when the parametrized model class is about right.
Then,ourmodelfitswell,i.e.,itneitheroverfitsnorunderfits.Thismeans
our model class is just rich enough to describe the dataset we are given.
Figure8.8(c)showsamodelthatfitsthegivendatasetfairlywell.Ideally,
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
y y y
272 WhenModelsMeetData
this is the model class we would want to work with since it has good
generalizationproperties.
In practice, we often define very rich model classes M with many pa-
θ
rameters,suchasdeepneuralnetworks.Tomitigatetheproblemofover-
fitting,wecanuseregularization(Section8.2.3)orpriors(Section8.3.2).
WewilldiscusshowtochoosethemodelclassinSection8.6.
8.3.4 Further Reading
When considering probabilistic models, the principle of maximum likeli-
hoodestimationgeneralizestheideaofleast-squaresregressionforlinear
models, which we will discuss in detail in Chapter 9. When restricting
the predictor to have linear form with an additional nonlinear function φ
appliedtotheoutput,i.e.,
p(y x ,θ) = φ(θ⊤x ), (8.21)
n n n
|
we can consider other models for other prediction tasks, such as binary
classification or modeling count data (McCullagh and Nelder, 1989). An
alternative view of this is to consider likelihoods that are from the ex-
ponential family (Section 6.6). The class of models, which have linear
dependence between parameters and data, and have potentially nonlin-
linkfunction ear transformation φ (called a link function), is referred to as generalized
generalizedlinear linearmodels(Agresti,2002,chapter4).
model Maximum likelihood estimation has a rich history, and was originally
proposedbySirRonaldFisherinthe1930s.Wewillexpandupontheidea
of a probabilistic model in Section 8.4. One debate among researchers
whouseprobabilisticmodels,isthediscussionbetweenBayesianandfre-
quentist statistics. As mentioned in Section 6.1.1, it boils down to the
definition of probability. Recall from Section 6.1 that one can consider
probabilitytobeageneralization(byallowinguncertainty)oflogicalrea-
soning (Cheeseman, 1985; Jaynes, 2003). The method of maximum like-
lihood estimation is frequentist in nature, and the interested reader is
pointed to Efron and Hastie (2016) for a balanced view of both Bayesian
andfrequentiststatistics.
There are some probabilistic models where maximum likelihood esti-
mationmaynotbepossible.Thereaderisreferredtomoreadvancedsta-
tisticaltextbooks,e.g.,CasellaandBerger(2002),forapproaches,suchas
methodofmoments,M-estimation,andestimatingequations.
8.4 Probabilistic Modeling and Inference
Inmachinelearning,wearefrequentlyconcernedwiththeinterpretation
and analysis of data, e.g., for prediction of future events and decision
making. To make this task more tractable, we often build models that
generativeprocess describethegenerativeprocessthatgeneratestheobserveddata.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.4 ProbabilisticModelingandInference 273
For example, we can describe the outcome of a coin-flip experiment
(“heads” or “tails”) in two steps. First, we define a parameter µ, which
describestheprobabilityof“heads”astheparameterofaBernoullidistri-
bution (Chapter 6); second, we can sample an outcome x head,tail
∈ { }
from the Bernoulli distribution p(x µ) = Ber(µ). The parameter µ gives
|
rise to a specific dataset and depends on the coin used. Since µ is un-
X
known in advance and can never be observed directly, we need mecha-
nisms to learn something about µ given observed outcomes of coin-flip
experiments.Inthefollowing,wewilldiscusshowprobabilisticmodeling
canbeusedforthispurpose.
8.4.1 Probabilistic Models
Aprobabilistic
Probabilistic models represent the uncertain aspects of an experiment as modelisspecified
probability distributions. The benefit of using probabilistic models is that bythejoint
distributionofall
they offer a unified and consistent set of tools from probability theory
randomvariables.
(Chapter6)formodeling,inference,prediction,andmodelselection.
In probabilistic modeling, the joint distribution p(x,θ) of the observed
variables x and the hidden parameters θ is of central importance: It en-
capsulatesinformationfromthefollowing:
Thepriorandthelikelihood(productrule,Section6.3).
The marginal likelihood p(x), which will play an important role in
modelselection(Section8.6),canbecomputedbytakingthejointdis-
tributionandintegratingouttheparameters(sumrule,Section6.3).
Theposterior,whichcanbeobtainedbydividingthejointbythemarginal
likelihood.
Only the joint distribution has this property. Therefore, a probabilistic
modelisspecifiedbythejointdistributionofallitsrandomvariables.
8.4.2 Bayesian Inference
Parameter
Akeytaskinmachinelearningistotakeamodelandthedatatouncover estimationcanbe
the values of the model’s hidden variables θ given the observed variables phrasedasan
optimization
x. In Section 8.3.1, we already discussed two ways for estimating model
problem.
parameters θ using maximum likelihood or maximum a posteriori esti-
mation. In both cases, we obtain a single-best value for θ so that the key
algorithmic problem of parameter estimation is solving an optimization
problem.Oncethesepointestimatesθ∗ areknown,weusethemtomake
predictions. More specifically, the predictive distribution will bep(x θ∗),
whereweuseθ∗ inthelikelihoodfunction. |
AsdiscussedinSection6.3,focusingsolelyonsomestatisticofthepos-
terior distribution (such as the parameter θ∗ that maximizes the poste-
rior) leads to loss of information, which can be critical in a system that
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
274 WhenModelsMeetData
uses the prediction p(x θ∗) to make decisions. These decision-making
|
Bayesianinference systems typically have different objective functions than the likelihood, a
isaboutlearningthe squared-error loss or a mis-classification error. Therefore, having the full
distributionof
posterior distribution around can be extremely useful and leads to more
randomvariables.
robust decisions. Bayesian inference is about finding this posterior distri-
Bayesianinference
bution(Gelmanetal.,2004).Foradataset ,aparameterpriorp(θ),and
X
alikelihoodfunction,theposterior
p( θ)p(θ) (cid:90)
p(θ ) = X | , p( ) = p( θ)p(θ)dθ, (8.22)
|X p( ) X X |
X
Bayesianinference is obtained by applying Bayes’ theorem. The key idea is to exploit Bayes’
invertsthe theoremtoinverttherelationshipbetweentheparametersθ andthedata
relationship (givenbythelikelihood)toobtaintheposteriordistributionp(θ ).
betweenparameters X |X
The implication of having a posterior distribution on the parameters is
andthedata.
that it can be used to propagate uncertainty from the parameters to the
data. More specifically, with a distribution p(θ) on the parameters our
predictionswillbe
(cid:90)
p(x) = p(x θ)p(θ)dθ = E [p(x θ)], (8.23)
θ
| |
and they no longer depend on the model parameters θ, which have been
marginalized/integrated out. Equation (8.23) reveals that the prediction
isanaverageoverallplausibleparametervaluesθ,wheretheplausibility
isencapsulatedbytheparameterdistributionp(θ).
HavingdiscussedparameterestimationinSection8.3andBayesianin-
ferencehere,letuscomparethesetwoapproachestolearning.Parameter
estimationviamaximumlikelihoodorMAPestimationyieldsaconsistent
point estimate θ∗ of the parameters, and the key computational problem
tobesolvedisoptimization.Incontrast,Bayesianinferenceyieldsa(pos-
terior) distribution, and the key computational problem to be solved is
integration.Predictionswithpointestimatesarestraightforward,whereas
predictionsintheBayesianframeworkrequiresolvinganotherintegration
problem; see (8.23). However, Bayesian inference gives us a principled
way to incorporate prior knowledge, account for side information, and
incorporate structural knowledge, all of which is not easily done in the
contextofparameterestimation.Moreover,thepropagationofparameter
uncertainty to the prediction can be valuable in decision-making systems
for risk assessment and exploration in the context of data-efficient learn-
ing(Deisenrothetal.,2015;KamtheandDeisenroth,2018).
WhileBayesianinferenceisamathematicallyprincipledframeworkfor
learning about parameters and making predictions, there are some prac-
tical challenges that come with it because of the integration problems we
needtosolve;see(8.22)and(8.23).Morespecifically,ifwedonotchoose
aconjugatepriorontheparameters(Section6.6.1),theintegralsin(8.22)
and(8.23)arenotanalyticallytractable,andwecannotcomputethepos-
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.4 ProbabilisticModelingandInference 275
terior, the predictions, or the marginal likelihood in closed form. In these
cases, we need to resort to approximations. Here, we can use stochas-
tic approximations, such as Markov chain Monte Carlo (MCMC) (Gilks
et al., 1996), or deterministic approximations, such as the Laplace ap-
proximation (Bishop, 2006; Barber, 2012; Murphy, 2012), variational in-
ference (Jordan et al., 1999; Blei et al., 2017), or expectation propaga-
tion(Minka,2001a).
Despite these challenges, Bayesian inference has been successfully ap-
pliedtoavarietyofproblems,includinglarge-scaletopicmodeling(Hoff-
man et al., 2013), click-through-rate prediction (Graepel et al., 2010),
data-efficientreinforcementlearningincontrolsystems(Deisenrothetal.,
2015),onlinerankingsystems(Herbrichetal.,2007),andlarge-scalerec-
ommender systems. There are generic tools, such as Bayesian optimiza-
tion (Brochu et al., 2009; Snoek et al., 2012; Shahriari et al., 2016), that
are very useful ingredients for an efficient search of meta parameters of
modelsoralgorithms.
Remark. In the machine learning literature, there can be a somewhat ar-
bitraryseparationbetween(random)“variables”and“parameters”.While
parameters are estimated (e.g., via maximum likelihood), variables are
usually marginalized out. In this book, we are not so strict with this sep-
aration because, in principle, we can place a prior on any parameter and
integrateitout,whichwouldthenturntheparameterintoarandomvari-
ableaccordingtotheaforementionedseparation.
♢
8.4.3 Latent-Variable Models
In practice, it is sometimes useful to have additional latent variables z latentvariable
(besides the model parameters θ) as part of the model (Moustaki et al.,
2015). These latent variables are different from the model parameters
θ as they do not parametrize the model explicitly. Latent variables may
describe the data-generating process, thereby contributing to the inter-
pretability of the model. They also often simplify the structure of the
model and allow us to define simpler and richer model structures. Sim-
plification of the model structure often goes hand in hand with a smaller
number of model parameters (Paquet, 2008; Murphy, 2012). Learning in
latent-variablemodels(atleastviamaximumlikelihood)canbedoneina
principledwayusingtheexpectationmaximization(EM)algorithm(Demp-
ster et al., 1977; Bishop, 2006). Examples, where such latent variables
are helpful, are principal component analysis for dimensionality reduc-
tion(Chapter10),Gaussianmixturemodelsfordensityestimation(Chap-
ter 11), hidden Markov models (Maybeck, 1979) or dynamical systems
(Ghahramani and Roweis, 1999; Ljung, 1999) for time-series modeling,
and meta learning and task generalization (Hausman et al., 2018; Sæ-
mundssonetal.,2018).Althoughtheintroductionoftheselatentvariables
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
276 WhenModelsMeetData
maymakethemodelstructureandthegenerativeprocesseasier,learning
inlatent-variablemodelsisgenerallyhard,aswewillseeinChapter11.
Since latent-variable models also allow us to define the process that
generatesdatafromparameters,letushavealookatthisgenerativepro-
cess. Denoting data by x, the model parameters by θ and the latent vari-
ablesbyz,weobtaintheconditionaldistribution
p(x z,θ) (8.24)
|
thatallowsustogeneratedataforanymodelparametersandlatentvari-
ables.Giventhatz arelatentvariables,weplaceapriorp(z)onthem.
As the models we discussed previously, models with latent variables
can be used for parameter learning and inference within the frameworks
we discussed in Sections 8.3 and 8.4.2. To facilitate learning (e.g., by
means of maximum likelihood estimation or Bayesian inference), we fol-
lowatwo-stepprocedure.First,wecomputethelikelihoodp(x θ)ofthe
|
model,whichdoesnotdependonthelatentvariables.Second,weusethis
likelihood for parameter estimation or Bayesian inference, where we use
exactlythesameexpressionsasinSections8.3and8.4.2,respectively.
Sincethelikelihoodfunctionp(x θ)isthepredictivedistributionofthe
|
data given the model parameters, we need to marginalize out the latent
variablessothat
(cid:90)
p(x θ) = p(x z,θ)p(z)dz, (8.25)
| |
where p(x z,θ) is given in (8.24) and p(z) is the prior on the latent
|
Thelikelihoodisa variables.Notethatthelikelihoodmustnotdependonthelatentvariables
functionofthedata z,butitisonlyafunctionofthedataxandthemodelparametersθ.
andthemodel
The likelihood in (8.25) directly allows for parameter estimation via
parameters,butis
maximum likelihood. MAP estimation is also straightforward with an ad-
independentofthe
latentvariables. ditional prior on the model parameters θ as discussed in Section 8.3.2.
Moreover, with the likelihood (8.25) Bayesian inference (Section 8.4.2)
in a latent-variable model works in the usual way: We place a prior p(θ)
on the model parameters and use Bayes’ theorem to obtain a posterior
distribution
p( θ)p(θ)
p(θ ) = X | (8.26)
|X p( )
X
overthemodelparametersgivenadataset .Theposteriorin(8.26)can
X
beusedforpredictionswithinaBayesianinferenceframework;see(8.23).
One challenge we have in this latent-variable model is that the like-
lihood p( θ) requires the marginalization of the latent variables ac-
X |
cording to (8.25). Except when we choose a conjugate prior p(z) for
p(x z,θ), the marginalization in (8.25) is not analytically tractable, and
|
we need to resort to approximations (Bishop, 2006; Paquet, 2008; Mur-
phy,2012;Moustakietal.,2015).
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.4 ProbabilisticModelingandInference 277
Similar to the parameter posterior (8.26) we can compute a posterior
onthelatentvariablesaccordingto
p( z)p(z) (cid:90)
p(z ) = X | , p( z) = p( z,θ)p(θ)dθ, (8.27)
|X p( ) X | X |
X
where p(z) is the prior on the latent variables and p( z) requires us to
X |
integrateoutthemodelparametersθ.
Giventhedifficultyofsolvingintegralsanalytically,itisclearthatmar-
ginalizing out both the latent variables and the model parameters at the
same time is not possible in general (Bishop, 2006; Murphy, 2012). A
quantitythatiseasiertocomputeistheposteriordistributiononthelatent
variables,butconditionedonthemodelparameters,i.e.,
p( z,θ)p(z)
p(z ,θ) = X | , (8.28)
|X p( θ)
X |
where p(z) is the prior on the latent variables and p( z,θ) is given
X |
in(8.24).
In Chapters 10 and 11, we derive the likelihood functions for PCA and
Gaussian mixture models, respectively. Moreover, we compute the poste-
riordistributions(8.28)onthelatentvariablesforbothPCAandGaussian
mixturemodels.
Remark. In the following chapters, we may not be drawing such a clear
distinction between latent variables z and uncertain model parameters θ
and call the model parameters “latent” or “hidden” as well because they
areunobserved.InChapters10and11,whereweusethelatentvariables
z, we will pay attention to the difference as we will have two different
typesofhiddenvariables:modelparametersθ andlatentvariablesz.
♢
Wecanexploitthefactthatalltheelementsofaprobabilisticmodelare
random variables to define a unified language for representing them. In
Section8.5,wewillseeaconcisegraphicallanguageforrepresentingthe
structure of probabilistic models. We will use this graphical language to
describetheprobabilisticmodelsinthesubsequentchapters.
8.4.4 Further Reading
Probabilistic models in machine learning (Bishop, 2006; Barber, 2012;
Murphy,2012)provideawayforuserstocaptureuncertaintyaboutdata
andpredictivemodelsinaprincipledfashion.Ghahramani(2015)presents
ashortreviewofprobabilisticmodelsinmachinelearning.Givenaproba-
bilisticmodel,wemaybeluckyenoughtobeabletocomputeparameters
of interest analytically. However, in general, analytic solutions are rare,
and computational methods such as sampling (Gilks et al., 1996; Brooks
et al., 2011) and variational inference (Jordan et al., 1999; Blei et al.,
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
278 WhenModelsMeetData
2017)areused.Moustakietal.(2015)andPaquet(2008)provideagood
overviewofBayesianinferenceinlatent-variablemodels.
In recent years, several programming languages have been proposed
that aim to treat the variables defined in software as random variables
corresponding to probability distributions. The objective is to be able to
writecomplexfunctionsofprobabilitydistributions,whileunderthehood
the compiler automatically takes care of the rules of Bayesian inference.
probabilistic Thisrapidlychangingfieldiscalledprobabilisticprogramming.
programming
8.5 Directed Graphical Models
In this section, we introduce a graphical language for specifying a prob-
directedgraphical abilistic model, called the directed graphical model. It provides a compact
model andsuccinctwaytospecifyprobabilisticmodels,andallowsthereaderto
visuallyparsedependenciesbetweenrandomvariables.Agraphicalmodel
visually captures the way in which the joint distribution over all random
variablescanbedecomposedintoaproductoffactorsdependingonlyon
a subset of these variables. In Section 8.4, we identified the joint distri-
bution of a probabilistic model as the key quantity of interest because it
comprises information about the prior, the likelihood, and the posterior.
Directedgraphical However, the joint distribution by itself can be quite complicated, and
modelsarealso it does not tell us anything about structural properties of the probabilis-
knownasBayesian tic model. For example, the joint distribution p(a,b,c) does not tell us
networks.
anything about independence relations. This is the point where graphical
modelscomeintoplay.Thissectionreliesontheconceptsofindependence
andconditionalindependence,asdescribedinSection6.4.5.
graphicalmodel In a graphical model, nodes are random variables. In Figure 8.9(a), the
nodesrepresenttherandomvariablesa,b,c.Edgesrepresentprobabilistic
relationsbetweenvariables,e.g.,conditionalprobabilities.
Remark. Noteverydistributioncanberepresentedinaparticularchoiceof
graphicalmodel.AdiscussionofthiscanbefoundinBishop(2006).
♢
Probabilisticgraphicalmodelshavesomeconvenientproperties:
Theyareasimplewaytovisualizethestructureofaprobabilisticmodel.
Theycanbeusedtodesignormotivatenewkindsofstatisticalmodels.
Inspectionofthegraphalonegivesusinsightintoproperties,e.g.,con-
ditionalindependence.
Complex computations for inference and learning in statistical models
canbeexpressedintermsofgraphicalmanipulations.
8.5.1 Graph Semantics
directedgraphical Directedgraphicalmodels/Bayesiannetworksareamethodforrepresenting
model/Bayesian conditional dependencies in a probabilistic model. They provide a visual
network
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.5 DirectedGraphicalModels 279
descriptionoftheconditionalprobabilities,hence,providingasimplelan-
guage for describing complex interdependence. The modular description Withadditional
alsoentailscomputationalsimplification.Directedlinks(arrows)between assumptions,the
arrowscanbeused
two nodes (random variables) indicate conditional probabilities. For ex-
toindicatecausal
ample, the arrow between a and b in Figure 8.9(a) gives the conditional
relationships(Pearl,
probabilityp(b a)ofbgivena.
2009).
|
Figure8.9
a b x 1 x 2 x 5 Examplesof
directedgraphical
models.
c x x
3 4
(a)Fullyconnected. (b)Notfullyconnected.
Directed graphical models can be derived from joint distributions if we
knowsomethingabouttheirfactorization.
Example 8.7
Considerthejointdistribution
p(a,b,c) = p(c a,b)p(b a)p(a) (8.29)
| |
ofthreerandomvariablesa,b,c.Thefactorizationofthejointdistribution
in (8.29) tells us something about the relationship between the random
variables:
cdependsdirectlyonaandb.
bdependsdirectlyona.
adependsneitheronbnoronc.
For the factorization in (8.29), we obtain the directed graphical model in
Figure8.9(a).
Ingeneral,wecanconstructthecorrespondingdirectedgraphicalmodel
fromafactorizedjointdistributionasfollows:
1. Createanodeforallrandomvariables.
2. For each conditional distribution, we add a directed link (arrow) to
the graph from the nodes corresponding to the variables on which the
distributionisconditioned.
Thegraphlayout
The graph layout depends on the choice of factorization of the joint dis- dependsonthe
factorizationofthe
tribution.
jointdistribution.
We discussed how to get from a known factorization of the joint dis-
tribution to the corresponding directed graphical model. Now, we will do
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
280 WhenModelsMeetData
exactly the opposite and describe how to extract the joint distribution of
asetofrandomvariablesfromagivengraphicalmodel.
Example 8.8
Looking at the graphical model in Figure 8.9(b), we exploit two proper-
ties:
The joint distribution p(x ,...,x ) we seek is the product of a set of
1 5
conditionals,oneforeachnodeinthegraph.Inthisparticularexample,
wewillneedfiveconditionals.
Each conditional depends only on the parents of the corresponding
nodeinthegraph.Forexample,x willbeconditionedonx .
4 2
These two properties yield the desired factorization of the joint distribu-
tion
p(x ,x ,x ,x ,x ) = p(x )p(x )p(x x )p(x x ,x )p(x x ). (8.30)
1 2 3 4 5 1 5 2 5 3 1 2 4 2
| | |
Ingeneral,thejointdistributionp(x) = p(x ,...,x )isgivenas
1 K
K
(cid:89)
p(x) = p(x Pa ), (8.31)
k k
|
k=1
where Pa means “the parent nodes of x ”. Parent nodes of x are nodes
k k k
thathavearrowspointingtox .
k
We conclude this subsection with a concrete example of the coin-flip
experiment. Consider a Bernoulli experiment (Example 6.8) where the
probabilitythattheoutcomexofthisexperimentis“heads”is
p(x µ) = Ber(µ). (8.32)
|
WenowrepeatthisexperimentN timesandobserveoutcomesx ,...,x
1 N
sothatweobtainthejointdistribution
N
(cid:89)
p(x ,...,x µ) = p(x µ). (8.33)
1 N n
| |
n=1
The expression on the right-hand side is a product of Bernoulli distribu-
tions on each individual outcome because the experiments are indepen-
dent. Recall from Section 6.4.5 that statistical independence means that
thedistributionfactorizes.Towritethegraphicalmodeldownforthisset-
ting, we make the distinction between unobserved/latent variables and
observedvariables.Graphically,observedvariablesaredenotedbyshaded
nodes so that we obtain the graphical model in Figure 8.10(a). We see
that the single parameter µ is the same for all x , n = 1,...,N as the
n
outcomes x are identically distributed. A more compact, but equivalent,
n
graphical model for this setting is given in Figure 8.10(b), where we use
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.5 DirectedGraphicalModels 281
Figure8.10
µ α µ β
Graphicalmodels
µ forarepeated
Bernoulli
x x experiment.
n n
x x
1 N n=1,...,N n=1,...,N
(a)Versionwithxnexplicit. (b) Version with (c) Hyperparameters α
platenotation. andβonthelatentµ.
theplatenotation.Theplate(box)repeatseverythinginside(inthiscase, plate
theobservationsx )N times.Therefore,bothgraphicalmodelsareequiv-
n
alent, but the plate notation is more compact. Graphical models immedi-
ately allow us to place a hyperprior on µ. A hyperprior is a second layer hyperprior
of prior distributions on the parameters of the first layer of priors. Fig-
ure 8.10(c) places a Beta(α,β) prior on the latent variable µ. If we treat
α and β as deterministic parameters, i.e., not random variables, we omit
thecirclearoundit.
8.5.2 Conditional Independence and d-Separation
Directedgraphicalmodelsallowustofindconditionalindependence(Sec-
tion6.4.5)relationshippropertiesofthejointdistributiononlybylooking
atthegraph.Aconceptcalledd-separation(Pearl,1988)iskeytothis. d-separation
Considerageneraldirectedgraphinwhich , , arearbitrarynonin-
A B C
tersecting sets of nodes (whose union may be smaller than the complete
setofnodesinthegraph).Wewishtoascertainwhetheraparticularcon-
ditional independence statement, “ is conditionally independent of
A B
given ”,denotedby
C
, (8.34)
A ⊥⊥ B|C
is implied by a given directed acyclic graph. To do so, we consider all
possible trails (paths that ignore the direction of the arrows) from any
node in to any nodes in . Any such path is said to be blocked if it
A B
includesanynodesuchthateitherofthefollowingaretrue:
The arrows on the path meet either head to tail or tail to tail at the
node,andthenodeisintheset .
C
The arrows meet head to head at the node, and neither the node nor
anyofitsdescendantsisintheset .
C
If all paths are blocked, then is said to be d-separated from by ,
A B C
andthejointdistributionoverallofthevariablesinthegraphwillsatisfy
.
A ⊥⊥ B|C
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
282 WhenModelsMeetData
Figure8.12 Three
typesofgraphical
a b a b a b
models:(a)Directed
graphicalmodels
(Bayesian
networks); c c c
(b)Undirected
graphicalmodels (a)Directedgraphicalmodel (b) Undirected graphical (c)Factorgraph
(Markovrandom model
fields);(c)Factor
graphs.
Example 8.9 (Conditional Independence)
Figure8.11 a b c
D-separation
example.
d
e
ConsiderthegraphicalmodelinFigure8.11.Visualinspectiongivesus
b d a,c (8.35)
⊥⊥ |
a c b (8.36)
⊥⊥ |
b d c (8.37)
⊥̸⊥ |
a c b,e (8.38)
⊥̸⊥ |
Directed graphical models allow a compact representation of proba-
bilistic models, and we will see examples of directed graphical models in
Chapters9,10,and11.Therepresentation,alongwiththeconceptofcon-
ditional independence, allows us to factorize the respective probabilistic
modelsintoexpressionsthatareeasiertooptimize.
The graphical representation of the probabilistic model allows us to
visually see the impact of design choices we have made on the structure
of the model. We often need to make high-level assumptions about the
structure of the model. These modeling assumptions (hyperparameters)
affect the prediction performance, but cannot be selected directly using
the approaches we have seen so far. We will discuss different ways to
choosethestructureinSection8.6.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.6 ModelSelection 283
8.5.3 Further Reading
An introduction to probabilistic graphical models can be found in Bishop
(2006, chapter 8), and an extensive description of the different applica-
tionsandcorrespondingalgorithmicimplicationscanbefoundinthebook
byKollerandFriedman(2009).Therearethreemaintypesofprobabilistic
graphicalmodels:
directedgraphical
Directedgraphicalmodels(Bayesiannetworks);seeFigure8.12(a) model
Undirectedgraphicalmodels(Markovrandomfields);seeFigure8.12(b) Bayesiannetwork
Factorgraphs;seeFigure8.12(c) undirectedgraphical
model
Graphical models allow for graph-based algorithms for inference and Markovrandom
learning, e.g., via local message passing. Applications range from rank- field
factorgraph
ing in online games (Herbrich et al., 2007) and computer vision (e.g.,
image segmentation, semantic labeling, image denoising, image restora-
tion (Kittler and Fo¨glein, 1984; Sucar and Gillies, 1994; Shotton et al.,
2006;Szeliskietal.,2008))tocodingtheory(McElieceetal.,1998),solv-
ing linear equation systems (Shental et al., 2008), and iterative Bayesian
stateestimationinsignalprocessing(Bicksonetal.,2007;Deisenrothand
Mohamed,2012).
One topic that is particularly important in real applications that we do
not discuss in this book is the idea of structured prediction (Bakir et al.,
2007; Nowozin et al., 2014), which allows machine learning models to
tackle predictions that are structured, for example sequences, trees, and
graphs. The popularity of neural network models has allowed more flex-
ible probabilistic models to be used, resulting in many useful applica-
tionsofstructuredmodels(Goodfellowetal.,2016,chapter16).Inrecent
years, there has been a renewed interest in graphical models due to their
applications to causal inference (Pearl, 2009; Imbens and Rubin, 2015;
Petersetal.,2017;Rosenbaum,2017).
8.6 Model Selection
Inmachinelearning,weoftenneedtomakehigh-levelmodelingdecisions
that critically influence the performance of the model. The choices we
make (e.g., the functional form of the likelihood) influence the number
and type of free parameters in the model and thereby also the flexibility
and expressivity of the model. More complex models are more flexible in Apolynomial
the sense that they can be used to describe more datasets. For instance, a y=a0+a1x+a2x2
polynomialofdegree1(aliney = a +a x)canonlybeusedtodescribe canalsodescribe
0 1
linearfunctionsby
linear relations between inputs x and observations y. A polynomial of
settinga2=0,i.e.,
degree2canadditionallydescribequadraticrelationshipsbetweeninputs itisstrictlymore
andobservations. expressivethana
Onewouldnowthinkthatveryflexiblemodelsaregenerallypreferable first-order
polynomial.
to simple models because they are more expressive. A general problem
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
284 WhenModelsMeetData
Figure8.13 Nested
Alllabeleddata
cross-validation.We
performtwolevels
ofK-fold
Alltrainingdata Testdata
cross-validation.
Totrainmodel Validation
is that at training time we can only use the training set to evaluate the
performance of the model and learn its parameters. However, the per-
formance on the training set is not really what we are interested in. In
Section 8.3, we have seen that maximum likelihood estimation can lead
to overfitting, especially when the training dataset is small. Ideally, our
model(also)workswellonthetestset(whichisnotavailableattraining
time). Therefore, we need some mechanisms for assessing how a model
generalizes to unseen test data. Model selection is concerned with exactly
thisproblem.
8.6.1 Nested Cross-Validation
Wehavealreadyseenanapproach(cross-validationinSection8.2.4)that
can be used for model selection. Recall that cross-validation provides an
estimateofthegeneralizationerrorbyrepeatedlysplittingthedatasetinto
training and validation sets. We can apply this idea one more time, i.e.,
for each split, we can perform another round of cross-validation. This is
nested sometimesreferredtoasnestedcross-validation;seeFigure8.13.Theinner
cross-validation level is used to estimate the performance of a particular choice of model
or hyperparameter on a internal validation set. The outer level is used to
estimate generalization performance for the best choice of model chosen
bytheinnerloop.Wecantestdifferentmodelandhyperparameterchoices
in the inner loop. To distinguish the two levels, the set used to estimate
testset thegeneralizationperformanceisoftencalledthetestsetandthesetused
validationset for choosing the best model is called the validation set. The inner loop
estimatestheexpectedvalueofthegeneralizationerrorforagivenmodel
(8.39),byapproximatingitusingtheempiricalerroronthevalidationset,
Thestandarderror i.e.,
isdefinedas √σ ,
whereKistheK
E [R( M)]
1 (cid:88)K
R( (k) M), (8.39)
numberof V V| ≈ K V |
experimentsandσ k=1
isthestandard whereR( M)istheempiricalrisk(e.g.,rootmeansquareerror)onthe
deviationoftherisk V|
validationset formodelM.Werepeatthisprocedureforallmodelsand
ofeachexperiment. V
choose the model that performs best. Note that cross-validation not only
gives us the expected generalization error, but we can also obtain high-
order statistics, e.g., the standard error, an estimate of how uncertain the
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.6 ModelSelection 285
Figure8.14
Evidence
Bayesianinference
embodiesOccam’s
razor.The
horizontalaxis
describesthespace
p( M )
D| 1 ofallpossible
datasetsD.The
evidence(vertical
axis)evaluateshow
wellamodel
p( D|M 2) predictsavailable
data.Since
p(D|Mi)needsto
integrateto1,we
shouldchoosethe
D modelwiththe
C greatestevidence.
Adapted
mean estimate is. Once the model is chosen, we can evaluate the final fromMacKay
performanceonthetestset. (2003).
8.6.2 Bayesian Model Selection
Therearemanyapproachestomodelselection,someofwhicharecovered
in this section. Generally, they all attempt to trade off model complexity
and data fit. We assume that simpler models are less prone to overfitting
thancomplexmodels,andhencetheobjectiveofmodelselectionistofind
thesimplestmodelthatexplainsthedatareasonablywell.Thisconceptis
alsoknownasOccam’srazor. Occam’srazor
Remark. If we treat model selection as a hypothesis testing problem, we
arelookingforthesimplesthypothesisthatisconsistentwiththedata(Mur-
phy,2012).
♢
Onemayconsiderplacingaprioronmodelsthatfavorssimplermodels.
However, it is not necessary to do this: An “automatic Occam’s Razor” is
quantitativelyembodiedintheapplicationofBayesianprobability(Smith
and Spiegelhalter, 1980; Jefferys and Berger, 1992; MacKay, 1992). Fig-
ure 8.14, adapted from MacKay (2003), gives us the basic intuition why
complex and very expressive models may turn out to be a less probable
choice for modeling a given dataset . Let us think of the horizontal axis Thesepredictions
D
representing the space of all possible datasets . If we are interested in arequantifiedbya
theposteriorprobabilityp(M )ofmodelMD giventhedata ,wecan normalized
i i
|D D probability
employ Bayes’ theorem. Assuming a uniform prior p(M) over all mod-
distributiononD,
els, Bayes’ theorem rewards models in proportion to how much they pre- i.e.,itneedsto
dicted the data that occurred. This prediction of the data given model integrate/sumto1.
M i, p( M i), is called the evidence for M i. A simple model M 1 can only evidence
D|
predict a small number of datasets, which is shown by p( M ); a more
1
D|
powerful model M that has, e.g., more free parameters than M , is able
2 1
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
286 WhenModelsMeetData
to predict a greater variety of datasets. This means, however, that M
2
does not predict the datasets in region C as well as M . Suppose that
1
equal prior probabilities have been assigned to the two models. Then, if
the dataset falls into region C, the less powerful model M is the more
1
probablemodel.
Earlierinthischapter,wearguedthatmodelsneedtobeabletoexplain
thedata,i.e.,thereshouldbeawaytogeneratedatafromagivenmodel.
Furthermore, if the model has been appropriately learned from the data,
thenweexpectthatthegenerateddatashouldbesimilartotheempirical
data. For this, it is helpful to phrase model selection as a hierarchical
inference problem, which allows us to compute the posterior distribution
overmodels.
Let us consider a finite number of models M = M ,...,M , where
1 K
{ }
Bayesianmodel each model M k possesses parameters θ k. In Bayesian model selection, we
selection place a prior p(M) on the set of models. The corresponding generative
generativeprocess processthatallowsustogeneratedatafromthismodelis
Figure8.15
Illustrationofthe M p(M) (8.40)
k
∼
hierarchical
θ p(θ M ) (8.41)
generativeprocess k ∼ | k
inBayesianmodel p( θ ) (8.42)
k
selection.Weplace D ∼ D|
apriorp(M)onthe and illustrated in Figure 8.15. Given a training set , we apply Bayes’
D
setofmodels.For theoremandcomputetheposteriordistributionovermodelsas
eachmodel,thereis
adistribution p(M ) p(M )p( M ). (8.43)
k k k
p(θ|M)onthe |D ∝ D|
corresponding Note that this posterior no longer depends on the model parameters θ k
modelparameters, becausetheyhavebeenintegratedoutintheBayesiansettingsince
whichisusedto
(cid:90)
generatethedataD.
p( M ) = p( θ )p(θ M )dθ , (8.44)
k k k k k
D| D| |
M
where p(θ M ) is the prior distribution of the model parameters θ of
k k k
|
modelM .Theterm(8.44)isreferredtoasthemodelevidenceormarginal
k
likelihood.Fromtheposteriorin(8.43),wedeterminetheMAPestimate
θ
M∗ = argmaxp(M ). (8.45)
k
Mk |D
With a uniform prior p(M ) = 1, which gives every model equal (prior)
k K
D
probability, determining the MAP estimate over models amounts to pick-
modelevidence
ingthemodelthatmaximizesthemodelevidence(8.44).
marginallikelihood
Remark(LikelihoodandMarginalLikelihood). Therearesomeimportant
differences between a likelihood and a marginal likelihood (evidence):
Whilethelikelihoodispronetooverfitting,themarginallikelihoodistyp-
ically not as the model parameters have been marginalized out (i.e., we
no longer have to fit the parameters). Furthermore, the marginal likeli-
hood automatically embodies a trade-off between model complexity and
datafit(Occam’srazor).
♢
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
8.6 ModelSelection 287
8.6.3 Bayes Factors for Model Comparison
Consider the problem of comparing two probabilistic models M ,M ,
1 2
givenadataset .Ifwecomputetheposteriorsp(M )andp(M ),
1 2
D |D |D
wecancomputetheratiooftheposteriors
p(M )
p(D|M1)p(M1)
p(M ) p( M )
1 |D = p(D) = 1 D| 1 . (8.46)
p(M
2
) p(D|M2)p(M2) p(M 2) p( M 2)
|D p(D) D|
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
posteriorodds priorodds Bayesfactor
The ratio of the posteriors is also called the posterior odds. The first frac- posteriorodds
tion on the right-hand side of (8.46), the prior odds, measures how much priorodds
ourprior(initial)beliefsfavorM overM .Theratioofthemarginallike-
1 2
lihoods (second fraction on the right-hand-side) is called the Bayes factor Bayesfactor
andmeasureshowwellthedata ispredictedbyM comparedtoM .
1 2
D
Remark. The Jeffreys-Lindley paradox states that the “Bayes factor always Jeffreys-Lindley
favorsthesimplermodelsincetheprobabilityofthedataunderacomplex paradox
model with a diffuse prior will be very small” (Murphy, 2012). Here, a
diffuse prior refers to a prior that does not favor specific models, i.e.,
manymodelsareaprioriplausibleunderthisprior.
♢
Ifwechooseauniformpriorovermodels,theprioroddstermin(8.46)
is1,i.e.,theposterioroddsistheratioofthemarginallikelihoods(Bayes
factor)
p( M )
D| 1 . (8.47)
p( M )
2
D|
If the Bayes factor is greater than 1, we choose model M , otherwise
1
model M . In a similar way to frequentist statistics, there are guidelines
2
on the size of the ratio that one should consider before ”significance” of
theresult(Jeffreys,1961).
Remark (Computing the Marginal Likelihood). The marginal likelihood
plays an important role in model selection: We need to compute Bayes
factors(8.46)andposteriordistributionsovermodels(8.43).
Unfortunately, computing the marginal likelihood requires us to solve
an integral (8.44). This integration is generally analytically intractable,
and we will have to resort to approximation techniques, e.g., numerical
integration (Stoer and Burlirsch, 2002), stochastic approximations using
MonteCarlo(Murphy,2012),orBayesianMonteCarlotechniques(O’Hagan,
1991;RasmussenandGhahramani,2003).
However,therearespecialcasesinwhichwecansolveit.InSection6.6.1,
wediscussedconjugatemodels.Ifwechooseaconjugateparameterprior
p(θ), we can compute the marginal likelihood in closed form. In Chap-
ter9,wewilldoexactlythisinthecontextoflinearregression.
♢
We have seen a brief introduction to the basic concepts of machine
learning in this chapter. For the rest of this part of the book we will see
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
288 WhenModelsMeetData
howthethreedifferentflavorsoflearninginSections8.2,8.3,and8.4are
appliedtothefourpillarsofmachinelearning(regression,dimensionality
reduction,densityestimation,andclassification).
8.6.4 Further Reading
Wementionedatthestartofthesectionthattherearehigh-levelmodeling
choicesthatinfluencetheperformanceofthemodel.Examplesincludethe
following:
Thedegreeofapolynomialinaregressionsetting
Thenumberofcomponentsinamixturemodel
Thenetworkarchitectureofa(deep)neuralnetwork
Thetypeofkernelinasupportvectormachine
ThedimensionalityofthelatentspaceinPCA
Thelearningrate(schedule)inanoptimizationalgorithm
Inparametric
models,thenumber Rasmussen and Ghahramani (2001) showed that the automatic Occam’s
ofparametersis razordoesnotnecessarilypenalizethenumberofparametersinamodel,
oftenrelatedtothe
but it is active in terms of the complexity of functions. They also showed
complexityofthe
that the automatic Occam’s razor also holds for Bayesian nonparametric
modelclass.
modelswithmanyparameters,e.g.,Gaussianprocesses.
Ifwefocusonthemaximumlikelihoodestimate,thereexistanumberof
heuristics for model selection that discourage overfitting. They are called
informationcriteria,andwechoosethemodelwiththelargestvalue.The
Akaikeinformation Akaikeinformationcriterion(AIC)(Akaike,1974)
criterion
logp(x θ) M (8.48)
| −
corrects for the bias of the maximum likelihood estimator by addition of
apenaltytermtocompensatefortheoverfittingofmorecomplexmodels
withlotsofparameters.Here,M isthenumberofmodelparameters.The
AICestimatestherelativeinformationlostbyagivenmodel.
Bayesian TheBayesianinformationcriterion(BIC)(Schwarz,1978)
information (cid:90) 1
criterion logp(x) = log p(x θ)p(θ)dθ logp(x θ) M logN (8.49)
| ≈ | − 2
can be used for exponential family distributions. Here, N is the number
of data points and M is the number of parameters. BIC penalizes model
complexitymoreheavilythanAIC.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
9
Linear Regression
In the following, we will apply the mathematical concepts from Chap-
ters 2, 5, 6, and 7 to solve linear regression (curve fitting) problems. In
regression,weaimtofindafunctionf thatmapsinputsx RD tocorre- regression
spondingfunctionvaluesf(x)
R.Weassumewearegive∈
nasetoftrain-
∈
inginputsx andcorrespondingnoisyobservationsy = f(x )+ϵ,where
n n n
ϵ is an i.i.d. random variable that describes measurement/observation
noise and potentially unmodeled processes (which we will not consider
further in this chapter). Throughout this chapter, we assume zero-mean
Gaussian noise. Our task is to find a function that not only models the
training data, but generalizes well to predicting function values at input
locations that are not part of the training data (see Chapter 8). An il-
lustration of such a regression problem is given in Figure 9.1. A typical
regression setting is given in Figure 9.1(a): For some input values x , we
n
observe (noisy) function values y = f(x )+ϵ. The task is to infer the
n n
functionf thatgeneratedthedataandgeneralizeswelltofunctionvalues
atnewinputlocations.ApossiblesolutionisgiveninFigure9.1(b),where
wealsoshowthreedistributionscenteredatthefunctionvaluesf(x)that
representthenoiseinthedata.
Regression is a fundamental problem in machine learning, and regres-
sion problems appear in a diverse range of research areas and applica-
Figure9.1
0.4 0.4 (a)Dataset;
(b)possiblesolution
0.2 0.2
totheregression
problem.
0.0 0.0
0.2 0.2
− −
0.4 0.4
− −
4 2 0 2 4 4 2 0 2 4
− − x − − x
(a)Regressionproblem:observednoisyfunc- (b) Regression solution: possible function
tionvaluesfromwhichwewishtoinferthe that could have generated the data (blue)
underlyingfunctionthatgeneratedthedata. withindicationofthemeasurementnoiseof
the function value at the corresponding in-
puts(orangedistributions).
289
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
y y
290 LinearRegression
tions, including time-series analysis (e.g., system identification), control
and robotics (e.g., reinforcement learning, forward/inverse model learn-
ing), optimization (e.g., line searches, global optimization), and deep-
learning applications (e.g., computer games, speech-to-text translation,
image recognition, automatic video annotation). Regression is also a key
ingredient of classification algorithms. Finding a regression function re-
quiressolvingavarietyofproblems,includingthefollowing:
Choice of the model (type) and the parametrization of the regres-
Normally,thetype sion function. Given a dataset, what function classes (e.g., polynomi-
ofnoisecouldalso als) are good candidates for modeling the data, and what particular
bea“modelchoice”,
parametrization (e.g., degree of the polynomial) should we choose?
butwefixthenoise
Model selection, as discussed in Section 8.6, allows us to compare var-
tobeGaussianin
thischapter. ious models to find the simplest model that explains the training data
reasonablywell.
Finding good parameters. Having chosen a model of the regression
function,howdowefindgoodmodelparameters?Here,wewillneedto
lookatdifferentloss/objectivefunctions(theydeterminewhata“good”
fitis)andoptimizationalgorithmsthatallowustominimizethisloss.
Overfitting and model selection. Overfitting is a problem when the
regression function fits the training data “too well” but does not gen-
eralize to unseen test data. Overfitting typically occurs if the underly-
ingmodel(oritsparametrization)isoverlyflexibleandexpressive;see
Section8.6.Wewilllookattheunderlyingreasonsanddiscusswaysto
mitigatetheeffectofoverfittinginthecontextoflinearregression.
Relationshipbetweenlossfunctionsandparameterpriors.Lossfunc-
tions(optimizationobjectives)areoftenmotivatedandinducedbyprob-
abilistic models. We will look at the connection between loss functions
andtheunderlyingpriorassumptionsthatinducetheselosses.
Uncertaintymodeling.Inanypracticalsetting,wehaveaccesstoonly
a finite, potentially large, amount of (training) data for selecting the
model class and the corresponding parameters. Given that this finite
amount of training data does not cover all possible scenarios, we may
wanttodescribetheremainingparameteruncertaintytoobtainamea-
sureofconfidenceofthemodel’spredictionattesttime;thesmallerthe
trainingset,themoreimportantuncertaintymodeling.Consistentmod-
elingofuncertaintyequipsmodelpredictionswithconfidencebounds.
In the following, we will be using the mathematical tools from Chap-
ters 3, 5, 6 and 7 to solve linear regression problems. We will discuss
maximumlikelihoodandmaximumaposteriori(MAP)estimationtofind
optimalmodelparameters.Usingtheseparameterestimates,wewillhave
a brief look at generalization errors and overfitting. Toward the end of
thischapter,wewilldiscussBayesianlinearregression,whichallowsusto
reasonaboutmodelparametersatahigherlevel,therebyremovingsome
oftheproblemsencounteredinmaximumlikelihoodandMAPestimation.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
9.1 ProblemFormulation 291
9.1 Problem Formulation
Because of the presence of observation noise, we will adopt a probabilis-
tic approach and explicitly model the noise using a likelihood function.
More specifically, throughout this chapter, we consider a regression prob-
lemwiththelikelihoodfunction
p(y x) =
(cid:0)
y f(x),
σ2(cid:1)
. (9.1)
| N |
Here, x RD are inputs and y R are noisy function values (targets).
∈ ∈
With(9.1),thefunctionalrelationshipbetweenxandy isgivenas
y = f(x)+ϵ, (9.2)
(cid:0) (cid:1)
where ϵ 0, σ2 is independent, identically distributed (i.i.d.) Gaus-
∼ N
sian measurement noise with mean 0 and variance σ2. Our objective is
to find a function that is close (similar) to the unknown function f that
generatedthedataandthatgeneralizeswell.
In this chapter, we focus on parametric models, i.e., we choose a para-
metrizedfunctionandfindparametersθthat“workwell”formodelingthe
data. For the time being, we assume that the noise variance σ2 is known
and focus on learning the model parameters θ. In linear regression, we
consider the special case that the parameters θ appear linearly in our
model.Anexampleoflinearregressionisgivenby
p(y x,θ) = (cid:0) y x⊤θ, σ2(cid:1) (9.3)
| N |
y = x⊤θ+ϵ, ϵ (cid:0) 0, σ2(cid:1) , (9.4)
⇐⇒ ∼ N
where θ RD are the parameters we seek. The class of functions de-
∈
scribed by (9.4) are straight lines that pass through the origin. In (9.4),
wechoseaparametrizationf(x) = x⊤θ. ADiracdelta(delta
The likelihood in (9.3) is the probability density function of y evalu- function)iszero
ated at x⊤θ. Note that the only source of uncertainty originates from the everywhereexcept
atasinglepoint,
observation noise (as x and θ are assumed known in (9.3)). Without ob-
anditsintegralis1.
servation noise, the relationship between x and y would be deterministic Itcanbeconsidered
and(9.3)wouldbeaDiracdelta. aGaussianinthe
limitofσ2→0.
likelihood
Example 9.1
For x,θ R the linear regression model in (9.4) describes straight lines
∈
(linear functions), and the parameter θ is the slope of the line. Fig-
ure9.2(a)showssomeexamplefunctionsfordifferentvaluesofθ.
Linearregression
The linear regression model in (9.3)–(9.4) is not only linear in the pa- referstomodelsthat
rameters, but also linear in the inputs x. Figure 9.2(a) shows examples arelinearinthe
parameters.
of such functions. We will see later that y = ϕ⊤(x)θ for nonlinear trans-
formationsϕisalsoalinearregressionmodelbecause“linearregression”
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
292 LinearRegression
Figure9.2 Linear 20
regressionexample. 10 10
(a)Example
0 0 0
functionsthatfall
intothiscategory; 10 10
20 − −
(b)trainingset; − 10 0 10 10 5 0 5 10 10 5 0 5 10
− x − − x − − x
(c)maximum
likelihoodestimate. (a)Examplefunctions(straight (b)Trainingset. (c) Maximum likelihood esti-
lines)thatcanbedescribedus- mate.
ingthelinearmodelin(9.4).
refers to models that are “linear in the parameters”, i.e., models that de-
scribe a function by a linear combination of input features. Here, a “fea-
ture”isarepresentationϕ(x)oftheinputsx.
In the following, we will discuss in more detail how to find good pa-
rameters θ and how to evaluate whether a parameter set “works well”.
Forthetimebeing,weassumethatthenoisevarianceσ2 isknown.
9.2 Parameter Estimation
Consider the linear regression setting (9.4) and assume we are given a
trainingset training set := (x 1,y 1),...,(x N,y N) consisting of N inputs x n
Figure9.3 RD and corrD espond{ ing observations/targe} ts y n R, n = 1,...,N. Th∈ e
∈
Probabilistic corresponding graphical model is given in Figure 9.3. Note that y and y
i j
graphicalmodelfor are conditionally independent given their respective inputs x ,x so that
i j
linearregression.
thelikelihoodfactorizesaccordingto
Observedrandom
variablesare
p( ,θ) = p(y ,...,y x ,...,x ,θ) (9.5a)
shaded, 1 N 1 N
Y|X |
deterministic/ N N
knownvaluesare = (cid:89) p(y x ,θ) = (cid:89) (cid:0) y x⊤θ, σ2(cid:1) , (9.5b)
withoutcircles. n | n N n | n
n=1 n=1
θ
where we defined := x ,...,x and := y ,...,y as the sets
1 N 1 N
σ X { } Y { }
of training inputs and corresponding targets, respectively. The likelihood
x n y n and the factors p(y n |x n,θ) are Gaussian due to the noise distribution;
see(9.3).
n=1,...,N In the following, we will discuss how to find optimal parameters θ∗
RD for the linear regression model (9.4). Once the parameters θ∗ ar∈ e
found, we can predict function values by using this parameter estimate
in (9.4) so that at an arbitrary test input x the distribution of the corre-
∗
spondingtargety is
∗
p(y x ,θ∗) = (cid:0) y x⊤θ∗, σ2(cid:1) . (9.6)
∗ | ∗ N ∗ | ∗
In the following, we will have a look at parameter estimation by maxi-
mizing the likelihood, a topic that we already covered to some degree in
Section8.3.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
y y y
9.2 ParameterEstimation 293
9.2.1 Maximum Likelihood Estimation
Awidelyusedapproachtofindingthedesiredparametersθ MLismaximum maximumlikelihood
likelihood estimation, where we find parameters θ that maximize the estimation
ML
likelihood (9.5b). Intuitively, maximizing the likelihood means maximiz- Maximizingthe
ingthepredictivedistributionofthetrainingdatagiventhemodelparam- likelihoodmeans
maximizingthe
eters.Weobtainthemaximumlikelihoodparametersas
predictive
θ argmaxp( ,θ). (9.7) distributionofthe
ML
∈ θ Y|X (training)data
giventhe
Remark. Thelikelihoodp(y x,θ)isnotaprobabilitydistributioninθ:It parameters.
|
is simply a function of the parameters θ but does not integrate to 1 (i.e., Thelikelihoodisnot
it is unnormalized), and may not even be integrable with respect to θ. aprobability
However, the likelihood in (9.7) is a normalized probability distribution distributioninthe
parameters.
iny.
♢
To find the desired parameters θ that maximize the likelihood, we
ML
typically perform gradient ascent (or gradient descent on the negative
likelihood). In the case of linear regression we consider here, however, Sincethelogarithm
a closed-form solution exists, which makes iterative gradient descent un- isa(strictly)
monotonically
necessary. In practice, instead of maximizing the likelihood directly, we
increasingfunction,
apply the log-transformation to the likelihood function and minimize the
theoptimumofa
negativelog-likelihood. functionf is
identicaltothe
Remark (Log-Transformation). Since the likelihood (9.5b) is a product of
optimumoflogf.
N Gaussiandistributions,thelog-transformationisusefulsince(a)itdoes
notsufferfromnumericalunderflow,and(b)thedifferentiationruleswill
turn out simpler. More specifically, numerical underflow will be a prob-
lem when we multiply N probabilities, where N is the number of data
points, since we cannot represent very small numbers, such as 10−256.
Furthermore, the log-transform will turn the product into a sum of log-
probabilities such that the corresponding gradient is a sum of individual
gradients, instead of a repeated application of the product rule (5.46) to
computethegradientofaproductofN terms.
♢
To find the optimal parameters θ of our linear regression problem,
ML
weminimizethenegativelog-likelihood
N N
(cid:89) (cid:88)
logp( ,θ) = log p(y x ,θ) = logp(y x ,θ), (9.8)
n n n n
− Y|X − | − |
n=1 n=1
where we exploited that the likelihood (9.5b) factorizes over the number
ofdatapointsduetoourindependenceassumptiononthetrainingset.
In the linear regression model (9.4), the likelihood is Gaussian (due to
theGaussianadditivenoiseterm),suchthatwearriveat
1
logp(y x ,θ) = (y x⊤θ)2+const, (9.9)
n | n −2σ2 n − n
wheretheconstantincludesalltermsindependentofθ.Using(9.9)inthe
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
294 LinearRegression
negativelog-likelihood(9.8),weobtain(ignoringtheconstantterms)
N
1 (cid:88)
(θ) := (y x⊤θ)2 (9.10a)
L 2σ2 n − n
n=1
1 1
= (y Xθ)⊤(y Xθ) = y Xθ 2, (9.10b)
2σ2 − − 2σ2∥ − ∥
Thenegative where we define the design matrix X := [x 1,...,x N]⊤ RN×D as the
log-likelihood collectionoftraininginputsandy := [y ,...,y ]⊤
RN∈
asavectorthat
1 N
functionisalso collectsalltrainingtargets.Notethatthenthrowin∈
thedesignmatrixX
callederrorfunction.
correspondstothetraininginputx .In(9.10b),weusedthefactthatthe
designmatrix n
sumofsquarederrorsbetweentheobservationsy andthecorresponding
Thesquarederroris n
modelpredictionx⊤θ equalsthesquareddistancebetweeny andXθ.
oftenusedasa n
measureofdistance. With(9.10b),wehavenowaconcreteformofthenegativelog-likelihood
Recallfrom functionweneedtooptimize.Weimmediatelyseethat(9.10b)isquadratic
Section3.1that in θ. This means that we can find a unique global solution θ for mini-
∥x∥2=x⊤xifwe ML
mizing the negative log-likelihood . We can find the global optimum by
choosethedot L
productastheinner computingthegradientof ,settingitto0andsolvingforθ.
L
product. Using the results from Chapter 5, we compute the gradient of with
L
respecttotheparametersas
(cid:18) (cid:19)
d d 1
L = (y Xθ)⊤(y Xθ) (9.11a)
dθ dθ 2σ2 − −
1 d (cid:16) (cid:17)
= y⊤y 2y⊤Xθ+θ⊤X⊤Xθ (9.11b)
2σ2dθ −
1
= ( y⊤X +θ⊤X⊤X) R1×D. (9.11c)
σ2 − ∈
The maximum likelihood estimator θ solves dL = 0⊤ (necessary opti-
ML dθ
Ignoringthe malitycondition)andweobtain
possibilityof
duplicatedata d L = 0⊤ (9.11c) θ⊤ X⊤X = y⊤X (9.12a)
points,rk(X)=D dθ ⇐⇒ ML
ifN ⩾D,i.e.,we
θ⊤ = y⊤X(X⊤X)−1 (9.12b)
donothavemore ⇐⇒ ML
parametersthan θ = (X⊤X)−1X⊤y. (9.12c)
ML
datapoints. ⇐⇒
Wecouldright-multiplythefirstequationby(X⊤X)−1 becauseX⊤X is
positivedefiniteifrk(X) = D,whererk(X)denotestherankofX.
Remark. Settingthegradientto0⊤ isanecessaryandsufficientcondition,
and we obtain a global minimum since the Hessian 2 (θ) = X⊤X
RD×D ispositivedefinite. ∇θL ∈
♢
Remark. Themaximumlikelihoodsolutionin(9.12c)requiresustosolve
a system of linear equations of the form Aθ = b with A = (X⊤X) and
b = X⊤y.
♢
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
9.2 ParameterEstimation 295
Example 9.2 (Fitting Lines)
LetushavealookatFigure9.2,whereweaimtofitastraightlinef(x) =
θx,whereθ isanunknownslope,toadatasetusingmaximumlikelihood
estimation. Examples of functions in this model class (straight lines) are
shown in Figure 9.2(a). For the dataset shown in Figure 9.2(b), we find
the maximum likelihood estimate of the slope parameter θ using (9.12c)
andobtainthemaximumlikelihoodlinearfunctioninFigure9.2(c).
Maximum Likelihood Estimation with Features
So far, we considered the linear regression setting described in (9.4),
which allowed us to fit straight lines to data using maximum likelihood
estimation. However, straight lines are not sufficiently expressive when it Linearregression
comestofittingmoreinterestingdata.Fortunately,linearregressionoffers refersto“linear-in-
the-parameters”
usawaytofitnonlinearfunctionswithinthelinearregressionframework:
regressionmodels,
Since “linear regression” only refers to “linear in the parameters”, we can
buttheinputscan
perform an arbitrary nonlinear transformation ϕ(x) of the inputs x and undergoany
then linearly combine the components of this transformation. The corre- nonlinear
spondinglinearregressionmodelis transformation.
p(y x,θ) = (cid:0) y ϕ⊤(x)θ, σ2(cid:1)
| N |
K (cid:88)−1 (9.13)
y = ϕ⊤(x)θ+ϵ = θ ϕ (x)+ϵ,
k k
⇐⇒
k=0
where ϕ : RD RK is a (nonlinear) transformation of the inputs x and
ϕ k : RD R i→ s the kth component of the feature vector ϕ. Note that the featurevector
→
modelparametersθ stillappearonlylinearly.
Example 9.3 (Polynomial Regression)
Weareconcernedwitharegressionproblemy = ϕ⊤(x)θ+ϵ,wherex R
andθ RK.Atransformationthatisoftenusedinthiscontextis ∈
∈
 
1
 
ϕ 0(x)  x 
 
 ϕ 1(x)   x2 
ϕ(x) =    . . .    =    x3    ∈ RK. (9.14)
 . . 
ϕ K−1(x)  . 
xK−1
This means that we “lift” the original one-dimensional input space into
a K-dimensional feature space consisting of all monomials xk for k =
0,...,K 1. With these features, we can model polynomials of degree
⩽ K 1w− ithintheframeworkoflinearregression:Apolynomialofdegree
−
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
296 LinearRegression
K 1is
−
K−1
(cid:88)
f(x) = θ xk = ϕ⊤(x)θ, (9.15)
k
k=0
where ϕ is defined in (9.14) and θ = [θ ,...,θ ]⊤ RK contains the
0 K−1
∈
(linear)parametersθ .
k
Letusnowhavealookatmaximumlikelihoodestimationoftheparam-
etersθ inthe linearregression model(9.13). Weconsidertraining inputs
featurematrix x n RD andtargetsy n R,n = 1,...,N,anddefinethefeaturematrix
∈ ∈
designmatrix (designmatrix)as
 
Φ :=


ϕ⊤(
. .
.x 1)
  =   
ϕ ϕ0 0(
( . .
.x x1 2)
)
· ·· ··
·
ϕ ϕK K− −1
1 . .
.( (x x1 2)
)  

∈ RN×K, (9.16)
ϕ⊤(x )
N ϕ (x ) ϕ (x )
0 N K−1 N
···
whereΦ = ϕ (x )andϕ : RD R.
ij j i j
→
Example 9.4 (Feature Matrix for Second-order Polynomials)
For a second-order polynomial and N training points x R,n =
n
∈
1,...,N,thefeaturematrixis

1 x
x2
1 1
Φ =


1
. .
x
.
.2 x
.
.2 2
  . (9.17)
. . . 
1 x x2
N N
WiththefeaturematrixΦdefinedin(9.16),thenegativelog-likelihood
forthelinearregressionmodel(9.13)canbewrittenas
1
logp( ,θ) = (y Φθ)⊤(y Φθ)+const. (9.18)
− Y|X 2σ2 − −
Comparing(9.18)withthenegativelog-likelihoodin(9.10b)forthe“fea-
ture-free” model, we immediately see we just need to replace X with Φ.
SincebothX andΦareindependentoftheparametersθ thatwewishto
maximumlikelihood optimize,wearriveimmediatelyatthemaximumlikelihoodestimate
estimate
θ = (Φ⊤Φ)−1Φ⊤y (9.19)
ML
forthelinearregressionproblemwithnonlinearfeaturesdefinedin(9.13).
Remark. When we were working without features, we required X⊤X to
be invertible, which is the case when rk(X) = D, i.e., the columns of X
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
9.2 ParameterEstimation 297
are linearly independent. In (9.19), we therefore require Φ⊤Φ RK×K
∈
tobeinvertible.Thisisthecaseifandonlyifrk(Φ) = K.
♢
Example 9.5 (Maximum Likelihood Polynomial Fit)
Figure9.4
Polynomial
4 4 Trainingdata
MLE regression:
2 2 (a)dataset
consistingof
0 0 (xn,yn)pairs,
n=1,...,10;
2 2
− − (b)maximum
4 4 likelihood
− −
polynomialof
4 2 0 2 4 4 2 0 2 4
− − x − − x degree4.
(a)Regressiondataset. (b)Polynomialofdegree4determinedbymax-
imumlikelihoodestimation.
Consider the dataset in Figure 9.4(a). The dataset consists of N = 10
pairs(x ,y ),wherex [ 5,5]andy = sin(x /5)+cos(x )+ϵ,
n n n n n n
(cid:0) (cid:1) ∼ U − −
whereϵ 0, 0.22 .
∼ N
We fit a polynomial of degree 4 using maximum likelihood estimation,
i.e.,parametersθ aregivenin(9.19).Themaximumlikelihoodestimate
ML
yields function values ϕ⊤(x )θ at any test location x . The result is
∗ ML ∗
showninFigure9.4(b).
Estimating the Noise Variance
Thus far, we assumed that the noise variance σ2 is known. However, we
canalsousetheprincipleofmaximumlikelihoodestimationtoobtainthe
maximum likelihood estimator σ2 for the noise variance. To do this, we
ML
follow the standard procedure: We write down the log-likelihood, com-
pute its derivative with respect to σ2 > 0, set it to 0, and solve. The
log-likelihoodisgivenby
N
log p( ,θ,σ2) = (cid:88) log (cid:0) y ϕ⊤(x )θ, σ2(cid:1) (9.20a)
n n
Y|X N |
n=1
N (cid:18) (cid:19)
(cid:88) 1 1 1
= log(2π) logσ2 (y ϕ⊤(x )θ)2 (9.20b)
−2 − 2 − 2σ2 n − n
n=1
N 1 (cid:88)N
= logσ2 (y ϕ⊤(x )θ)2+const. (9.20c)
− 2 − 2σ2 n − n
n=1
(cid:124) (cid:123)(cid:122) (cid:125)
=:s
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
y y
298 LinearRegression
Thepartialderivativeofthelog-likelihoodwithrespecttoσ2 isthen
∂logp( ,θ,σ2) N 1
Y|X = + s = 0 (9.21a)
∂σ2 −2σ2 2σ4
N s
= (9.21b)
⇐⇒ 2σ2 2σ4
sothatweidentify
s 1 (cid:88)N
σ2 = = (y ϕ⊤(x )θ)2. (9.22)
ML N N n − n
n=1
Therefore, the maximum likelihood estimate of the noise variance is the
empirical mean of the squared distances between the noise-free function
values ϕ⊤(x )θ and the corresponding noisy observations y at input lo-
n n
cationsx .
n
9.2.2 Overfitting in Linear Regression
We just discussed how to use maximum likelihood estimation to fit lin-
ear models (e.g., polynomials) to data. We can evaluate the quality of
the model by computing the error/loss incurred. One way of doing this
is to compute the negative log-likelihood (9.10b), which we minimized
to determine the maximum likelihood estimator. Alternatively, given that
the noise parameter σ2 is not a free model parameter, we can ignore the
scaling by 1/σ2, so that we end up with a squared-error-loss function
rootmeansquare y Φθ 2 .Insteadofusingthissquaredloss,weoftenusetherootmean
∥ − ∥
error squareerror(RMSE)
RMSE
(cid:118)
(cid:114) (cid:117) N
1 y Φθ 2 = (cid:117) (cid:116) 1 (cid:88) (y ϕ⊤(x )θ)2, (9.23)
N ∥ − ∥ N n − n
n=1
which (a) allows us to compare errors of datasets with different sizes
TheRMSEis and (b) has the same scale and the same units as the observed func-
normalized. tion values y . For example, if we fit a model that maps post-codes (x
n
is given in latitude, longitude) to house prices (y-values are EUR) then
the RMSE is also measured in EUR, whereas the squared error is given
Thenegative in EUR2. If we choose to include the factor σ2 from the original negative
log-likelihoodis log-likelihood (9.10b), then we end up with a unitless objective, i.e., in
unitless. theprecedingexample,ourobjectivewouldnolongerbeinEURorEUR2.
For model selection (see Section 8.6), we can use the RMSE (or the
negativelog-likelihood)todeterminethebestdegreeofthepolynomialby
findingthepolynomialdegreeM thatminimizestheobjective.Giventhat
the polynomial degree is a natural number, we can perform a brute-force
search and enumerate all (reasonable) values of M. For a training set of
size N it is sufficient to test 0 ⩽ M ⩽ N 1. For M < N, the maximum
likelihood estimator is unique. For M
⩾−
N, we have more parameters
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
9.2 ParameterEstimation 299
Figure9.5
4 Trainingdata 4 Trainingdata 4 Trainingdata
MLE MLE MLE Maximum
2 2 2 likelihoodfitsfor
0 0 0 differentpolynomial
−2 −2 −2 degreesM.
4 4 4
− − −
4 2 0 2 4 4 2 0 2 4 4 2 0 2 4
− − x − − x − − x
(a)M =0 (b)M =1 (c)M =3
4 Trainingdata 4 Trainingdata 4 Trainingdata
MLE MLE MLE
2 2 2
0 0 0
2 2 2
− − −
4 4 4
− − −
4 2 0 2 4 4 2 0 2 4 4 2 0 2 4
− − x − − x − − x
(d)M =4 (e)M =6 (f)M =9
thandatapoints,andwouldneedtosolveanunderdeterminedsystemof
linear equations (Φ⊤Φ in (9.19) would also no longer be invertible) so
thatthereareinfinitelymanypossiblemaximumlikelihoodestimators.
Figure9.5showsanumberofpolynomialfitsdeterminedbymaximum
likelihood for the dataset from Figure 9.4(a) with N = 10 observations.
We notice that polynomials of low degree (e.g., constants (M = 0) or
linear (M = 1)) fit the data poorly and, hence, are poor representations
of the true underlying function. For degrees M = 3,...,6, the fits look
plausibleandsmoothlyinterpolatethedata.Whenwegotohigher-degree Thecaseof
polynomials, we notice that they fit the data better and better. In the ex- M =N−1is
tremecaseofM = N 1 = 9,thefunctionwillpassthrougheverysingle extremeinthesense
− thatotherwisethe
data point. However, these high-degree polynomials oscillate wildly and
nullspaceofthe
are a poor representation of the underlying function that generated the corresponding
data,suchthatwesufferfromoverfitting. systemoflinear
Remember that the goal is to achieve good generalization by making equationswouldbe
non-trivial,andwe
accurate predictions for new (unseen) data. We obtain some quantita-
wouldhave
tiveinsightintothedependenceofthegeneralizationperformanceonthe infinitelymany
polynomialofdegreeM byconsideringaseparatetestsetcomprising200 optimalsolutionsto
data points generated using exactly the same procedure used to generate thelinearregression
problem.
thetrainingset.Astestinputs,wechosealineargridof200pointsinthe
overfitting
intervalof[ 5,5].ForeachchoiceofM,weevaluatetheRMSE(9.23)for
− Notethatthenoise
boththetrainingdataandthetestdata.
varianceσ2>0.
Looking now at the test error, which is a qualitive measure of the gen-
eralizationpropertiesofthecorrespondingpolynomial,wenoticethatini-
tially the test error decreases; see Figure 9.6 (orange). For fourth-order
polynomials,thetesterrorisrelativelylowandstaysrelativelyconstantup
todegree5.However,fromdegree6onwardthetesterrorincreasessignif-
icantly, and high-order polynomials have very bad generalization proper-
ties.Inthisparticularexample,thisalsoisevidentfromthecorresponding
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
y
y
y
y
y
y
300 LinearRegression
Figure9.6 Training 10
andtesterror. Trainingerror
8 Testerror
6
4
2
0
0 2 4 6 8 10
Degreeofpolynomial
trainingerror maximum likelihood fits in Figure 9.5. Note that the training error (blue
curveinFigure9.6)neverincreaseswhenthedegreeofthepolynomialin-
creases.Inourexample,thebestgeneralization(thepointofthesmallest
testerror testerror)isobtainedforapolynomialofdegreeM = 4.
9.2.3 Maximum A Posteriori Estimation
We just saw that maximum likelihood estimation is prone to overfitting.
We often observe that the magnitude of the parameter values becomes
relativelylargeifwerunintooverfitting(Bishop,2006).
To mitigate the effect of huge parameter values, we can place a prior
distribution p(θ) on the parameters. The prior distribution explicitly en-
codeswhatparametervaluesareplausible(beforehavingseenanydata).
(cid:0) (cid:1)
For example, a Gaussian prior p(θ) = 0, 1 on a single parameter
N
θ encodes that parameter values are expected lie in the interval [ 2,2]
−
(two standard deviations around the mean value). Once a dataset ,
X Y
isavailable,insteadofmaximizingthelikelihoodweseekparametersthat
maximize the posterior distribution p(θ , ). This procedure is called
|X Y
maximuma maximumaposteriori(MAP)estimation.
posteriori The posterior over the parameters θ, given the training data , , is
X Y
MAP obtainedbyapplyingBayes’theorem(Section6.3)as
p( ,θ)p(θ)
p(θ , ) = Y|X . (9.24)
|X Y p( )
Y|X
Since the posterior explicitly depends on the parameter prior p(θ), the
priorwillhaveaneffectontheparametervectorwefindasthemaximizer
of the posterior. We will see this more explicitly in the following. The
parameter vector θ that maximizes the posterior (9.24) is the MAP
MAP
estimate.
To find the MAP estimate, we follow steps that are similar in flavor
to maximum likelihood estimation. We start with the log-transform and
computethelog-posterioras
logp(θ , ) = logp( ,θ)+logp(θ)+const, (9.25)
|X Y Y|X
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
ESMR
9.2 ParameterEstimation 301
wheretheconstantcomprisesthetermsthatareindependentofθ.Wesee
thatthelog-posteriorin(9.25)isthesumofthelog-likelihoodp( ,θ)
Y|X
andthelog-priorlogp(θ)sothattheMAPestimatewillbea“compromise”
between the prior (our suggestion for plausible parameter values before
observingdata)andthedata-dependentlikelihood.
TofindtheMAPestimateθ ,weminimizethenegativelog-posterior
MAP
distributionwithrespecttoθ,i.e.,wesolve
θ argmin logp( ,θ) logp(θ) . (9.26)
MAP
∈ θ {− Y|X − }
Thegradientofthenegativelog-posteriorwithrespecttoθ is
dlogp(θ , ) dlogp( ,θ) dlogp(θ)
|X Y = Y|X , (9.27)
− dθ − dθ − dθ
where we identify the first term on the right-hand side as the gradient of
thenegativelog-likelihoodfrom(9.11c).
(cid:0) (cid:1)
Witha(conjugate)Gaussianpriorp(θ) = 0, b2I ontheparameters
N
θ, the negative log-posterior for the linear regression setting (9.13), we
obtainthenegativelogposterior
1 1
logp(θ , ) = (y Φθ)⊤(y Φθ)+ θ⊤θ+const. (9.28)
− |X Y 2σ2 − − 2b2
Here,thefirsttermcorrespondstothecontributionfromthelog-likelihood,
andthesecondtermoriginatesfromthelog-prior.Thegradientofthelog-
posteriorwithrespecttotheparametersθ isthen
dlogp(θ , ) 1 1
|X Y = (θ⊤Φ⊤Φ y⊤Φ)+ θ⊤. (9.29)
− dθ σ2 − b2
We will find the MAP estimate θ by setting this gradient to 0⊤ and
MAP
solvingforθ .Weobtain
MAP
1 1
(θ⊤Φ⊤Φ y⊤Φ)+ θ⊤ = 0⊤ (9.30a)
σ2 − b2
(cid:18) (cid:19)
1 1 1
θ⊤ Φ⊤Φ+ I y⊤Φ = 0⊤ (9.30b)
⇐⇒ σ2 b2 − σ2
(cid:18) σ2 (cid:19)
θ⊤ Φ⊤Φ+ I = y⊤Φ (9.30c)
⇐⇒ b2
(cid:18) σ2 (cid:19)−1
θ⊤ = y⊤Φ Φ⊤Φ+ I (9.30d)
⇐⇒ b2
sothattheMAPestimateis(bytransposingbothsidesofthelastequality) Φ⊤Φissymmetric,
positivesemi
(cid:18) σ2 (cid:19)−1
θ = Φ⊤Φ+ I Φ⊤y. (9.31) definite.The
MAP b2 additionalterm
in(9.31)isstrictly
Comparing the MAP estimate in (9.31) with the maximum likelihood es- positivedefiniteso
timate in (9.19), we see that the only difference between both solutions thattheinverse
is the additional term
σ2
I in the inverse matrix. This term ensures that
exists.
b2
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
302 LinearRegression
Φ⊤Φ + σ2 I is symmetric and strictly positive definite (i.e., its inverse
b2
exists and the MAP estimate is the unique solution of a system of linear
equations).Moreover,itreflectstheimpactoftheregularizer.
Example 9.6 (MAP Estimation for Polynomial Regression)
InthepolynomialregressionexamplefromSection9.2.1,weplaceaGaus-
(cid:0) (cid:1)
sian prior p(θ) = 0, I on the parameters θ and determine the MAP
N
estimatesaccordingto(9.31).InFigure9.7,weshowboththemaximum
likelihood and the MAP estimates for polynomials of degree 6 (left) and
degree 8 (right). The prior (regularizer) does not play a significant role
for the low-degree polynomial, but keeps the function relatively smooth
for higher-degree polynomials. Although the MAP estimate can push the
boundaries of overfitting, it is not a general solution to this problem, so
weneedamoreprincipledapproachtotackleoverfitting.
Figure9.7
Polynomial
4 4 Trainingdata
regression: MLE
maximumlikelihood 2 2 MAP
andMAPestimates.
(a)Polynomialsof 0 0
degree6;
2 Trainingdata 2
(b)polynomialsof − −
MLE
degree8.
4 MAP 4
− −
4 2 0 2 4 4 2 0 2 4
− − x − − x
(a)Polynomialsofdegree6. (b)Polynomialsofdegree8.
9.2.4 MAP Estimation as Regularization
Instead of placing a prior distribution on the parameters θ, it is also pos-
sible to mitigate the effect of overfitting by penalizing the amplitude of
regularization the parameter by means of regularization. In regularized least squares, we
regularizedleast considerthelossfunction
squares
y Φθ 2 +λ θ 2 , (9.32)
∥ − ∥ ∥ ∥2
which we minimize with respect to θ (see Section 8.2.3). Here, the first
data-fitterm term is a data-fit term (also called misfit term), which is proportional to
misfitterm the negative log-likelihood; see (9.10b). The second term is called the
regularizer regularizer, and the regularization parameter λ ⩾ 0 controls the “strict-
regularization ness”oftheregularization.
parameter
Remark. Instead of the Euclidean norm , we can choose any p-norm
∥·∥2
in (9.32). In practice, smaller values for p lead to sparser solutions.
∥·∥p
Here, “sparse” means that many parameter values θ = 0, which is also
d
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
y y
9.3 BayesianLinearRegression 303
useful for variable selection. For p = 1, the regularizer is called LASSO LASSO
(leastabsoluteshrinkageandselectionoperator)andwasproposedbyTib-
shirani(1996).
♢
The regularizer λ θ 2 in (9.32) can be interpreted as a negative log-
∥ ∥2
Gaussianprior,whichweuseinMAPestimation;see(9.26).Morespecif-
(cid:0) (cid:1)
ically, with a Gaussian prior p(θ) = 0, b2I , we obtain the negative
N
log-Gaussianprior
1
logp(θ) = θ 2 +const (9.33)
− 2b2 ∥ ∥2
so that for λ = 1 the regularization term and the negative log-Gaussian
2b2
priorareidentical.
Giventhattheregularizedleast-squareslossfunctionin(9.32)consists
oftermsthatarecloselyrelatedtothenegativelog-likelihoodplusaneg-
ative log-prior, it is not surprising that, when we minimize this loss, we
obtainasolutionthatcloselyresemblestheMAPestimatein(9.31).More
specifically,minimizingtheregularizedleast-squareslossfunctionyields
θ = (Φ⊤Φ+λI)−1Φ⊤y, (9.34)
RLS
which is identical to the MAP estimate in (9.31) for λ = σ2 , where σ2 is
b2
the noise variance and b2 the variance of the (isotropic) Gaussian prior
(cid:0) (cid:1)
p(θ) = 0, b2I . Apointestimateisa
N
So far, we have covered parameter estimation using maximum likeli- singlespecific
hood and MAP estimation where we found point estimates θ∗ that op- parametervalue,
unlikeadistribution
timize an objective function (likelihood or posterior). We saw that both
overplausible
maximum likelihood and MAP estimation can lead to overfitting. In the parametersettings.
next section, we will discuss Bayesian linear regression, where we use
Bayesian inference (Section 8.4) to find a posterior distribution over the
unknown parameters, which we subsequently use to make predictions.
Morespecifically,forpredictionswewillaverageoverallplausiblesetsof
parametersinsteadoffocusingonapointestimate.
9.3 Bayesian Linear Regression
Previously,welookedatlinearregressionmodelswhereweestimatedthe
model parameters θ, e.g., by means of maximum likelihood or MAP esti-
mation.WediscoveredthatMLEcanleadtosevereoverfitting,inparticu-
lar, in the small-data regime. MAP addresses this issue by placing a prior
ontheparametersthatplaystheroleofaregularizer. Bayesianlinear
Bayesian linear regression pushes the idea of the parameter prior a step regression
further and does not even attempt to compute a point estimate of the
parameters,butinsteadthefullposteriordistributionovertheparameters
is taken into account when making predictions. This means we do not fit
any parameters, but we compute a mean over all plausible parameters
settings(accordingtotheposterior).
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
304 LinearRegression
9.3.1 Model
InBayesianlinearregression,weconsiderthemodel
(cid:0) (cid:1)
prior p(θ) = m , S ,
0 0
N (9.35)
likelihood p(y x,θ) = (cid:0) y ϕ⊤(x)θ, σ2(cid:1) ,
| N |
(cid:0) (cid:1)
Figure9.8 wherewenowexplicitlyplaceaGaussianpriorp(θ) = m 0, S 0 onθ,
N
Graphicalmodelfor which turns the parameter vector into a random variable. This allows us
Bayesianlinear
towritedownthecorrespondinggraphicalmodelinFigure9.8,wherewe
regression.
made the parameters of the Gaussian prior on θ explicit. The full proba-
m 0 S 0 bilistic model, i.e., the joint distribution of observed and unobserved ran-
domvariables,y andθ,respectively,is
θ
p(y,θ x) = p(y x,θ)p(θ). (9.36)
σ
| |
x y
9.3.2 Prior Predictions
Inpractice,weareusuallynotsomuchinterestedintheparametervalues
θ themselves. Instead, our focus often lies in the predictions we make
withthoseparametervalues.InaBayesiansetting,wetaketheparameter
distribution and average over all plausible parameter settings when we
make predictions. More specifically, to make predictions at an input x ,
∗
weintegrateoutθ andobtain
(cid:90)
p(y x ) = p(y x ,θ)p(θ)dθ = E [p(y x ,θ)], (9.37)
∗ ∗ ∗ ∗ θ ∗ ∗
| | |
which we can interpret as the average prediction of y x ,θ for all plau-
∗ ∗
|
sibleparametersθ accordingtothepriordistributionp(θ).Notethatpre-
dictions using the prior distribution only require us to specify the input
x ,butnotrainingdata.
∗
In our model (9.35), we chose a conjugate (Gaussian) prior on θ so
that the predictive distribution is Gaussian as well (and can be computed
(cid:0) (cid:1)
inclosedform):Withthepriordistributionp(θ) = m , S ,weobtain
0 0
N
thepredictivedistributionas
p(y x ) = (cid:0) ϕ⊤(x )m , ϕ⊤(x )S ϕ(x )+σ2(cid:1) , (9.38)
∗ ∗ ∗ 0 ∗ 0 ∗
| N
where we exploited that (i) the prediction is Gaussian due to conjugacy
(seeSection6.6)andthemarginalizationpropertyofGaussians(seeSec-
tion6.5),(ii)theGaussiannoiseisindependentsothat
V[y ] = V [ϕ⊤(x )θ]+V [ϵ], (9.39)
∗ θ ∗ ϵ
and (iii) y is a linear transformation of θ so that we can apply the rules
∗
for computing the mean and covariance of the prediction analytically by
using(6.50)and(6.51),respectively.In(9.38),thetermϕ⊤(x )S ϕ(x )
∗ 0 ∗
inthepredictivevarianceexplicitlyaccountsfortheuncertaintyassociated
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
9.3 BayesianLinearRegression 305
with the parameters θ, whereas σ2 is the uncertainty contribution due to
themeasurementnoise.
If we are interested in predicting noise-free function values f(x ) =
∗
ϕ⊤(x )θ insteadofthenoise-corruptedtargetsy weobtain
∗ ∗
p(f(x )) = (cid:0) ϕ⊤(x )m , ϕ⊤(x )S ϕ(x )(cid:1) , (9.40)
∗ ∗ 0 ∗ 0 ∗
N
whichonlydiffersfrom(9.38)intheomissionofthenoisevarianceσ2 in
thepredictivevariance.
Remark (Distribution over Functions). Since we can represent the distri- Theparameter
bution p(θ) using a set of samples θ and every sample θ gives rise to a distributionp(θ)
i i
function f ( ) = θ⊤ϕ( ), it follows that the parameter distribution p(θ) inducesa
i · i · distributionover
inducesadistributionp(f( ))overfunctions.Hereweusethenotation( )
· · functions.
toexplicitlydenoteafunctionalrelationship.
♢
Example 9.7 (Prior over Functions)
Figure9.9 Prior
overfunctions.
4 4
(a)Distributionover
functions
2 2
representedbythe
meanfunction
0 0
(blackline)andthe
marginal
2 2
− − uncertainties
4 4 (shaded),
− −
representingthe
4 2 0 2 4 4 2 0 2 4
− − x − − x 67%and95%
confidencebounds,
(a)Priordistributionoverfunctions. (b) Samples from the prior distribution over
respectively;
functions.
(b)samplesfrom
Let us consider a Bayesian linear regression problem with polynomials thepriorover
of degree 5. We choose a parameter prior p(θ) =
(cid:0)
0,
1I(cid:1)
. Figure 9.9
functions,whichare
N 4 inducedbythe
visualizestheinducedpriordistributionoverfunctions(shadedarea:dark
samplesfromthe
gray:67%confidencebound;lightgray:95%confidencebound)induced parameterprior.
bythisparameterprior,includingsomefunctionsamplesfromthisprior.
A function sample is obtained by first sampling a parameter vector
θ p(θ) and then computing f ( ) = θ⊤ϕ( ). We used 200 input lo-
i ∼ i · i ·
cations x [ 5,5] to which we apply the feature function ϕ( ). The
∗
∈ − ·
uncertainty(representedbytheshadedarea)inFigure9.9issolelydueto
theparameteruncertaintybecauseweconsideredthenoise-freepredictive
distribution(9.40).
So far, we looked at computing predictions using the parameter prior
p(θ). However, when we have a parameter posterior (given some train-
ing data , ), the same principles for prediction and inference hold
X Y
as in (9.37) – we just need to replace the prior p(θ) with the posterior
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
y y
306 LinearRegression
p(θ , ). In the following, we will derive the posterior distribution in
|X Y
detailbeforeusingittomakepredictions.
9.3.3 Posterior Distribution
Given a training set of inputs x RD and corresponding observations
n
y R, n = 1,...,N, we comp∈ ute the posterior over the parameters
n
∈
usingBayes’theoremas
p( ,θ)p(θ)
p(θ , ) = Y|X , (9.41)
|X Y p( )
Y|X
where is the set of training inputs and the collection of correspond-
X Y
ing training targets. Furthermore, p( ,θ) is the likelihood, p(θ) the
Y|X
parameterprior,and
(cid:90)
p( ) = p( ,θ)p(θ)dθ = E [p( ,θ)] (9.42)
θ
Y|X Y|X Y|X
marginallikelihood the marginal likelihood/evidence, which is independent of the parameters
evidence θ and ensures that the posterior is normalized, i.e., it integrates to 1. We
Themarginal can think of the marginal likelihood as the likelihood averaged over all
likelihoodisthe possibleparametersettings(withrespecttothepriordistributionp(θ)).
expectedlikelihood
undertheparameter Theorem 9.1 (Parameter Posterior). In our model (9.35), the parameter
prior.
posterior(9.41)canbecomputedinclosedformas
(cid:0) (cid:1)
p(θ , ) = θ m , S , (9.43a)
N N
|X Y N |
S = (S−1+σ−2Φ⊤Φ)−1, (9.43b)
N 0
m = S (S−1m +σ−2Φ⊤y), (9.43c)
N N 0 0
wherethesubscriptN indicatesthesizeofthetrainingset.
Proof Bayes’ theorem tells us that the posterior p(θ , ) is propor-
|X Y
tionaltotheproductofthelikelihoodp( ,θ)andthepriorp(θ):
Y|X
p( ,θ)p(θ)
Posterior p(θ , ) = Y|X (9.44a)
|X Y p( )
Y|X
Likelihood p( ,θ) =
(cid:0)
y Φθ,
σ2I(cid:1)
(9.44b)
Y|X N |
(cid:0) (cid:1)
Prior p(θ) = θ m , S . (9.44c)
0 0
N |
Instead of looking at the product of the prior and the likelihood, we
can transform the problem into log-space and solve for the mean and
covarianceoftheposteriorbycompletingthesquares.
Thesumofthelog-priorandthelog-likelihoodis
log
(cid:0)
y Φθ,
σ2I(cid:1)
+log
(cid:0)
θ m , S
(cid:1)
(9.45a)
0 0
N | N |
= 1(cid:0) σ−2(y Φθ)⊤(y Φθ)+(θ m )⊤S−1(θ m )(cid:1) +const
−2 − − − 0 0 − 0
(9.45b)
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
9.3 BayesianLinearRegression 307
where the constant contains terms independent of θ. We will ignore the
constantinthefollowing.Wenowfactorize(9.45b),whichyields
1(cid:0)
σ−2y⊤y 2σ−2y⊤Φθ+θ⊤σ−2Φ⊤Φθ+θ⊤S−1θ
− 2 − 0 (9.46a)
2m⊤S−1θ+m⊤S−1m (cid:1)
− 0 0 0 0 0
= 1(cid:0) θ⊤(σ−2Φ⊤Φ+S−1)θ 2(σ−2Φ⊤y+S−1m )⊤θ(cid:1) +const,
− 2 0 − 0 0
(9.46b)
where the constant contains the black terms in (9.46a), which are inde-
pendent of θ. The orange terms are terms that are linear in θ, and the
blue terms are the ones that are quadratic in θ. Inspecting (9.46b), we
find that this equation is quadratic in θ. The fact that the unnormalized
log-posterior distribution is a (negative) quadratic form implies that the
posteriorisGaussian,i.e.,
p(θ , ) = exp(logp(θ , )) exp(logp( ,θ)+logp(θ))
|X Y |X Y ∝ Y|X
(9.47a)
exp(cid:16) 1(cid:0)
θ⊤(σ−2Φ⊤Φ+S−1)θ 2(σ−2Φ⊤y+S−1m
)⊤θ(cid:1)(cid:17)
,
∝ − 2 0 − 0 0
(9.47b)
whereweused(9.46b)inthelastexpression.
Theremainingtaskisittobringthis(unnormalized)Gaussianintothe
(cid:0) (cid:1)
form that is proportional to θ m , S , i.e., we need to identify the
N N
N |
mean m and the covariance matrix S . To do this, we use the concept
N N
ofcompletingthesquares.Thedesiredlog-posterioris completingthe
squares
log (cid:0) θ m , S (cid:1) = 1 (θ m )⊤S−1(θ m )+const (9.48a)
N | N N −2 − N N − N
= 1(cid:0) θ⊤S−1θ 2m⊤S−1θ+m⊤S−1m (cid:1) . (9.48b)
−2 N − N N N N N
Here, we factorized the quadratic form (θ −m N)⊤S− N1(θ −m N) into a Sin (cid:0)cep(θ|X, (cid:1)Y)=
termthatisquadraticinθalone(blue),atermthatislinearinθ(orange), N mN,SN ,it
holdsthat
and a constant term (black). This allows us now to find S and m by
N N
matchingthecoloredexpressionsin(9.46b)and(9.48b),whichyields
θ MAP=mN.
S−1 = Φ⊤σ−2IΦ+S−1 (9.49a)
N 0
S = (σ−2Φ⊤Φ+S−1)−1 (9.49b)
⇐⇒ N 0
and
m⊤S−1 = (σ−2Φ⊤y+S−1m )⊤ (9.50a)
N N 0 0
m = S (σ−2Φ⊤y+S−1m ). (9.50b)
⇐⇒ N N 0 0
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
308 LinearRegression
Remark (General Approach to Completing the Squares). If we are given
anequation
x⊤Ax 2a⊤x+const , (9.51)
1
−
where A is symmetric and positive definite, which we wish to bring into
theform
(x µ)⊤Σ(x µ)+const , (9.52)
2
− −
wecandothisbysetting
Σ := A, (9.53)
µ := Σ−1a (9.54)
andconst = const µ⊤Σµ.
2 1
− ♢
We can see that the terms inside the exponential in (9.47b) are of the
form(9.51)with
A := σ−2Φ⊤Φ+S−1, (9.55)
0
a := σ−2Φ⊤y+S−1m . (9.56)
0 0
Since A,a can be difficult to identify in equations like (9.46a), it is of-
ten helpful to bring these equations into the form (9.51) that decouples
quadratic term, linear terms, and constants, which simplifies finding the
desiredsolution.
9.3.4 Posterior Predictions
In (9.37), we computed the predictive distribution of y at a test input
∗
x using the parameter prior p(θ). In principle, predicting with the pa-
∗
rameter posterior p(θ , ) is not fundamentally different given that
|X Y
in our conjugate model the prior and posterior are both Gaussian (with
different parameters). Therefore, by following the same reasoning as in
Section9.3.2,weobtainthe(posterior)predictivedistribution
(cid:90)
p(y , ,x ) = p(y x ,θ)p(θ , )dθ (9.57a)
∗ ∗ ∗ ∗
|X Y | |X Y
(cid:90)
= (cid:0) y ϕ⊤(x )θ, σ2(cid:1) (cid:0) θ m , S (cid:1) dθ (9.57b)
∗ ∗ N N
N | N |
= (cid:0) y ϕ⊤(x )m , ϕ⊤(x )S ϕ(x )+σ2(cid:1) . (9.57c)
∗ ∗ N ∗ N ∗
N |
E[y∗|X,Y,x∗]= The term ϕ⊤(x ∗)S Nϕ(x ∗) reflects the posterior uncertainty associated
ϕ⊤(x∗)mN = with the parameters θ. Note that S
N
depends on the training inputs
ϕ⊤(x∗)θ MAP. through Φ; see (9.43b). The predictive mean ϕ⊤(x )m coincides with
∗ N
thepredictionsmadewiththeMAPestimateθ .
MAP
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
9.3 BayesianLinearRegression 309
Remark (Marginal Likelihood and Posterior Predictive Distribution). By
replacingtheintegralin(9.57a),thepredictivedistributioncanbeequiv-
alently written as the expectation E [p(y x ,θ)], where the expec-
θ|X,Y ∗ ∗
|
tationistakenwithrespecttotheparameterposteriorp(θ , ).
|X Y
Writing the posterior predictive distribution in this way highlights a
close resemblance to the marginal likelihood (9.42). The key difference
between the marginal likelihood and the posterior predictive distribution
are (i) the marginal likelihood can be thought of predicting the training
targets y and not the test targets y , and (ii) the marginal likelihood av-
∗
erages with respect to the parameter prior and not the parameter poste-
rior.
♢
Remark (Mean and Variance of Noise-Free Function Values). In many
cases, we are not interested in the predictive distribution p(y , ,x )
∗ ∗
|X Y
of a (noisy) observation y . Instead, we would like to obtain the distribu-
∗
tion of the (noise-free) function values f(x ) = ϕ⊤(x )θ. We determine
∗ ∗
the corresponding moments by exploiting the properties of means and
variances,whichyields
E[f(x ) , ] = E [ϕ⊤(x )θ , ] = ϕ⊤(x )E [θ , ]
∗ θ ∗ ∗ θ
|X Y |X Y |X Y (9.58)
= ϕ⊤(x )m = m⊤ϕ(x ),
∗ N N ∗
V [f(x ) , ] = V [ϕ⊤(x )θ , ]
θ ∗ θ ∗
|X Y |X Y
= ϕ⊤(x )V [θ , ]ϕ(x ) (9.59)
∗ θ ∗
|X Y
= ϕ⊤(x )S ϕ(x ).
∗ N ∗
We see that the predictive mean is the same as the predictive mean for
noisy observations as the noise has mean 0, and the predictive variance
onlydiffersbyσ2,whichisthevarianceofthemeasurementnoise:When
we predict noisy function values, we need to include σ2 as a source of
uncertainty, but this term is not needed for noise-free predictions. Here,
theonlyremaininguncertaintystemsfromtheparameterposterior.
♢ Integratingout
Remark (Distribution over Functions). The fact that we integrate out the parametersinduces
parameters θ induces a distribution over functions: If we sample θ adistributionover
i
p(θ , ) from the parameter posterior, we obtain a single function r∼ e- functions.
aliza| tX ionY θ⊤ϕ( ). The mean function, i.e., the set of all expected function meanfunction
values E [i f( )· θ, , ], of this distribution over functions is m⊤ϕ( ).
θ · | X Y N ·
The(marginal)variance,i.e.,thevarianceofthefunctionf( ),isgivenby
ϕ⊤( )S ϕ( ). ·
N
· · ♢
Example 9.8 (Posterior over Functions)
Let us revisit the Bayesian linear regression problem with polynomials
of degree 5. We choose a parameter prior p(θ) =
(cid:0)
0,
1I(cid:1)
. Figure 9.9
N 4
visualizes the prior over functions induced by the parameter prior and
samplefunctionsfromthisprior.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
310 LinearRegression
Figure 9.10 shows the posterior over functions that we obtain via
Bayesian linear regression. The training dataset is shown in panel (a);
panel (b) shows the posterior distribution over functions, including the
functions we would obtain via maximum likelihood and MAP estimation.
The function we obtain using the MAP estimate also corresponds to the
posteriormeanfunctionintheBayesianlinearregressionsetting.Panel(c)
shows some plausible realizations (samples) of functions under that pos-
terioroverfunctions.
Figure9.10
Bayesianlinear 4 4 4
regressionand
2 2 2
posteriorover
0 0 0 functions. Trainingdata
(a)trainingdata; −2 −2 M ML AE
P
−2
(b)posterior 4 4 BLR 4
− − −
distributionover 4 2 0 2 4 4 2 0 2 4 4 2 0 2 4
− − x − − x − − x
functions;
(a)Trainingdata. (b)Posterioroverfunctionsrep-(c)Samplesfromtheposterior
(c)Samplesfrom
resentedbythemarginaluncer-over functions, which are in-
theposteriorover
tainties (shaded) showing theducedbythesamplesfromthe
functions.
67% and 95% predictive con-parameterposterior.
fidence bounds, the maximum
likelihood estimate (MLE) and
the MAP estimate (MAP), the
latter of which is identical to
theposteriormeanfunction.
Figure 9.11 shows some posterior distributions over functions induced
by the parameter posterior. For different polynomial degrees M, the left
panels show the maximum likelihood function θ⊤ ϕ( ), the MAP func-
tionθ⊤ ϕ( )(whichisidenticaltotheposteriormM eL an· function),andthe
MAP ·
67% and 95% predictive confidence bounds obtained by Bayesian linear
regression,representedbytheshadedareas.
Therightpanelsshowsamplesfromtheposterioroverfunctions:Here,
we sampled parameters θ from the parameter posterior and computed
i
the function ϕ⊤(x )θ , which is a single realization of a function under
∗ i
the posterior distribution over functions. For low-order polynomials, the
parameter posterior does not allow the parameters to vary much: The
sampled functions are nearly identical. When we make the model more
flexible by adding more parameters (i.e., we end up with a higher-order
polynomial),theseparametersarenotsufficientlyconstrainedbythepos-
terior,andthesampledfunctionscanbeeasilyvisuallyseparated.Wealso
seeinthecorrespondingpanelsonthelefthowtheuncertaintyincreases,
especiallyattheboundaries.
Althoughforaseventh-orderpolynomialtheMAPestimateyieldsarea-
sonablefit,theBayesianlinearregressionmodeladditionallytellsusthat
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
y y y
9.3 BayesianLinearRegression 311
Figure9.11
4 4 Bayesianlinear
regression.Left
2 2 panels:Shaded
areasindicatethe
0 0 67%(darkgray)
Trainingdata
and95%(light
2 MLE 2
− − gray)predictive
MAP
confidencebounds.
4 BLR 4
− − Themeanofthe
4 2 0 2 4 4 2 0 2 4 Bayesianlinear
− − x − − x
regressionmodel
(a)PosteriordistributionforpolynomialsofdegreeM =3(left)andsamplesfromthepos- coincideswiththe
terioroverfunctions(right). MAPestimate.The
predictive
uncertaintyisthe
4 4 sumofthenoise
termandthe
2 2
posteriorparameter
uncertainty,which
0 0
dependsonthe
Trainingdata
locationofthetest
2 MLE 2
− MAP − input.Rightpanels:
4 BLR 4 sampledfunctions
− − fromtheposterior
4 2 0 2 4 4 2 0 2 4
− − x − − x distribution.
(b) Posterior distribution for polynomials of degree M = 5 (left) and samples from the
posterioroverfunctions(right).
4 Trainingdata 4
MLE
2 MAP 2
BLR
0 0
2 2
− −
4 4
− −
4 2 0 2 4 4 2 0 2 4
− − x − − x
(c)PosteriordistributionforpolynomialsofdegreeM =7(left)andsamplesfromthepos-
terioroverfunctions(right).
the posterior uncertainty is huge. This information can be critical when
we use these predictions in a decision-making system, where bad deci-
sions can have significant consequences (e.g., in reinforcement learning
orrobotics).
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
y
y
y
y
y
y
312 LinearRegression
9.3.5 Computing the Marginal Likelihood
InSection8.6.2,wehighlightedtheimportanceofthemarginallikelihood
for Bayesian model selection. In the following, we compute the marginal
likelihood for Bayesian linear regression with a conjugate Gaussian prior
ontheparameters,i.e.,exactlythesettingwehavebeendiscussinginthis
chapter.
Justtorecap,weconsiderthefollowinggenerativeprocess:
(cid:0) (cid:1)
θ m , S (9.60a)
0 0
∼ N
y x ,θ (cid:0) x⊤θ, σ2(cid:1) , (9.60b)
n | n ∼ N n
Themarginal n = 1,...,N.Themarginallikelihoodisgivenby
likelihoodcanbe (cid:90)
interpretedasthe p( ) = p( ,θ)p(θ)dθ (9.61a)
expectedlikelihood Y|X Y|X
undertheprior,i.e., (cid:90)
E θ[p(Y|X,θ)]. =
N(cid:0)
y |Xθ,
σ2I(cid:1) N(cid:0)
θ |m 0, S
0(cid:1)
dθ, (9.61b)
whereweintegrateoutthemodelparametersθ.Wecomputethemarginal
likelihood in two steps: First, we show that the marginal likelihood is
Gaussian (as a distribution in y); second, we compute the mean and co-
varianceofthisGaussian.
1. ThemarginallikelihoodisGaussian:FromSection6.5.2,weknowthat
(i)theproductoftwoGaussianrandomvariablesisan(unnormalized)
Gaussian distribution, and (ii) a linear transformation of a Gaussian
randomvariableisGaussiandistributed.In(9.61b),werequirealinear
(cid:0) (cid:1) (cid:0) (cid:1)
transformationtobring y Xθ, σ2I intotheform θ µ, Σ for
N | N |
someµ,Σ.Oncethisisdone,theintegralcanbesolvedinclosedform.
The result is the normalizing constant of the product of the two Gaus-
sians.ThenormalizingconstantitselfhasGaussianshape;see(6.76).
2. Mean and covariance. We compute the mean and covariance matrix
ofthemarginallikelihoodbyexploitingthestandardresultsformeans
andcovariancesofaffinetransformationsofrandomvariables;seeSec-
tion6.4.4.Themeanofthemarginallikelihoodiscomputedas
E[ ] = E [Xθ+ϵ] = XE [θ] = Xm . (9.62)
θ,ϵ θ 0
Y|X
(cid:0) (cid:1)
Note that ϵ 0, σ2I is a vector of i.i.d. random variables. The
∼ N
covariancematrixisgivenas
Cov[ ] = Cov [Xθ+ϵ] = Cov [Xθ]+σ2I (9.63a)
θ,ϵ θ
Y|X
= XCov [θ]X⊤+σ2I = XS X⊤+σ2I. (9.63b)
θ 0
Hence,themarginallikelihoodis
N 1
p( ) = (2π)− 2 det(XS 0X⊤+σ2I)− 2 (9.64a)
Y|X
exp(cid:0) 1(y Xm )⊤(XS X⊤+σ2I)−1(y Xm )(cid:1)
· − 2 − 0 0 − 0
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
9.4 MaximumLikelihoodasOrthogonalProjection 313
4 4 Figure9.12
Geometric
interpretationof
2 2
leastsquares.
(a)Dataset;
0 0
(b)maximum
Projection likelihoodsolution
2 2
− − Observations interpretedasa
Maximumlikelihoodestimate
projection.
4 4
− 4 2 0 2 4 − 4 2 0 2 4
− − x − − x
(a) Regression dataset consisting of noisy ob- (b) The orange dots are the projections of
servationsyn (blue)offunctionvaluesf(xn) the noisy observations (blue dots) onto the
atinputlocationsxn. lineθ MLx.Themaximumlikelihoodsolutionto
a linear regression problem finds a subspace
(line) onto which the overall projection er-
ror(orangelines)oftheobservationsismini-
mized.
=
(cid:0)
y Xm , XS
X⊤+σ2I(cid:1)
. (9.64b)
0 0
N |
Given the close connection with the posterior predictive distribution (see
Remark on Marginal Likelihood and Posterior Predictive Distribution ear-
lierinthissection),thefunctionalformofthemarginallikelihoodshould
notbetoosurprising.
9.4 Maximum Likelihood as Orthogonal Projection
Having crunched through much algebra to derive maximum likelihood
and MAP estimates, we will now provide a geometric interpretation of
maximumlikelihoodestimation.Letusconsiderasimplelinearregression
setting
y = xθ+ϵ, ϵ
(cid:0)
0,
σ2(cid:1)
, (9.65)
∼ N
in which we consider linear functions f : R R that go through the
→
origin(weomitfeatureshereforclarity).Theparameterθdeterminesthe
slopeoftheline.Figure9.12(a)showsaone-dimensionaldataset.
With a training data set (x ,y ),...,(x ,y ) we recall the results
1 1 N N
{ }
from Section 9.2.1 and obtain the maximum likelihood estimator for the
slopeparameteras
X⊤y
θ = (X⊤X)−1X⊤y = R, (9.66)
ML X⊤X ∈
whereX = [x ,...,x ]⊤ RN,y = [y ,...,y ]⊤ RN.
1 N 1 N
∈ ∈
ThismeansforthetraininginputsX weobtaintheoptimal(maximum
likelihood)reconstructionofthetrainingtargetsas
X⊤y XX⊤
Xθ = X = y, (9.67)
ML X⊤X X⊤X
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
y y
314 LinearRegression
i.e., we obtain the approximation with the minimum least-squares error
betweeny andXθ.
As we are looking for a solution of y = Xθ, we can think of linear
Linearregression regression as a problem for solving systems of linear equations. There-
canbethoughtofas fore,wecanrelatetoconceptsfromlinearalgebraandanalyticgeometry
amethodforsolving
that we discussed in Chapters 2 and 3. In particular, looking carefully
systemsoflinear
at (9.67) we see that the maximum likelihood estimator θ in our ex-
equations. ML
ample from (9.65) effectively does an orthogonal projection of y onto
Maximum theone-dimensionalsubspacespannedbyX.Recallingtheresultsonor-
likelihoodlinear thogonalprojectionsfromSection3.8,weidentify XX⊤ astheprojection
regressionperforms
X⊤X
matrix,θ asthecoordinatesoftheprojectionontotheone-dimensional
anorthogonal ML
subspace of RN spanned by X and Xθ as the orthogonal projection of
projection. ML
y ontothissubspace.
Therefore, the maximum likelihood solution provides also a geometri-
cally optimal solution by finding the vectors in the subspace spanned by
X that are “closest” to the corresponding observations y, where “clos-
est” means the smallest (squared) distance of the function values y to
n
x θ.Thisisachievedbyorthogonalprojections.Figure9.12(b)showsthe
n
projectionofthenoisyobservationsontothesubspacethatminimizesthe
squareddistancebetweentheoriginaldatasetanditsprojection(notethat
the x-coordinate is fixed), which corresponds to the maximum likelihood
solution.
Inthegenerallinearregressioncasewhere
y = ϕ⊤(x)θ+ϵ, ϵ (cid:0) 0, σ2(cid:1) (9.68)
∼ N
withvector-valuedfeaturesϕ(x) RK,weagaincaninterpretthemaxi-
∈
mumlikelihoodresult
y Φθ , (9.69)
ML
≈
θ = (Φ⊤Φ)−1Φ⊤y (9.70)
ML
as a projection onto a K-dimensional subspace of RN, which is spanned
bythecolumnsofthefeaturematrixΦ;seeSection3.8.2.
If the feature functions ϕ that we use to construct the feature ma-
k
trix Φ are orthonormal (see Section 3.7), we obtain a special case where
the columns of Φ form an orthonormal basis (see Section 3.5), such that
Φ⊤Φ = I.Thiswillthenleadtotheprojection
(cid:32) (cid:33)
K
(cid:88)
Φ(Φ⊤Φ)−1Φ⊤y = ΦΦ⊤y = ϕ ϕ⊤ y (9.71)
k k
k=1
sothatthemaximumlikelihoodprojectionissimplythesumofprojections
of y onto the individual basis vectors ϕ , i.e., the columns of Φ. Further-
k
more,thecouplingbetweendifferentfeatureshasdisappearedduetothe
orthogonalityofthebasis.Manypopularbasisfunctionsinsignalprocess-
ing, such as wavelets and Fourier bases, are orthogonal basis functions.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
9.5 FurtherReading 315
When the basis is not orthogonal, one can convert a set of linearly inde-
pendentbasisfunctionstoanorthogonalbasisbyusingtheGram-Schmidt
process;seeSection3.8.3and(Strang,2003).
9.5 Further Reading
In this chapter, we discussed linear regression for Gaussian likelihoods
and conjugate Gaussian priors on the parameters of the model. This al-
lowed for closed-form Bayesian inference. However, in some applications
we may want to choose a different likelihood function. For example, in
a binary classification setting, we observe only two possible (categorical) classification
outcomes, and a Gaussian likelihood is inappropriate in this setting. In-
stead,wecanchooseaBernoullilikelihoodthatwillreturnaprobabilityof
thepredictedlabeltobe1(or0).WerefertothebooksbyBarber(2012),
Bishop(2006),andMurphy(2012)foranin-depthintroductiontoclassifi-
cationproblems.Adifferentexamplewherenon-Gaussianlikelihoodsare
importantiscountdata.Countsarenon-negativeintegers,andinthiscase
aBinomialorPoissonlikelihoodwouldbeabetterchoicethanaGaussian.
Alltheseexamplesfallintothecategoryofgeneralizedlinearmodels,aflex- generalizedlinear
ible generalization of linear regression that allows for response variables model
thathaveerrordistributionsotherthanaGaussiandistribution.TheGLM Generalizedlinear
generalizes linear regression by allowing the linear model to be related modelsarethe
totheobservedvaluesviaasmoothandinvertiblefunctionσ( )thatmay buildingblocksof
be nonlinear so that y = σ(f(x)), where f(x) = θ⊤ϕ(x) is· the linear deepneural
networks.
regression model from (9.13). We can therefore think of a generalized
linear model in terms of function composition y = σ f, where f is a
◦
linearregressionmodelandσ theactivationfunction.Notethatalthough
we are talking about “generalized linear models”, the outputs y are no
longer linear in the parameters θ. In logistic regression, we choose the logisticregression
logistic sigmoid σ(f) = 1 [0,1], which can be interpreted as the logisticsigmoid
1+exp(−f) ∈
probability of observing y = 1 of a Bernoulli random variable y 0,1 .
∈ { }
The function σ( ) is called transfer function or activation function, and its transferfunction
·
inverse is called the canonical link function. From this perspective, it is activationfunction
alsoclearthatgeneralizedlinearmodelsarethebuildingblocksof(deep) canonicallink
function
feedforward neural networks: If we consider a generalized linear model
Forordinarylinear
y = σ(Ax+b),whereAisaweightmatrixandbabiasvector,weiden-
regressionthe
tify this generalized linear model as a single-layer neural network with activationfunction
activationfunctionσ( ).Wecannowrecursivelycomposethesefunctions wouldsimplybethe
· identity.
via
Agreatpostonthe
x = f (x )
k+1 k k (9.72) relationbetween
f (x ) = σ (A x +b ) GLMsanddeep
k k k k k k
networksis
for k = 0,...,K 1, where x are the input features and x = y are availableat
0 K
−
the observed outputs, such that f f is a K-layer deep neural https://tinyurl.
K−1 ◦···◦ 0 com/glm-dnn.
network. Therefore, the building blocks of this deep neural network are
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
316 LinearRegression
thegeneralizedlinearmodelsdefinedin(9.72).Neuralnetworks(Bishop,
1995;Goodfellowetal.,2016)aresignificantlymoreexpressiveandflexi-
blethanlinearregressionmodels.However,maximumlikelihoodparame-
terestimationisanon-convexoptimizationproblem,andmarginalization
oftheparametersinafullyBayesiansettingisanalyticallyintractable.
We briefly hinted at the fact that a distribution over parameters in-
Gaussianprocess duces a distribution over regression functions. Gaussian processes (Ras-
mussen and Williams, 2006) are regression models where the concept of
a distribution over function is central. Instead of placing a distribution
over parameters, a Gaussian process places a distribution directly on the
space of functions without the “detour” via the parameters. To do so, the
kerneltrick Gaussian process exploits the kernel trick (Scho¨lkopf and Smola, 2002),
which allows us to compute inner products between two function values
f(x ),f(x ) only by looking at the corresponding input x ,x . A Gaus-
i j i j
sian process is closely related to both Bayesian linear regression and sup-
port vector regression but can also be interpreted as a Bayesian neural
network with a single hidden layer where the number of units tends to
infinity(Neal,1996;Williams,1997).ExcellentintroductionstoGaussian
processes can be found in MacKay (1998) and Rasmussen and Williams
(2006).
WefocusedonGaussianparameterpriorsinthediscussionsinthischap-
ter,becausetheyallowforclosed-forminferenceinlinearregressionmod-
els. However, even in a regression setting with Gaussian likelihoods, we
maychooseanon-Gaussianprior.Considerasetting,wheretheinputsare
x RD andourtrainingsetissmallandofsizeN D.Thismeansthat
∈ ≪
the regression problem is underdetermined. In this case, we can choose
a parameter prior that enforces sparsity, i.e., a prior that tries to set as
variableselection many parameters to 0 as possible (variable selection). This prior provides
astrongerregularizerthantheGaussianprior,whichoftenleadstoanin-
creasedpredictionaccuracyandinterpretabilityofthemodel.TheLaplace
prior is one example that is frequently used for this purpose. A linear re-
gression model with the Laplace prior on the parameters is equivalent to
LASSO linear regression with L1 regularization (LASSO) (Tibshirani, 1996). The
Laplacedistributionissharplypeakedatzero(itsfirstderivativeisdiscon-
tinuous) and it concentrates its probability mass closer to zero than the
Gaussian distribution, which encourages parameters to be 0. Therefore,
the nonzero parameters are relevant for the regression problem, which is
thereasonwhywealsospeakof“variableselection”.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10
Dimensionality Reduction with Principal
Component Analysis
Workingdirectlywithhigh-dimensionaldata,suchasimages,comeswith A640×480pixel
somedifficulties:Itishardtoanalyze,interpretationisdifficult,visualiza- colorimageisadata
pointina
tion is nearly impossible, and (from a practical point of view) storage of
million-dimensional
the data vectors can be expensive. However, high-dimensional data often
space,whereevery
haspropertiesthatwecanexploit.Forexample,high-dimensionaldatais pixelrespondsto
often overcomplete, i.e., many dimensions are redundant and can be ex- threedimensions,
plained by a combination of other dimensions. Furthermore, dimensions oneforeachcolor
channel(red,green,
inhigh-dimensionaldataareoftencorrelatedsothatthedatapossessesan
blue).
intrinsic lower-dimensional structure. Dimensionality reduction exploits
structureandcorrelationandallowsustoworkwithamorecompactrep-
resentation of the data, ideally without losing information. We can think
ofdimensionalityreductionasacompressiontechnique,similartojpegor
mp3,whicharecompressionalgorithmsforimagesandmusic.
In this chapter, we will discuss principal component analysis (PCA), an principalcomponent
algorithm for linear dimensionality reduction. PCA, proposed by Pearson analysis
(1901) and Hotelling (1933), has been around for more than 100 years PCA
dimensionality
and is still one of the most commonly used techniques for data compres-
reduction
sion and data visualization. It is also used for the identification of simple
patterns, latent factors, and structures of high-dimensional data. In the
Figure10.1
Illustration:
4 4
dimensionality
reduction.(a)The
2 2 originaldataset
doesnotvarymuch
0 0 alongthex2
direction.(b)The
datafrom(a)canbe
2 2
− − representedusing
thex1-coordinate
4 4 alonewithnearlyno
− −
loss.
5.0 2.5 0.0 2.5 5.0 5.0 2.5 0.0 2.5 5.0
− − x1 − − x1
(a)Datasetwithx1andx2coordinates. (b)Compresseddatasetwhereonlythex1coor-
dinateisrelevant.
317
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
2x 2x
318 DimensionalityReductionwithPrincipalComponentAnalysis
Karhunen-Lo`eve signal processing community, PCA is also known as the Karhunen-Lo`eve
transform transform.Inthischapter,wederivePCAfromfirstprinciples,drawingon
our understanding of basis and basis change (Sections 2.6.1 and 2.7.2),
projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distribu-
tions(Section6.5),andconstrainedoptimization(Section7.2).
Dimensionality reduction generally exploits a property of high-dimen-
sionaldata(e.g.,images)thatitoftenliesonalow-dimensionalsubspace.
Figure 10.1 gives an illustrative example in two dimensions. Although
the data in Figure 10.1(a) does not quite lie on a line, the data does not
vary much in the x -direction, so that we can express it as if it were on
2
a line – with nearly no loss; see Figure 10.1(b). To describe the data in
Figure 10.1(b), only the x -coordinate is required, and the data lies in a
1
one-dimensionalsubspaceofR2.
10.1 Problem Setting
In PCA,we areinterested infinding projectionsx˜ ofdata pointsx that
n n
areassimilartotheoriginaldatapointsaspossible,butwhichhaveasig-
nificantly lower intrinsic dimensionality. Figure 10.1 gives an illustration
ofwhatthiscouldlooklike.
Moreconcretely,weconsiderani.i.d.dataset = x ,...,x ,x
1 N n
datacovariance
RD,withmean0thatpossessesthedatacovariaX ncem{
atrix
(6.42)} ∈
matrix
N
1 (cid:88)
S = x x⊤. (10.1)
N n n
n=1
Furthermore, we assume there exists a low-dimensional compressed rep-
resentation(code)
z = B⊤x RM (10.2)
n n
∈
ofx ,wherewedefinetheprojectionmatrix
n
B := [b ,...,b ] RD×M . (10.3)
1 M
∈
WeassumethatthecolumnsofBareorthonormal(Definition3.7)sothat
T b1h ,e .c .o .l ,u bm Mns
ofB
sb u⊤ i bb sj pa= ce0 Uif an Rd Do ,n dly imif (Ui ̸=
)
=j Mand <b D⊤ i b oi n= to1 w. hW ice hs wee ek pa ron jeM ct- td hi em de an ts aio .n Wa el
formabasisofthe denotethep⊆ rojecteddatabyx˜ U,andtheircoordinates(withrespect
n
M-dimensional ∈
tothebasisvectorsb ,...,b ofU)byz .Ouraimistofindprojections
subspaceinwhich 1 M n
theprojecteddata x˜ n RD (or equivalently the codes z n and the basis vectors b 1,...,b M)
∈
x˜ =BB⊤x∈RD so that they are as similar to the original data x and minimize the loss
n
live. duetocompression.
Example 10.1 (Coordinate Representation/Code)
Consider R2 with the canonical basis e = [1,0]⊤, e = [0,1]⊤. From
1 2
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.1 ProblemSetting 319
Original Reconstructed Figure10.2
RD RD Graphical
Compressed illustrationofPCA.
InPCA,wefinda
RM
compressedversion
x z x˜ zoforiginaldatax.
Thecompressed
datacanbe
reconstructedinto
x˜,whichlivesinthe
originaldataspace,
buthasanintrinsic
lower-dimensional
Chapter2, weknow thatx R2 canbe representedas alinear combina- representationthan
tionofthesebasisvectors,e∈ .g., x.
(cid:20) (cid:21)
5
= 5e +3e . (10.4)
3 1 2
However,whenweconsidervectorsoftheform
(cid:20) (cid:21)
0
x˜ = R2, z R, (10.5)
z ∈ ∈
they can always be written as 0e +ze . To represent these vectors it is
1 2
sufficient to remember/store the coordinate/code z of x˜ with respect to
thee 2 vector. Thedimensionofa
More precisely, the set of x˜ vectors (with the standard vector addition vectorspace
correspondstothe
and scalar multiplication) forms a vector subspace U (see Section 2.4)
numberofitsbasis
withdim(U) = 1becauseU = span[e 2]. vectors(see
Section2.6.1).
In Section 10.2, we will find low-dimensional representations that re-
tain as much information as possible and minimize the compression loss.
An alternative derivation of PCA is given in Section 10.3, where we will
2
belookingatminimizingthesquaredreconstructionerror x x˜ be-
n n
∥ − ∥
tweentheoriginaldatax anditsprojectionx˜ .
n n
Figure 10.2 illustrates the setting we consider in PCA, where z repre-
sentsthelower-dimensionalrepresentationofthecompresseddatax˜ and
plays the role of a bottleneck, which controls how much information can
flowbetweenxandx˜.InPCA,weconsideralinearrelationshipbetween
the original data x and its low-dimensional code z so that z = B⊤x and
x˜ = Bz for a suitable matrix B. Based on the motivation of thinking
of PCA as a data compression technique, we can interpret the arrows in
Figure 10.2 as a pair of operations representing encoders and decoders.
The linear mapping represented by B can be thought of as a decoder,
whichmapsthelow-dimensionalcodez RM backintotheoriginaldata
spaceRD.Similarly,B⊤ canbethoughto∈ fanencoder,whichencodesthe
originaldataxasalow-dimensional(compressed)codez.
Throughout this chapter, we will use the MNIST digits dataset as a re-
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
320 DimensionalityReductionwithPrincipalComponentAnalysis
Figure10.3
Examplesof
handwrittendigits
fromtheMNIST
dataset.http:
//yann.lecun.
com/exdb/mnist/.
occurringexample,whichcontains60,000examplesofhandwrittendigits
0through9.Eachdigitisagrayscaleimageofsize28 28,i.e.,itcontains
×
784pixelssothatwecaninterpreteveryimageinthisdatasetasavector
x R784.ExamplesofthesedigitsareshowninFigure10.3.
∈
10.2 Maximum Variance Perspective
Figure 10.1 gave an example of how a two-dimensional dataset can be
represented using a single coordinate. In Figure 10.1(b), we chose to ig-
nore the x -coordinate of the data because it did not add too much in-
2
formation so that the compressed data is similar to the original data in
Figure 10.1(a). We could have chosen to ignore the x -coordinate, but
1
thenthecompresseddatahadbeenverydissimilarfromtheoriginaldata,
andmuchinformationinthedatawouldhavebeenlost.
If we interpret information content in the data as how “space filling”
thedatasetis,thenwecandescribetheinformationcontainedinthedata
bylookingatthespreadofthedata.FromSection6.4.1,weknowthatthe
varianceisanindicatorofthespreadofthedata,andwecanderivePCAas
a dimensionality reduction algorithm that maximizes the variance in the
low-dimensionalrepresentationofthedatatoretainasmuchinformation
aspossible.Figure10.4illustratesthis.
Considering the setting discussed in Section 10.1, our aim is to find
a matrix B (see (10.3)) that retains as much information as possible
when compressing data by projecting it onto the subspace spanned by
thecolumnsb ,...,b ofB.Retainingmostinformationafterdatacom-
1 M
pression is equivalent to capturing the largest amount of variance in the
low-dimensionalcode(Hotelling,1933).
Remark. (Centered Data) For the data covariance matrix in (10.1), we
assumedcentereddata.Wecanmakethisassumptionwithoutlossofgen-
erality:Letusassumethatµisthemeanofthedata.Usingtheproperties
ofthevariance,whichwediscussedinSection6.4.4,weobtain
V [z] = V [B⊤(x µ)] = V [B⊤x B⊤µ] = V [B⊤x], (10.6)
z x x x
− −
i.e., the variance of the low-dimensional code does not depend on the
meanofthedata.Therefore,weassumewithoutlossofgeneralitythatthe
data has mean 0 for the remainder of this section. With this assumption
themeanofthelow-dimensionalcodeisalso0sinceE [z] = E [B⊤x] =
z x
B⊤E [x] = 0.
x
♢
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.2 MaximumVariancePerspective 321
Figure10.4 PCA
findsa
lower-dimensional
subspace(line)that
maintainsasmuch
variance(spreadof
thedata)aspossible
whenthedata
(blue)isprojected
ontothissubspace
(orange).
10.2.1 Direction with Maximal Variance
Wemaximizethevarianceofthelow-dimensionalcodeusingasequential
approach.Westartbyseekingasinglevectorb 1 RD thatmaximizesthe Thevectorb1will
∈
variance of the projected data, i.e., we aim to maximize the variance of bethefirstcolumn
thefirstcoordinatez ofz RM sothat ofthematrixBand
1
∈ thereforethefirstof
M orthonormal
N
V := V[z ] = 1 (cid:88) z2 (10.7) basisvectorsthat
1 1 N 1n spanthe
n=1 lower-dimensional
subspace.
is maximized, where we exploited the i.i.d. assumption of the data and
defined z as the first coordinate of the low-dimensional representation
1n
z RM ofx RD.Notethatfirstcomponentofz isgivenby
n n n
∈ ∈
z = b⊤x , (10.8)
1n 1 n
i.e., it is the coordinate of the orthogonal projection of x onto the one-
n
dimensional subspace spanned by b (Section 3.8). We substitute (10.8)
1
into(10.7),whichyields
N N
1 (cid:88) 1 (cid:88)
V = (b⊤x )2 = b⊤x x⊤b (10.9a)
1 N 1 n N 1 n n 1
n=1 n=1
(cid:32) (cid:33)
N
1 (cid:88)
= b⊤ x x⊤ b = b⊤Sb , (10.9b)
1 N n n 1 1 1
n=1
where S is the data covariance matrix defined in (10.1). In (10.9a), we
have used the fact that the dot product of two vectors is symmetric with
respecttoitsarguments,thatis,b⊤x = x⊤b .
1 n n 1
Notice that arbitrarily increasing the magnitude of the vector b in-
1
creases V , that is, a vector b that is two times longer can result in V
1 1 1
that is potentially four times larger. Therefore, we restrict all solutions to ∥b1∥2=1
b
1
2 = 1, which results in a constrained optimization problem in which ⇐⇒ ∥b1∥=1.
∥ ∥
weseekthedirectionalongwhichthedatavariesmost.
With the restriction of the solution space to unit vectors the vector b
1
that points in the direction of maximum variance can be found by the
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
322 DimensionalityReductionwithPrincipalComponentAnalysis
constrainedoptimizationproblem
maxb⊤Sb
1 1
b1 (10.10)
subjectto b 2 = 1.
1
∥ ∥
FollowingSection7.2,weobtaintheLagrangian
L(b ,λ) = b⊤Sb +λ (1 b⊤b ) (10.11)
1 1 1 1 − 1 1
to solve this constrained optimization problem. The partial derivatives of
Lwithrespecttob andλ are
1 1
∂L ∂L
= 2b⊤S 2λ b⊤, = 1 b⊤b , (10.12)
∂b 1 − 1 1 ∂λ − 1 1
1 1
respectively.Settingthesepartialderivativesto0givesustherelations
Sb = λ b , (10.13)
1 1 1
b⊤b = 1. (10.14)
1 1
By comparing this with the definition of an eigenvalue decomposition
(Section 4.4), we see that b is an eigenvector of the data covariance
1
matrixS,andtheLagrangemultiplierλ playstheroleofthecorrespond-
1
√
Thequantity λ1is ingeigenvalue.Thiseigenvectorproperty(10.13)allowsustorewriteour
alsocalledthe varianceobjective(10.10)as
loadingoftheunit
vectorb1and V
1
= b⊤ 1Sb
1
= λ 1b⊤ 1b
1
= λ 1, (10.15)
representsthe
standarddeviation i.e., the variance of the data projected onto a one-dimensional subspace
ofthedata equalstheeigenvaluethatisassociatedwiththebasisvectorb thatspans
1
accountedforbythe
thissubspace.Therefore,tomaximizethevarianceofthelow-dimensional
principalsubspace
code, we choose the basis vector associated with the largest eigenvalue
span[b1].
ofthedatacovariancematrix.Thiseigenvectoriscalledthefirstprincipal
principalcomponent
component.Wecandeterminetheeffect/contributionoftheprincipalcom-
ponent b in the original data space by mapping the coordinate z back
1 1n
intodataspace,whichgivesustheprojecteddatapoint
x˜ = b z = b b⊤x RD (10.16)
n 1 1n 1 1 n ∈
intheoriginaldataspace.
Remark. Although x˜ is a D-dimensional vector, it only requires a single
n
coordinatez torepresentitwithrespecttothebasisvectorb RD.
1n 1
∈ ♢
10.2.2 M-dimensional Subspace with Maximal Variance
Assumewehavefoundthefirstm 1principalcomponentsasthem 1
− −
eigenvectors of S that are associated with the largest m 1 eigenvalues.
−
SinceS issymmetric,thespectraltheorem(Theorem4.15)statesthatwe
can use these eigenvectors to construct an orthonormal eigenbasis of an
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.2 MaximumVariancePerspective 323
(m 1)-dimensional subspace of RD. Generally, the mth principal com-
−
ponent can be found by subtracting the effect of the first m 1 principal
−
components b ,...,b from the data, thereby trying to find principal
1 m−1
components that compress the remaining information. We then arrive at
thenewdatamatrix
m−1
(cid:88)
Xˆ := X b b⊤X = X B X, (10.17)
− i i − m−1
i=1
where X = [x 1,...,x N] RD×N contains the data points as column ThematrixXˆ :=
tv he ect so ur bs sa pn ad ceB sm pa− n1 n: e= d(cid:80) bym i= b− 1∈1 ,b ..ib .,⊤ i bisap .rojectionmatrixthatprojectsonto [ Rxˆ D1, ×. N.., inxˆ N (1] 0∈
.17)
1 m−1
containsthe
Remark (Notation). Throughout this chapter, we do not follow the con- informationinthe
vention of collecting data x ,...,x as the rows of the data matrix, but datathathasnotyet
1 N
beencompressed.
we define them to be the columns of X. This means that our data ma-
trixX isaD N matrixinsteadoftheconventionalN D matrix.The
× ×
reason for our choice is that the algebra operations work out smoothly
without the need to either transpose the matrix or to redefine vectors as
rowvectorsthatareleft-multipliedontomatrices.
♢
Tofindthemthprincipalcomponent,wemaximizethevariance
N N
1 (cid:88) 1 (cid:88)
V = V[z ] = z2 = (b⊤xˆ )2 = b⊤Sˆb , (10.18)
m m N mn N m n m m
n=1 n=1
2
subject to b = 1, where we followed the same steps as in (10.9b)
m
∥ ∥
and defined Sˆ as the data covariance matrix of the transformed dataset
ˆ := xˆ ,...,xˆ . As previously, when we looked at the first principal
1 N
X { }
component alone, we solve a constrained optimization problem and dis-
coverthattheoptimalsolutionb istheeigenvectorofSˆ thatisassociated
m
withthelargesteigenvalueofSˆ.
Itturnsoutthatb isalsoaneigenvectorofS.Moregenerally,thesets
m
of eigenvectors of S and Sˆ are identical. Since both S and Sˆ are sym-
metric, we can find an ONB of eigenvectors (spectral theorem 4.15), i.e.,
there exist D distinct eigenvectors for both S and Sˆ. Next, we show that
every eigenvector of S is an eigenvector of Sˆ. Assume we have already
found eigenvectors b ,...,b of Sˆ. Consider an eigenvector b of S,
1 m−1 i
i.e.,Sb = λ b .Ingeneral,
i i i
Sˆb = 1 XˆXˆ⊤ b = 1 (X B X)(X B X)⊤b (10.19a)
i N i N − m−1 − m−1 i
= (S SB B S +B SB )b . (10.19b)
m−1 m−1 m−1 m−1 i
− −
We distinguish between two cases. If i ⩾ m, i.e., b is an eigenvector
i
thatisnotamongthefirstm 1principalcomponents,thenb isorthogo-
i
−
naltothefirstm 1principalcomponentsandB b = 0.Ifi < m,i.e.,
m−1 i
−
b isamongthefirstm 1principalcomponents,thenb isabasisvector
i i
−
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
324 DimensionalityReductionwithPrincipalComponentAnalysis
of the principal subspace onto which B projects. Since b ,...,b
m−1 1 m−1
are an ONB of this principal subspace, we obtain B b = b . The two
m−1 i i
casescanbesummarizedasfollows:
B b = b ifi < m, B b = 0 ifi ⩾ m. (10.20)
m−1 i i m−1 i
In the case i ⩾ m, by using (10.20) in (10.19b), we obtain Sˆb = (S
i
−
B S)b = Sb = λ b , i.e., b is also an eigenvector of Sˆ with eigen-
m−1 i i i i i
valueλ .Specifically,
i
Sˆb = Sb = λ b . (10.21)
m m m m
Equation (10.21) reveals that b is not only an eigenvector of S but also
m
of Sˆ. Specifically, λ is the largest eigenvalue of Sˆ and λ is the mth
m m
largesteigenvalueofS,andbothhavetheassociatedeigenvectorb .
m
Inthecasei < m,byusing(10.20)in(10.19b),weobtain
Sˆb = (S SB B S +B SB )b = 0 = 0b (10.22)
i m−1 m−1 m−1 m−1 i i
− −
This means that b ,...,b are also eigenvectors of Sˆ, but they are as-
1 m−1
sociatedwitheigenvalue0sothatb ,...,b spanthenullspaceofSˆ.
1 m−1
Overall, every eigenvector of S is also an eigenvector of Sˆ. However,
if the eigenvectors of S are part of the (m 1) dimensional principal
−
Thisderivation subspace,thentheassociatedeigenvalueofSˆ is0.
showsthatthereis With the relation (10.21) and b⊤b = 1, the variance of the data pro-
m m
anintimate jectedontothemthprincipalcomponentis
connectionbetween
theM-dimensional V = b⊤Sb (10 =.21) λ b⊤b = λ . (10.23)
subspacewith m m m m m m m
maximalvariance This means that the variance of the data, when projected onto an M-
andtheeigenvalue
dimensional subspace, equals the sum of the eigenvalues that are associ-
decomposition.We
ated with the corresponding eigenvectors of the data covariance matrix.
willrevisitthis
connectionin
Section10.4.
Example 10.2 (Eigenvalues of MNIST “8”)
Figure10.5
Propertiesofthe 50
500
trainingdataof
40
MNIST“8”.(a) 400
Eigenvaluessorted 30
indescendingorder; 300
(b)Variance 20
200
capturedbythe
10
principal 100
components 0
0 50 100 150 200 0 50 100 150 200
associatedwiththe
Index Numberofprincipalcomponents
largesteigenvalues.
(a)Eigenvalues(sortedindescendingorder)of (b)Variancecapturedbytheprincipalcompo-
the data covariance matrix of all digits “8” in nents.
theMNISTtrainingset.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
eulavnegiE
ecnairavderutpaC
10.3 ProjectionPerspective 325
Figure10.6
Illustrationofthe
projection
approach:Finda
subspace(line)that
minimizesthe
lengthofthe
differencevector
betweenprojected
(orange)and
original(blue)data.
Takingalldigits“8”intheMNISTtrainingdata,wecomputetheeigen-
valuesofthedatacovariancematrix.Figure10.5(a)showsthe200largest
eigenvalues of the data covariance matrix. We see that only a few of
them have a value that differs significantly from 0. Therefore, most of
thevariance,whenprojectingdataontothesubspacespannedbythecor-
respondingeigenvectors,iscapturedbyonlyafewprincipalcomponents,
asshowninFigure10.5(b).
Overall,tofindanM-dimensionalsubspaceofRD thatretainsasmuch
information as possible, PCA tells us to choose the columns of the matrix
B in (10.3) as the M eigenvectors of the data covariance matrix S that
are associated with the M largest eigenvalues. The maximum amount of
variancePCAcancapturewiththefirstM principalcomponentsis
M
(cid:88)
V = λ , (10.24)
M m
m=1
wheretheλ aretheM largesteigenvaluesofthedatacovariancematrix
m
S.Consequently,thevariancelostbydatacompressionviaPCAis
D
(cid:88)
J := λ = V V . (10.25)
M j D M
−
j=M+1
Instead of these absolute quantities, we can define the relative variance
capturedas VM,andtherelativevariancelostbycompressionas1 VM.
VD − VD
10.3 Projection Perspective
In the following, we will derive PCA as an algorithm that directly mini-
mizes the average reconstruction error. This perspective allows us to in-
terpretPCAasimplementinganoptimallinearauto-encoder.Wewilldraw
heavilyfromChapters2and3.
In the previous section, we derived PCA by maximizing the variance
in the projected space to retain as much information as possible. In the
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
326 DimensionalityReductionwithPrincipalComponentAnalysis
Figure10.7 2.5 2.5
Simplified
projectionsetting. 2.0 2.0
(a)Avectorx∈R2
(redcross)shallbe 1.5 1.5
projectedontoa
one-dimensional 1.0 1.0
subspaceU ⊆R2
U U
spannedbyb.(b) 0.5 0.5
showsthedifference b b
vectorsbetweenx 0.0 0.0
andsome
candidatesx˜.
0.5 0.5
− 1.0 0.5 0.0 0.5 1.0 1.5 2.0 − 1.0 0.5 0.0 0.5 1.0 1.5 2.0
− − x1 − − x1
(a)Setting. (b) Differences x−x˜i for 50 different x˜i are
shownbytheredlines.
following,wewilllookatthedifferencevectorsbetweentheoriginaldata
x andtheirreconstructionx˜ andminimizethisdistancesothatx and
n n n
x˜ areascloseaspossible.Figure10.6illustratesthissetting.
n
10.3.1 Setting and Objective
Assume an (ordered) orthonormal basis (ONB) B = (b ,...,b ) of RD,
1 D
i.e.,b⊤b = 1ifandonlyifi = j and0otherwise.
i j
From Section 2.5 we know that for a basis (b ,...,b ) of RD any x
1 D
RD canbewrittenasalinearcombinationofthebasisvectorsofRD,i.e∈ .,
Vectorsx˜ ∈U could
bevectorsona
D M D
planeinR3.The (cid:88) (cid:88) (cid:88)
x = ζ b = ζ b + ζ b (10.26)
dimensionalityof d d m m j j
theplaneis2,but d=1 m=1 j=M+1
thevectorsstillhave
forsuitablecoordinatesζ R.
threecoordinates d
∈
withrespecttothe We are interested in finding vectors x˜ RD, which live in lower-
standardbasisof dimensionalsubspaceU RD,dim(U) = M∈ ,sothat
R3. ⊆
M
(cid:88)
x˜ = z b U RD (10.27)
m m
∈ ⊆
m=1
is as similar to x as possible. Note that at this point we need to assume
thatthecoordinatesz ofx˜ andζ ofxarenotidentical.
m m
Inthefollowing,weuseexactlythiskindofrepresentationofx˜ tofind
optimal coordinates z and basis vectors b ,...,b such that x˜ is as sim-
1 M
ilar to the original data point x as possible, i.e., we aim to minimize the
(Euclidean)distance x x˜ .Figure10.7illustratesthissetting.
∥ − ∥
Withoutlossofgenerality,weassumethatthedataset = x ,...,x ,
1 N
x RD,iscenteredat0,i.e.,E[ ] = 0.WithoutthezeX ro-m{ eanassump-}
n
∈ X
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2x 2x
10.3 ProjectionPerspective 327
tion,wewouldarriveatexactlythesamesolution,butthenotationwould
besubstantiallymorecluttered.
Weareinterestedinfindingthebestlinearprojectionof ontoalower-
dimensionalsubspaceU ofRD withdim(U) = M andortX honormalbasis
vectors b 1,...,b M. We will call this subspace U the principal subspace. principalsubspace
Theprojectionsofthedatapointsaredenotedby
M
(cid:88)
x˜ := z b = Bz RD, (10.28)
n mn m n
∈
m=1
where z := [z ,...,z ]⊤ RM is the coordinate vector of x˜ with
n 1n Mn n
∈
respect to the basis (b ,...,b ). More specifically, we are interested in
1 M
havingthex˜ assimilartox aspossible.
n n
The similarity measure we use in the following is the squared distance
2
(Euclideannorm) x x˜ betweenxandx˜.Wethereforedefineourob-
∥ − ∥
jectiveasminimizingtheaveragesquaredEuclideandistance(reconstruction reconstructionerror
error)(Pearson,1901)
N
1 (cid:88)
J := x x˜ 2, (10.29)
M N ∥ n − n ∥
n=1
where we make it explicit that the dimension of the subspace onto which
we project the data is M. In order to find this optimal linear projection,
we need to find the orthonormal basis of the principal subspace and the
coordinatesz RM oftheprojectionswithrespecttothisbasis.
n
∈
To find the coordinates z and the ONB of the principal subspace, we
n
follow a two-step approach. First, we optimize the coordinates z for a
n
givenONB(b ,...,b );second,wefindtheoptimalONB.
1 M
10.3.2 Finding Optimal Coordinates
Letusstartbyfindingtheoptimalcoordinatesz ,...,z oftheprojec-
1n Mn
tions x˜ for n = 1,...,N. Consider Figure 10.7(b), where the principal
n
subspace is spanned by a single vector b. Geometrically speaking, finding
theoptimalcoordinatesz correspondstofindingtherepresentationofthe
linearprojectionx˜ withrespecttobthatminimizesthedistancebetween
x˜ x. From Figure 10.7(b), it is clear that this will be the orthogonal
−
projection,andinthefollowingwewillshowexactlythis.
We assume an ONB (b ,...,b ) of U RD. To find the optimal co-
1 M
⊆
ordinates z with respect to this basis, we require the partial derivatives
m
∂J ∂J ∂x˜
M = M n , (10.30a)
∂z ∂x˜ ∂z
in n in
∂J 2
M = (x x˜ )⊤ R1×D, (10.30b)
∂x˜ −N n − n ∈
n
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
328 DimensionalityReductionwithPrincipalComponentAnalysis
Figure10.8 3.25 2.5
Optimalprojection
ofavectorx∈R2 3.00
2.0
ontoa 2.75
one-dimensional 1.5
2.50
subspace
(continuationfrom 2.25 1.0
Figure10.7). U
2.00
(a)Distances 0.5 x˜
∥x−x˜∥forsome 1.75 b
x˜ ∈U. 1.50 0.0
(b)Orthogonal
1.25 0.5
projectionand 1.0 0.5 0.0 0.5 1.0 1.5 2.0 − 1.0 0.5 0.0 0.5 1.0 1.5 2.0
optimalcoordinates.
− − x1 − − x1
(a)Distances∥x−x˜∥forsomex˜ = z1b ∈ (b)Thevectorx˜thatminimizesthedistance
U =span[b];seepanel(b)forthesetting. inpanel(a)isitsorthogonalprojectiononto
U. The coordinate of the projection x˜ with
respect to the basis vector b that spans U
is the factor we need to scale b in order to
“reach”x˜.
(cid:32) (cid:33)
∂x˜
n (10 =.28)
∂ (cid:88)M
z b = b (10.30c)
∂z ∂z mn m i
in in m=1
fori = 1,...,M,suchthatweobtain
(cid:32) (cid:33)⊤
∂J
M
( (1 10
0
=. .3 30 0b c)) 2
(x x˜ )⊤b (10 =.28)
2
x
(cid:88)M
z b b
∂z −N n − n i −N n − mn m i
in m=1
(10.31a)
2 2
O =NB (x⊤b z b⊤b ) = (x⊤b z ). (10.31b)
−N n i − in i i −N n i − in
Thecoordinatesof since b⊤ i b i = 1. Setting this partial derivative to 0 yields immediately the
theoptimal optimalcoordinates
projectionofxn
withrespecttothe z = x⊤b = b⊤x (10.32)
in n i i n
basisvectors
b1,...,bM arethe for i = 1,...,M and n = 1,...,N. This means that the optimal co-
coordinatesofthe ordinates z of the projection x˜ are the coordinates of the orthogonal
in n
orthogonal
projection (see Section 3.8) of the original data point x onto the one-
projectionofxn n
ontotheprincipal dimensionalsubspacethatisspannedbyb i.Consequently:
subspace.
Theoptimallinearprojectionx˜ ofx isanorthogonalprojection.
n n
The coordinates of x˜ with respect to the basis (b ,...,b ) are the
n 1 M
coordinates of the orthogonal projection of x onto the principal sub-
n
space.
An orthogonal projection is the best linear mapping given the objec-
tive(10.29).
Thecoordinatesζ ofxin(10.26)andthecoordinatesz ofx˜ in(10.27)
m m
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
x˜
x
k
−
k
2x
10.3 ProjectionPerspective 329
must be identical for m = 1,...,M since U⊥ = span[b ,...,b ] is
M+1 D
theorthogonalcomplement(seeSection3.6)ofU = span[b ,...,b ].
1 M
Remark (Orthogonal Projections with Orthonormal Basis Vectors). Let us
brieflyrecaporthogonalprojectionsfromSection3.8.If(b ,...,b )isan
1 D
orthonormalbasisofRD then b⊤xisthe
j
coordinateofthe
x˜ = b j(b⊤
j
b j)−1b⊤
j
x = b jb⊤
j
x
∈
RD (10.33) orthogonal
projectionofxonto
istheorthogonalprojectionofxontothesubspacespannedbythejthba- thesubspace
sisvector,andz
j
= b⊤
j
xisthecoordinateofthisprojectionwithrespectto spannedbybj.
thebasisvectorb thatspansthatsubspacesincez b = x˜.Figure10.8(b)
j j j
illustratesthissetting.
More generally, if we aim to project onto an M-dimensional subspace
ofRD,weobtaintheorthogonalprojectionofxontotheM-dimensional
subspacewithorthonormalbasisvectorsb ,...,b as
1 M
x˜ = B(B⊤B)−1B⊤x = BB⊤x, (10.34)
(cid:124) (cid:123)(cid:122) (cid:125)
=I
where we defined B := [b ,...,b ] RD×M. The coordinates of this
1 M
projection with respect to the ordered∈ basis (b ,...,b ) are z := B⊤x
1 M
asdiscussedinSection3.8.
We can think of the coordinates as a representation of the projected
vector in a new coordinate system defined by (b ,...,b ). Note that al-
1 M
though x˜ RD, we only need M coordinates z ,...,z to represent
1 M
∈
thisvector;theotherD M coordinateswithrespecttothebasisvectors
−
(b ,...,b )arealways0.
M+1 D
♢
So far we have shown that for a given ONB we can find the optimal
coordinatesofx˜ byanorthogonalprojectionontotheprincipalsubspace.
Inthefollowing,wewilldeterminewhatthebestbasisis.
10.3.3 Finding the Basis of the Principal Subspace
To determine the basis vectors b ,...,b of the principal subspace, we
1 M
rephrase the loss function (10.29) using the results we have so far. This
will make it easier to find the basis vectors. To reformulate the loss func-
tion,weexploitourresultsfrombeforeandobtain
M M
x˜ = (cid:88) z b (10 =.32) (cid:88) (x⊤b )b . (10.35)
n mn m n m m
m=1 m=1
Wenowexploitthesymmetryofthedotproduct,whichyields
(cid:32) (cid:33)
M
(cid:88)
x˜ = b b⊤ x . (10.36)
n m m n
m=1
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
330 DimensionalityReductionwithPrincipalComponentAnalysis
Figure10.9
Orthogonal 6 U⊥
projectionand
displacement 4
vectors.When
2
projectingdata
pointsxn(blue)
0 ontosubspaceU1, U
weobtainx˜n
2
(orange).The −
displacementvector
4
x˜n−xnlies −
completelyinthe 6
orthogonal −
5 0 5
complementU2of − x
1
U1.
Sincewecangenerallywritetheoriginaldatapointx asalinearcombi-
n
nationofallbasisvectors,itholdsthat
(cid:32) (cid:33)
D D D
x = (cid:88) z b (10 =.32) (cid:88) (x⊤b )b = (cid:88) b b⊤ x (10.37a)
n dn d n d d d d n
d=1 d=1 d=1
(cid:32) (cid:33) (cid:32) (cid:33)
M D
(cid:88) (cid:88)
= b b⊤ x + b b⊤ x , (10.37b)
m m n j j n
m=1 j=M+1
where we split the sum with D terms into a sum over M and a sum
overD M terms.Withthisresult,wefindthatthedisplacementvector
−
x x˜ ,i.e.,thedifferencevectorbetweentheoriginaldatapointandits
n n
−
projection,is
(cid:32) (cid:33)
D
(cid:88)
x x˜ = b b⊤ x (10.38a)
n − n j j n
j=M+1
D
(cid:88)
= (x⊤b )b . (10.38b)
n j j
j=M+1
This means the difference is exactly the projection of the data point onto
theorthogonalcomplementoftheprincipalsubspace:Weidentifythema-
trix (cid:80)D b b⊤ in (10.38a) as the projection matrix that performs this
j=M+1 j j
projection. Hence the displacement vector x x˜ lies in the subspace
n n
−
thatisorthogonaltotheprincipalsubspaceasillustratedinFigure10.9.
Remark(Low-RankApproximation). In(10.38a),wesawthattheprojec-
tionmatrix,whichprojectsxontox˜,isgivenby
M
(cid:88)
b b⊤ = BB⊤. (10.39)
m m
m=1
Byconstructionasasumofrank-onematricesb b⊤ weseethatBB⊤ is
m m
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
x 2
10.3 ProjectionPerspective 331
symmetricandhasrankM.Therefore,theaveragesquaredreconstruction
errorcanalsobewrittenas
1 (cid:88)N
x x˜ 2 =
1 (cid:88)N (cid:13)
(cid:13)x BB⊤x
(cid:13) (cid:13)2
(10.40a)
N ∥ n − n ∥ N (cid:13) n − n(cid:13)
n=1 n=1
1 (cid:88)N (cid:13) (cid:13)2
= (cid:13)(I BB⊤)x (cid:13) . (10.40b)
N (cid:13) − n(cid:13)
n=1
Findingorthonormalbasisvectorsb 1,...,b M,whichminimizethediffer- PCAfindsthebest
ence between the original data x and their projections x˜ , is equivalent rank-M
n n
to finding the best rank-M approximation BB⊤ of the identity matrix I approximationof
theidentitymatrix.
(seeSection4.6).
♢
Nowwehaveallthetoolstoreformulatethelossfunction(10.29).
(cid:13) (cid:13)2
J =
1 (cid:88)N
x x˜ 2 (10 =.38b)
1 (cid:88)N (cid:13)
(cid:13)
(cid:88)D
(b⊤x )b
(cid:13)
(cid:13) . (10.41)
M N ∥ n − n ∥ N (cid:13) (cid:13) j n j(cid:13) (cid:13)
n=1 n=1 j=M+1
Wenowexplicitlycomputethesquarednormandexploitthefactthatthe
b formanONB,whichyields
j
N D N D
1 (cid:88) (cid:88) 1 (cid:88) (cid:88)
J = (b⊤x )2 = b⊤x b⊤x (10.42a)
M N j n N j n j n
n=1j=M+1 n=1j=M+1
N D
1 (cid:88) (cid:88)
= b⊤x x⊤b , (10.42b)
N j n n j
n=1j=M+1
where we exploited the symmetry of the dot product in the last step to
writeb⊤x = x⊤b .Wenowswapthesumsandobtain
j n n j
(cid:32) (cid:33)
D N D
(cid:88) 1 (cid:88) (cid:88)
J = b⊤ x x⊤ b = b⊤Sb (10.43a)
M j N n n j j j
j=M+1 n=1 j=M+1
(cid:124) (cid:123)(cid:122) (cid:125)
=:S
(cid:88)D (cid:88)D (cid:16)(cid:16) (cid:88)D (cid:17) (cid:17)
= tr(b⊤Sb ) = tr(Sb b⊤) = tr b b⊤ S ,
j j j j j j
j=M+1 j=M+1 j=M+1
(cid:124) (cid:123)(cid:122) (cid:125)
projectionmatrix
(10.43b)
whereweexploitedthepropertythatthetraceoperatortr( )(see(4.18))
·
is linear and invariant to cyclic permutations of its arguments. Since we
assumedthatourdatasetiscentered,i.e.,E[ ] = 0,weidentifyS asthe
X
data covariance matrix. Since the projection matrix in (10.43b) is con-
structedasasumofrank-onematricesb b⊤ ititselfisofrankD M.
j j −
Equation (10.43a) implies that we can formulate the average squared
reconstruction error equivalently as the covariance matrix of the data,
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
332 DimensionalityReductionwithPrincipalComponentAnalysis
projectedontotheorthogonalcomplementoftheprincipalsubspace.Min-
Minimizingthe imizingtheaveragesquaredreconstructionerroristhereforeequivalentto
averagesquared minimizingthevarianceofthedatawhenprojectedontothesubspacewe
reconstructionerror
ignore,i.e.,theorthogonalcomplementoftheprincipalsubspace.Equiva-
isequivalentto
lently, we maximize the variance of the projection that we retain in the
minimizingthe
projectionofthe principal subspace, which links the projection loss immediately to the
datacovariance maximum-varianceformulationofPCAdiscussedinSection10.2.Butthis
matrixontothe then also means that we will obtain the same solution that we obtained
orthogonal
for the maximum-variance perspective. Therefore, we omit a derivation
complementofthe
principalsubspace. that is identical to the one presented in Section 10.2 and summarize the
Minimizingthe resultsfromearlierinthelightoftheprojectionperspective.
averagesquared Theaveragesquaredreconstructionerror,whenprojectingontotheM-
reconstructionerror dimensionalprincipalsubspace,is
isequivalentto
maximizingthe D
(cid:88)
varianceofthe J = λ , (10.44)
M j
projecteddata.
j=M+1
where λ are the eigenvalues of the data covariance matrix. Therefore,
j
to minimize (10.44) we need to select the smallest D M eigenvalues,
−
which then implies that their corresponding eigenvectors are the basis of
the orthogonal complement of the principal subspace. Consequently, this
meansthatthebasisoftheprincipalsubspacecomprisestheeigenvectors
b ,...,b thatareassociatedwiththelargestM eigenvaluesofthedata
1 M
covariancematrix.
Example 10.3 (MNIST Digits Embedding)
Figure10.10
Embeddingof
MNISTdigits0
(blue)and1
(orange)ina
two-dimensional
principalsubspace
usingPCA.Four
embeddingsofthe
digits“0”and“1”in
theprincipal
subspaceare
highlightedinred
withtheir
corresponding
originaldigit.
Figure 10.10 visualizes the training data of the MMIST digits “0” and “1”
embedded in the vector subspace spanned by the first two principal com-
ponents.Weobservearelativelyclearseparationbetween“0”s(bluedots)
and “1”s (orange dots), and we see the variation within each individual
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.4 EigenvectorComputationandLow-RankApproximations 333
cluster.Fourembeddingsofthedigits“0”and“1”intheprincipalsubspace
are highlighted in red with their corresponding original digit. The figure
revealsthatthevariationwithinthesetof“0”issignificantlygreaterthan
thevariationwithinthesetof“1”.
10.4 Eigenvector Computation and Low-Rank Approximations
In the previous sections, we obtained the basis of the principal subspace
astheeigenvectorsthatareassociatedwiththelargesteigenvaluesofthe
datacovariancematrix
N
1 (cid:88) 1
S = x x⊤ = XX⊤, (10.45)
N n n N
n=1
X = [x ,...,x ] RD×N . (10.46)
1 N
∈
Note that X is a D N matrix, i.e., it is the transpose of the “typical”
×
data matrix (Bishop, 2006; Murphy, 2012). To get the eigenvalues (and
thecorrespondingeigenvectors)ofS,wecanfollowtwoapproaches: Use
eigendecomposition
We perform an eigendecomposition (see Section 4.2) and compute the orSVDtocompute
eigenvaluesandeigenvectorsofS directly. eigenvectors.
We use a singular value decomposition (see Section 4.5). Since S is
symmetricandfactorizesintoXX⊤ (ignoringthefactor 1),theeigen-
N
valuesofS arethesquaredsingularvaluesofX.
Morespecifically,theSVDofX isgivenby
X = U Σ V⊤ , (10.47)
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
D×N D×DD×NN×N
where U RD×D and V⊤ RN×N are orthogonal matrices and Σ
RD×N isa∈ matrixwhoseonly∈ nonzeroentriesarethesingularvaluesσ ⩾∈
ii
0.Itthenfollowsthat
1 1 1
S = XX⊤ = UΣV⊤V Σ⊤U⊤ = UΣΣ⊤U⊤. (10.48)
N N (cid:124) (cid:123)(cid:122) (cid:125) N
=IN
With the results from Section 4.5, we get that the columns of U are the ThecolumnsofU
eigenvectors of XX⊤ (and therefore S). Furthermore, the eigenvalues aretheeigenvectors
λ ofS arerelatedtothesingularvaluesofX via ofS.
d
σ2
λ = d . (10.49)
d N
This relationship between the eigenvalues of S and the singular values
ofX providestheconnectionbetweenthemaximumvarianceview(Sec-
tion10.2)andthesingularvaluedecomposition.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
334 DimensionalityReductionwithPrincipalComponentAnalysis
10.4.1 PCA Using Low-Rank Matrix Approximations
To maximize the variance of the projected data (or minimize the average
squared reconstruction error), PCA chooses the columns of U in (10.48)
to be the eigenvectors that are associated with the M largest eigenvalues
ofthedatacovariancematrixSsothatweidentifyU astheprojectionma-
trixBin(10.3),whichprojectstheoriginaldataontoalower-dimensional
Eckart-Young subspace of dimension M. The Eckart-Young theorem (Theorem 4.25 in
theorem Section4.6)offersadirectwaytoestimatethelow-dimensionalrepresen-
tation.Considerthebestrank-M approximation
X˜ := argmin X A RD×N (10.50)
M rk(A)⩽M ∥ − ∥2 ∈
ofX,where isthespectralnormdefinedin(4.93).TheEckart-Young
∥·∥2
theorem states that X˜ is given by truncating the SVD at the top-M
M
singularvalue.Inotherwords,weobtain
X˜ = U Σ V⊤ RD×N (10.51)
M (cid:124)(cid:123)(cid:122)M (cid:125) (cid:124)(cid:123)(cid:122)M (cid:125) (cid:124)(cid:123)(cid:122)M (cid:125) ∈
D×MM×MM×N
with orthogonal matrices U := [u ,...,u ] RD×M and V :=
M 1 M M
[v ,...,v ] RN×M and a diagonal matrix Σ ∈ RM×M whose diago-
1 M M
∈ ∈
nalentriesaretheM largestsingularvaluesofX.
10.4.2 Practical Aspects
Finding eigenvalues and eigenvectors is also important in other funda-
mentalmachinelearningmethodsthatrequirematrixdecompositions.In
theory,aswediscussedinSection4.2,wecansolvefortheeigenvaluesas
roots of the characteristic polynomial. However, for matrices larger than
4 4thisisnotpossiblebecausewewouldneedtofindtherootsofapoly-
×
Abel-Ruffini nomial of degree 5 or higher. However, the Abel-Ruffini theorem (Ruffini,
theorem 1799; Abel, 1826) states that there exists no algebraic solution to this
problem for polynomials of degree 5 or more. Therefore, in practice, we
np.linalg.eigh
solveforeigenvaluesorsingularvaluesusingiterativemethods,whichare
or
np.linalg.svd implementedinallmodernpackagesforlinearalgebra.
In many applications (such as PCA presented in this chapter), we only
require a few eigenvectors. It would be wasteful to compute the full de-
composition, and then discard all eigenvectors with eigenvalues that are
beyond the first few. It turns out that if we are interested in only the first
few eigenvectors (with the largest eigenvalues), then iterative processes,
whichdirectlyoptimizetheseeigenvectors,arecomputationallymoreeffi-
cientthanafulleigendecomposition(orSVD).Intheextremecaseofonly
poweriteration needing the first eigenvector, a simple method called the power iteration
isveryefficient.Poweriterationchoosesarandomvectorx thatisnotin
0
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.5 PCAinHighDimensions 335
thenullspaceofS andfollowstheiteration
Sx
x = k , k = 0,1,... . (10.52)
k+1
Sx
k
∥ ∥
This means the vector x k is multiplied by S in every iteration and then IfSisinvertible,it
normalized,i.e.,wealwayshave x = 1.Thissequenceofvectorscon- issufficientto
k
vergestotheeigenvectorassociat∥ edw∥
iththelargesteigenvalueofS.The
ensurethatx0̸=0.
original Google PageRank algorithm (Page et al., 1999) uses such an al-
gorithmforrankingwebpagesbasedontheirhyperlinks.
10.5 PCA in High Dimensions
InordertodoPCA,weneedtocomputethedatacovariancematrix.InD
dimensions,thedatacovariancematrixisaD Dmatrix.Computingthe
×
eigenvalues and eigenvectors of this matrix is computationally expensive
asitscalescubicallyinD.Therefore,PCA,aswediscussedearlier,willbe
infeasibleinveryhighdimensions.Forexample,ifourx areimageswith
n
10,000 pixels (e.g., 100 100 pixel images), we would need to compute
×
the eigendecomposition of a 10,000 10,000 covariance matrix. In the
×
following,weprovideasolutiontothisproblemforthecasethatwehave
substantiallyfewerdatapointsthandimensions,i.e.,N D.
Assume we have a centered dataset x ,...,x , x ≪ RD. Then the
1 N n
∈
datacovariancematrixisgivenas
1
S = XX⊤ RD×D, (10.53)
N ∈
where X = [x ,...,x ] is a D N matrix whose columns are the data
1 N
×
points.
WenowassumethatN D,i.e.,thenumberofdatapointsissmaller
≪
than the dimensionality of the data. If there are no duplicate data points,
therankofthecovariancematrixS isN,soithasD N+1manyeigen-
−
valuesthatare0.Intuitively,thismeansthattherearesomeredundancies.
Inthefollowing,wewillexploitthisandturntheD Dcovariancematrix
×
intoanN N covariancematrixwhoseeigenvaluesareallpositive.
×
InPCA,weendedupwiththeeigenvectorequation
Sb = λ b , m = 1,...,M , (10.54)
m m m
where b is a basis vector of the principal subspace. Let us rewrite this
m
equationabit:WithS definedin(10.53),weobtain
1
Sb = XX⊤b = λ b . (10.55)
m N m m m
WenowmultiplyX⊤ RN×D fromtheleft-handside,whichyields
∈
1 1
X⊤XX⊤b = λ X⊤b X⊤Xc = λ c , (10.56)
N (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) m (cid:125) m m ⇐⇒ N m m m
N×N =:cm
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
336 DimensionalityReductionwithPrincipalComponentAnalysis
and we get a new eigenvector/eigenvalue equation: λ remains eigen-
m
value, which confirms our results from Section 4.5.3 that the nonzero
eigenvaluesofXX⊤ equalthenonzeroeigenvaluesofX⊤X.Weobtain
the eigenvector of the matrix 1X⊤X RN×N associated with λ as
N ∈ m
c := X⊤b . Assuming we have no duplicate data points, this matrix
m m
hasrankN andisinvertible.Thisalsoimpliesthat 1X⊤X hasthesame
N
(nonzero)eigenvaluesasthedatacovariancematrixS.Butthisisnowan
N N matrix, so that we can compute the eigenvalues and eigenvectors
×
muchmoreefficientlythanfortheoriginalD Ddatacovariancematrix.
Now that we have the eigenvectors of
1X×⊤X,
we are going to re-
N
cover the original eigenvectors, which we still need for PCA. Currently,
weknowtheeigenvectorsof 1X⊤X.Ifweleft-multiplyoureigenvalue/
N
eigenvectorequationwithX,weget
1
XX⊤Xc = λ Xc (10.57)
N m m m
(cid:124) (cid:123)(cid:122) (cid:125)
S
and we recover the data covariance matrix again. This now also means
thatwerecoverXc asaneigenvectorofS.
m
Remark. IfwewanttoapplythePCAalgorithmthatwediscussedinSec-
tion 10.6, we need to normalize the eigenvectors Xc of S so that they
m
havenorm1.
♢
10.6 Key Steps of PCA in Practice
In the following, we will go through the individual steps of PCA using a
running example, which is summarized in Figure 10.11. We are given a
two-dimensional dataset (Figure 10.11(a)), and we want to use PCA to
projectitontoaone-dimensionalsubspace.
1. Mean subtraction We start by centering the data by computing the
mean µ of the dataset and subtracting it from every single data point.
Thisensuresthatthedatasethasmean0(Figure10.11(b)).Meansub-
tractionisnotstrictlynecessarybutreducestheriskofnumericalprob-
lems.
2. Standardization Dividethedatapointsbythestandarddeviationσ
d
of the dataset for every dimension d = 1,...,D. Now the data is unit
free, and it has variance 1 along each axis, which is indicated by the
standardization twoarrowsinFigure10.11(c).Thisstepcompletesthestandardization
ofthedata.
3. Eigendecomposition of the covariance matrix Compute the data
covariancematrixanditseigenvaluesandcorrespondingeigenvectors.
Since the covariance matrix is symmetric, the spectral theorem (The-
orem 4.15) states that we can find an ONB of eigenvectors. In Fig-
ure10.11(d),theeigenvectorsarescaledbythemagnitudeofthecor-
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.6 KeyStepsofPCAinPractice 337
Figure10.11 Steps
5.0 5.0 5.0 ofPCA.(a)Original
dataset;
2.5 2.5 2.5 (b)centering;
(c)divideby
0.0 0.0 0.0
standarddeviation;
2.5 2.5 2.5 (d)eigendecomposi-
− − −
tion;(e)projection;
0 5 0 5 0 5
x1 x1 x1 (f)mappingbackto
(a)Originaldataset. (b) Step 1: Centering by sub- (c) Step 2: Dividing by the originaldataspace.
tracting the mean from each standard deviation to make
datapoint. the data unit free. Data has
variance1alongeachaxis.
5.0 5.0 5.0
2.5 2.5 2.5
0.0 0.0 0.0
2.5 2.5 2.5
− − −
0 5 0 5 0 5
x1 x1 x1
(d)Step3:Computeeigenval- (e) Step 4: Project data onto (f) Undo the standardization
uesandeigenvectors(arrows) theprincipalsubspace. andmoveprojecteddataback
of the data covariance matrix into the original data space
(ellipse). from(a).
respondingeigenvalue.Thelongervectorspanstheprincipalsubspace,
which we denote by U. The data covariance matrix is represented by
theellipse.
4. Projection Wecanprojectanydatapointx RD ontotheprincipal
∗
∈
subspace: To get this right, we need to standardize x using the mean
∗
µ andstandarddeviationσ ofthetrainingdatainthedthdimension,
d d
respectively,sothat
x(d) µ
x(d) ∗ − d , d = 1,...,D, (10.58)
∗ ← σ
d
wherex(d) isthedthcomponentofx .Weobtaintheprojectionas
∗ ∗
x˜ = BB⊤x (10.59)
∗ ∗
withcoordinates
z = B⊤x (10.60)
∗ ∗
with respect to the basis of the principal subspace. Here, B is the ma-
trix that contains the eigenvectors that are associated with the largest
eigenvaluesofthedatacovariancematrixascolumns.PCAreturnsthe
coordinates(10.60),nottheprojectionsx .
∗
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
2x
2x
2x
2x
2x
2x
338 DimensionalityReductionwithPrincipalComponentAnalysis
Havingstandardizedourdataset,(10.59)onlyyieldstheprojectionsin
the context of the standardized dataset. To obtain our projection in the
original data space (i.e., before standardization), we need to undo the
standardization (10.58) and multiply by the standard deviation before
addingthemeansothatweobtain
x˜(d) x˜(d)σ +µ , d = 1,...,D. (10.61)
∗ ← ∗ d d
Figure10.11(f)illustratestheprojectionintheoriginaldataspace.
Example 10.4 (MNIST Digits: Reconstruction)
In the following, we will apply PCA to the MNIST digits dataset, which
contains 60,000 examples of handwritten digits 0 through 9. Each digit is
animageofsize28 28,i.e.,itcontains784pixelssothatwecaninterpret
everyimageinthis× datasetasavectorx R784.Examplesofthesedigits
∈
areshowninFigure10.3.
Figure10.12 Effect
ofincreasingthe Original
numberofprincipal
componentson
reconstruction.
PCs: 1
PCs: 10
PCs: 100
PCs: 500
Forillustrationpurposes,weapplyPCAtoasubsetoftheMNISTdigits,
and we focus on the digit “8”. We used 5,389 training images of the digit
“8” and determined the principal subspace as detailed in this chapter. We
then used the learned projection matrix to reconstruct a set of test im-
ages, which is illustrated in Figure 10.12. The first row of Figure 10.12
shows a set of four original digits from the test set. The following rows
show reconstructions of exactly these digits when using a principal sub-
space of dimensions 1, 10, 100, and 500, respectively. We see that even
with a single-dimensional principal subspace we get a halfway decent re-
construction of the original digits, which, however, is blurry and generic.
Withanincreasingnumberofprincipalcomponents(PCs),thereconstruc-
tions become sharper and more details are accounted for. With 500 prin-
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.7 LatentVariablePerspective 339
cipal components, we effectively obtain a near-perfect reconstruction. If
weweretochoose784PCs,wewouldrecovertheexactdigitwithoutany
compressionloss.
Figure10.13showstheaveragesquaredreconstructionerror,whichis
N D
1 (cid:88) x x˜ 2 = (cid:88) λ , (10.62)
N ∥ n − n ∥ i
n=1 i=M+1
as a function of the number M of principal components. We can see that
the importance of the principal components drops off rapidly, and only
marginalgainscanbeachievedbyaddingmorePCs.Thismatchesexactly
our observation in Figure 10.5, where we discovered that the most of the
variance of the projected data is captured by only a few principal compo-
nents.Withabout550PCs,wecanessentiallyfullyreconstructthetraining
datathatcontainsthedigit“8”(somepixelsaroundtheboundariesshow
novariationacrossthedatasetastheyarealwaysblack).
Figure10.13
500
Averagesquared
reconstructionerror
400
asafunctionofthe
numberofprincipal
300
components.The
averagesquared
200
reconstructionerror
isthesumofthe
100
eigenvaluesinthe
0 orthogonal
0 200 400 600 800 complementofthe
NumberofPCs principalsubspace.
10.7 Latent Variable Perspective
In the previous sections, we derived PCA without any notion of a prob-
abilistic model using the maximum-variance and the projection perspec-
tives. On the one hand, this approach may be appealing as it allows us to
sidestep all the mathematical difficulties that come with probability the-
ory,butontheotherhand,aprobabilisticmodelwouldofferusmoreflex-
ibilityandusefulinsights.Morespecifically,aprobabilisticmodelwould
Come with a likelihood function, and we can explicitly deal with noisy
observations(whichwedidnotevendiscussearlier)
Allow us to do Bayesian model comparison via the marginal likelihood
asdiscussedinSection8.6
ViewPCAasagenerativemodel,whichallowsustosimulatenewdata
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
rorrenoitcurtsnocerderauqsegarevA
340 DimensionalityReductionwithPrincipalComponentAnalysis
Allowustomakestraightforwardconnectionstorelatedalgorithms
Deal with data dimensions that are missing at random by applying
Bayes’theorem
Giveusanotionofthenoveltyofanewdatapoint
Giveusaprincipledwaytoextendthemodel,e.g.,toamixtureofPCA
models
HavethePCAwederivedinearliersectionsasaspecialcase
Allow for a fully Bayesian treatment by marginalizing out the model
parameters
By introducing a continuous-valued latent variable z RM it is possible
∈
tophrasePCAasaprobabilisticlatent-variablemodel.TippingandBishop
probabilisticPCA (1999) proposed this latent-variable model as probabilistic PCA (PPCA).
PPCA PPCA addresses most of the aforementioned issues, and the PCA solution
that we obtained by maximizing the variance in the projected space or
by minimizing the reconstruction error is obtained as the special case of
maximumlikelihoodestimationinanoise-freesetting.
10.7.1 Generative Process and Probabilistic Model
In PPCA, we explicitly write down the probabilistic model for linear di-
mensionality reduction. For this we assume a continuous latent variable
z RM with a standard-normal prior p(z) = (cid:0) 0, I(cid:1) and a linear rela-
∈ N
tionshipbetweenthelatentvariablesandtheobservedxdatawhere
x = Bz+µ+ϵ RD, (10.63)
∈
where ϵ (cid:0) 0, σ2I(cid:1) is Gaussian observation noise and B RD×M
and µ
∼RDN
describe the linear/affine mapping from latent
to∈
observed
∈
variables.Therefore,PPCAlinkslatentandobservedvariablesvia
p(x z,B,µ,σ2) = (cid:0) x Bz+µ, σ2I(cid:1) . (10.64)
| N |
Overall,PPCAinducesthefollowinggenerativeprocess:
(cid:0) (cid:1)
z z 0, I (10.65)
n
∼ N |
x z
(cid:0)
x Bz +µ,
σ2I(cid:1)
(10.66)
n n n
| ∼ N |
To generate a data point that is typical given the model parameters, we
ancestralsampling follow an ancestral sampling scheme: We first sample a latent variable z n
fromp(z).Thenweusez in(10.64)tosampleadatapointconditioned
n
onthesampledz ,i.e.,x p(x z ,B,µ,σ2).
n n n
∼ |
Thisgenerativeprocessallowsustowritedowntheprobabilisticmodel
(i.e.,thejointdistributionofallrandomvariables;seeSection8.4)as
p(x,z B,µ,σ2) = p(x z,B,µ,σ2)p(z), (10.67)
| |
whichimmediatelygivesrisetothegraphicalmodelinFigure10.14using
theresultsfromSection8.5.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.7 LatentVariablePerspective 341
Figure10.14
z n Graphicalmodelfor
probabilisticPCA.
Theobservationsxn
B µ explicitlydependon
corresponding
latentvariables
x n σ zn∼N(cid:0) 0,I(cid:1) .The
modelparameters
n=1,...,N
B,µandthe
likelihood
parameterσare
Remark. Notethedirectionofthearrowthatconnectsthelatentvariables sharedacrossthe
z and the observed data x: The arrow points from z to x, which means dataset.
thatthePPCAmodelassumesalower-dimensionallatentcausezforhigh-
dimensional observations x. In the end, we are obviously interested in
finding something out about z given some observations. To get there we
willapplyBayesianinferenceto“invert”thearrowimplicitlyandgofrom
observationstolatentvariables.
♢
Example 10.5 (Generating New Data Using Latent Variables)
Figure10.15
Generatingnew
MNISTdigits.The
latentvariablesz
canbeusedto
generatenewdata
x˜ =Bz.Thecloser
westaytothe
trainingdata,the
morerealisticthe
generateddata.
Figure10.15showsthelatentcoordinatesoftheMNISTdigits“8”found
byPCAwhenusingatwo-dimensionalprincipalsubspace(bluedots).We
can query any vector z in this latent space and generate an image x˜ =
∗ ∗
Bz thatresemblesthedigit“8”.Weshoweightofsuchgeneratedimages
∗
withtheircorrespondinglatentspacerepresentation.Dependingonwhere
we query the latent space, the generated images look different (shape,
rotation,size,etc.).Ifwequeryawayfromthetrainingdata,weseemore
and more artifacts, e.g., the top-left and top-right digits. Note that the
intrinsicdimensionalityofthesegeneratedimagesisonlytwo.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
342 DimensionalityReductionwithPrincipalComponentAnalysis
10.7.2 Likelihood and Joint Distribution
Thelikelihooddoes
notdependonthe Using the results from Chapter 6, we obtain the likelihood of this proba-
latentvariablesz.
bilistic model by integrating out the latent variable z (see Section 8.4.3)
sothat
(cid:90)
p(x B,µ,σ2) = p(x z,B,µ,σ2)p(z)dz (10.68a)
| |
(cid:90)
=
(cid:0)
x Bz+µ,
σ2I(cid:1) (cid:0)
z 0,
I(cid:1)
dz. (10.68b)
N | N |
FromSection6.5,weknowthatthesolutiontothisintegralisaGaussian
distributionwithmean
E [x] = E [Bz+µ]+E [ϵ] = µ (10.69)
x z ϵ
andwithcovariancematrix
V[x] = V [Bz+µ]+V [ϵ] = V [Bz]+σ2I (10.70a)
z ϵ z
= BV [z]B⊤+σ2I = BB⊤+σ2I. (10.70b)
z
The likelihood in (10.68b) can be used for maximum likelihood or MAP
estimationofthemodelparameters.
Remark. We cannot use the conditional distribution in (10.64) for maxi-
mum likelihood estimation as it still depends on the latent variables. The
likelihood function we require for maximum likelihood (or MAP) estima-
tion should only be a function of the data x and the model parameters,
butmustnotdependonthelatentvariables.
♢
From Section 6.5, we know that a Gaussian random variable z and
a linear/affine transformation x = Bz of it are jointly Gaussian dis-
(cid:0) (cid:1)
tributed. We already know the marginals p(z) = z 0, I and p(x) =
(cid:0)
x µ,
BB⊤+σ2I(cid:1) .Themissingcross-covariaN ncei|
sgivenas
N |
Cov[x,z] = Cov [Bz+µ] = BCov [z,z] = B. (10.71)
z z
Therefore, the probabilistic model of PPCA, i.e., the joint distribution of
latentandobservedrandomvariablesisexplicitlygivenby
(cid:18)(cid:20) x(cid:21) (cid:12) (cid:12)(cid:20) µ(cid:21) (cid:20) BB⊤+σ2I B(cid:21)(cid:19)
p(x,z B,µ,σ2) = (cid:12) , , (10.72)
| N z (cid:12) 0 B⊤ I
with a mean vector of length D + M and a covariance matrix of size
(D+M) (D+M).
×
10.7.3 Posterior Distribution
The joint Gaussian distribution p(x,z B,µ,σ2) in (10.72) allows us to
|
determine the posterior distribution p(z x) immediately by applying the
|
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.8 FurtherReading 343
rules of Gaussian conditioning fromSection 6.5.1. The posterior distribu-
tionofthelatentvariablegivenanobservationxisthen
(cid:0) (cid:1)
p(z x) = z m, C , (10.73)
| N |
m = B⊤(BB⊤+σ2I)−1(x µ), (10.74)
−
C = I B⊤(BB⊤+σ2I)−1B. (10.75)
−
Note that the posterior covariance does not depend on the observed data
x. For a new observation x in data space, we use (10.73) to determine
∗
the posteriordistribution of the correspondinglatent variablez . The co-
∗
variance matrix C allows us to assess how confident the embedding is. A
covariancematrixC withasmalldeterminant(whichmeasuresvolumes)
tells us that the latent embedding z is fairly certain. If we obtain a pos-
∗
terior distribution p(z x ) with much variance, we may be faced with
∗ ∗
|
an outlier. However, we can explore this posterior distribution to under-
stand what other data points x are plausible under this posterior. To do
this, we exploit the generative process underlying PPCA, which allows us
to explore the posterior distribution on the latent variables by generating
newdatathatisplausibleunderthisposterior:
1. Samplealatentvariablez p(z x )fromtheposteriordistribution
∗ ∗
∼ |
overthelatentvariables(10.73).
2. Sampleareconstructedvectorx˜ p(x z ,B,µ,σ2)from(10.64).
∗ ∗
∼ |
If we repeat this process many times, we can explore the posterior dis-
tribution (10.73) on the latent variables z and its implications on the
∗
observeddata.Thesamplingprocesseffectivelyhypothesizesdata,which
isplausibleundertheposteriordistribution.
10.8 Further Reading
WederivedPCAfromtwoperspectives:(a)maximizingthevarianceinthe
projected space; (b) minimizing the average reconstruction error. How-
ever,PCAcanalsobeinterpretedfromdifferentperspectives.Letusrecap
what we have done: We took high-dimensional data x RD and used
a matrix B⊤ to find a lower-dimensional representation∈ z RM. The
∈
columnsofBaretheeigenvectorsofthedatacovariancematrixSthatare
associated with the largest eigenvalues. Once we have a low-dimensional
representationz,wecangetahigh-dimensionalversionofit(intheorig-
inal data space) as x x˜ = Bz = BB⊤x RD, where BB⊤ is a
≈ ∈
projectionmatrix.
We can also think of PCA as a linear auto-encoder as illustrated in Fig- auto-encoder
ure10.16.Anauto-encoderencodesthedatax n RD toacodez n RM code
∈ ∈
and decodes it to a x˜ similar to x . The mapping from the data to the
n n
codeiscalledtheencoder,andthemappingfromthecodebacktotheorig- encoder
inaldataspaceiscalledthedecoder.Ifweconsiderlinearmappingswhere decoder
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
344 DimensionalityReductionwithPrincipalComponentAnalysis
Figure10.16 PCA Original
canbeviewedasa
RD RD
linearauto-encoder.
Code
Itencodesthe
high-dimensional RM
B B
dataxintoa ⊤
lower-dimensional x z x˜
representation
(code)z∈RM and
decodeszusinga
decoder.The
decodedvectorx˜is
theorthogonal Encoder Decoder
projectionofthe
originaldataxonto
theM-dimensional
principalsubspace. thecodeisgivenbyz = B⊤x RM andweareinterestedinminimiz-
n n
∈
ingtheaveragesquarederrorbetweenthedatax anditsreconstruction
n
x˜ = Bz ,n = 1,...,N,weobtain
n n
1 (cid:88)N 1 (cid:88)N (cid:13) (cid:13)2
x x˜ 2 = (cid:13)x BB⊤x (cid:13) . (10.76)
N ∥ n − n ∥ N (cid:13) n − n(cid:13)
n=1 n=1
Thismeansweendupwiththesameobjectivefunctionasin(10.29)that
wediscussedinSection10.3sothatweobtainthePCAsolutionwhenwe
minimize the squared auto-encoding loss. If we replace the linear map-
ping of PCA with a nonlinear mapping, we get a nonlinear auto-encoder.
Aprominentexampleofthisisadeepauto-encoderwherethelinearfunc-
tionsarereplacedwithdeepneuralnetworks.Inthiscontext,theencoder
recognitionnetwork is also known as a recognition network or inference network, whereas the
inferencenetwork decoderisalsocalledagenerator.
generator Another interpretation of PCA is related to information theory. We can
think of the code as a smaller or compressed version of the original data
point. When we reconstruct our original data using the code, we do not
get the exact data point back, but a slightly distorted or noisy version
Thecodeisa of it. This means that our compression is “lossy”. Intuitively, we want
compressedversion to maximize the correlation between the original data and the lower-
oftheoriginaldata.
dimensionalcode.Moreformally,thisisrelatedtothemutualinformation.
WewouldthengetthesamesolutiontoPCAwediscussedinSection10.3
bymaximizingthemutualinformation,acoreconceptininformationthe-
ory(MacKay,2003).
In our discussion on PPCA, we assumed that the parameters of the
model, i.e., B,µ, and the likelihood parameter σ2, are known. Tipping
andBishop(1999)describehowtoderivemaximumlikelihoodestimates
for these parameters in the PPCA setting (note that we use a different
notationinthischapter).Themaximumlikelihoodparameters,whenpro-
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.8 FurtherReading 345
jectingD-dimensionaldataontoanM-dimensionalsubspace,are
N
1 (cid:88)
µ = x , (10.77)
ML N n
n=1
B
ML
= T(Λ σ2I)1 2R, (10.78)
−
D
1 (cid:88)
σ2 = λ , (10.79)
ML D M j
− j=M+1
whereT RD×M containsM eigenvectorsofthedatacovariancematrix, ThematrixΛ−σ2I
Λ = diag∈ (λ ,...,λ ) RM×M isadiagonalmatrixwiththeeigenvalues in(10.78)is
1 M
associated with the pri∈ ncipal axes on its diagonal, and R RM×M is guaranteedtobe
∈ positivesemidefinite
an arbitrary orthogonal matrix. The maximum likelihood solution B is
ML asthesmallest
unique up to an arbitrary orthogonal transformation, e.g., we can right- eigenvalueofthe
multiply B with any rotation matrix R so that (10.78) essentially is a datacovariance
ML
singularvaluedecomposition(seeSection4.5).Anoutlineoftheproofis matrixisbounded
frombelowbythe
givenbyTippingandBishop(1999).
noisevarianceσ2.
The maximum likelihood estimate for µ given in (10.77) is the sample
mean of the data. The maximum likelihood estimator for the observation
noise variance σ2 given in (10.79) is the average variance in the orthog-
onalcomplementoftheprincipalsubspace,i.e.,theaverageleftovervari-
ance that we cannot capture with the first M principal components is
treatedasobservationnoise.
In the noise-free limit where σ 0, PPCA and PCA provide identical
→
solutions: Since the data covariance matrix S is symmetric, it can be di-
agonalized (see Section 4.4), i.e., there exists a matrix T of eigenvectors
ofS sothat
S = TΛT−1. (10.80)
InthePPCAmodel,thedatacovariancematrixisthecovariancematrixof
theGaussianlikelihoodp(x B,µ,σ2),whichisBB⊤+σ2I,see(10.70b).
For σ 0, we obtain
BB⊤|
so that this data covariance must equal the
→
PCAdatacovariance(anditsfactorizationgivenin(10.80))sothat
Cov[ ] = TΛT−1 = BB⊤ B = TΛ1 2R, (10.81)
X ⇐⇒
i.e., we obtain the maximum likelihood estimate in (10.78) for σ = 0.
From (10.78) and (10.80), it becomes clear that (P)PCA performs a de-
compositionofthedatacovariancematrix.
In a streaming setting, where data arrives sequentially, it is recom-
mendedtousetheiterativeexpectationmaximization(EM)algorithmfor
maximumlikelihoodestimation(Roweis,1998).
To determine the dimensionality of the latent variables (the length of
thecode,thedimensionalityofthelower-dimensionalsubspaceontowhich
we project the data), Gavish and Donoho (2014) suggest the heuristic
that, if we can estimate the noise variance σ2 of the data, we should
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
346 DimensionalityReductionwithPrincipalComponentAnalysis
√
discard all singular values smaller than 4σ√ D. Alternatively, we can use
3
(nested) cross-validation (Section 8.6.1) or Bayesian model selection cri-
teria (discussed in Section 8.6.2) to determine a good estimate of the
intrinsicdimensionalityofthedata(Minka,2001b).
SimilartoourdiscussiononlinearregressioninChapter9,wecanplace
a prior distribution on the parameters of the model and integrate them
out. By doing so, we (a) avoid point estimates of the parameters and the
issues that come with these point estimates (see Section 8.6) and (b) al-
lowforanautomaticselectionoftheappropriatedimensionalityM ofthe
BayesianPCA latentspace.InthisBayesianPCA,whichwasproposedbyBishop(1999),
a prior p(µ,B,σ2) is placed on the model parameters. The generative
processallowsustointegratethemodelparametersoutinsteadofcondi-
tioningonthem,whichaddressesoverfittingissues.Sincethisintegration
isanalyticallyintractable,Bishop(1999)proposestouseapproximatein-
ference methods, such as MCMC or variational inference. We refer to the
workbyGilksetal.(1996)andBleietal.(2017)formoredetailsonthese
approximateinferencetechniques.
(cid:0)
In PPCA, we considered the linear model p(x z ) = x Bz +
n n n n
(cid:1) (cid:0) (cid:1) | N |
µ, σ2I with prior p(z ) = 0, I , where all observation dimensions
n
N
are affected by the same amount of noise. If we allow each observation
factoranalysis dimension d to have a different variance σ2, we obtain factor analysis
d
(FA) (Spearman, 1904; Bartholomew et al., 2011). This means that FA
gives the likelihood some more flexibility than PPCA, but still forces the
Anoverlyflexible data to be explained by the model parameters B, µ.However, FA no
likelihoodwouldbe longer allows for a closed-form maximum likelihood solution so that we
abletoexplainmore
need to use an iterative scheme, such as the expectation maximization
thanjustthenoise.
algorithm, to estimate the model parameters. While in PPCA all station-
ary points are global optima, this no longer holds for FA. Compared to
PPCA,FAdoesnotchangeifwescalethedata,butitdoesreturndifferent
solutionsifwerotatethedata.
independent An algorithm that is also closely related to PCA is independent com-
componentanalysis ponent analysis (ICA (Hyvarinen et al., 2001)). Starting again with the
(cid:0) (cid:1)
ICA latent-variable perspective p(x z ) = x Bz +µ, σ2I we now
n n n n
| N |
change the prior on z to non-Gaussian distributions. ICA can be used
n
blind-source for blind-source separation. Imagine you are in a busy train station with
separation many people talking. Your ears play the role of microphones, and they
linearlymixdifferentspeechsignalsinthetrainstation.Thegoalofblind-
sourceseparationistoidentifytheconstituentpartsofthemixedsignals.
As discussed previously in the context of maximum likelihood estimation
forPPCA,theoriginalPCAsolutionisinvarianttoanyrotation.Therefore,
PCA can identify the best lower-dimensional subspace in which the sig-
nals live, but not the signals themselves (Murphy, 2012). ICA addresses
this issue by modifying the prior distribution p(z) on the latent sources
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
10.8 FurtherReading 347
to require non-Gaussian priors p(z). We refer to the books by Hyvarinen
etal.(2001)andMurphy(2012)formoredetailsonICA.
PCA,factoranalysis,andICAarethreeexamplesfordimensionalityre-
ductionwithlinearmodels.CunninghamandGhahramani(2015)provide
abroadersurveyoflineardimensionalityreduction.
The (P)PCA model we discussed here allows for several important ex-
tensions. In Section 10.5, we explained how to do PCA when the in-
put dimensionality D is significantly greater than the number N of data
points.ByexploitingtheinsightthatPCAcanbeperformedbycomputing
(many)innerproducts,thisideacanbepushedtotheextremebyconsid-
ering infinite-dimensional features. The kernel trick is the basis of kernel kerneltrick
PCA and allows us to implicitly compute inner products between infinite- kernelPCA
dimensionalfeatures(Scho¨lkopfetal.,1998;Scho¨lkopfandSmola,2002).
There are nonlinear dimensionality reduction techniques that are de-
rived from PCA (Burges (2010) provides a good overview). The auto-
encoder perspective of PCA that we discussed previously in this section
can be used to render PCA as a special case of a deep auto-encoder. In the deepauto-encoder
deep auto-encoder, both the encoder and the decoder are represented by
multilayer feedforward neural networks, which themselves are nonlinear
mappings.Ifwesettheactivationfunctionsintheseneuralnetworkstobe
theidentity,themodelbecomesequivalenttoPCA.Adifferentapproachto
nonlinear dimensionality reduction is the Gaussian process latent-variable Gaussianprocess
model(GP-LVM)proposedbyLawrence(2005).TheGP-LVMstartsoffwith latent-variable
model
the latent-variable perspective that we used to derive PPCA and replaces
GP-LVM
thelinearrelationshipbetweenthelatentvariableszandtheobservations
x with a Gaussian process (GP). Instead of estimating the parameters of
themapping(aswedoinPPCA),theGP-LVMmarginalizesoutthemodel
parameters and makes point estimates of the latent variables z. Similar
to Bayesian PCA, the Bayesian GP-LVM proposed by Titsias and Lawrence BayesianGP-LVM
(2010)maintainsadistributiononthelatentvariablesz andusesapprox-
imateinferencetointegratethemoutaswell.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
11
Density Estimation with Gaussian Mixture
Models
In earlier chapters, we covered already two fundamental problems in
machine learning: regression (Chapter 9) and dimensionality reduction
(Chapter 10). In this chapter, we will have a look at a third pillar of ma-
chine learning: density estimation. On our journey, we introduce impor-
tant concepts, such as the expectation maximization (EM) algorithm and
alatentvariableperspectiveofdensityestimationwithmixturemodels.
When we apply machine learning to data we often aim to represent
data in some way. A straightforward way is to take the data points them-
selves as the representation of the data; see Figure 11.1 for an example.
However, this approach may be unhelpful if the dataset is huge or if we
are interested in representing characteristics of the data. In density esti-
mation,werepresentthedatacompactlyusingadensityfromaparamet-
ric family, e.g., a Gaussian or Beta distribution. For example, we may be
looking for the mean and variance of a dataset in order to represent the
datacompactlyusingaGaussiandistribution.Themeanandvariancecan
be found using tools we discussed in Section 8.3: maximum likelihood or
maximumaposterioriestimation.Wecanthenusethemeanandvariance
ofthisGaussiantorepresentthedistributionunderlyingthedata,i.e.,we
think of the dataset to be a typical realization from this distribution if we
weretosamplefromit.
Figure11.1
Two-dimensional
4
datasetthatcannot
bemeaningfully
representedbya 2
Gaussian.
0
2
−
4
−
5 0 5
− x
1
348
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
x
2
11.1 GaussianMixtureModel 349
Inpractice,theGaussian(orsimilarlyallotherdistributionsweencoun-
tered so far) have limited modeling capabilities. For example, a Gaussian
approximationofthedensitythatgeneratedthedatainFigure11.1would
be a poor approximation. In the following, we will look at a more ex-
pressive family of distributions, which we can use for density estimation:
mixturemodels. mixturemodel
Mixturemodelscanbeusedtodescribeadistributionp(x)byaconvex
combinationofK simple(base)distributions
K
(cid:88)
p(x) = π p (x) (11.1)
k k
k=1
K
(cid:88)
0 ⩽ π ⩽ 1, π = 1, (11.2)
k k
k=1
where the components p are members of a family of basic distributions,
k
e.g., Gaussians, Bernoullis, or Gammas, and the π k are mixture weights. mixtureweight
Mixture models are more expressive than the corresponding base distri-
butionsbecausetheyallowformultimodaldatarepresentations,i.e.,they
candescribedatasetswithmultiple“clusters”,suchastheexampleinFig-
ure11.1.
We will focus on Gaussian mixture models (GMMs), where the basic
distributions are Gaussians. For a given dataset, we aim to maximize the
likelihood of the model parameters to train the GMM. For this purpose,
we will use results from Chapter 5, Chapter 6, and Section 7.2. However,
unlike other applications we discussed earlier (linear regression or PCA),
we will not find a closed-form maximum likelihood solution. Instead, we
will arrive at a set of dependent simultaneous equations, which we can
onlysolveiteratively.
11.1 Gaussian Mixture Model
A Gaussian mixture model is a density model where we combine a finite Gaussianmixture
(cid:0) (cid:1)
numberofK Gaussiandistributions x µ , Σ sothat model
N | k k
K
(cid:88) (cid:0) (cid:1)
p(x θ) = π x µ , Σ (11.3)
| k N | k k
k=1
K
(cid:88)
0 ⩽ π ⩽ 1, π = 1, (11.4)
k k
k=1
where we defined θ := µ ,Σ ,π : k = 1,...,K as the collection of
{ k k k }
all parameters of the model. This convex combination of Gaussian distri-
bution gives us significantly more flexibility for modeling complex densi-
tiesthanasimpleGaussiandistribution(whichwerecoverfrom(11.3)for
K = 1). An illustration is given in Figure 11.2, displaying the weighted
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
350 DensityEstimationwithGaussianMixtureModels
Figure11.2 0.30
Component1
Gaussianmixture
Component2
model.The 0.25
Component3
Gaussianmixture GMMdensity
distribution(black) 0.20
iscomposedofa
0.15
convexcombination
ofGaussian
0.10
distributionsandis
moreexpressive
0.05
thananyindividual
component.Dashed 0.00
linesrepresentthe 4 2 0 2 4 6 8
weightedGaussian − − x
components.
componentsandthemixturedensity,whichisgivenas
p(x θ) = 0.5
(cid:0)
x 2,
1(cid:1)
+0.2
(cid:0)
x 1,
2(cid:1)
+0.3
(cid:0)
x 4,
1(cid:1)
. (11.5)
| N | − 2 N | N |
11.2 Parameter Learning via Maximum Likelihood
Assume we are given a dataset = x ,...,x , where x , n =
1 N n
X { }
1,...,N, are drawn i.i.d. from an unknown distribution p(x). Our ob-
jective is to find a good approximation/representation of this unknown
distribution p(x) by means of a GMM with K mixture components. The
parameters of the GMM are the K means µ , the covariances Σ , and
k k
mixture weights π . We summarize all these free parameters in θ :=
k
π ,µ ,Σ : k = 1,...,K .
{ k k k }
Example 11.1 (Initial Setting)
F s (e big t lt au i cnr kge ):1 wG1 iM. t3 hMInitial 00 .. 23 50 π π π1 2 3N N N( ( (x x x| | |µ µ µ1 2 3, , ,σ σ σ1 2 32 2 2) )
)
mixturethree GMMdensity
0.20
mixturecomponents
(dashed)andseven 0.15
datapoints(discs).
0.10
0.05
0.00
5 0 5 10 15
− x
Throughout this chapter, we will have a simple running example that
helpsusillustrateandvisualizeimportantconcepts.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
)x(p
)x(p
11.2 ParameterLearningviaMaximumLikelihood 351
We consider a one-dimensional dataset = 3, 2.5, 1,0,2,4,5
X {− − − }
consisting of seven data points and wish to find a GMM with K = 3
componentsthatmodelsthedensityofthedata.Weinitializethemixture
componentsas
(cid:0) (cid:1)
p (x) = x 4, 1 (11.6)
1
N | −
(cid:0) (cid:1)
p (x) = x 0, 0.2 (11.7)
2
N |
(cid:0) (cid:1)
p (x) = x 8, 3 (11.8)
3
N |
and assign them equal weights π = π = π = 1. The corresponding
1 2 3 3
model(andthedatapoints)areshowninFigure11.3.
In the following, we detail how to obtain a maximum likelihood esti-
mate θ of the model parameters θ. We start by writing down the like-
ML
lihood, i.e., the predictive distribution of the training data given the pa-
rameters. We exploit our i.i.d. assumption, which leads to the factorized
likelihood
N K
(cid:89) (cid:88) (cid:0) (cid:1)
p( θ) = p(x θ), p(x θ) = π x µ , Σ , (11.9)
X | n | n | k N n | k k
n=1 k=1
where every individual likelihood term p(x θ) is a Gaussian mixture
n
|
density.Thenweobtainthelog-likelihoodas
N N K
(cid:88) (cid:88) (cid:88) (cid:0) (cid:1)
logp( θ) = logp(x θ) = log π x µ , Σ . (11.10)
X | n | k N n | k k
n=1 n=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125)
=:L
Weaimtofindparametersθ∗ thatmaximizethelog-likelihood defined
ML L
in (11.10). Our “normal” procedure would be to compute the gradient
d /dθ of the log-likelihood with respect to the model parameters θ, set
L
it to 0, and solve for θ. However, unlike our previous examples for max-
imum likelihood estimation (e.g., when we discussed linear regression in
Section 9.2), we cannot obtain a closed-form solution. However, we can
exploitaniterativeschemetofindgoodmodelparametersθ ,whichwill
ML
turnouttobetheEMalgorithmforGMMs.Thekeyideaistoupdateone
modelparameteratatimewhilekeepingtheothersfixed.
Remark. If we were to consider a single Gaussian as the desired density,
thesumoverk in(11.10)vanishes,andthelog canbeapplieddirectlyto
theGaussiancomponent,suchthatweget
log (cid:0) x µ, Σ(cid:1) = D log(2π) 1 logdet(Σ) 1(x µ)⊤Σ−1(x µ).
N | −2 − 2 − 2 − −
(11.11)
This simple form allows us to find closed-form maximum likelihood esti-
matesofµandΣ,asdiscussedinChapter8.In(11.10),wecannotmove
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
352 DensityEstimationwithGaussianMixtureModels
thelogintothesumoverk sothatwecannotobtainasimpleclosed-form
maximumlikelihoodsolution.
♢
Any local optimum of a function exhibits the property that its gradi-
entwithrespecttotheparametersmustvanish(necessarycondition);see
Chapter7.Inourcase,weobtainthefollowingnecessaryconditionswhen
weoptimizethelog-likelihoodin(11.10)withrespecttotheGMMparam-
etersµ ,Σ ,π :
k k k
∂
L = 0⊤
(cid:88)N ∂logp(x
n
|θ)
= 0⊤, (11.12)
∂µ ⇐⇒ ∂µ
k n=1 k
∂
L = 0
(cid:88)N ∂logp(x
n
|θ)
= 0, (11.13)
∂Σ ⇐⇒ ∂Σ
k n=1 k
∂
L = 0
(cid:88)N ∂logp(x
n
|θ)
= 0. (11.14)
∂π ⇐⇒ ∂π
k n=1 k
For all three necessary conditions, by applying the chain rule (see Sec-
tion5.2.2),werequirepartialderivativesoftheform
∂logp(x θ) 1 ∂p(x θ)
n | = n | , (11.15)
∂θ p(x θ) ∂θ
n
|
whereθ = µ ,Σ ,π ,k = 1,...,K arethemodelparametersand
{ k k k }
1 1
= . (11.16)
p(x n |θ) (cid:80)K j=1π j N(cid:0) x n |µ j, Σ j(cid:1)
In the following, we will compute the partial derivatives (11.12) through
(11.14). But before we do this, we introduce a quantity that will play a
centralroleintheremainderofthischapter:responsibilities.
11.2.1 Responsibilities
Wedefinethequantity
(cid:0) (cid:1)
π x µ , Σ
r := k N n | k k (11.17)
nk (cid:80)K
π
(cid:0)
x µ , Σ
(cid:1)
j=1 j N n | j j
responsibility as the responsibility of the kth mixture component for the nth data point.
The responsibility r of the kth mixture component for data point x is
nk n
proportionaltothelikelihood
(cid:0) (cid:1)
p(x π ,µ ,Σ ) = π x µ , Σ (11.18)
n | k k k k N n | k k
rnfollowsa of the mixture component given the data point. Therefore, mixture com-
Boltzmann/Gibbs ponents have a high responsibility for a data point when the data point
distribution.
could be a plausible sample from that mixture component. Note that
r := [r ,...,r ]⊤ RK is a (normalized) probability vector, i.e.,
n n1 nK
∈
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
11.2 ParameterLearningviaMaximumLikelihood 353
(cid:80) r = 1 with r ⩾ 0. This probability vector distributes probabil-
k nk nk
ity mass among the K mixture components, and we can think of r as a
n
“soft assignment” of x n to the K mixture components. Therefore, the re- Theresponsibility
sponsibility r
nk
from (11.17) represents the probability that x
n
has been r nkisthe
generatedbythekthmixturecomponent. probabilitythatthe
kthmixture
component
Example 11.2 (Responsibilities) generatedthenth
datapoint.
ForourexamplefromFigure11.3,wecomputetheresponsibilitiesr
nk
 1.0 0.0 0.0 
 1.0 0.0 0.0 
 
0.057 0.943 0.0 
 0.001 0.999 0.0   RN×K. (11.19)
  ∈
 0.0 0.066 0.934
 
 0.0 0.0 1.0 
0.0 0.0 1.0
Here the nth row tells us the responsibilities of all mixture components
for x . The sum of all K responsibilities for a data point (sum of every
n
row) is 1. The kth column gives us an overview of the responsibility of
thekthmixturecomponent.Wecanseethatthethirdmixturecomponent
(third column) is not responsible for any of the first four data points, but
takes much responsibility of the remaining data points. The sum of all
entries of a column gives us the values N , i.e., the total responsibility of
k
the kth mixture component. In our example, we get N = 2.058, N =
1 2
2.008, N = 2.934.
3
In the following, we determine the updates of the model parameters
µ ,Σ ,π for given responsibilities. We will see that the update equa-
k k k
tions all depend on the responsibilities, which makes a closed-form solu-
tiontothemaximumlikelihoodestimationproblemimpossible.However,
for given responsibilities we will be updating one model parameter at a
time, while keeping the others fixed. After this, we will recompute the
responsibilities.Iteratingthesetwostepswilleventuallyconvergetoalo-
cal optimum and is a specific instantiation of the EM algorithm. We will
discussthisinsomemoredetailinSection11.3.
11.2.2 Updating the Means
Theorem 11.1 (Update of the GMM Means). The update of the mean pa-
rametersµ ,k = 1,...,K,oftheGMMisgivenby
k
(cid:80)N
r x
µnew = n=1 nk n , (11.20)
k (cid:80)N
r
n=1 nk
wheretheresponsibilitiesr aredefinedin(11.17).
nk
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
354 DensityEstimationwithGaussianMixtureModels
Remark. The update of the means µ of the individual mixture compo-
k
nentsin(11.20)dependsonallmeans,covariancematricesΣ ,andmix-
k
ture weights π via r given in (11.17). Therefore, we cannot obtain a
k nk
closed-formsolutionforallµ atonce.
k ♢
Proof From (11.15), we see that the gradient of the log-likelihood with
respecttothemeanparametersµ ,k = 1,...,K,requiresustocompute
k
thepartialderivative
∂p(x
n
|θ)
=
(cid:88)K
π
∂ N(cid:0) x
n
|µ j, Σ j(cid:1)
= π
∂ N(cid:0) x
n
|µ k, Σ k(cid:1)
(11.21a)
∂µ j ∂µ k ∂µ
k j=1 k k
= π (x µ )⊤Σ−1 (cid:0) x µ , Σ (cid:1) , (11.21b)
k n − k k N n | k k
whereweexploitedthatonlythekthmixturecomponentdependsonµ .
k
Weuseourresultfrom(11.21b)in(11.15)andputeverythingtogether
sothatthedesiredpartialderivativeof withrespecttoµ isgivenas
L k
∂ (cid:88)N ∂logp(x
n
θ) (cid:88)N 1 ∂p(x
n
θ)
L = | = | (11.22a)
∂µ ∂µ p(x θ) ∂µ
k n=1 k n=1 n | k
=
(cid:88)N
(x µ )⊤Σ−1
π
k
N(cid:0) x
n
|µ k, Σ k(cid:1)
(11.22b)
n − k k (cid:80)K π (cid:0) x µ , Σ (cid:1)
n=1 j=1 j N n | j j
(cid:124) (cid:123)(cid:122) (cid:125)
=rnk
N
(cid:88)
= r (x µ )⊤Σ−1. (11.22c)
nk n − k k
n=1
Hereweusedtheidentityfrom(11.16)andtheresultofthepartialderiva-
tivein(11.21b)togetto(11.22b).Thevaluesr aretheresponsibilities
nk
wedefinedin(11.17).
Wenowsolve(11.22c)forµnew sothat ∂L(µn kew) = 0⊤ andobtain
k ∂µ
k
(cid:88)N
r x =
(cid:88)N
r µnew µnew =
(cid:80)N n=1r nkx
n =
1 (cid:88)N
r x ,
n=1 nk n n=1 nk k ⇐⇒ k (cid:80)N n=1r nk N k n=1 nk n
(11.23)
wherewedefined
N
(cid:88)
N := r (11.24)
k nk
n=1
as the total responsibility of the kth mixture component for the entire
dataset.ThisconcludestheproofofTheorem11.1.
Intuitively,(11.20)canbeinterpretedasanimportance-weightedMonte
Carlo estimate of the mean, where the importance weights of data point
x are the responsibilities r of the kth cluster for x , k = 1,...,K.
n nk n
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
11.2 ParameterLearningviaMaximumLikelihood 355
Therefore, the mean µ k is pulled toward a data point x n with strength Figure11.4 Update
givenbyr .Themeansarepulledstrongertowarddatapointsforwhich ofthemean
nk
parameterof
thecorrespondingmixturecomponenthasahighresponsibility,i.e.,ahigh
mixturecomponent
likelihood.Figure11.4illustratesthis.Wecanalsointerpretthemeanup-
inaGMM.The
date in (11.20) as the expected value of all data points under the distri- meanµisbeing
butiongivenby pulledtoward
individualdata
r k := [r 1k,...,r Nk]⊤/N k, (11.25) pointswiththe
weightsgivenbythe
whichisanormalizedprobabilityvector,i.e.,
corresponding
µ E [ ]. (11.26) responsibilities.
k ← rk X x2 x3
r2
Example 11.3 (Mean Updates)
x1
r1 r3
µ
Figure11.5 Effect
0.30 π1N(x |µ1,σ12) 0.30 π1N(x |µ1,σ12) ofupdatingthe
0.25 π π G2 3 MN N M( (x x| | dµ µ e2 3 n, , sσ σ it2 32 2 y) ) 0.25 π π G2 3 MN N M( (x x| | dµ µ e2 3 n, , sσ σ it2 32 2 y) ) m GMea Mn .v (a al )ue Gs Min Ma
0.20 0.20 beforeupdatingthe
0.15 0.15 meanvalues;
0.10 0.10 (b)GMMafter
updatingthemean
0.05 0.05
valuesµ kwhile
0.00 0.00
retainingthe
5 0 5 10 15 5 0 5 10 15
− x − x variancesand
(a) GMM density and individual components (b) GMM density and individual components mixtureweights.
priortoupdatingthemeanvalues. afterupdatingthemeanvalues.
In our example from Figure 11.3, the mean values are updated as fol-
lows:
µ : 4 2.7 (11.27)
1
− → −
µ : 0 0.4 (11.28)
2
→ −
µ : 8 3.7 (11.29)
3
→
Here we see that the means of the first and third mixture component
move toward the regime of the data, whereas the mean of the second
component does not change so dramatically. Figure 11.5 illustrates this
change, where Figure 11.5(a) shows the GMM density prior to updating
themeansandFigure11.5(b)showstheGMMdensityafterupdatingthe
meanvaluesµ .
k
The update of the mean parameters in (11.20) look fairly straight-
forward. However, note that the responsibilities r are a function of
nk
π ,µ ,Σ for all j = 1,...,K, such that the updates in (11.20) depend
j j j
on all parameters of the GMM, and a closed-form solution, which we ob-
tained for linear regression in Section 9.2 or PCA in Chapter 10, cannot
beobtained.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
)x(p )x(p
356 DensityEstimationwithGaussianMixtureModels
11.2.3 Updating the Covariances
Theorem 11.2 (Updates of the GMM Covariances). The update of the co-
varianceparametersΣ ,k = 1,...,K oftheGMMisgivenby
k
N
1 (cid:88)
Σnew = r (x µ )(x µ )⊤, (11.30)
k N nk n − k n − k
k n=1
wherer andN aredefinedin(11.17)and(11.24),respectively.
nk k
Proof To prove Theorem 11.2, our approach is to compute the partial
derivativesofthelog-likelihood withrespecttothecovariancesΣ ,set
k
L
themto0,andsolveforΣ .Westartwithourgeneralapproach
k
∂
L =
(cid:88)N ∂logp(x
n
|θ)
=
(cid:88)N 1 ∂p(x
n
|θ)
. (11.31)
∂Σ ∂Σ p(x θ) ∂Σ
k n=1 k n=1 n | k
We already know 1/p(x θ) from (11.16). To obtain the remaining par-
n
|
tialderivative∂p(x θ)/∂Σ ,wewritedownthedefinitionoftheGaus-
n k
|
siandistributionp(x θ)(see(11.9))anddropalltermsbutthekth.We
n
|
thenobtain
∂p(x θ)
n
| (11.32a)
∂Σ
k
= ∂Σ∂ (cid:18) π k(2π)−D 2 det(Σ k)−1 2 exp(cid:0) −1 2(x n −µ k)⊤Σ− k1(x n −µ k)(cid:1)(cid:19)
k
(11.32b)
= π k(2π)−D 2 (cid:20) ∂Σ∂ det(Σ k)−1 2 exp(cid:0) −1 2(x n −µ k)⊤Σ− k1(x n −µ k)(cid:1)
k
+det(Σ k)−1 2 ∂Σ∂ exp(cid:0) −1 2(x n −µ k)⊤Σ− k1(x n −µ k)(cid:1)(cid:21) . (11.32c)
k
Wenowusetheidentities
∂ 1 1 1
∂Σ
det(Σ k)− 2 (5. =101)
−2
det(Σ k)− 2Σ− k1, (11.33)
k
∂
(x µ )⊤Σ−1(x µ ) (5. =103) Σ−1(x µ )(x µ )⊤Σ−1
∂Σ n − k k n − k − k n − k n − k k
k
(11.34)
andobtain(aftersomerearranging)thedesiredpartialderivativerequired
in(11.31)as
∂p(x n |θ) = π (cid:0) x µ , Σ (cid:1)
∂Σ k N n | k k
k
(cid:2) 1(Σ−1 Σ−1(x µ )(x µ )⊤Σ−1)(cid:3) . (11.35)
· −2 k − k n − k n − k k
Putting everything together, the partial derivative of the log-likelihood
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
11.2 ParameterLearningviaMaximumLikelihood 357
withrespecttoΣ isgivenby
k
∂ (cid:88)N ∂logp(x
n
θ) (cid:88)N 1 ∂p(x
n
θ)
L = | = | (11.36a)
∂Σ ∂Σ p(x θ) ∂Σ
k n=1 k n=1 n | k
=
(cid:88)N π
k
N(cid:0) x
n
|µ k, Σ k(cid:1)
(cid:80)K
π
(cid:0)
x µ , Σ
(cid:1)
n=1 j=1 j N n | j j
(cid:124) (cid:123)(cid:122) (cid:125)
=rnk
(cid:2) 1(Σ−1 Σ−1(x µ )(x µ )⊤Σ−1)(cid:3) (11.36b)
· −2 k − k n − k n − k k
N
1 (cid:88)
= r (Σ−1 Σ−1(x µ )(x µ )⊤Σ−1) (11.36c)
−2 nk k − k n − k n − k k
n=1
(cid:32) (cid:33)
N N
1 (cid:88) 1 (cid:88)
= Σ−1 r + Σ−1 r (x µ )(x µ )⊤ Σ−1.
−2 k nk 2 k nk n − k n − k k
n=1 n=1
(cid:124) (cid:123)(cid:122) (cid:125)
=Nk
(11.36d)
We see that the responsibilities r also appear in this partial derivative.
nk
Setting this partial derivative to 0, we obtain the necessary optimality
condition
(cid:32) (cid:33)
N
(cid:88)
N Σ−1 = Σ−1 r (x µ )(x µ )⊤ Σ−1 (11.37a)
k k k nk n − k n − k k
n=1
(cid:32) (cid:33)
N
(cid:88)
N I = r (x µ )(x µ )⊤ Σ−1. (11.37b)
⇐⇒ k nk n − k n − k k
n=1
BysolvingforΣ ,weobtain
k
N
1 (cid:88)
Σnew = r (x µ )(x µ )⊤, (11.38)
k N nk n − k n − k
k n=1
wherer istheprobabilityvectordefinedin(11.25).Thisgivesusasim-
k
pleupdateruleforΣ fork = 1,...,K andprovesTheorem11.2.
k
Similar to the update of µ in (11.20), we can interpret the update of
k
the covariance in (11.30) as an importance-weighted expected value of
thesquareofthecentereddata ˜ := x µ ,...,x µ .
Xk { 1 − k N − k}
Example 11.4 (Variance Updates)
InourexamplefromFigure11.3,thevariancesareupdatedasfollows:
σ2 : 1 0.14 (11.39)
1 →
σ2 : 0.2 0.44 (11.40)
2 →
σ2 : 3 1.53 (11.41)
3 →
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
358 DensityEstimationwithGaussianMixtureModels
Here we see that the variances of the first and third component shrink
significantly, whereas the variance of the second component increases
slightly.
Figure 11.6 illustrates this setting. Figure 11.6(a) is identical (but
zoomed in) to Figure 11.5(b) and shows the GMM density and its indi-
vidual components prior to updating the variances. Figure 11.6(b) shows
theGMMdensityafterupdatingthevariances.
Figure11.6 Effect
ofupdatingthe 0.30 π1N(x |µ1,σ12) 0.35 π1N(x |µ1,σ12)
v (aa )ri Gan Mc Mes bin efa orG eMM. 0.25 π π G2 3 MN N M( (x x| | dµ µ e2 3 n, , sσ σ it2 32 2 y) ) 00 .. 23 50 π π G2 3 MN N M( (x x| | dµ µ e2 3 n, , sσ σ it2 32 2 y) )
updatingthe 0.20
0.20
variances;(b)GMM 0.15
0.15
afterupdatingthe 0.10 0.10
varianceswhile
0.05 0.05
retainingthemeans
0.00 0.00
andmixture
4 2 0 2 4 6 8 4 2 0 2 4 6 8
weights. − − x − − x
(a) GMM density and individual components (b) GMM density and individual components
priortoupdatingthevariances. afterupdatingthevariances.
Similartotheupdateofthemeanparameters,wecaninterpret(11.30)
as a Monte Carlo estimate of the weighted covariance of data points x
n
associated with the kth mixture component, where the weights are the
responsibilities r . As with the updates of the mean parameters, this up-
nk
datedependsonallπ ,µ ,Σ , j = 1,...,K,throughtheresponsibilities
j j j
r ,whichprohibitsaclosed-formsolution.
nk
11.2.4 Updating the Mixture Weights
Theorem11.3(UpdateoftheGMMMixtureWeights). Themixtureweights
oftheGMMareupdatedas
N
πnew = k , k = 1,...,K, (11.42)
k N
whereN isthenumberofdatapointsandN isdefinedin(11.24).
k
Proof To find the partial derivative of the log-likelihood with respect
to the weight parameters π , k = 1,...,K, we account for the con-
k
(cid:80)
straint π = 1 by using Lagrange multipliers (see Section 7.2). The
k k
Lagrangianis
(cid:32) (cid:33)
K
(cid:88)
L = +λ π 1 (11.43a)
k
L −
k=1
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
)x(p )x(p
11.2 ParameterLearningviaMaximumLikelihood 359
(cid:32) (cid:33)
N K K
(cid:88) (cid:88) (cid:0) (cid:1) (cid:88)
= log π x µ , Σ +λ π 1 , (11.43b)
k N n | k k k −
n=1 k=1 k=1
where is the log-likelihood from (11.10) and the second term encodes
L
for the equality constraint that all the mixture weights need to sum up to
1.Weobtainthepartialderivativewithrespecttoπ as
k
∂L
=
(cid:88)N N(cid:0) x
n
|µ k, Σ k(cid:1)
+λ (11.44a)
∂π k n=1 (cid:80)K j=1π j N(cid:0) x n |µ j, Σ j(cid:1)
=
1 (cid:88)N π
k
N(cid:0) x
n
|µ k, Σ k(cid:1)
+λ =
N
k +λ, (11.44b)
π k n=1 (cid:80)K j=1π j N(cid:0) x n |µ j, Σ j(cid:1) π k
(cid:124) (cid:123)(cid:122) (cid:125)
=Nk
andthepartialderivativewithrespecttotheLagrangemultiplierλas
∂L (cid:88)K
= π 1. (11.45)
∂λ k −
k=1
Setting both partial derivatives to 0 (necessary condition for optimum)
yieldsthesystemofequations
N
π = k , (11.46)
k − λ
K
(cid:88)
1 = π . (11.47)
k
k=1
Using(11.46)in(11.47)andsolvingforπ ,weobtain
k
(cid:88)K
π = 1
(cid:88)K N
k = 1
N
= 1 λ = N .
k ⇐⇒ − λ ⇐⇒ −λ ⇐⇒ −
k=1 k=1
(11.48)
Thisallowsustosubstitute N forλin(11.46)toobtain
−
N
πnew = k , (11.49)
k N
whichgivesustheupdatefortheweightparametersπ andprovesTheo-
k
rem11.3.
We can identify the mixture weight in (11.42) as the ratio of the to-
tal responsibility of the kth cluster and the number of data points. Since
(cid:80)
N = N , the number of data points can also be interpreted as the
k k
totalresponsibilityofallmixturecomponentstogether,suchthatπ isthe
k
relativeimportanceofthekthmixturecomponentforthedataset.
Remark. Since N =
(cid:80)N
r , the update equation (11.42) for the mix-
k i=1 nk
ture weights π also depends on all π ,µ ,Σ ,j = 1,...,K via the re-
k j j j
sponsibilitiesr .
nk
♢
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
360 DensityEstimationwithGaussianMixtureModels
Example 11.5 (Weight Parameter Updates)
Figure11.7 Effect
o mf ixu tp ud ra eti wn eg igth he tsina 00 .. 33 05 π π π1 2 3N N N( ( (x x x| | |µ µ µ1 2 3, , ,σ σ σ1 2 32 2 2) )
)
00 .. 23 50 π π π1 2 3N N N( ( (x x x| | |µ µ µ1 2 3, , ,σ σ σ1 2 32 2 2) )
)
GMM.(a)GMM 0.25 GMMdensity GMMdensity
0.20
beforeupdatingthe 0.20
0.15
mixtureweights; 0.15
(b)GMMafter 0.10 0.10
updatingthe 0.05 0.05
mixtureweights 0.00 0.00
whileretainingthe −4 −2 0 x2 4 6 8 −4 −2 0 x2 4 6 8
meansand
(a)GMMdensityandindividualcomponents (b)GMMdensityandindividualcomponents
variances.Notethe
priortoupdatingthemixtureweights. afterupdatingthemixtureweights.
differentscalesof
theverticalaxes. In our running example from Figure 11.3, the mixture weights are up-
datedasfollows:
π : 1 0.29 (11.50)
1 3 →
π : 1 0.29 (11.51)
2 3 →
π : 1 0.42 (11.52)
3 3 →
Here we see that the third component gets more weight/importance,
while the other components become slightly less important. Figure 11.7
illustrates the effect of updating the mixture weights. Figure 11.7(a) is
identicaltoFigure11.6(b)andshowstheGMMdensityanditsindividual
components prior to updating the mixture weights. Figure 11.7(b) shows
theGMMdensityafterupdatingthemixtureweights.
Overall, having updated the means, the variances, and the weights
once, we obtain the GMM shown in Figure 11.7(b). Compared with the
initializationshowninFigure11.3,wecanseethattheparameterupdates
causedtheGMMdensitytoshiftsomeofitsmasstowardthedatapoints.
After updating the means, variances, and weights once, the GMM fit
in Figure 11.7(b) is already remarkably better than its initialization from
Figure11.3.Thisisalsoevidencedbythelog-likelihoodvalues,whichin-
creasedfrom28.3(initialization)to14.4afteronecompleteupdatecycle.
11.3 EM Algorithm
Unfortunately,theupdatesin(11.20),(11.30),and(11.42)donotconsti-
tute a closed-form solution for the updates of the parameters µ ,Σ ,π
k k k
ofthemixturemodelbecausetheresponsibilitiesr dependonthosepa-
nk
rametersinacomplexway.However,theresultssuggestasimpleiterative
scheme for finding a solution to the parameters estimation problem via
EMalgorithm maximum likelihood. The expectation maximization algorithm (EM algo-
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
)x(p )x(p
11.3 EMAlgorithm 361
rithm) was proposed by Dempster et al. (1977) and is a general iterative
schemeforlearningparameters(maximumlikelihoodorMAP)inmixture
modelsand,moregenerally,latent-variablemodels.
InourexampleoftheGaussianmixturemodel,wechooseinitialvalues
forµ ,Σ ,π andalternateuntilconvergencebetween
k k k
E-step: Evaluate the responsibilities r (posterior probability of data
nk
pointnbelongingtomixturecomponentk).
M-step: Use the updated responsibilities to reestimate the parameters
µ ,Σ ,π .
k k k
EverystepintheEMalgorithmincreasesthelog-likelihoodfunction(Neal
and Hinton, 1999). For convergence, we can check the log-likelihood or
the parameters directly. A concrete instantiation of the EM algorithm for
estimatingtheparametersofaGMMisasfollows:
1. Initializeµ ,Σ ,π .
k k k
2. E-step: Evaluate responsibilities r for every data point x using cur-
nk n
rentparametersπ ,µ ,Σ :
k k k
(cid:0) (cid:1)
π x µ , Σ
r = k N n | k k . (11.53)
nk (cid:80) (cid:0) (cid:1)
π x µ , Σ
j j N n | j j
3. M-step: Reestimate parameters π ,µ ,Σ using the current responsi-
k k k
bilitiesr nk (fromE-step): Havingupdatedthe
meansµ
N k
1 (cid:88) in(11.54),theyare
µ = r x , (11.54)
k N nk n subsequentlyused
k n=1 in(11.55)toupdate
1 (cid:88)N thecorresponding
Σ = r (x µ )(x µ )⊤, (11.55) covariances.
k N nk n − k n − k
k n=1
N
π = k . (11.56)
k N
Example 11.6 (GMM Fit)
Figure11.8 EM
0.30 π1N(x |µ1,σ12) 28 algorithmappliedto
π2N(x |µ2,σ22) 26 theGMMfrom
0.25 π G3 MN M(x | dµ e3 n, sσ it32 y) 24 Figure11.2.(a)
0.20 FinalGMMfit;
22
0.15 (b)negative
20
0.10 log-likelihoodasa
18
functionoftheEM
0.05 16
iteration.
0.00 14
−5 0
x
5 10 15 0 1 2 Iteration3 4 5
(a)FinalGMMfit.Afterfiveiterations,theEM (b)Negativelog-likelihoodasafunctionofthe
algorithmconvergesandreturnsthisGMM. EMiterations.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
)x(p
doohilekil-golevitageN
362 DensityEstimationwithGaussianMixtureModels
Figure11.9 10
Illustrationofthe 104
EMalgorithmfor 5
fittingaGaussian
mixturemodelwith 0
threecomponentsto 6 103
×
atwo-dimensional
5
−
dataset.(a)Dataset;
(b)negative
10
4 ×103
log-likelihood − −10 −5 x0
1
5 10 0 20 EMiteratio4 n0 60
(lowerisbetter)as
(a)Dataset. (b)Negativelog-likelihood.
afunctionoftheEM
iterations.Thered
10 10
dotsindicatethe
iterationsforwhich
5 5
themixture
componentsofthe
0 0
correspondingGMM
fitsareshownin(c)
through(f).The −5 −5
yellowdiscsindicate
themeansofthe −10 10 5 0 5 10 −10 10 5 0 5 10
Gaussianmixture
− − x1 − − x1
components. (c)EMinitialization. (d)EMafteroneiteration.
Figure11.10(a)
showsthefinal 10 10
GMMfit.
5 5
0 0
5 5
− −
10 10
− 10 5 0 5 10 − 10 5 0 5 10
− − x1 − − x1
(e)EMafter10iterations. (f)EMafter62iterations.
WhenwerunEMonourexamplefromFigure11.3,weobtainthefinal
result shown in Figure 11.8(a) after five iterations, and Figure 11.8(b)
shows how the negative log-likelihood evolves as a function of the EM
iterations.ThefinalGMMisgivenas
(cid:0) (cid:1) (cid:0) (cid:1)
p(x) = 0.29 x 2.75, 0.06 +0.28 x 0.50, 0.25
N | − N | − (11.57)
(cid:0) (cid:1)
+0.43 x 3.64, 1.63 .
N |
We applied the EM algorithm to the two-dimensional dataset shown
in Figure 11.1 with K = 3 mixture components. Figure 11.9 illustrates
some steps of the EM algorithm and shows the negative log-likelihood as
a function of the EM iteration (Figure 11.9(b)). Figure 11.10(a) shows
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
2x
2x
2x
doohilekil-golevitageN
2x
2x
11.4 Latent-VariablePerspective 363
6 6 Figure11.10 GMM
fitand
4 4
responsibilities
2 2 whenEMconverges.
0 0 (a)GMMfitwhen
EMconverges;
2 2
− − (b)eachdatapoint
4 4 iscoloredaccording
− −
tothe
6 6
− −5 x0
1
5 − −5 x0
1
5 responsibilitiesof
themixture
(a)GMMfitafter62iterations. (b)Datasetcoloredaccordingtotherespon-
components.
sibilitiesofthemixturecomponents.
the corresponding final GMM fit. Figure 11.10(b) visualizes the final re-
sponsibilitiesofthemixturecomponentsforthedatapoints.Thedatasetis
coloredaccordingtotheresponsibilitiesofthemixturecomponentswhen
EM converges. While a single mixture component is clearly responsible
for the data on the left, the overlap of the two data clusters on the right
could have been generated by two mixture components. It becomes clear
that there are data points that cannot be uniquely assigned to a single
component (either blue or yellow), such that the responsibilities of these
twoclustersforthosepointsarearound0.5.
11.4 Latent-Variable Perspective
WecanlookattheGMMfromtheperspectiveofadiscretelatent-variable
model, i.e., where the latent variable z can attain only a finite set of val-
ues.ThisisincontrasttoPCA,wherethelatentvariableswerecontinuous-
valuednumbersinRM.
The advantages of the probabilistic perspective are that (i) it will jus-
tifysomeadhocdecisionswemadeintheprevioussections,(ii)itallows
for a concrete interpretation of the responsibilities as posterior probabil-
ities, and (iii) the iterative algorithm for updating the model parameters
can be derived in a principled manner as the EM algorithm for maximum
likelihoodparameterestimationinlatent-variablemodels.
11.4.1 Generative Process and Probabilistic Model
ToderivetheprobabilisticmodelforGMMs,itisusefultothinkaboutthe
generativeprocess,i.e.,theprocessthatallowsustogeneratedata,using
aprobabilisticmodel.
We assume a mixture model with K components and that a data point
x can be generated by exactly one mixture component. We introduce a
binaryindicatorvariablez 0,1 withtwostates(seeSection6.2)that
k
∈ { }
indicates whether the kth mixture component generated that data point
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
2x 2x
364 DensityEstimationwithGaussianMixtureModels
sothat
(cid:0) (cid:1)
p(x z = 1) = x µ , Σ . (11.58)
| k N | k k
We define z := [z ,...,z ]⊤ RK as a probability vector consisting of
1 K
∈
K 1many0sandexactlyone1.Forexample,forK = 3,avalidzwould
−
be z = [z ,z ,z ]⊤ = [0,1,0]⊤, which would select the second mixture
1 2 3
componentsincez = 1.
2
Remark. Sometimes this kind of probability distribution is called “multi-
noulli”, a generalization of the Bernoulli distribution to more than two
values(Murphy,2012).
♢
one-hotencoding The properties of z imply that
(cid:80)K
k=1z k = 1. Therefore, z is a one-hot
1-of-K encoding(also:1-of-K representation).
representation Thus far, we assumed that the indicator variables z are known. How-
k
ever,inpractice,thisisnotthecase,andweplaceapriordistribution
K
(cid:88)
p(z) = π = [π ,...,π ]⊤, π = 1, (11.59)
1 K k
k=1
onthelatentvariablez.Thenthekthentry
π = p(z = 1) (11.60)
k k
of this probability vector describes the probability that the kth mixture
Figure11.11 componentgenerateddatapointx.
Graphicalmodelfor
Remark(SamplingfromaGMM). Theconstructionofthislatent-variable
aGMMwithasingle
model (see the corresponding graphical model in Figure 11.11) lends it-
datapoint.
selftoaverysimplesamplingprocedure(generativeprocess)togenerate
π
data:
z 1. Samplez(i) p(z).
∼
2. Samplex(i) p(x z(i) = 1).
∼ |
µ
k In the first step, we select a mixture component i (via the one-hot encod-
Σ k x ing z) at random according to p(z) = π; in the second step we draw a
k=1,...,K samplefromthecorrespondingmixturecomponent.Whenwediscardthe
samples of the latent variable so that we are left with the x(i), we have
valid samples from the GMM. This kind of sampling, where samples of
random variables depend on samples from the variable’s parents in the
ancestralsampling graphicalmodel,iscalledancestralsampling.
♢
Generally, a probabilistic model is defined by the joint distribution of
the data and the latent variables (see Section 8.4). With the prior p(z)
definedin(11.59)and(11.60)andtheconditionalp(x z)from(11.58),
|
weobtainallK componentsofthisjointdistributionvia
(cid:0) (cid:1)
p(x,z = 1) = p(x z = 1)p(z = 1) = π x µ , Σ (11.61)
k | k k k N | k k
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
11.4 Latent-VariablePerspective 365
fork = 1,...,K,sothat
   (cid:0) (cid:1) 
p(x,z = 1) π x µ , Σ
1 1 N | 1 1
. .
p(x,z) =   . .   =   . .   , (11.62)
(cid:0) (cid:1)
p(x,z = 1) π x µ , Σ
K K N | K K
whichfullyspecifiestheprobabilisticmodel.
11.4.2 Likelihood
To obtain the likelihood p(x θ) in a latent-variable model, we need to
|
marginalize out the latent variables (see Section 8.4.3). In our case, this
can be done by summing out all latent variables from the joint p(x,z)
in(11.62)sothat
(cid:88)
p(x θ) = p(x θ,z)p(z θ), θ := µ ,Σ ,π : k = 1,...,K .
| | | { k k k }
z
(11.63)
Wenowexplicitlyconditionontheparametersθoftheprobabilisticmodel,
whichwepreviouslyomitted.In(11.63),wesumoverallK possibleone-
(cid:80)
hot encodings of z, which is denoted by . Since there is only a single
z
nonzero single entry in each z there are only K possible configurations/
settingsofz.Forexample,ifK = 3,thenz canhavetheconfigurations
     
1 0 0
0, 1, 0 . (11.64)
0 0 1
Summing over all possible configurations of z in (11.63) is equivalent to
lookingatthenonzeroentryofthez-vectorandwriting
(cid:88)
p(x θ) = p(x θ,z)p(z θ) (11.65a)
| | |
z
K
(cid:88)
= p(x θ,z = 1)p(z = 1 θ) (11.65b)
k k
| |
k=1
sothatthedesiredmarginaldistributionisgivenas
K
p(x θ) (11 =.65b) (cid:88) p(x θ,z = 1)p(z = 1 θ) (11.66a)
k k
| | |
k=1
K
(cid:88) (cid:0) (cid:1)
= π x µ , Σ , (11.66b)
k N | k k
k=1
whichweidentifyastheGMMmodelfrom(11.3).Givenadataset ,we
X
immediatelyobtainthelikelihood
N N K
p( θ) = (cid:89) p(x θ) (11 =.66b) (cid:89)(cid:88) π (cid:0) x µ , Σ (cid:1) , (11.67)
X | n | k N n | k k
n=1 n=1k=1
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
366 DensityEstimationwithGaussianMixtureModels
π
Figure11.12
Graphicalmodelfor
aGMMwithN data
points.
z
n
µ
k
Σ k x n
k=1,...,K
n=1,...,N
which is exactly the GMM likelihood from (11.9). Therefore, the latent-
variable model with latent indicators z is an equivalent way of thinking
k
aboutaGaussianmixturemodel.
11.4.3 Posterior Distribution
Letushaveabrieflookattheposteriordistributiononthelatentvariable
z.AccordingtoBayes’theorem,theposteriorofthekthcomponenthaving
generateddatapointx
p(z = 1)p(x z = 1)
p(z = 1 x) = k | k , (11.68)
k | p(x)
where the marginal p(x) is given in (11.66b). This yields the posterior
distributionforthekthindicatorvariablez
k
(cid:0) (cid:1)
p(z = 1)p(x z = 1) π x µ , Σ
p(z = 1 x) = k | k = k N | k k ,
k | (cid:80)K p(z = 1)p(x z = 1) (cid:80)K π (cid:0) x µ , Σ (cid:1)
j=1 j | j j=1 j N | j j
(11.69)
which we identify as the responsibility of the kth mixture component for
datapointx.NotethatweomittedtheexplicitconditioningontheGMM
parametersπ ,µ ,Σ wherek = 1,...,K.
k k k
11.4.4 Extension to a Full Dataset
Thus far, we have only discussed the case where the dataset consists only
of a single data point x. However, the concepts of the prior and posterior
canbedirectlyextendedtothecaseofN datapoints := x ,...,x .
1 N
X { }
IntheprobabilisticinterpretationoftheGMM,everydatapointx pos-
n
sessesitsownlatentvariable
z = [z ,...,z ]⊤ RK. (11.70)
n n1 nK
∈
Previously (when we only considered a single data point x), we omitted
theindexn,butnowthisbecomesimportant.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
11.4 Latent-VariablePerspective 367
We share the same prior distribution π across all latent variables z .
n
The corresponding graphical model is shown in Figure 11.12, where we
usetheplatenotation.
The conditional distribution p(x ,...,x z ,...,z ) factorizes over
1 N 1 N
|
thedatapointsandisgivenas
N
(cid:89)
p(x ,...,x z ,...,z ) = p(x z ). (11.71)
1 N 1 N n n
| |
n=1
To obtain the posterior distribution p(z = 1 x ), we follow the same
nk n
|
reasoningasinSection11.4.3andapplyBayes’theoremtoobtain
p(x z = 1)p(z = 1)
p(z = 1 x ) = n | nk nk (11.72a)
nk | n (cid:80)K p(x z = 1)p(z = 1)
j=1 n | nj nj
(cid:0) (cid:1)
π x µ , Σ
= k N n | k k = r . (11.72b)
(cid:80)K
π
(cid:0)
x µ , Σ
(cid:1) nk
j=1 j N n | j j
This means that p(z = 1 x ) is the (posterior) probability that the kth
k n
|
mixture component generated data point x and corresponds to the re-
n
sponsibility r we introduced in (11.17). Now the responsibilities also
nk
have not only an intuitive but also a mathematically justified interpreta-
tionasposteriorprobabilities.
11.4.5 EM Algorithm Revisited
TheEMalgorithmthatweintroducedasaniterativeschemeformaximum
likelihood estimation can be derived in a principled way from the latent-
variableperspective.Givenacurrentsettingθ(t) ofmodelparameters,the
E-stepcalculatestheexpectedlog-likelihood
Q(θ θ(t)) = E [logp(x,z θ)] (11.73a)
|
z|x,θ(t)
|
(cid:90)
= logp(x,z θ)p(z x,θ(t))dz, (11.73b)
| |
wheretheexpectationoflogp(x,z θ)istakenwithrespecttotheposte-
|
riorp(z x,θ(t))ofthelatentvariables.TheM-stepselectsanupdatedset
|
ofmodelparametersθ(t+1) bymaximizing(11.73b).
Although an EM iteration does increase the log-likelihood, there are
no guarantees that EM converges to the maximum likelihood solution.
It is possible that the EM algorithm converges to a local maximum of
the log-likelihood. Different initializations of the parameters θ could be
used in multiple EM runs to reduce the risk of ending up in a bad local
optimum.Wedonotgointofurtherdetailshere,butrefertotheexcellent
expositionsbyRogersandGirolami(2016)andBishop(2006).
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
368 DensityEstimationwithGaussianMixtureModels
11.5 Further Reading
The GMM can be considered a generative model in the sense that it is
straightforward to generate new data using ancestral sampling (Bishop,
2006). For given GMM parameters π ,µ ,Σ , k = 1,...,K, we sample
k k k
an index k from the probability vector [π ,...,π ]⊤ and then sample a
1 K
(cid:0) (cid:1)
datapointx µ , Σ .IfwerepeatthisN times,weobtainadataset
∼ N k k
thathasbeengeneratedbyaGMM.Figure11.1wasgeneratedusingthis
procedure.
Throughout this chapter, we assumed that the number of components
K isknown.Inpractice,thisisoftennotthecase.However,wecoulduse
nestedcross-validation,asdiscussedinSection8.6.1,tofindgoodmodels.
GaussianmixturemodelsarecloselyrelatedtotheK-meansclustering
algorithm. K-means also uses the EM algorithm to assign data points to
clusters. If we treat the means in the GMM as cluster centers and ignore
the covariances (or set them to I), we arrive at K-means. As also nicely
describedbyMacKay(2003),K-meansmakesa“hard”assignmentofdata
points to cluster centers µ , whereas a GMM makes a “soft” assignment
k
viatheresponsibilities.
Weonlytoucheduponthelatent-variableperspectiveofGMMsandthe
EMalgorithm.NotethatEMcanbeusedforparameterlearningingeneral
latent-variable models, e.g., nonlinear state-space models (Ghahramani
andRoweis,1999;RoweisandGhahramani,1999)andforreinforcement
learningasdiscussedbyBarber(2012).Therefore,thelatent-variableper-
spective of a GMM is useful to derive the corresponding EM algorithm in
aprincipledway(Bishop,2006;Barber,2012;Murphy,2012).
We only discussed maximum likelihood estimation (via the EM algo-
rithm)forfindingGMMparameters.Thestandardcriticismsofmaximum
likelihoodalsoapplyhere:
As in linear regression, maximum likelihood can suffer from severe
overfitting. In the GMM case, this happens when the mean of a mix-
turecomponentisidenticaltoadatapointandthecovariancetendsto
0. Then, the likelihood approaches infinity. Bishop (2006) and Barber
(2012)discussthisissueindetail.
We only obtain a point estimate of the parameters π ,µ ,Σ for k =
k k k
1,...,K, which does not give any indication of uncertainty in the pa-
rametervalues.ABayesianapproachwouldplaceapriorontheparam-
eters,whichcanbeusedtoobtainaposteriordistributionontheparam-
eters.Thisposteriorallowsustocomputethemodelevidence(marginal
likelihood),whichcanbeusedformodelcomparison,whichgivesusa
principled way to determine the number of mixture components. Un-
fortunately,closed-forminferenceisnotpossibleinthissettingbecause
there is no conjugate prior for this model. However, approximations,
such as variational inference, can be used to obtain an approximate
posterior(Bishop,2006).
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
11.5 FurtherReading 369
0.30 Figure11.13
Data Histogram(orange
0.25 KDE bars)andkernel
Histogram densityestimation
0.20
(blueline).The
kerneldensity
0.15
estimatorproduces
0.10 asmoothestimate
oftheunderlying
0.05 density,whereasthe
histogramisan
0.00 unsmoothedcount
4 2 0 2 4 6 8
− − x measureofhow
manydatapoints
(black)fallintoa
In this chapter, we discussed mixture models for density estimation. singlebin.
Thereisaplethoraofdensityestimationtechniquesavailable.Inpractice,
weoftenusehistogramsandkerneldensityestimation. histogram
Histograms provide a nonparametric way to represent continuous den-
sities and have been proposed by Pearson (1895). A histogram is con-
structedby“binning”thedataspaceandcount,howmanydatapointsfall
intoeachbin.Thenabarisdrawnatthecenterofeachbin,andtheheight
ofthebarisproportionaltothenumberofdatapointswithinthatbin.The
bin size is a critical hyperparameter, and a bad choice can lead to overfit-
ting and underfitting. Cross-validation, as discussed in Section 8.2.4, can
beusedtodetermineagoodbinsize. kerneldensity
Kerneldensityestimation,independentlyproposedbyRosenblatt(1956) estimation
and Parzen (1962), is a nonparametric way for density estimation. Given
N i.i.d. samples, the kernel density estimator represents the underlying
distributionas
N (cid:18) (cid:19)
p(x) = 1 (cid:88) k x −x n , (11.74)
Nh h
n=1
wherekisakernelfunction,i.e.,anonnegativefunctionthatintegratesto
1 and h > 0 is a smoothing/bandwidth parameter, which plays a similar
role as the bin size in histograms. Note that we place a kernel on every
single data point x in the dataset. Commonly used kernel functions are
n
theuniformdistributionandtheGaussiandistribution.Kerneldensityesti-
matesarecloselyrelatedtohistograms,butbychoosingasuitablekernel,
we can guarantee smoothness of the density estimate. Figure 11.13 illus-
trates the difference between a histogram and a kernel density estimator
(withaGaussian-shapedkernel)foragivendatasetof250datapoints.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
)x(p
12
Classification with Support Vector Machines
In many situations, we want our machine learning algorithm to predict
oneofanumberof(discrete)outcomes.Forexample,anemailclientsorts
mail into personal mail and junk mail, which has two outcomes. Another
example is a telescope that identifies whether an object in the night sky
isagalaxy,star,orplanet.Thereareusuallyasmallnumberofoutcomes,
and more importantly there is usually no additional structure on these
Anexampleof outcomes. In this chapter, we consider predictors that output binary val-
structureisifthe ues,i.e.,thereareonlytwopossibleoutcomes.Thismachinelearningtask
outcomeswere
is called binary classification. This is in contrast to Chapter 9, where we
ordered,likeinthe
consideredapredictionproblemwithcontinuous-valuedoutputs.
caseofsmall,
medium,andlarge Forbinaryclassification,thesetofpossiblevaluesthatthelabel/output
t-shirts. can attain is binary, and for this chapter we denote them by +1, 1 . In
{ − }
binaryclassification otherwords,weconsiderpredictorsoftheform
f : RD +1, 1 . (12.1)
→ { − }
Recall from Chapter 8 that we represent each example (data point) x
n
Inputexamplexn as a feature vector of D real numbers. The labels are often referred to as
mayalsobereferred the positive and negative classes, respectively. One should be careful not
toasinputs,data
to infer intuitive attributes of positiveness of the +1 class. For example,
points,features,or
in a cancer detection task, a patient with cancer is often labeled +1. In
instances.
class principle, any two distinct values can be used, e.g., True,False , 0,1
{ } { }
Forprobabilistic or red,blue . The problem of binary classification is well studied, and
{ }
models,itis wedeferasurveyofotherapproachestoSection12.6.
mathematically
We present an approach known as the support vector machine (SVM),
convenienttouse
whichsolvesthebinaryclassificationtask.Asinregression,wehaveasu-
{0,1}asabinary
representation;see pervised learning task, where we have a set of examples x n RD along
∈
theremarkafter with their corresponding (binary) labels y +1, 1 . Given a train-
n
Example6.12. ingdatasetconsistingofexample–labelpairs ∈ (x{ ,y )− ,..} .,(x ,y ) ,we
1 1 N N
{ }
wouldliketoestimateparametersofthemodelthatwillgivethesmallest
classification error. Similar to Chapter 9, we consider a linear model, and
hide away the nonlinearity in a transformation ϕ of the examples (9.13).
WewillrevisitϕinSection12.4.
The SVM provides state-of-the-art results in many applications, with
sound theoretical guarantees (Steinwart and Christmann, 2008). There
aretwomainreasonswhywechosetoillustratebinaryclassificationusing
370
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
ClassificationwithSupportVectorMachines 371
Figure12.1
Example2Ddata,
illustratingthe
intuitionofdata
wherewecanfinda
linearclassifierthat
separatesorange
crossesfromblue
discs.
x(1)
SVMs.First,theSVMallowsforageometricwaytothinkaboutsupervised
machinelearning.WhileinChapter9weconsideredthemachinelearning
problem in terms of probabilistic models and attacked it using maximum
likelihood estimation and Bayesian inference, here we will consider an
alternative approach where we reason geometrically about the machine
learning task. It relies heavily on concepts, such as inner products and
projections,whichwediscussedinChapter3.Thesecondreasonwhywe
find SVMs instructive is that in contrast to Chapter 9, the optimization
problem for SVM does not admit an analytic solution so that we need to
resorttoavarietyofoptimizationtoolsintroducedinChapter7.
The SVM view of machine learning is subtly different from the max-
imum likelihood view of Chapter 9. The maximum likelihood view pro-
posesamodelbasedonaprobabilisticviewofthedatadistribution,from
whichanoptimizationproblemisderived.Incontrast,theSVMviewstarts
bydesigningaparticularfunctionthatistobeoptimizedduringtraining,
based on geometric intuitions. We have seen something similar already
in Chapter 10, where we derived PCA from geometric principles. In the
SVM case, we start by designing a loss function that is to be minimized
on training data, following the principles of empirical risk minimization
(Section8.2).
Let us derive the optimization problem corresponding to training an
SVM on example–label pairs. Intuitively, we imagine binary classification
data,whichcanbeseparatedbyahyperplaneasillustratedinFigure12.1.
Here, every example x (a vector of dimension 2) is a two-dimensional
n
location (x(1) and x(2)), and the corresponding binary label y is one of
n n n
twodifferentsymbols(orangecrossorbluedisc).“Hyperplane”isaword
that is commonly used in machine learning, and we encountered hyper-
planes already in Section 2.8. A hyperplane is an affine subspace of di-
mension D 1 (if the corresponding vector space is of dimension D).
−
The examples consist of two classes (there are two possible labels) that
have features (the components of the vector representing the example)
arranged in such a way as to allow us to separate/classify them by draw-
ingastraightline.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
)2(x
372 ClassificationwithSupportVectorMachines
In the following, we formalize the idea of finding a linear separator
of the two classes. We introduce the idea of the margin and then extend
linear separators to allow for examples to fall on the “wrong” side, incur-
ring a classification error. We present two equivalent ways of formalizing
the SVM: the geometric view (Section 12.2.4) and the loss function view
(Section 12.2.5). We derive the dual version of the SVM using Lagrange
multipliers (Section 7.2). The dual SVM allows us to observe a third way
of formalizing the SVM: in terms of the convex hulls of the examples of
eachclass(Section12.3.2).Weconcludebybrieflydescribingkernelsand
howtonumericallysolvethenonlinearkernel-SVMoptimizationproblem.
12.1 Separating Hyperplanes
Giventwoexamplesrepresentedasvectorsx andx ,onewaytocompute
i j
thesimilaritybetweenthemisusinganinnerproduct x ,x .Recallfrom
i j
⟨ ⟩
Section 3.2 that inner products are closely related to the angle between
twovectors.Thevalueoftheinnerproductbetweentwovectorsdepends
on the length (norm) of each vector. Furthermore, inner products allow
ustorigorouslydefinegeometricconceptssuchasorthogonalityandpro-
jections.
The main idea behind many classification algorithms is to represent
data in RD and then partition this space, ideally in a way that examples
with the same label (and no other examples) are in the same partition.
In the case of binary classification, the space would be divided into two
parts corresponding to the positive and negative classes, respectively. We
consider a particularly convenient partition, which is to (linearly) split
the space into two halves using a hyperplane. Let example x RD be an
∈
elementofthedataspace.Considerafunction
f : RD R (12.2a)
→
x f(x) := w,x +b, (12.2b)
(cid:55)→ ⟨ ⟩
parametrized by w RD and b R. Recall from Section 2.8 that hy-
∈ ∈
perplanes are affine subspaces. Therefore, we define the hyperplane that
separatesthetwoclassesinourbinaryclassificationproblemas
(cid:8) x RD : f(x) = 0(cid:9) . (12.3)
∈
An illustration of the hyperplane is shown in Figure 12.2, where the
vectorw isavectornormaltothehyperplaneandbtheintercept.Wecan
derive that w is a normal vector to the hyperplane in (12.3) by choosing
any two examples x and x on the hyperplane and showing that the
a b
vectorbetweenthemisorthogonaltow.Intheformofanequation,
f(x ) f(x ) = w,x +b ( w,x +b) (12.4a)
a b a b
− ⟨ ⟩ − ⟨ ⟩
= w,x x , (12.4b)
a b
⟨ − ⟩
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
12.1 SeparatingHyperplanes 373
Figure12.2
w Equationofa
separating
w hyperplane(12.3).
(a)Thestandard
b
wayofrepresenting
.
theequationin3D.
Positive
(b)Foreaseof
. .
drawing,welookat
0 Negative thehyperplaneedge
on.
(a)Separatinghyperplanein3D (b)Projectionofthesettingin(a)onto
aplane
where the second line is obtained by the linearity of the inner product
(Section 3.2). Since we have chosen x and x to be on the hyperplane,
a b
this implies that f(x ) = 0 and f(x ) = 0 and hence w,x x = 0.
a b a b
⟨ − ⟩
Recall that two vectors are orthogonal when their inner product is zero. wisorthogonalto
Therefore,weobtainthatwisorthogonaltoanyvectoronthehyperplane. anyvectoronthe
hyperplane.
Remark. Recall from Chapter 2 that we can think of vectors in different
ways. In this chapter, we think of the parameter vector w as an arrow
indicating a direction, i.e., we consider w to be a geometric vector. In
contrast, we think of the example vector x as a data point (as indicated
by its coordinates), i.e., we consider x to be the coordinates of a vector
withrespecttothestandardbasis.
♢
When presented with a test example, we classify the example as pos-
itive or negative depending on the side of the hyperplane on which it
occurs.Notethat(12.3)notonlydefinesahyperplane;itadditionallyde-
fines a direction. In other words, it defines the positive and negative side
of the hyperplane. Therefore, to classify a test example x , we calcu-
test
late the value of the function f(x ) and classify the example as +1 if
test
f(x ) ⩾ 0 and 1 otherwise. Thinking geometrically, the positive ex-
test
−
ampleslie“above”thehyperplaneandthenegativeexamples“below”the
hyperplane.
When training the classifier, we want to ensure that the examples with
positivelabelsareonthepositivesideofthehyperplane,i.e.,
w,x +b ⩾ 0 when y = +1 (12.5)
n n
⟨ ⟩
andtheexampleswithnegativelabelsareonthenegativeside,i.e.,
w,x +b < 0 when y = 1. (12.6)
n n
⟨ ⟩ −
Refer to Figure 12.2 for a geometric intuition of positive and negative
examples.Thesetwoconditionsareoftenpresentedinasingleequation
y ( w,x +b) ⩾ 0. (12.7)
n n
⟨ ⟩
Equation(12.7)isequivalentto(12.5)and(12.6)whenwemultiplyboth
sidesof(12.5)and(12.6)withy = 1andy = 1,respectively.
n n
−
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
374 ClassificationwithSupportVectorMachines
Figure12.3
Possibleseparating
hyperplanes.There
aremanylinear
classifiers(green
lines)thatseparate
orangecrossesfrom
bluediscs.
x(1)
12.2 Primal Support Vector Machine
Based on the concept of distances from points to a hyperplane, we now
are in a position to discuss the support vector machine. For a dataset
(x ,y ),...,(x ,y ) thatislinearlyseparable,wehaveinfinitelymany
1 1 N N
{ }
candidate hyperplanes (refer to Figure 12.3), and therefore classifiers,
thatsolveourclassificationproblemwithoutany(training)errors.Tofind
a unique solution, one idea is to choose the separating hyperplane that
maximizes the margin between the positive and negative examples. In
otherwords,wewantthepositiveandnegativeexamplestobeseparated
Aclassifierwith byalargemargin(Section12.2.1).Inthefollowing,wecomputethedis-
largemarginturns tance between an example and a hyperplane to derive the margin. Recall
outtogeneralize
that the closest point on the hyperplane to a given point (example x ) is
n
well(Steinwartand
obtainedbytheorthogonalprojection(Section3.8).
Christmann,2008).
12.2.1 Concept of the Margin
margin The concept of the margin is intuitively simple: It is the distance of the
Therecouldbetwo separating hyperplane to the closest examples in the dataset, assuming
ormoreclosest that the dataset is linearly separable. However, when trying to formalize
examplestoa
thisdistance,thereisatechnicalwrinklethatmaybeconfusing.Thetech-
hyperplane.
nical wrinkle is that we need to define a scale at which to measure the
distance.Apotentialscaleistoconsiderthescaleofthedata,i.e.,theraw
values of x . There are problems with this, as we could change the units
n
of measurement of x and change the values in x , and, hence, change
n n
thedistancetothehyperplane.Aswewillseeshortly,wedefinethescale
basedontheequationofthehyperplane(12.3)itself.
Consider a hyperplane w,x +b, and an example x as illustrated in
a
⟨ ⟩
Figure 12.4. Without loss of generality, we can consider the example x
a
to be on the positive side of the hyperplane, i.e., w,x + b > 0. We
a
⟨ ⟩
would like to compute the distance r > 0 of x from the hyperplane. We
a
do so by considering the orthogonal projection (Section 3.8) of x onto
a
the hyperplane, which we denote by x′. Since w is orthogonal to the
a
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
)2(x
12.2 PrimalSupportVectorMachine 375
. Figure12.4 Vector
x
a
r additiontoexpress
w
. distanceto
x
′a hyperplane:
xa=x′ a+r ∥ww ∥.
.
0
hyperplane,weknowthatthedistancer isjustascalingofthisvectorw.
If the length of w is known, then we can use this scaling factor r factor
to work out the absolute distance between x and x′. For convenience,
a a
we choose to use a vector of unit length (its norm is 1) and obtain this
by dividing w by its norm, w . Using vector addition (Section 2.4), we
∥w∥
obtain
w
x = x′ +r . (12.8)
a a w
∥ ∥
Another way of thinking about r is that it is the coordinate of x in the
a
subspacespannedbyw/ w .Wehavenowexpressedthedistanceofx
a
∥ ∥
from the hyperplane as r, and if we choose x to be the point closest to
a
thehyperplane,thisdistancer isthemargin.
Recall that we would like the positive examples to be further than r
from the hyperplane, and the negative examples to be further than dis-
tance r (in the negative direction) from the hyperplane. Analogously to
the combination of (12.5) and (12.6) into (12.7), we formulate this ob-
jectiveas
y ( w,x +b) ⩾ r. (12.9)
n n
⟨ ⟩
In other words, we combine the requirements that examples are at least
r away from the hyperplane (in the positive and negative direction) into
onesingleinequality.
Since we are interested only in the direction, we add an assumption to
our model that the parameter vector w is of unit length, i.e., w = 1,
∥ ∥
where we use the Euclidean norm w = √w⊤w (Section 3.1). This Wewillseeother
∥ ∥
assumption also allows a more intuitive interpretation of the distance r choicesofinner
products
(12.8)sinceitisthescalingfactorofavectoroflength1.
(Section3.2)in
Remark. A reader familiar with other presentations of the margin would Section12.4.
notice that our definition of w = 1 is different from the standard
∥ ∥
presentation if the SVM was the one provided by Scho¨lkopf and Smola
(2002), for example. In Section 12.2.3, we will show the equivalence of
bothapproaches.
♢
Collectingthethreerequirementsintoasingleconstrainedoptimization
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
376 ClassificationwithSupportVectorMachines
Figure12.5 .
x
a
Derivationofthe r
w
margin:r= 1 . .
∥w∥ x
′a
⟨w
,
⟨w x
,
⟩
x +
⟩ b
+
=
b
1
=
0
problem,weobtaintheobjective
max r
w,b,r (cid:124)(cid:123)(cid:122)(cid:125)
margin
(12.10)
subjectto y ( w,x +b) ⩾ r, w = 1, r > 0,
n n
⟨ ⟩ ∥ ∥
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
datafitting normalization
which says that we want to maximize the margin r while ensuring that
thedataliesonthecorrectsideofthehyperplane.
Remark. Theconceptofthemarginturnsouttobehighlypervasiveinma-
chine learning. It was used by Vladimir Vapnik and Alexey Chervonenkis
to show that when the margin is large, the “complexity” of the function
class is low, and hence learning is possible (Vapnik, 2000). It turns out
that the concept is useful for various different approaches for theoret-
ically analyzing generalization error (Steinwart and Christmann, 2008;
Shalev-ShwartzandBen-David,2014).
♢
12.2.2 Traditional Derivation of the Margin
Intheprevioussection,wederived(12.10)bymakingtheobservationthat
we are only interested in the direction of w and not its length, leading to
the assumption that w = 1. In this section, we derive the margin max-
∥ ∥
imizationproblembymakingadifferentassumption.Insteadofchoosing
that the parameter vector is normalized, we choose a scale for the data.
Wechoosethisscalesuchthatthevalueofthepredictor w,x +bis1at
⟨ ⟩
Recallthatwe the closest example. Let us also denote the example in the dataset that is
currentlyconsider closesttothehyperplanebyx .
a
linearlyseparable
Figure12.5isidenticaltoFigure12.4,exceptthatnowwerescaledthe
data.
axes, such that the example x lies exactly on the margin, i.e., w,x +
a a
⟨ ⟩
b = 1. Since x′ is the orthogonal projection of x onto the hyperplane, it
a a
mustbydefinitionlieonthehyperplane,i.e.,
w,x′ +b = 0. (12.11)
⟨ a⟩
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
12.2 PrimalSupportVectorMachine 377
Bysubstituting(12.8)into(12.11),weobtain
(cid:28) (cid:29)
w
w,x r +b = 0. (12.12)
a
− w
∥ ∥
Exploitingthebilinearityoftheinnerproduct(seeSection3.2),weget
w,w
w,x +b r⟨ ⟩ = 0. (12.13)
a
⟨ ⟩ − w
∥ ∥
Observethatthefirsttermis1byourassumptionofscale,i.e., w,x +
a
⟨ ⟩
b = 1. From (3.16) in Section 3.1, we know that w,w = w 2. Hence,
⟨ ⟩ ∥ ∥
thesecondtermreducestor w .Usingthesesimplifications,weobtain
∥ ∥
1
r = . (12.14)
w
∥ ∥
This means we derived the distance r in terms of the normal vector w
of the hyperplane. At first glance, this equation is counterintuitive as we Wecanalsothinkof
seem to have derived the distance from the hyperplane in terms of the thedistanceasthe
projectionerrorthat
length of the vector w, but we do not yet know this vector. One way to
incurswhen
think about it is to consider the distance r to be a temporary variable
projectingxaonto
that we only use for this derivation. Therefore, for the rest of this section thehyperplane.
we will denote the distance to the hyperplane by 1 . In Section 12.2.3,
∥w∥
we will see that the choice that the margin equals 1 is equivalent to our
previousassumptionof w = 1inSection12.2.1.
∥ ∥
Similar to the argument to obtain (12.9), we want the positive and
negativeexamplestobeatleast1awayfromthehyperplane,whichyields
thecondition
y ( w,x +b) ⩾ 1. (12.15)
n n
⟨ ⟩
Combining the margin maximization with the fact that examples need to
beonthecorrectsideofthehyperplane(basedontheirlabels)givesus
1
max (12.16)
w,b w
∥ ∥
subjecttoy ( w,x +b) ⩾ 1 forall n = 1,...,N. (12.17)
n n
⟨ ⟩
Instead of maximizing the reciprocal of the norm as in (12.16), we often
minimize thesquared norm. Wealso often includea constant 1 that does Thesquarednorm
2
not affect the optimal w,b but yields a tidier form when we compute the resultsinaconvex
quadratic
gradient.Then,ourobjectivebecomes
programming
1 problemforthe
min w 2 (12.18)
SVM(Section12.5).
w,b 2∥ ∥
subjecttoy ( w,x +b) ⩾ 1 forall n = 1,...,N . (12.19)
n n
⟨ ⟩
Equation (12.18) is known as the hard margin SVM. The reason for the hardmarginSVM
expression “hard” is because the formulation does not allow for any vi-
olations of the margin condition. We will see in Section 12.2.4 that this
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
378 ClassificationwithSupportVectorMachines
“hard” condition can be relaxed to accommodate violations if the data is
notlinearlyseparable.
12.2.3 Why We Can Set the Margin to 1
In Section 12.2.1, we argued that we would like to maximize some value
r,whichrepresentsthedistanceoftheclosestexampletothehyperplane.
In Section 12.2.2, we scaled the data such that the closest example is of
distance1tothehyperplane.Inthissection,werelatethetwoderivations,
andshowthattheyareequivalent.
Theorem 12.1. Maximizing the margin r, where we consider normalized
weightsasin(12.10),
max r
w,b,r (cid:124)(cid:123)(cid:122)(cid:125)
margin
(12.20)
subjectto y ( w,x +b) ⩾ r, w = 1, r > 0,
n n
⟨ ⟩ ∥ ∥
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
datafitting normalization
isequivalenttoscalingthedata,suchthatthemarginisunity:
1
2
min w
w,b 2 ∥ ∥
(cid:124) (cid:123)(cid:122) (cid:125)
margin (12.21)
subjectto y ( w,x +b) ⩾ 1 .
n n
⟨ ⟩
(cid:124) (cid:123)(cid:122) (cid:125)
datafitting
Proof Consider (12.20). Since the square is a strictly monotonic trans-
formationfornon-negativearguments,themaximumstaysthesameifwe
consider r2 in the objective. Since w = 1 we can reparametrize the
∥ ∥
equationwithanewweightvectorw′ thatisnotnormalizedbyexplicitly
using
w′
.Weobtain
∥w′∥
max r2
w′,b,r
(cid:18)(cid:28) w′ (cid:29) (cid:19) (12.22)
subjectto y ,x +b ⩾ r, r > 0.
n w′ n
∥ ∥
Equation(12.22)explicitlystatesthatthedistancerispositive.Therefore,
Notethatr>0 wecandividethefirstconstraintbyr,whichyields
becausewe
assumedlinear max r2
w′,b,r
separability,and
 
hencethereisno
(cid:42) (cid:43)
issuetodividebyr.  w′ b  (12.23)
subjectto y  ,x +  ⩾ 1, r > 0
n w′ r n r 
 
(cid:124)∥ (cid:123)(cid:122)∥ (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
w′′ b′′
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
12.2 PrimalSupportVectorMachine 379
Figure12.6
(a)Linearly
separableand
(b)non-linearly
separabledata.
x(1) x(1)
(a)Linearlyseparabledata,withalarge (b)Non-linearlyseparabledata
margin
renamingtheparameterstow′′ andb′′.Sincew′′ = w′ ,rearrangingfor
∥w′∥r
r gives
(cid:13) (cid:13) w′ (cid:13) (cid:13) 1 (cid:13) (cid:13) w′ (cid:13) (cid:13) 1
w′′ = (cid:13) (cid:13) = (cid:13) (cid:13) = . (12.24)
∥ ∥ (cid:13) w′ r(cid:13) r ·(cid:13) w′ (cid:13) r
∥ ∥ ∥ ∥
Bysubstitutingthisresultinto(12.23),weobtain
1
max
w′′,b′′ w′′ 2 (12.25)
∥ ∥
subjectto y ( w′′,x +b′′) ⩾ 1.
n n
⟨ ⟩
Thefinalstepistoobservethatmaximizing 1 yieldsthesamesolution
∥w′′∥2
asminimizing 1 w′′ 2 ,whichconcludestheproofofTheorem12.1.
2 ∥ ∥
12.2.4 Soft Margin SVM: Geometric View
In the case where data is not linearly separable, we may wish to allow
some examples to fall within the margin region, or even to be on the
wrongsideofthehyperplaneasillustratedinFigure12.6.
The model that allows for some classification errors is called the soft softmarginSVM
marginSVM.Inthissection,wederivetheresultingoptimizationproblem
using geometric arguments. In Section 12.2.5, we will derive an equiv-
alent optimization problem using the idea of a loss function. Using La-
grange multipliers (Section 7.2), we will derive the dual optimization
problem of the SVM in Section 12.3. This dual optimization problem al-
lowsustoobserveathirdinterpretationoftheSVM:asahyperplanethat
bisects the line between convex hulls corresponding to the positive and
negativedataexamples(Section12.3.2).
Thekeygeometricideaistointroduceaslackvariableξ n corresponding slackvariable
toeachexample–labelpair(x ,y )thatallowsaparticularexampletobe
n n
within the margin or even on the wrong side of the hyperplane (refer to
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
)2(x )2(x
380 ClassificationwithSupportVectorMachines
Figure12.7 Soft
marginSVMallows .
w
examplestobe
withinthemarginor
ξ
onthewrongsideof
thehyperplane.The
slackvariableξ .
measuresthe x + ⟨w ,
distanceofa ⟨w x
,
positiveexample x ⟩ +
x+tothepositive ⟩
+
b
=
marginhyperplane b
1
⟨w,x⟩+b=1 =
0
whenx+isonthe
wrongside.
Figure 12.7). We subtract the value of ξ from the margin, constraining
n
ξ to be non-negative. To encourage correct classification of the samples,
n
weaddξ totheobjective
n
N
1 (cid:88)
min w 2+C ξ (12.26a)
n
w,b,ξ 2∥ ∥
n=1
subjectto y ( w,x +b) ⩾ 1 ξ (12.26b)
n n n
⟨ ⟩ −
ξ ⩾ 0 (12.26c)
n
forn = 1,...,N.In contrasttothe optimizationproblem(12.18) forthe
softmarginSVM hard margin SVM, this one is called the soft margin SVM. The parameter
C > 0tradesoffthesizeofthemarginandthetotalamountofslackthat
regularization we have. This parameter is called the regularization parameter since, as
parameter wewillseeinthefollowingsection,themargintermintheobjectivefunc-
tion (12.26a) is a regularization term. The margin term w 2 is called
∥ ∥
regularizer the regularizer, and in many books on numerical optimization, the reg-
ularization parameter is multiplied with this term (Section 8.2.3). This
is in contrast to our formulation in this section. Here a large value of C
implies low regularization, as we give the slack variables larger weight,
hence giving more priority to examples that do not lie on the correct side
Thereare ofthemargin.
alternative
Remark. In the formulation of the soft margin SVM (12.26a) w is reg-
parametrizationsof
thisregularization, ularized, but b is not regularized. We can see this by observing that the
whichis regularization term does not contain b. The unregularized term b com-
why(12.26a)isalso plicatestheoreticalanalysis(SteinwartandChristmann,2008,chapter1)
oftenreferredtoas
anddecreasescomputationalefficiency(Fanetal.,2008).
theC-SVM. ♢
12.2.5 Soft Margin SVM: Loss Function View
Let us consider a different approach for deriving the SVM, following the
principle of empirical risk minimization (Section 8.2). For the SVM, we
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
12.2 PrimalSupportVectorMachine 381
choosehyperplanesasthehypothesisclass,thatis
f(x) = w,x +b. (12.27)
⟨ ⟩
We will see in this section that the margin corresponds to the regulariza-
tion term. The remaining question is, what is the loss function? In con- lossfunction
trast to Chapter 9, where we consider regression problems (the output
of the predictor is a real number), in this chapter, we consider binary
classification problems (the output of the predictor is one of two labels
+1, 1 ). Therefore, the error/loss function for each single example–
{ − }
label pair needs to be appropriate for binary classification. For example,
the squared loss that is used for regression (9.10b) is not suitable for bi-
naryclassification.
Remark. Theideallossfunctionbetweenbinarylabelsistocountthenum-
ber of mismatches between the prediction and the label. This means that
forapredictorf appliedtoanexamplex ,wecomparetheoutputf(x )
n n
with the label y . We define the loss to be zero if they match, and one if
n
they do not match. This is denoted by 1(f(x ) = y ) and is called the
n n
̸
zero-one loss. Unfortunately, the zero-one loss results in a combinatorial zero-oneloss
optimizationproblemforfindingthebestparametersw,b.Combinatorial
optimization problems (in contrast to continuous optimization problems
discussedinChapter7)areingeneralmorechallengingtosolve.
♢
WhatisthelossfunctioncorrespondingtotheSVM?Considertheerror
between the output of a predictor f(x ) and the label y . The loss de-
n n
scribes the error that is made on the training data. An equivalent way to
derive(12.26a)istousethehingeloss hingeloss
ℓ(t) = max 0,1 t where t = yf(x) = y( w,x +b). (12.28)
{ − } ⟨ ⟩
If f(x) is on the correct side (based on the corresponding label y) of the
hyperplane, and further than distance 1, this means that t ⩾ 1 and the
hinge loss returns a value of zero. If f(x) is on the correct side but too
close to the hyperplane (0 < t < 1), the example x is within the margin,
and the hinge loss returns a positive value. When the example is on the
wrongsideofthehyperplane(t < 0),thehingelossreturnsanevenlarger
value,whichincreaseslinearly.Inotherwords,wepayapenaltyoncewe
are closer than the margin to the hyperplane, even if the prediction is
correct, and the penalty increases linearly. An alternative way to express
thehingelossisbyconsideringitastwolinearpieces
(cid:40)
0 if t ⩾ 1
ℓ(t) = , (12.29)
1 t if t < 1
−
as illustrated in Figure 12.8. The loss corresponding to the hard margin
SVM12.18isdefinedas
(cid:40)
0 if t ⩾ 1
ℓ(t) = . (12.30)
if t < 1
∞
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
382 ClassificationwithSupportVectorMachines
Figure12.8 The
4
hingelossisa Zero-oneloss
convexupperbound Hingeloss
ofzero-oneloss.
2
0
2 0 2
−
t
This loss can be interpreted as never allowing any examples inside the
margin.
For a given training set (x ,y ),...,(x ,y ) , we seek to minimize
1 1 N N
{ }
the total loss, while regularizing the objective with ℓ -regularization (see
2
Section 8.2.3). Using the hinge loss (12.28) gives us the unconstrained
optimizationproblem
N
1 (cid:88)
min w 2+C max 0,1 y ( w,x +b) . (12.31)
n n
w,b 2∥ ∥ { − ⟨ ⟩ }
(cid:124) (cid:123)(cid:122) (cid:125) n=1
(cid:124) (cid:123)(cid:122) (cid:125)
regularizer
errorterm
regularizer Thefirsttermin(12.31)iscalledtheregularizationtermortheregularizer
lossterm (seeSection8.2.3),andthesecondtermiscalledthelosstermortheerror
errorterm term.RecallfromSection12.2.4thattheterm 1 w 2 arisesdirectlyfrom
2 ∥ ∥
the margin. In other words, margin maximization can be interpreted as
regularization regularization.
In principle, the unconstrained optimization problem in (12.31) can
be directly solved with (sub-)gradient descent methods as described in
Section7.1.Toseethat(12.31)and(12.26a)areequivalent,observethat
thehingeloss(12.28)essentiallyconsistsoftwolinearparts,asexpressed
in(12.29).Considerthehingelossforasingleexample-labelpair(12.28).
We can equivalently replace minimization of the hinge loss over t with a
minimizationofaslackvariableξ withtwoconstraints.Inequationform,
minmax 0,1 t (12.32)
t { − }
isequivalentto
min ξ
ξ,t (12.33)
subjectto ξ ⩾ 0, ξ ⩾ 1 t.
−
By substituting this expression into (12.31) and rearranging one of the
constraints,weobtainexactlythesoftmarginSVM(12.26a).
Remark. Letuscontrastourchoiceofthelossfunctioninthissectiontothe
loss function for linear regression in Chapter 9. Recall from Section 9.2.1
that for finding maximum likelihood estimators, we usually minimize the
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
t
1,0
xam
} −
{
12.3 DualSupportVectorMachine 383
negative log-likelihood. Furthermore, since the likelihood term for linear
regressionwithGaussiannoiseisGaussian,thenegativelog-likelihoodfor
eachexampleisasquarederrorfunction.Thesquarederrorfunctionisthe
lossfunctionthatisminimizedwhenlookingforthemaximumlikelihood
solution.
♢
12.3 Dual Support Vector Machine
The description of the SVM in the previous sections, in terms of the vari-
ableswandb,isknownastheprimalSVM.Recallthatweconsiderinputs
x RD with D features. Since w is of the same dimension as x, this
∈
means that the number of parameters (the dimension of w) of the opti-
mizationproblemgrowslinearlywiththenumberoffeatures.
In the following, we consider an equivalent optimization problem (the
so-called dual view), which is independent of the number of features. In-
stead, the number of parameters increases with the number of examples
inthetrainingset.WesawasimilarideaappearinChapter10,wherewe
expressedthelearningprobleminawaythatdoesnotscalewiththenum-
ber of features. This is useful for problems where we have more features
than the number of examples in the training dataset. The dual SVM also
has the additional advantage that it easily allows kernels to be applied,
as we shall see at the end of this chapter. The word “dual” appears often
in mathematical literature, and in this particular case it refers to convex
duality.Thefollowingsubsectionsareessentiallyanapplicationofconvex
duality,whichwediscussedinSection7.2.
12.3.1 Convex Duality via Lagrange Multipliers
Recall the primal soft margin SVM (12.26a). We call the variables w, b,
andξ correspondingtotheprimalSVMtheprimalvariables.Weuseα n ⩾ InChapter7,we
0astheLagrangemultipliercorrespondingtotheconstraint(12.26b)that usedλasLagrange
the examples are classified correctly and γ ⩾ 0 as the Lagrange multi- multipliers.Inthis
n
section,wefollow
plier corresponding to the non-negativity constraint of the slack variable;
thenotation
see(12.26c).TheLagrangianisthengivenby commonlychosenin
SVMliterature,and
useαandγ.
N
1 (cid:88)
L(w,b,ξ,α,γ) = w 2+C ξ (12.34)
n
2∥ ∥
n=1
N N
(cid:88) (cid:88)
α (y ( w,x +b) 1+ξ ) γ ξ .
n n n n n n
− ⟨ ⟩ − −
n=1 n=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
constraint(12.26b) constraint(12.26c)
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
384 ClassificationwithSupportVectorMachines
BydifferentiatingtheLagrangian(12.34)withrespecttothethreeprimal
variablesw,b,andξ respectively,weobtain
∂L (cid:88)N
= w⊤ α y x ⊤, (12.35)
∂w − n n n
n=1
∂L (cid:88)N
= α y , (12.36)
∂b − n n
n=1
∂L
= C α γ . (12.37)
∂ξ − n − n
n
We now find the maximum of the Lagrangian by setting each of these
partialderivativestozero.Bysetting(12.35)tozero,wefind
N
(cid:88)
w = α y x , (12.38)
n n n
n=1
representertheorem which is a particular instance of the representer theorem (Kimeldorf and
Therepresenter Wahba, 1970). Equation (12.38) states that the optimal weight vector in
theoremisactually the primal is a linear combination of the examples x . Recall from Sec-
n
acollectionof
tion 2.6.1 that this means that the solution of the optimization problem
theoremssaying
lies in the span of training data. Additionally, the constraint obtained by
thatthesolutionof
minimizing setting (12.36) to zero implies that the optimal weight vector is an affine
empiricalriskliesin combination of the examples. The representer theorem turns out to hold
thesubspace for very general settings of regularized empirical risk minimization (Hof-
(Section2.4.3)
mann et al., 2008; Argyriou and Dinuzzo, 2014). The theorem has more
definedbythe
examples. general versions (Scho¨lkopf et al., 2001), and necessary and sufficient
conditionsonitsexistencecanbefoundinYuetal.(2013).
Remark. The representer theorem (12.38) also provides an explanation
of the name “support vector machine.” The examples x , for which the
n
corresponding parameters α = 0, do not contribute to the solution w at
n
supportvector all. The other examples, where α n > 0, are called support vectors since
they“support”thehyperplane.
♢
By substituting the expression for w into the Lagrangian (12.34), we
obtainthedual
(cid:42) (cid:43)
N N N N
1 (cid:88)(cid:88) (cid:88) (cid:88)
D(ξ,α,γ) = y y α α x ,x y α y α x ,x
i j i j i j i i j j j i
2 ⟨ ⟩−
i=1 j=1 i=1 j=1
N N N N N
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
+C ξ b y α + α α ξ γ ξ .
i i i i i i i i
− − −
i=1 i=1 i=1 i=1 i=1
(12.39)
Note that there are no longer any terms involving the primal variable w.
Bysetting(12.36)tozero,weobtain(cid:80)N
y α = 0.Therefore,theterm
n=1 n n
involving b also vanishes. Recall that inner products are symmetric and
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
12.3 DualSupportVectorMachine 385
bilinear (see Section 3.2). Therefore, the first two terms in (12.39) are
over the same objects. These terms (colored blue) can be simplified, and
weobtaintheLagrangian
N N N N
1 (cid:88)(cid:88) (cid:88) (cid:88)
D(ξ,α,γ) = y y α α x ,x + α + (C α γ )ξ .
i j i j i j i i i i
−2 ⟨ ⟩ − −
i=1 j=1 i=1 i=1
(12.40)
Thelastterminthisequationisacollectionofalltermsthatcontainslack
variablesξ .Bysetting(12.37)tozero,weseethatthelasttermin(12.40)
i
is also zero. Furthermore, by using the same equation and recalling that
the Lagrange multiplers γ are non-negative, we conclude that α ⩽ C.
i i
We now obtain the dual optimization problem of the SVM, which is ex-
pressed exclusively in terms of the Lagrange multipliers α . Recall from
i
Lagrangian duality (Definition 7.1) that we maximize the dual problem.
This is equivalent to minimizing the negative dual problem, such that we
endupwiththedualSVM dualSVM
N N N
1 (cid:88)(cid:88) (cid:88)
min y y α α x ,x α
i j i j i j i
α 2 ⟨ ⟩−
i=1 j=1 i=1
N (12.41)
(cid:88)
subjectto y α = 0
i i
i=1
0 ⩽ α ⩽ C forall i = 1,...,N .
i
The equality constraint in (12.41) is obtained from setting (12.36) to
zero. The inequality constraint α ⩾ 0 is the condition imposed on La-
i
grange multipliers of inequality constraints (Section 7.2). The inequality
constraintα ⩽ C isdiscussedinthepreviousparagraph.
i
ThesetofinequalityconstraintsintheSVMarecalled“boxconstraints”
because they limit the vector α = [α , ,α ]⊤ RN of Lagrange mul-
1 N
··· ∈
tipliers to be inside the box defined by 0 and C on each axis. These
axis-aligned boxes are particularly efficient to implement in numerical
solvers(Dosta´l,2009,chapter5). Itturnsoutthat
Once we obtain the dual parameters α, we can recover the primal pa- examplesthatlie
exactlyonthe
rameters w by using the representer theorem (12.38). Let us call the op-
marginare
timal primal parameter w∗. However, there remains the question on how
exampleswhose
to obtain the parameter b∗. Consider an example x n that lies exactly on dualparameterslie
themargin’sboundary,i.e., w∗,x +b = y .Recallthaty iseither+1 strictlyinsidethe
n n n
or 1.Therefore,theonlyu⟨ nknown⟩ isb,whichcanbecomputedby boxconstraints,
− 0<αi<C.Thisis
b∗ = y w∗,x . (12.42) derivedusingthe
n −⟨ n ⟩ KarushKuhnTucker
conditions,for
Remark. In principle, there may be no examples that lie exactly on the
examplein
margin. In this case, we should compute y w∗,x for all support Scho¨lkopfand
n n
| −⟨ ⟩|
vectors and take the median value of this absolute value difference to be Smola(2002).
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
386 ClassificationwithSupportVectorMachines
Figure12.9 Convex
hulls.(a)Convex
hullofpoints,some
ofwhichliewithin
theboundary;
(b)convexhulls
aroundpositiveand
negativeexamples.
c
d
(a)Convexhull. (b) Convex hulls around positive (blue) and
negative (orange) examples. The distance be-
tweenthe twoconvexsetsis thelengthof the
differencevectorc−d.
the value of b∗. A derivation of this can be found in http://fouryears.
eu/2012/06/07/the-svm-bias-term-conspiracy/.
♢
12.3.2 Dual SVM: Convex Hull View
Another approach to obtain the dual SVM is to consider an alternative
geometricargument.Considerthesetofexamplesx withthesamelabel.
n
We would like to build a convex set that contains all the examples such
that it is the smallest possible set. This is called the convex hull and is
illustratedinFigure12.9.
Let us first build some intuition about a convex combination of points.
Consider two points x and x and corresponding non-negative weights
1 2
α ,α ⩾ 0suchthatα +α = 1.Theequationα x +α x describeseach
1 2 1 2 1 1 2 2
pointonalinebetweenx andx .Considerwhathappenswhenweadd
1 2
a third point x along with a weight α ⩾ 0 such that (cid:80)3 α = 1.
3 3 n=1 n
The convex combination of these three points x ,x ,x spans a two-
1 2 3
convexhull dimensional area. The convex hull of this area is the triangle formed by
theedgescorrespondingtoeachpairofofpoints.Asweaddmorepoints,
and the number of points becomes greater than the number of dimen-
sions, some of the points will be inside the convex hull, as we can see in
Figure12.9(a).
In general, building a convex convex hull can be done by introducing
non-negative weights α ⩾ 0 corresponding to each example x . Then
n n
theconvexhullcanbedescribedastheset
(cid:40) (cid:41)
N N
(cid:88) (cid:88)
conv(X) = α x with α = 1 and α ⩾ 0, (12.43)
n n n n
n=1 n=1
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
12.3 DualSupportVectorMachine 387
for all n = 1,...,N. If the two clouds of points corresponding to the
positive and negative classes are separated, then the convex hulls do not
overlap.Giventhetrainingdata(x ,y ),...,(x ,y ),weformtwocon-
1 1 N N
vex hulls, corresponding to the positive and negative classes respectively.
We pick a point c, which is in the convex hull of the set of positive exam-
ples, and is closest to the negative class distribution. Similarly, we pick a
pointdintheconvexhullofthesetofnegativeexamplesandisclosestto
the positive class distribution; see Figure 12.9(b). We define a difference
vectorbetweendandcas
w := c d. (12.44)
−
Picking the points c and d as in the preceding cases, and requiring them
tobeclosesttoeachotherisequivalenttominimizingthelength/normof
w,sothatweendupwiththecorrespondingoptimizationproblem
1
argmin w = argmin w 2 . (12.45)
w ∥ ∥ w 2 ∥ ∥
Sincecmustbeinthepositiveconvexhull,itcanbeexpressedasaconvex
combination of the positive examples, i.e., for non-negative coefficients
α+
n
(cid:88)
c = α+x . (12.46)
n n
n:yn=+1
In (12.46), we use the notation n : y = +1 to indicate the set of indices
n
nforwhichy = +1.Similarly,fortheexampleswithnegativelabels,we
n
obtain
(cid:88)
d = α−x . (12.47)
n n
n:yn=−1
Bysubstituting(12.44),(12.46),and(12.47)into(12.45),weobtainthe
objective
(cid:13) (cid:13)2
1 (cid:13) (cid:88) (cid:88) (cid:13)
min (cid:13) α+x α−x (cid:13) . (12.48)
α 2 (cid:13) (cid:13) n n − n n(cid:13) (cid:13)
n:yn=+1 n:yn=−1
Letαbethesetofallcoefficients,i.e.,theconcatenationofα+andα−.
Recallthatwerequirethatforeachconvexhullthattheircoefficientssum
toone,
(cid:88) (cid:88)
α+ = 1 and α− = 1. (12.49)
n n
n:yn=+1 n:yn=−1
Thisimpliestheconstraint
N
(cid:88)
y α = 0. (12.50)
n n
n=1
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
388 ClassificationwithSupportVectorMachines
Thisresultcanbeseenbymultiplyingouttheindividualclasses
N
(cid:88) (cid:88) (cid:88)
y α = (+1)α++ ( 1)α− (12.51a)
n n n − n
n=1 n:yn=+1 n:yn=−1
(cid:88) (cid:88)
= α+ α− = 1 1 = 0. (12.51b)
n − n −
n:yn=+1 n:yn=−1
Theobjectivefunction(12.48)andtheconstraint(12.50),alongwiththe
assumptionthatα ⩾ 0,giveusaconstrained(convex)optimizationprob-
lem. This optimization problem can be shown to be the same as that of
thedualhardmarginSVM(BennettandBredensteiner,2000a).
Remark. Toobtainthesoftmargindual,weconsiderthereducedhull.The
reducedhull reduced hull is similar to the convex hull but has an upper bound to the
size of the coefficients α. The maximum possible value of the elements
of α restricts the size that the convex hull can take. In other words, the
bound on α shrinks the convex hull to a smaller volume (Bennett and
Bredensteiner,2000b).
♢
12.4 Kernels
Consider the formulation of the dual SVM (12.41). Notice that the in-
ner product in the objective occurs only between examples x and x .
i j
There are no inner products between the examples and the parameters.
Therefore, if we consider a set of features ϕ(x ) to represent x , the only
i i
change in the dual SVM will be to replace the inner product. This mod-
ularity, where the choice of the classification method (the SVM) and the
choice of the feature representation ϕ(x) can be considered separately,
provides flexibility for us to explore the two problems independently. In
thissection,wediscusstherepresentationϕ(x)andbrieflyintroducethe
ideaofkernels,butdonotgointothetechnicaldetails.
Sinceϕ(x)couldbeanon-linearfunction,wecanusetheSVM(which
assumes a linear classifier) to construct classifiers that are nonlinear in
the examples x . This provides a second avenue, in addition to the soft
n
margin, for users to deal with a dataset that is not linearly separable. It
turnsoutthattherearemanyalgorithmsandstatisticalmethodsthathave
this property that we observed in the dual SVM: the only inner products
are those that occur between examples. Instead of explicitly defining a
non-linear feature map ϕ( ) and computing the resulting inner product
·
betweenexamplesx andx ,wedefineasimilarityfunctionk(x ,x )be-
i j i j
kernel tweenx i andx j.Foracertainclassofsimilarityfunctions,calledkernels,
the similarity function implicitly defines a non-linear feature map ϕ( ).
TheinputsX ofthe Kernels are by definition functions k : R for which there exis· ts
X ×X →
kernelfunctioncan aHilbertspace andϕ : afeaturemapsuchthat
beverygeneraland H X → H
arenotnecessarily k(x ,x ) = ϕ(x ),ϕ(x ) . (12.52)
restrictedtoRD. i j ⟨ i j ⟩H
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
12.4 Kernels 389
Figure12.10 SVM
withdifferent
kernels.Notethat
whilethedecision
boundaryis
nonlinear,the
underlyingproblem
beingsolvedisfora
linearseparating
hyperplane(albeit
withanonlinear
kernel).
Firstfeature Firstfeature
(a)SVMwithlinearkernel (b)SVMwithRBFkernel
Firstfeature Firstfeature
(c)SVMwithpolynomial(degree2)kernel (d)SVMwithpolynomial(degree3)kernel
There is a unique reproducing kernel Hilbert space associated with every
kernel k (Aronszajn, 1950; Berlinet and Thomas-Agnan, 2004). In this
unique association, ϕ(x) = k( ,x) is called the canonical feature map. canonicalfeature
·
The generalization from an inner product to a kernel function (12.52) is map
knownasthekerneltrick(Scho¨lkopfandSmola,2002;Shawe-Taylorand kerneltrick
Cristianini,2004),asithidesawaytheexplicitnon-linearfeaturemap.
ThematrixK RN×N,resultingfromtheinnerproductsortheappli-
∈
cation of k( , ) to a dataset, is called the Gram matrix, and is often just Grammatrix
· ·
referred to as the kernel matrix. Kernels must be symmetric and positive kernelmatrix
semidefinite functions so that every kernel matrix K is symmetric and
positivesemidefinite(Section3.2.3):
z RN : z⊤Kz ⩾ 0. (12.53)
∀ ∈
Some popular examples of kernels for multivariate real-valued data x
i
RD are the polynomial kernel, the Gaussian radial basis function kerne∈ l,
andtherationalquadratickernel(Scho¨lkopfandSmola,2002;Rasmussen
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
erutaefdnoceS
erutaefdnoceS
erutaefdnoceS
erutaefdnoceS
390 ClassificationwithSupportVectorMachines
andWilliams,2006).Figure12.10illustratestheeffectofdifferentkernels
on separating hyperplanes on an example dataset. Note that we are still
solving for hyperplanes, that is, the hypothesis class of functions are still
linear.Thenon-linearsurfacesareduetothekernelfunction.
Remark. Unfortunately for the fledgling machine learner, there are mul-
tiple meanings of the word “kernel.” In this chapter, the word “kernel”
comesfromtheideaofthereproducingkernelHilbertspace(RKHS)(Aron-
szajn,1950;Saitoh,1988).Wehavediscussedtheideaofthekernelinlin-
earalgebra(Section2.7.3),wherethekernelisanotherwordforthenull
space. The third common use of theword “kernel” in machine learning is
thesmoothingkernelinkerneldensityestimation(Section11.5).
♢
Since the explicit representation ϕ(x) is mathematically equivalent to
the kernel representation k(x ,x ), a practitioner will often design the
i j
kernel function such that it can be computed more efficiently than the
inner product between explicit feature maps. For example, consider the
polynomial kernel (Scho¨lkopf and Smola, 2002), where the number of
terms in the explicit expansion grows very quickly (even for polynomials
of low degree) when the input dimension is large. The kernel function
only requires one multiplication per input dimension, which can provide
significant computational savings. Another example is the Gaussian ra-
dial basis function kernel (Scho¨lkopf and Smola, 2002; Rasmussen and
Williams,2006),wherethecorrespondingfeaturespaceisinfinitedimen-
sional. In this case, we cannot explicitly represent the feature space but
Thechoiceof canstillcomputesimilaritiesbetweenapairofexamplesusingthekernel.
kernel,aswellas Another useful aspect of the kernel trick is that there is no need for
theparametersof
the original data to be already represented as multivariate real-valued
thekernel,isoften
data. Note that theinner product is defined on theoutput of the function
chosenusingnested
cross-validation ϕ( ), but does not restrict the input to real numbers. Hence, the function
·
(Section8.6.1). ϕ( ) and the kernel function k( , ) can be defined on any object, e.g.,
· · ·
sets, sequences, strings, graphs, and distributions (Ben-Hur et al., 2008;
Ga¨rtner,2008;Shietal.,2009;Sriperumbuduretal.,2010;Vishwanathan
etal.,2010).
12.5 Numerical Solution
We conclude our discussion of SVMs by looking at how to express the
problems derived in this chapter in terms of the concepts presented in
Chapter 7. We consider two different approaches for finding the optimal
solutionfortheSVM.FirstweconsiderthelossviewofSVM8.2.2andex-
pressthisasanunconstrainedoptimizationproblem.Thenweexpressthe
constrained versions of the primal and dual SVMs as quadratic programs
instandardform7.3.2.
Consider the loss function view of the SVM (12.31). This is a convex
unconstrainedoptimizationproblem,butthehingeloss(12.28)isnotdif-
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
12.5 NumericalSolution 391
ferentiable. Therefore, we apply a subgradient approach for solving it.
However, the hinge loss is differentiable almost everywhere, except for
one single point at the hinge t = 1. At this point, the gradient is a set of
possiblevaluesthatliebetween0and 1.Therefore,thesubgradientg of
−
thehingelossisgivenby

1 t < 1

−
g(t) = [ 1,0] t = 1 . (12.54)
 −
0 t > 1
Usingthissubgradient,wecanapplytheoptimizationmethodspresented
inSection7.1.
Both the primal and the dual SVM result in a convex quadratic pro-
grammingproblem(constrainedoptimization).NotethattheprimalSVM
in (12.26a) has optimization variables that have the size of the dimen-
sion D of the input examples. The dual SVM in (12.41) has optimization
variablesthathavethesizeofthenumberN ofexamples.
To express the primal SVM in the standard form (7.45) for quadratic
programming, let us assume that we use the dot product (3.5) as the
inner product. We rearrange the equation for the primal SVM (12.26a), Recallfrom
suchthattheoptimizationvariablesareallontherightandtheinequality Section3.2thatwe
usethephrasedot
oftheconstraintmatchesthestandardform.Thisyieldstheoptimization
producttomeanthe
N innerproducton
min 1 w 2+C(cid:88) ξ Euclideanvector
n
w,b,ξ 2∥ ∥ space.
n=1 (12.55)
y x⊤w y b ξ ⩽ 1
subjectto − ξn ⩽n
0
− n − n −
n
−
n = 1,...,N.Byconcatenatingthevariablesw,b,x intoasinglevector,
n
andcarefullycollectingtheterms,weobtainthefollowingmatrixformof
thesoftmarginSVM:
 ⊤    
w (cid:20) (cid:21) w w
wm ,bin
,ξ
21  ξb 0 NI +D
1,D
00 ND +, 1N ,N+ +1
1
 ξb+(cid:2) 0 D+1,1 C1 N,1(cid:3)⊤  ξb
 
(cid:20) (cid:21) w (cid:20) (cid:21)
YX y I 1
subjectto − − − N b ⩽ − N,1 .
0 I 0
N,D+1 − N ξ N,1
(12.56)
In the preceding optimization problem, the minimization is over the pa-
rameters [w⊤,b,ξ⊤]⊤ RD+1+N, and we use the notation: I to rep-
m
∈
resent the identity matrix of size m m, 0 to represent the matrix
m,n
×
of zeros of size m n, and 1 to represent the matrix of ones of size
m,n
×
m n. In addition, y is the vector of labels [y , ,y ]⊤, Y = diag(y)
1 N
× ···
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
392 ClassificationwithSupportVectorMachines
is an N by N matrix where the elements of the diagonal are from y, and
X RN×D isthematrixobtainedbyconcatenatingalltheexamples.
∈
Wecansimilarlyperformacollectionoftermsforthedualversionofthe
SVM (12.41). To express the dual SVM in standard form, we first have to
expressthekernelmatrixK suchthateachentryisK = k(x ,x ).Ifwe
ij i j
have an explicit feature representation x then we define K = x ,x .
i ij i j
⟨ ⟩
Forconvenienceofnotationweintroduceamatrixwithzeroseverywhere
except on the diagonal, where we store the labels, that is, Y = diag(y).
ThedualSVMcanbewrittenas
1
min α⊤YKYα 1⊤ α
α 2 − N,1
 y⊤ 
subjectto  −y⊤  α ⩽ (cid:20) 0 N+2,1(cid:21) . (12.57)
 I N C1 N,1
−
I
N
Remark. In Sections 7.3.1 and 7.3.2, we introduced the standard forms
of the constraints to be inequality constraints. We will express the dual
SVM’sequalityconstraintastwoinequalityconstraints,i.e.,
Ax = b isreplacedby Ax ⩽ b and Ax ⩾ b. (12.58)
Particularsoftwareimplementationsofconvexoptimizationmethodsmay
providetheabilitytoexpressequalityconstraints.
♢
Since there are many different possible views of the SVM, there are
many approaches for solving the resulting optimization problem. The ap-
proach presented here, expressing the SVM problem in standard convex
optimizationform,isnotoftenusedinpractice.Thetwomainimplemen-
tations of SVM solvers are Chang and Lin (2011) (which is open source)
andJoachims(1999).SinceSVMshaveaclearandwell-definedoptimiza-
tion problem, many approaches based on numerical optimization tech-
niques (Nocedal and Wright, 2006) can be applied (Shawe-Taylor and
Sun,2011).
12.6 Further Reading
The SVM is one of many approaches for studying binary classification.
Other approaches include the perceptron, logistic regression, Fisher dis-
criminant,nearestneighbor,naiveBayes,andrandomforest(Bishop,2006;
Murphy, 2012). A short tutorial on SVMs and kernels on discrete se-
quencescanbefoundinBen-Huretal.(2008).ThedevelopmentofSVMs
is closely linked to empirical risk minimization, discussed in Section 8.2.
Hence, the SVM has strong theoretical properties (Vapnik, 2000; Stein-
wart and Christmann, 2008). The book about kernel methods (Scho¨lkopf
and Smola, 2002) includes many details of support vector machines and
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
12.6 FurtherReading 393
how to optimize them. A broader book about kernel methods (Shawe-
TaylorandCristianini,2004)alsoincludesmanylinearalgebraapproaches
fordifferentmachinelearningproblems.
An alternative derivation of the dual SVM can be obtained using the
idea of the Legendre–Fenchel transform (Section 7.3.3). The derivation
considerseachtermoftheunconstrainedformulationoftheSVM(12.31)
separately and calculates their convex conjugates (Rifkin and Lippert,
2007). Readers interested in the functional analysis view (also the reg-
ularization methods view) of SVMs are referred to the work by Wahba
(1990). Theoretical exposition of kernels (Aronszajn, 1950; Schwartz,
1964;Saitoh,1988;MantonandAmblard,2015)requiresabasicground-
inginlinearoperators(AkhiezerandGlazman,1993).Theideaofkernels
have been generalized to Banach spaces (Zhang et al., 2009) and Kre˘ın
spaces(Ongetal.,2004;Looslietal.,2016).
Observe that the hinge loss has three equivalent representations, as
shown in (12.28) and (12.29), as well as the constrained optimization
problemin(12.33).Theformulation(12.28)isoftenusedwhencompar-
ing the SVM loss function with other loss functions (Steinwart, 2007).
The two-piece formulation (12.29) is convenient for computing subgra-
dients, as each piece is linear. The third formulation (12.33), as seen
in Section 12.5, enables the use of convex quadratic programming (Sec-
tion7.3.2)tools.
Since binary classification is a well-studied task in machine learning,
other words are also sometimes used, such as discrimination, separation,
anddecision.Furthermore,therearethreequantitiesthatcanbetheout-
put of a binary classifier. First is the output of the linear function itself
(oftencalledthescore),whichcantakeanyrealvalue.Thisoutputcanbe
used for ranking the examples, and binary classification can be thought
ofaspickingathresholdontherankedexamples(Shawe-TaylorandCris-
tianini, 2004). The second quantity that is often considered the output
of a binary classifier is the output determined after it is passed through
a non-linear function to constrain its value to a bounded range, for ex-
ample in the interval [0,1]. A common non-linear function is the sigmoid
function(Bishop,2006).Whenthenon-linearityresultsinwell-calibrated
probabilities (Gneiting and Raftery, 2007; Reid and Williamson, 2011),
this is called class probability estimation. The third output of a binary
classifieristhefinalbinarydecision +1, 1 ,whichistheonemostcom-
{ − }
monlyassumedtobetheoutputoftheclassifier.
The SVM is a binary classifier that does not naturally lend itself to a
probabilistic interpretation. There are several approaches for converting
the raw output of the linear function (the score) into a calibrated class
probability estimate (P(Y = 1 X = x)) that involve an additional cal-
|
ibration step (Platt, 2000; Zadrozny and Elkan, 2001; Lin et al., 2007).
From the training perspective, there are many related probabilistic ap-
proaches. We mentioned at the end of Section 12.2.5 that there is a re-
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
394 ClassificationwithSupportVectorMachines
lationship between loss function and the likelihood (also compare Sec-
tions 8.2 and 8.3). The maximum likelihood approach corresponding to
a well-calibrated transformation during training is called logistic regres-
sion,whichcomesfromaclassofmethodscalledgeneralizedlinearmod-
els. Details of logistic regression from this point of view can be found in
Agresti (2002, chapter 5) and McCullagh and Nelder (1989, chapter 4).
Naturally,onecouldtakeamoreBayesianviewoftheclassifieroutputby
estimating a posterior distribution using Bayesian logistic regression. The
Bayesian view also includes the specification of the prior, which includes
design choices such as conjugacy (Section 6.6.1) with the likelihood. Ad-
ditionally, one could consider latent functions as priors, which results in
Gaussian process classification (Rasmussen and Williams, 2006, chapter
3).
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
References
Abel, Niels H. 1826. D´emonstration de l’Impossibilit´e de la R´esolution Alg´ebrique des
E´quationsG´en´eralesquiPassentleQuatri`emeDegr´e. GrøndahlandSøn.
Adhikari,Ani,andDeNero,John.2018. ComputationalandInferentialThinking:The
FoundationsofDataScience. Gitbooks.
Agarwal, Arvind, and Daum´e III, Hal. 2010. A Geometric View of Conjugate Priors.
MachineLearning,81(1),99–113.
Agresti,A.2002. CategoricalDataAnalysis. Wiley.
Akaike, Hirotugu. 1974. A New Look at the Statistical Model Identification. IEEE
TransactionsonAutomaticControl,19(6),716–723.
Akhiezer,NaumI.,andGlazman,IzrailM.1993. TheoryofLinearOperatorsinHilbert
Space. DoverPublications.
Alpaydin,Ethem.2010. IntroductiontoMachineLearning. MITPress.
Amari,Shun-ichi.2016. InformationGeometryandItsApplications. Springer.
Argyriou, Andreas, and Dinuzzo, Francesco. 2014. A Unifying View of Representer
Theorems. In:ProceedingsoftheInternationalConferenceonMachineLearning.
Aronszajn,Nachman.1950. TheoryofReproducingKernels. TransactionsoftheAmer-
icanMathematicalSociety,68,337–404.
Axler,Sheldon.2015. LinearAlgebraDoneRight. Springer.
Bakir,Go¨khan,Hofmann,Thomas,Scho¨lkopf,Bernhard,Smola,AlexanderJ.,Taskar,
Ben,andVishwanathan,S.V.N.(eds).2007.PredictingStructuredData.MITPress.
Barber,David.2012. BayesianReasoningandMachineLearning. CambridgeUniversity
Press.
Barndorff-Nielsen,Ole.2014. InformationandExponentialFamilies:InStatisticalThe-
ory. Wiley.
Bartholomew,David,Knott,Martin,andMoustaki,Irini.2011. LatentVariableModels
andFactorAnalysis:AUnifiedApproach. Wiley.
Baydin, Atılım G., Pearlmutter, Barak A., Radul, Alexey A., and Siskind, Jeffrey M.
2018.AutomaticDifferentiationinMachineLearning:ASurvey.JournalofMachine
LearningResearch,18,1–43.
Beck,Amir,andTeboulle,Marc.2003.MirrorDescentandNonlinearProjectedSubgra-
dient Methods for Convex Optimization. Operations Research Letters, 31(3), 167–
175.
Belabbas, Mohamed-Ali, and Wolfe, Patrick J. 2009. Spectral Methods in Machine
Learning and New Strategies for Very Large Datasets. Proceedings of the National
AcademyofSciences,0810600105.
Belkin, Mikhail, and Niyogi, Partha. 2003. Laplacian Eigenmaps for Dimensionality
ReductionandDataRepresentation. NeuralComputation,15(6),1373–1396.
Ben-Hur,Asa,Ong,ChengSoon,Sonnenburg,So¨ren,Scho¨lkopf,Bernhard,andRa¨tsch,
Gunnar. 2008. Support Vector Machines and Kernels for Computational Biology.
PLoSComputationalBiology,4(10),e1000173.
395
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
396 References
Bennett, Kristin P., and Bredensteiner, Erin J. 2000a. Duality and Geometry in SVM
Classifiers. In:ProceedingsoftheInternationalConferenceonMachineLearning.
Bennett, Kristin P., and Bredensteiner, Erin J. 2000b. Geometry in Learning. Pages
132–145of:GeometryatWork. MathematicalAssociationofAmerica.
Berlinet,Alain,andThomas-Agnan,Christine.2004.ReproducingKernelHilbertSpaces
inProbabilityandStatistics. Springer.
Bertsekas,DimitriP.1999. NonlinearProgramming. AthenaScientific.
Bertsekas,DimitriP.2009. ConvexOptimizationTheory. AthenaScientific.
Bickel, Peter J., and Doksum, Kjell. 2006. Mathematical Statistics, Basic Ideas and
SelectedTopics. Vol.1. PrenticeHall.
Bickson, Danny, Dolev, Danny, Shental, Ori, Siegel, Paul H., and Wolf, Jack K. 2007.
LinearDetectionviaBeliefPropagation. In:ProceedingsoftheAnnualAllertonCon-
ferenceonCommunication,Control,andComputing.
Billingsley,Patrick.1995. ProbabilityandMeasure. Wiley.
Bishop, Christopher M. 1995. Neural Networks for Pattern Recognition. Clarendon
Press.
Bishop,ChristopherM.1999. BayesianPCA. In:AdvancesinNeuralInformationPro-
cessingSystems.
Bishop,ChristopherM.2006. PatternRecognitionandMachineLearning. Springer.
Blei,DavidM.,Kucukelbir,Alp,andMcAuliffe,JonD.2017. VariationalInference:A
ReviewforStatisticians. JournaloftheAmericanStatisticalAssociation,112(518),
859–877.
Blum,Arvim,andHardt,Moritz.2015. TheLadder:AReliableLeaderboardforMa-
chineLearningCompetitions. In:InternationalConferenceonMachineLearning.
Bonnans,J.Fr´ed´eric,Gilbert,J.Charles,Lemar´echal,Claude,andSagastiza´bal,Clau-
diaA.2006. NumericalOptimization:TheoreticalandPracticalAspects. Springer.
Borwein, Jonathan M., and Lewis, Adrian S. 2006. Convex Analysis and Nonlinear
Optimization.2ndedn. CanadianMathematicalSociety.
Bottou, L´eon. 1998. Online Algorithms and Stochastic Approximations. Pages 9–42
of:OnlineLearningandNeuralNetworks. CambridgeUniversityPress.
Bottou,L´eon,Curtis,FrankE.,andNocedal,Jorge.2018. OptimizationMethodsfor
Large-ScaleMachineLearning. SIAMReview,60(2),223–311.
Boucheron, Stephane, Lugosi, Gabor, and Massart, Pascal. 2013. Concentration In-
equalities:ANonasymptoticTheoryofIndependence. OxfordUniversityPress.
Boyd, Stephen, and Vandenberghe, Lieven. 2004. Convex Optimization. Cambridge
UniversityPress.
Boyd,Stephen,andVandenberghe,Lieven.2018. IntroductiontoAppliedLinearAlge-
bra. CambridgeUniversityPress.
Brochu, Eric, Cora, Vlad M., and de Freitas, Nando. 2009. A Tutorial on Bayesian
Optimization of Expensive Cost Functions, with Application to Active User Modeling
and Hierarchical Reinforcement Learning. Tech. rept. TR-2009-023. Department of
ComputerScience,UniversityofBritishColumbia.
Brooks,Steve,Gelman,Andrew,Jones,GalinL.,andMeng,Xiao-Li(eds).2011.Hand-
bookofMarkovChainMonteCarlo. ChapmanandHall/CRC.
Brown,LawrenceD.1986. FundamentalsofStatisticalExponentialFamilies:WithAp-
plicationsinStatisticalDecisionTheory. InstituteofMathematicalStatistics.
Bryson, Arthur E. 1961. A Gradient Method for Optimizing Multi-Stage Allocation
Processes.In:ProceedingsoftheHarvardUniversitySymposiumonDigitalComputers
andTheirApplications.
Bubeck,S´ebastien.2015. ConvexOptimization:AlgorithmsandComplexity. Founda-
tionsandTrendsinMachineLearning,8(3-4),231–357.
Bu¨hlmann,Peter,andVanDeGeer,Sara.2011. StatisticsforHigh-DimensionalData.
Springer.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
References 397
Burges, Christopher. 2010. Dimension Reduction: A Guided Tour. Foundations and
TrendsinMachineLearning,2(4),275–365.
Carroll, J Douglas, and Chang, Jih-Jie. 1970. Analysis of Individual Differences in
Multidimensional Scaling via an N-Way Generalization of “Eckart-Young” Decom-
position. Psychometrika,35(3),283–319.
Casella,George,andBerger,RogerL.2002. StatisticalInference. Duxbury.
C¸inlar,Erhan.2011. ProbabilityandStochastics. Springer.
Chang,Chih-Chung,andLin,Chih-Jen.2011. LIBSVM:ALibraryforSupportVector
Machines. ACMTransactionsonIntelligentSystemsandTechnology,2,27:1–27:27.
Cheeseman,Peter.1985. InDefenseofProbability. In:ProceedingsoftheInternational
JointConferenceonArtificialIntelligence.
Chollet,Francois,andAllaire,J.J.2018. DeepLearningwithR. ManningPublications.
Codd,EdgarF.1990. TheRelationalModelforDatabaseManagement. Addison-Wesley
LongmanPublishing.
Cunningham,JohnP.,andGhahramani,Zoubin.2015. LinearDimensionalityReduc-
tion: Survey, Insights, and Generalizations. Journal of Machine Learning Research,
16,2859–2900.
Datta,BiswaN.2010. NumericalLinearAlgebraandApplications. SIAM.
Davidson,AnthonyC.,andHinkley,DavidV.1997. BootstrapMethodsandTheirAppli-
cation. CambridgeUniversityPress.
Dean, Jeffrey, Corrado, Greg S., Monga, Rajat, and Chen, et al. 2012. Large Scale
DistributedDeepNetworks. In:AdvancesinNeuralInformationProcessingSystems.
Deisenroth,Marc P.,and Mohamed,Shakir. 2012. ExpectationPropagation inGaus-
sianProcessDynamicalSystems. Pages2618–2626of:AdvancesinNeuralInforma-
tionProcessingSystems.
Deisenroth, Marc P., and Ohlsson, Henrik. 2011. A General Perspective on Gaussian
Filtering and Smoothing: Explaining Current and Deriving New Algorithms. In:
ProceedingsoftheAmericanControlConference.
Deisenroth, Marc P., Fox, Dieter, and Rasmussen, Carl E. 2015. Gaussian Processes
for Data-Efficient Learning in Robotics and Control. IEEE Transactions on Pattern
AnalysisandMachineIntelligence,37(2),408–423.
Dempster,ArthurP.,Laird,NanM.,andRubin,DonaldB.1977. MaximumLikelihood
fromIncompleteDataviatheEMAlgorithm. JournaloftheRoyalStatisticalSociety,
39(1),1–38.
Deng, Li, Seltzer, Michael L., Yu, Dong, Acero, Alex, Mohamed, Abdel-rahman, and
Hinton, Geoffrey E. 2010. Binary Coding of Speech Spectrograms Using a Deep
Auto-Encoder. In:ProceedingsofInterspeech.
Devroye,Luc.1986. Non-UniformRandomVariateGeneration. Springer.
Donoho, David L., and Grimes, Carrie. 2003. Hessian Eigenmaps: Locally Linear
Embedding Techniques for High-Dimensional Data. Proceedings of the National
AcademyofSciences,100(10),5591–5596.
Dosta´l,Zden˘ek.2009. OptimalQuadraticProgrammingAlgorithms:WithApplications
toVariationalInequalities. Springer.
Douven, Igor. 2017. Abduction. In: The Stanford Encyclopedia of Philosophy. Meta-
physicsResearchLab,StanfordUniversity.
Downey, Allen B. 2014. Think Stats: Exploratory Data Analysis. 2nd edn. O’Reilly
Media.
Dreyfus, Stuart. 1962. The Numerical Solution of Variational Problems. Journal of
MathematicalAnalysisandApplications,5(1),30–45.
Drumm,Volker,andWeil,Wolfgang.2001. LineareAlgebraundAnalytischeGeometrie.
LectureNotes,Universita¨tKarlsruhe(TH).
Dudley,RichardM.2002. RealAnalysisandProbability. CambridgeUniversityPress.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
398 References
Eaton, Morris L. 2007. Multivariate Statistics: A Vector Space Approach. Institute of
MathematicalStatisticsLectureNotes.
Eckart,Carl,andYoung,Gale.1936. TheApproximationofOneMatrixbyAnotherof
LowerRank. Psychometrika,1(3),211–218.
Efron,Bradley,andHastie,Trevor.2016.ComputerAgeStatisticalInference:Algorithms,
EvidenceandDataScience. CambridgeUniversityPress.
Efron,Bradley,andTibshirani,RobertJ.1993. AnIntroductiontotheBootstrap. Chap-
manandHall/CRC.
Elliott, Conal. 2009. Beautiful Differentiation. In: International Conference on Func-
tionalProgramming.
Evgeniou, Theodoros, Pontil, Massimiliano, and Poggio, Tomaso. 2000. Statistical
LearningTheory:APrimer. InternationalJournalofComputerVision,38(1),9–13.
Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang, Xiang-Rui, and Lin, Chih-Jen.
2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine
LearningResearch,9,1871–1874.
Gal,Yarin,vanderWilk,Mark,andRasmussen,CarlE.2014. DistributedVariational
Inference in Sparse Gaussian Process Regression and Latent Variable Models. In:
AdvancesinNeuralInformationProcessingSystems.
Ga¨rtner,Thomas.2008. KernelsforStructuredData. WorldScientific.
Gavish,Matan,andDonoho,DavidL.2014. TheOptimalHardThresholdforSingular
√
Valuesis4 3. IEEETransactionsonInformationTheory,60(8),5040–5053.
Gelman,Andrew,Carlin,JohnB.,Stern,HalS.,andRubin,DonaldB.2004. Bayesian
DataAnalysis. ChapmanandHall/CRC.
Gentle, James E. 2004. Random Number Generation and Monte Carlo Methods.
Springer.
Ghahramani,Zoubin.2015. ProbabilisticMachineLearningandArtificialIntelligence.
Nature,521,452–459.
Ghahramani,Zoubin,andRoweis,SamT.1999. LearningNonlinearDynamicalSys-
temsUsinganEMAlgorithm.In:AdvancesinNeuralInformationProcessingSystems.
MITPress.
Gilks,WalterR.,Richardson,Sylvia,andSpiegelhalter,DavidJ.1996. MarkovChain
MonteCarloinPractice. ChapmanandHall/CRC.
Gneiting, Tilmann, and Raftery, Adrian E. 2007. Strictly Proper Scoring Rules, Pre-
diction,andEstimation. JournaloftheAmericanStatisticalAssociation,102(477),
359–378.
Goh,Gabriel.2017. WhyMomentumReallyWorks. Distill.
Gohberg,Israel,Goldberg,Seymour,andKrupnik,Nahum.2012. TracesandDetermi-
nantsofLinearOperators. Birkha¨user.
Golan, Jonathan S. 2007. The Linear Algebra a Beginning Graduate Student Ought to
Know. Springer.
Golub,GeneH.,andVanLoan,CharlesF.2012. MatrixComputations. JHUPress.
Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. 2016. Deep Learning. MIT
Press.
Graepel, Thore, Candela, Joaquin Quin˜onero-Candela, Borchert, Thomas, and Her-
brich,Ralf.2010. Web-ScaleBayesianClick-throughRatePredictionforSponsored
SearchAdvertisinginMicrosoft’sBingSearchEngine. In:ProceedingsoftheInterna-
tionalConferenceonMachineLearning.
Griewank,Andreas,andWalther,Andrea.2003. IntroductiontoAutomaticDifferenti-
ation. In:ProceedingsinAppliedMathematicsandMechanics.
Griewank,Andreas,andWalther,Andrea.2008. EvaluatingDerivatives,Principlesand
TechniquesofAlgorithmicDifferentiation. SIAM.
Grimmett,GeoffreyR.,andWelsh,Dominic.2014.Probability:AnIntroduction.Oxford
UniversityPress.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
References 399
Grinstead,CharlesM.,andSnell,J.Laurie.1997.IntroductiontoProbability.American
MathematicalSociety.
Hacking,Ian.2001. ProbabilityandInductiveLogic. CambridgeUniversityPress.
Hall,Peter.1992. TheBootstrapandEdgeworthExpansion. Springer.
Hallin, Marc, Paindaveine, Davy, and Sˇiman, Miroslav. 2010. Multivariate Quan-
tilesandMultiple-OutputRegressionQuantiles:Fromℓ OptimizationtoHalfspace
1
Depth. AnnalsofStatistics,38,635–669.
Hasselblatt, Boris, and Katok, Anatole. 2003. A First Course in Dynamics with a
PanoramaofRecentDevelopments. CambridgeUniversityPress.
Hastie,Trevor,Tibshirani,Robert,andFriedman,Jerome.2001. TheElementsofSta-
tisticalLearning–DataMining,Inference,andPrediction. Springer.
Hausman, Karol, Springenberg, Jost T., Wang, Ziyu, Heess, Nicolas, and Riedmiller,
Martin. 2018. Learning an Embedding Space for Transferable Robot Skills. In:
ProceedingsoftheInternationalConferenceonLearningRepresentations.
Hazan, Elad. 2015. Introduction to Online Convex Optimization. Foundations and
TrendsinOptimization,2(3–4),157–325.
Hensman, James, Fusi, Nicolo`, and Lawrence, Neil D. 2013. Gaussian Processes for
BigData. In:ProceedingsoftheConferenceonUncertaintyinArtificialIntelligence.
Herbrich, Ralf, Minka, Tom, and Graepel, Thore. 2007. TrueSkill(TM): A Bayesian
SkillRatingSystem. In:AdvancesinNeuralInformationProcessingSystems.
Hiriart-Urruty,Jean-Baptiste,andLemar´echal,Claude.2001. FundamentalsofConvex
Analysis. Springer.
Hoffman, Matthew D., Blei, David M., and Bach, Francis. 2010. Online Learning for
LatentDirichletAllocation. AdvancesinNeuralInformationProcessingSystems.
Hoffman,MatthewD.,Blei,DavidM.,Wang,Chong,andPaisley,John.2013.Stochas-
ticVariationalInference. JournalofMachineLearningResearch,14(1),1303–1347.
Hofmann,Thomas,Scho¨lkopf,Bernhard,andSmola,AlexanderJ.2008. KernelMeth-
odsinMachineLearning. AnnalsofStatistics,36(3),1171–1220.
Hogben,Leslie.2013. HandbookofLinearAlgebra. ChapmanandHall/CRC.
Horn,RogerA.,andJohnson,CharlesR.2013. MatrixAnalysis. CambridgeUniversity
Press.
Hotelling,Harold.1933. AnalysisofaComplexofStatisticalVariablesintoPrincipal
Components. JournalofEducationalPsychology,24,417–441.
Hyvarinen,Aapo,Oja,Erkki,andKarhunen,Juha.2001.IndependentComponentAnal-
ysis. Wiley.
Imbens, Guido W., and Rubin, Donald B. 2015. Causal Inference for Statistics, Social
andBiomedicalSciences. CambridgeUniversityPress.
Jacod,Jean,andProtter,Philip.2004. ProbabilityEssentials. Springer.
Jaynes,EdwinT.2003. ProbabilityTheory:TheLogicofScience. CambridgeUniversity
Press.
Jefferys,WilliamH.,andBerger,JamesO.1992. Ockham’sRazorandBayesianAnal-
ysis. AmericanScientist,80,64–72.
Jeffreys,Harold.1961. TheoryofProbability. OxfordUniversityPress.
JimenezRezende,Danilo,andMohamed,Shakir.2015.VariationalInferencewithNor-
malizingFlows.In:ProceedingsoftheInternationalConferenceonMachineLearning.
Jimenez Rezende, Danilo, Mohamed, Shakir, and Wierstra, Daan. 2014. Stochastic
Backpropagation and Approximate Inference in Deep Generative Models. In: Pro-
ceedingsoftheInternationalConferenceonMachineLearning.
Joachims,Thorsten.1999. AdvancesinKernelMethods–SupportVectorLearning. MIT
Press. Chap.MakingLarge-ScaleSVMLearningPractical,pages169–184.
Jordan,MichaelI.,Ghahramani,Zoubin,Jaakkola,TommiS.,andSaul,LawrenceK.
1999.AnIntroductiontoVariationalMethodsforGraphicalModels.MachineLearn-
ing,37,183–233.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
400 References
Julier,SimonJ.,andUhlmann,JeffreyK.1997. ANewExtensionoftheKalmanFilter
toNonlinearSystems.In:ProceedingsofAeroSenseSymposiumonAerospace/Defense
Sensing,SimulationandControls.
Kaiser,Marcus,andHilgetag,ClausC.2006. NonoptimalComponentPlacement,but
ShortProcessingPaths,DuetoLong-DistanceProjectionsinNeuralSystems. PLoS
ComputationalBiology,2(7),e95.
Kalman,Dan.1996. ASingularlyValuableDecomposition:TheSVDofaMatrix. Col-
legeMathematicsJournal,27(1),2–23.
Kalman,RudolfE.1960.ANewApproachtoLinearFilteringandPredictionProblems.
TransactionsoftheASME–JournalofBasicEngineering,82(SeriesD),35–45.
Kamthe,Sanket,andDeisenroth,MarcP.2018.Data-EfficientReinforcementLearning
with Probabilistic Model Predictive Control. In: Proceedings of the International
ConferenceonArtificialIntelligenceandStatistics.
Katz,VictorJ.2004. AHistoryofMathematics. Pearson/Addison-Wesley.
Kelley,HenryJ.1960. GradientTheoryofOptimalFlightPaths. ArsJournal,30(10),
947–954.
Kimeldorf,GeorgeS.,andWahba,Grace.1970. ACorrespondencebetweenBayesian
EstimationonStochasticProcessesandSmoothingbySplines. AnnalsofMathemat-
icalStatistics,41(2),495–502.
Kingma, Diederik P., and Welling, Max. 2014. Auto-Encoding Variational Bayes. In:
ProceedingsoftheInternationalConferenceonLearningRepresentations.
Kittler,Josef,andFo¨glein,Janos.1984.ContextualClassificationofMultispectralPixel
Data. ImageandVisionComputing,2(1),13–29.
Kolda,TamaraG.,andBader,BrettW.2009. TensorDecompositionsandApplications.
SIAMReview,51(3),455–500.
Koller,Daphne,andFriedman,Nir.2009. ProbabilisticGraphicalModels. MITPress.
Kong,Linglong,andMizera,Ivan.2012. QuantileTomography:UsingQuantileswith
MultivariateData. StatisticaSinica,22,1598–1610.
Lang,Serge.1987. LinearAlgebra. Springer.
Lawrence,NeilD.2005. ProbabilisticNon-LinearPrincipalComponentAnalysiswith
Gaussian Process Latent Variable Models. Journal of Machine Learning Research,
6(Nov.),1783–1816.
Leemis, Lawrence M., and McQueston, Jacquelyn T. 2008. Univariate Distribution
Relationships. AmericanStatistician,62(1),45–53.
Lehmann, Erich L., and Romano, Joseph P. 2005. Testing Statistical Hypotheses.
Springer.
Lehmann,ErichLeo,andCasella,George.1998. TheoryofPointEstimation. Springer.
Liesen,Jo¨rg,andMehrmann,Volker.2015. LinearAlgebra. Springer.
Lin,Hsuan-Tien,Lin,Chih-Jen,andWeng,RubyC.2007.ANoteonPlatt’sProbabilistic
OutputsforSupportVectorMachines. MachineLearning,68,267–276.
Ljung,Lennart.1999. SystemIdentification:TheoryfortheUser. PrenticeHall.
Loosli,Ga¨elle,Canu,St´ephane,andOng,ChengSoon.2016. LearningSVMinKre˘ın
Spaces.IEEETransactionsofPatternAnalysisandMachineIntelligence,38(6),1204–
1216.
Luenberger,DavidG.1969. OptimizationbyVectorSpaceMethods. Wiley.
MacKay,DavidJ.C.1992. BayesianInterpolation. NeuralComputation,4,415–447.
MacKay, David J. C. 1998. Introduction to Gaussian Processes. Pages 133–165 of:
Bishop,C.M.(ed),NeuralNetworksandMachineLearning. Springer.
MacKay, David J. C. 2003. Information Theory, Inference, and Learning Algorithms.
CambridgeUniversityPress.
Magnus,JanR.,andNeudecker,Heinz.2007. MatrixDifferentialCalculuswithAppli-
cationsinStatisticsandEconometrics. Wiley.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
References 401
Manton, Jonathan H., and Amblard, Pierre-Olivier. 2015. A Primer on Reproducing
KernelHilbertSpaces. FoundationsandTrendsinSignalProcessing,8(1–2),1–126.
Markovsky, Ivan. 2011. Low Rank Approximation: Algorithms, Implementation, Appli-
cations. Springer.
Maybeck,PeterS.1979. StochasticModels,Estimation,andControl. AcademicPress.
McCullagh,Peter,andNelder,JohnA.1989. GeneralizedLinearModels. CRCPress.
McEliece,RobertJ.,MacKay,DavidJ.C.,andCheng,Jung-Fu.1998. TurboDecoding
as an Instance of Pearl’s “Belief Propagation” Algorithm. IEEE Journal on Selected
AreasinCommunications,16(2),140–152.
Mika, Sebastian, Ra¨tsch, Gunnar, Weston, Jason, Scho¨lkopf, Bernhard, and Mu¨ller,
Klaus-Robert. 1999. Fisher Discriminant Analysis with Kernels. Pages 41–48 of:
ProceedingsoftheWorkshoponNeuralNetworksforSignalProcessing.
Minka,ThomasP.2001a. AFamilyofAlgorithmsforApproximateBayesianInference.
Ph.D.thesis,MassachusettsInstituteofTechnology.
Minka, Tom. 2001b. Automatic Choice of Dimensionality of PCA. In: Advances in
NeuralInformationProcessingSystems.
Mitchell,Tom.1997. MachineLearning. McGraw-Hill.
Mnih, Volodymyr, Kavukcuoglu, Koray, and Silver, David, et al. 2015. Human-Level
ControlthroughDeepReinforcementLearning. Nature,518,529–533.
Moonen, Marc, and De Moor, Bart. 1995. SVD and Signal Processing, III: Algorithms,
ArchitecturesandApplications. Elsevier.
Moustaki,Irini,Knott,Martin,andBartholomew,DavidJ.2015. Latent-VariableMod-
eling. AmericanCancerSociety. Pages1–10.
Mu¨ller, Andreas C., and Guido, Sarah. 2016. Introduction to Machine Learning with
Python:AGuideforDataScientists. O’ReillyPublishing.
Murphy,KevinP.2012. MachineLearning:AProbabilisticPerspective. MITPress.
Neal,RadfordM.1996. BayesianLearningforNeuralNetworks. Ph.D.thesis,Depart-
mentofComputerScience,UniversityofToronto.
Neal, Radford M., and Hinton, Geoffrey E. 1999. A View of the EM Algorithm that
Justifies Incremental, Sparse, and Other Variants. Pages 355–368 of: Learning in
GraphicalModels. MITPress.
Nelsen,Roger.2006. AnIntroductiontoCopulas. Springer.
Nesterov,Yuri.2018. LecturesonConvexOptimization. Springer.
Neumaier,Arnold.1998. SolvingIll-ConditionedandSingularLinearSystems:ATu-
torialonRegularization. SIAMReview,40,636–666.
Nocedal,Jorge,andWright,StephenJ.2006. NumericalOptimization. Springer.
Nowozin, Sebastian, Gehler, Peter V., Jancsary, Jeremy, and Lampert, Christoph H.
(eds).2014. AdvancedStructuredPrediction. MITPress.
O’Hagan, Anthony. 1991. Bayes-Hermite Quadrature. Journal of Statistical Planning
andInference,29,245–260.
Ong,ChengSoon,Mary,Xavier,Canu,St´ephane,andSmola,AlexanderJ.2004.Learn-
ing with Non-Positive Kernels. In: Proceedings of the International Conference on
MachineLearning.
Ormoneit, Dirk, Sidenbladh, Hedvig, Black, Michael J., and Hastie, Trevor. 2001.
Learning and Tracking Cyclic Human Motion. In: Advances in Neural Information
ProcessingSystems.
Page, Lawrence, Brin, Sergey, Motwani, Rajeev, and Winograd, Terry. 1999. The
PageRank Citation Ranking: Bringing Order to the Web. Tech. rept. Stanford Info-
Lab.
Paquet,Ulrich.2008. BayesianInferenceforLatentVariableModels. Ph.D.thesis,Uni-
versityofCambridge.
Parzen, Emanuel. 1962. On Estimation of a Probability Density Function and Mode.
AnnalsofMathematicalStatistics,33(3),1065–1076.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
402 References
Pearl,Judea.1988. ProbabilisticReasoninginIntelligentSystems:NetworksofPlausible
Inference. MorganKaufmann.
Pearl,Judea.2009. Causality:Models,ReasoningandInference.2ndedn. Cambridge
UniversityPress.
Pearson,Karl.1895. ContributionstotheMathematicalTheoryofEvolution.II.Skew
VariationinHomogeneousMaterial. PhilosophicalTransactionsoftheRoyalSociety
A:Mathematical,PhysicalandEngineeringSciences,186,343–414.
Pearson,Karl.1901. OnLinesandPlanesofClosestFittoSystemsofPointsinSpace.
PhilosophicalMagazine,2(11),559–572.
Peters, Jonas, Janzing, Dominik, and Scho¨lkopf, Bernhard. 2017. Elements of Causal
Inference:FoundationsandLearningAlgorithms. MITPress.
Petersen,KaareB.,andPedersen,MichaelS.2012. TheMatrixCookbook. Tech.rept.
TechnicalUniversityofDenmark.
Platt,JohnC.2000. ProbabilisticOutputsforSupportVectorMachinesandCompar-
isonstoRegularizedLikelihoodMethods. In:AdvancesinLargeMarginClassifiers.
Pollard, David. 2002. A User’s Guide to Measure Theoretic Probability. Cambridge
UniversityPress.
Polyak,RomanA.2016.TheLegendreTransformationinModernOptimization.Pages
437–507of:Goldengorin,B.(ed),OptimizationandItsApplicationsinControland
DataSciences. Springer.
Press, William H., Teukolsky, Saul A., Vetterling, William T., and Flannery, Brian P.
2007. Numerical Recipes: The Art of Scientific Computing. Cambridge University
Press.
Proschan,MichaelA.,andPresnell,Brett.1998. ExpecttheUnexpectedfromCondi-
tionalExpectation. AmericanStatistician,52(3),248–252.
Raschka, Sebastian, and Mirjalili, Vahid. 2017. Python Machine Learning: Machine
LearningandDeepLearningwithPython,scikit-learn,andTensorFlow.PacktPublish-
ing.
Rasmussen,CarlE.,andGhahramani,Zoubin.2001. Occam’sRazor. In:Advancesin
NeuralInformationProcessingSystems.
Rasmussen, Carl E., and Ghahramani, Zoubin. 2003. Bayesian Monte Carlo. In: Ad-
vancesinNeuralInformationProcessingSystems.
Rasmussen,CarlE.,andWilliams,ChristopherK.I.2006. GaussianProcessesforMa-
chineLearning. MITPress.
Reid, Mark, and Williamson, Robert C. 2011. Information, Divergence and Risk for
BinaryExperiments. JournalofMachineLearningResearch,12,731–817.
Rifkin,RyanM.,andLippert,RossA.2007. ValueRegularizationandFenchelDuality.
JournalofMachineLearningResearch,8,441–479.
Rockafellar,RalphT.1970. ConvexAnalysis. PrincetonUniversityPress.
Rogers,Simon,andGirolami,Mark.2016. AFirstCourseinMachineLearning. Chap-
manandHall/CRC.
Rosenbaum, Paul R. 2017. Observation and Experiment: An Introduction to Causal
Inference. HarvardUniversityPress.
Rosenblatt, Murray. 1956. Remarks on Some Nonparametric Estimates of a Density
Function. AnnalsofMathematicalStatistics,27(3),832–837.
Roweis,SamT.1998. EMAlgorithmsforPCAandSPCA. Pages626–632of:Advances
inNeuralInformationProcessingSystems.
Roweis,SamT.,andGhahramani,Zoubin.1999.AUnifyingReviewofLinearGaussian
Models. NeuralComputation,11(2),305–345.
Roy, Anindya, and Banerjee, Sudipto. 2014. Linear Algebra and Matrix Analysis for
Statistics. ChapmanandHall/CRC.
Rubinstein, Reuven Y., and Kroese, Dirk P. 2016. Simulation and the Monte Carlo
Method. Wiley.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
References 403
Ruffini,Paolo.1799. TeoriaGeneraledelleEquazioni,incuisiDimostraImpossibilela
SoluzioneAlgebraicadelleEquazioniGeneralidiGradoSuperiorealQuarto. Stampe-
riadiS.Tommasod’Aquino.
Rumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. 1986. Learning
RepresentationsbyBack-PropagatingErrors. Nature,323(6088),533–536.
Sæmundsson, Steindo´r, Hofmann, Katja, and Deisenroth, Marc P. 2018. Meta Rein-
forcementLearningwithLatentVariableGaussianProcesses. In:Proceedingsofthe
ConferenceonUncertaintyinArtificialIntelligence.
Saitoh,Saburou. 1988. Theoryof ReproducingKernelsandits Applications. Longman
ScientificandTechnical.
Sa¨rkka¨,Simo.2013. BayesianFilteringandSmoothing. CambridgeUniversityPress.
Scho¨lkopf,Bernhard,andSmola,AlexanderJ.2002. LearningwithKernels–Support
VectorMachines,Regularization,Optimization,andBeyond. MITPress.
Scho¨lkopf, Bernhard, Smola, Alexander J., and Mu¨ller, Klaus-Robert. 1997. Kernel
Principal Component Analysis. In: Proceedings of the International Conference on
ArtificialNeuralNetworks.
Scho¨lkopf,Bernhard,Smola,AlexanderJ.,andMu¨ller,Klaus-Robert.1998. Nonlinear
ComponentAnalysisasaKernelEigenvalueProblem. NeuralComputation,10(5),
1299–1319.
Scho¨lkopf, Bernhard, Herbrich, Ralf, and Smola, Alexander J. 2001. A Generalized
RepresenterTheorem. In:ProceedingsoftheInternationalConferenceonComputa-
tionalLearningTheory.
Schwartz,Laurent.1964. SousEspacesHilbertiensd’EspacesVectorielsTopologiques
etNoyauxAssoci´es. Journald’AnalyseMath´ematique,13,115–256.
Schwarz,GideonE.1978. EstimatingtheDimensionofaModel. AnnalsofStatistics,
6(2),461–464.
Shahriari,Bobak,Swersky,Kevin,Wang,Ziyu,Adams,RyanP.,andDeFreitas,Nando.
2016. Taking the Human out of the Loop: A Review of Bayesian Optimization.
ProceedingsoftheIEEE,104(1),148–175.
Shalev-Shwartz, Shai, and Ben-David, Shai. 2014. Understanding Machine Learning:
FromTheorytoAlgorithms. CambridgeUniversityPress.
Shawe-Taylor,John,andCristianini,Nello.2004. KernelMethodsforPatternAnalysis.
CambridgeUniversityPress.
Shawe-Taylor,John,andSun,Shiliang.2011.AReviewofOptimizationMethodologies
inSupportVectorMachines. Neurocomputing,74(17),3609–3618.
Shental,Ori,Siegel,PaulH.,Wolf,JackK.,Bickson,Danny,andDolev,Danny.2008.
Gaussian Belief Propagation Solver for Systems of Linear Equations. Pages 1863–
1867of:ProceedingsoftheInternationalSymposiumonInformationTheory.
Shewchuk,JonathanR.1994. AnIntroductiontotheConjugateGradientMethodwith-
outtheAgonizingPain.
Shi, Jianbo, and Malik, Jitendra. 2000. Normalized Cuts and Image Segmentation.
IEEETransactionsonPatternAnalysisandMachineIntelligence,22(8),888–905.
Shi, Qinfeng, Petterson, James, Dror, Gideon, Langford, John, Smola, Alexander J.,
and Vishwanathan, S. V. N. 2009. Hash Kernels for Structured Data. Journal of
MachineLearningResearch,2615–2637.
Shiryayev,AlbertN.1984. Probability. Springer.
Shor,NaumZ.1985. MinimizationMethodsforNon-DifferentiableFunctions. Springer.
Shotton, Jamie, Winn, John, Rother, Carsten, and Criminisi, Antonio. 2006. Texton-
Boost:JointAppearance,ShapeandContextModelingforMulti-ClassObjectRecog-
nition and Segmentation. In: Proceedings of the European Conference on Computer
Vision.
Smith,AdrianF.M.,andSpiegelhalter,David.1980.BayesFactorsandChoiceCriteria
forLinearModels. JournaloftheRoyalStatisticalSocietyB,42(2),213–220.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
404 References
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. 2012. Practical Bayesian Op-
timization of Machine Learning Algorithms. In: Advances in Neural Information
ProcessingSystems.
Spearman, Charles. 1904. “General Intelligence,” Objectively Determined and Mea-
sured. AmericanJournalofPsychology,15(2),201–292.
Sriperumbudur, Bharath K., Gretton, Arthur, Fukumizu, Kenji, Scho¨lkopf, Bernhard,
andLanckriet,GertR.G.2010. HilbertSpaceEmbeddingsandMetricsonProba-
bilityMeasures. JournalofMachineLearningResearch,11,1517–1561.
Steinwart, Ingo. 2007. How to Compare Different Loss Functions and Their Risks.
ConstructiveApproximation,26,225–287.
Steinwart,Ingo,andChristmann,Andreas.2008. SupportVectorMachines. Springer.
Stoer,Josef,andBurlirsch,Roland.2002.IntroductiontoNumericalAnalysis.Springer.
Strang, Gilbert. 1993. The Fundamental Theorem of Linear Algebra. The American
MathematicalMonthly,100(9),848–855.
Strang,Gilbert.2003. IntroductiontoLinearAlgebra. Wellesley-CambridgePress.
Stray,Jonathan.2016. TheCuriousJournalist’sGuidetoData. TowCenterforDigital
JournalismatColumbia’sGraduateSchoolofJournalism.
Strogatz, Steven. 2014. Writing about Math for the Perplexed and the Traumatized.
NoticesoftheAmericanMathematicalSociety,61(3),286–291.
Sucar, Luis E., and Gillies, Duncan F. 1994. Probabilistic Reasoning in High-Level
Vision. ImageandVisionComputing,12(1),42–60.
Szeliski, Richard, Zabih, Ramin, and Scharstein, Daniel, et al. 2008. A Compar-
ative Study of Energy Minimization Methods for Markov Random Fields with
Smoothness-Based Priors. IEEE Transactions on Pattern Analysis and Machine In-
telligence,30(6),1068–1080.
Tandra, Haryono. 2014. The Relationship between the Change of Variable Theorem
and the Fundamental Theorem of Calculus for the Lebesgue Integral. Teaching of
Mathematics,17(2),76–83.
Tenenbaum,JoshuaB.,DeSilva,Vin,andLangford,JohnC.2000.AGlobalGeometric
Framework for Nonlinear Dimensionality Reduction. Science, 290(5500), 2319–
2323.
Tibshirani,Robert.1996. RegressionSelectionandShrinkageviatheLasso. Journal
oftheRoyalStatisticalSocietyB,58(1),267–288.
Tipping,MichaelE.,andBishop,ChristopherM.1999. ProbabilisticPrincipalCompo-
nentAnalysis. JournaloftheRoyalStatisticalSociety:SeriesB,61(3),611–622.
Titsias, Michalis K., and Lawrence, Neil D. 2010. Bayesian Gaussian Process Latent
Variable Model. In: Proceedings of the International Conference on Artificial Intelli-
genceandStatistics.
Toussaint,Marc. 2012. SomeNotes onGradientDescent. https://ipvs.informatik.uni-
stuttgart.de/mlr/marc/notes/gradientDescent.pdf.
Trefethen,LloydN.,andBauIII,David.1997. NumericalLinearAlgebra. SIAM.
Tucker,LedyardR.1966. SomeMathematicalNotesonThree-ModeFactorAnalysis.
Psychometrika,31(3),279–311.
Vapnik,VladimirN.1998. StatisticalLearningTheory. Wiley.
Vapnik,VladimirN.1999. AnOverviewofStatisticalLearningTheory. IEEETransac-
tionsonNeuralNetworks,10(5),988–999.
Vapnik,VladimirN.2000. TheNatureofStatisticalLearningTheory. Springer.
Vishwanathan, S. V. N., Schraudolph, Nicol N., Kondor, Risi, and Borgwardt,
KarstenM.2010. GraphKernels. JournalofMachineLearningResearch,11,1201–
1242.
von Luxburg, Ulrike, and Scho¨lkopf, Bernhard. 2011. Statistical Learning Theory:
Models, Concepts, and Results. Pages 651–706 of: D. M. Gabbay, S. Hartmann,
J.Woods(ed),HandbookoftheHistoryofLogic,vol.10. Elsevier.
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
References 405
Wahba,Grace.1990. SplineModelsforObservationalData. SocietyforIndustrialand
AppliedMathematics.
Walpole, Ronald E., Myers, Raymond H., Myers, Sharon L., and Ye, Keying. 2011.
ProbabilityandStatisticsforEngineersandScientists. PrenticeHall.
Wasserman,Larry.2004. AllofStatistics. Springer.
Wasserman,Larry.2007. AllofNonparametricStatistics. Springer.
Whittle,Peter.2000. ProbabilityviaExpectation. Springer.
Wickham,Hadley.2014. TidyData. JournalofStatisticalSoftware,59,1–23.
Williams,ChristopherK.I.1997. ComputingwithInfiniteNetworks. In:Advancesin
NeuralInformationProcessingSystems.
Yu, Yaoliang, Cheng, Hao, Schuurmans, Dale, and Szepesva´ri, Csaba. 2013. Charac-
terizingtheRepresenterTheorem. In:ProceedingsoftheInternationalConferenceon
MachineLearning.
Zadrozny, Bianca, and Elkan, Charles. 2001. Obtaining Calibrated Probability Esti-
mates from Decision Trees and Naive Bayesian Classifiers. In: Proceedings of the
InternationalConferenceonMachineLearning.
Zhang,Haizhang,Xu,Yuesheng,andZhang,Jun.2009. ReproducingKernelBanach
SpacesforMachineLearning.JournalofMachineLearningResearch,10,2741–2775.
Zia,RoyceK.P.,Redish,EdwardF.,andMcKay,SusanR.2009. MakingSenseofthe
LegendreTransform. AmericanJournalofPhysics,77(614),614–622.
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).

Index
1-of-K representation,364 canonicalbasis,45
ℓ norm,71 canonicalfeaturemap,389
1
ℓ norm,72 canonicallinkfunction,315
2
abduction,258 categoricalvariable,180
Abel-Ruffinitheorem,334 Cauchy-Schwarzinequality,75
Abeliangroup,36 change-of-variabletechnique,219
absolutelyhomogeneous,71 characteristicpolynomial,104
activationfunction,315 Choleskydecomposition,114
affinemapping,63 Choleskyfactor,114
affinesubspace,61 Choleskyfactorization,114
Akaikeinformationcriterion,288 class,370
algebra,17 classification,315
algebraicmultiplicity,106 closure,36
analytic,143 code,343
ancestralsampling,340,364 codirected,105
angle,76 codomain,58,139
associativity,24,26,36 collinear,105
attribute,253 column,22
augmentedmatrix,29 columnspace,59
auto-encoder,343 columnvector,22,38
automaticdifferentiation,161 completingthesquares,307
automorphism,49 concavefunction,236
conditionnumber,230
backpropagation,159
conditionalprobability,179
basicvariable,30
conditionallyindependent,195
basis,44
conjugate,208
basisvector,45
conjugateprior,208
Bayesfactor,287
convexconjugate,242
Bayes’law,185
convexfunction,236
Bayes’rule,185
convexhull,386
Bayes’theorem,185
convexoptimizationproblem,236,239
BayesianGP-LVM,347
convexset,236
Bayesianinference,274
coordinate,50
Bayesianinformationcriterion,288
coordinaterepresentation,50
Bayesianlinearregression,303
coordinatevector,50
Bayesianmodelselection,286
correlation,191
Bayesiannetwork,278,283
covariance,190
BayesianPCA,346
covariancematrix,190,198
Bernoullidistribution,205
covariate,253
Betadistribution,206
CPdecomposition,136
bilinearmapping,72
cross-covariance,191
bijective,48
cross-validation,258,263
binaryclassification,370
cumulativedistributionfunction,178,
Binomialdistribution,206
181
blind-sourceseparation,346
Borelσ-algebra,180 d-separation,281
407
ThismaterialispublishedbyCambridgeUniversityPressasMathematicsforMachineLearningby
MarcPeterDeisenroth,A.AldoFaisal,andChengSoonOng(2020).Thisversionisfreetoview
anddownloadforpersonaluseonly.Notforre-distribution,re-sale,oruseinderivativeworks.
©byM.P.Deisenroth,A.A.Faisal,andC.S.Ong,2024.https://mml-book.com.
408 Index
datacovariancematrix,318 feature,253
datapoint,253 featuremap,254
data-fitterm,302 featurematrix,296
decoder,343 featurevector,295
deepauto-encoder,347 Fisherdiscriminantanalysis,136
defective,111 Fisher-Neymantheorem,210
denominatorlayout,151 forwardmode,161
derivative,141 freevariable,30
designmatrix,294,296 fullrank,47
determinant,99 fullSVD,128
diagonalmatrix,115 fundamentaltheoremoflinear
diagonalizable,116 mappings,60
diagonalization,116 Gaussianelimination,31
differencequotient,141 Gaussianmixturemodel,349
dimension,45 Gaussianprocess,316
dimensionalityreduction,317 Gaussianprocesslatent-variablemodel,
directedgraphicalmodel,278,283 347
direction,61 generallineargroup,37
directionspace,61 generalsolution,28,30
distance,75 generalizedlinearmodel,272,315
distribution,177 generatingset,44
distributivity,24,26 generativeprocess,272,286
domain,58,139 generator,344
dotproduct,72 geometricmultiplicity,108
dualSVM,385 Givensrotation,94
Eckart-Youngtheorem,131,334 globalminimum,225
eigendecomposition,116 GP-LVM,347
eigenspace,106 gradient,146
eigenspectrum,106 Grammatrix,389
eigenvalue,105 Gram-Schmidtorthogonalization,89
eigenvalueequation,105 graphicalmodel,278
eigenvector,105 group,36
elementarytransformations,28 Hadamardproduct,23
EMalgorithm,360 hardmarginSVM,377
embarrassinglyparallel,264 Hessian,164
empiricalcovariance,192 Hessianeigenmaps,136
empiricalmean,192 Hessianmatrix,165
empiricalrisk,260 hingeloss,381
empiricalriskminimization,257,260 histogram,369
encoder,343 hyperparameter,258
endomorphism,49 hyperplane,61,62
epigraph,236 hyperprior,281
equivalent,56
i.i.d.,195
errorfunction,294
ICA,346
errorterm,382
identityautomorphism,49
Euclideandistance,72,75
identitymapping,49
Euclideannorm,72
identitymatrix,23
Euclideanvectorspace,73
image,58,139
eventspace,175
independentandidenticallydistributed,
evidence,186,285,306
195,260,266
example,253
independentcomponentanalysis,346
expectedrisk,261
inferencenetwork,344
expectedvalue,187
injective,48
exponentialfamily,205,211
innerproduct,73
extendedKalmanfilter,170
innerproductspace,73
factoranalysis,346 intermediatevariables,162
factorgraph,283 inverse,24
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Index 409
inverseelement,36 margin,374
invertible,24 marginal,190
Isomap,136 marginallikelihood,186,286,306
isomorphism,49 marginalprobability,179
Jacobian,146,150 marginalizationproperty,184
Jacobiandeterminant,152 Markovrandomfield,283
Jeffreys-Lindleyparadox,287 matrix,22
Jensen’sinequality,239 matrixfactorization,98
jointprobability,178 maximumaposteriori,300
maximumaposterioriestimation,269
Karhunen-Lo`evetransform,318
maximumlikelihood,257
kernel,33,47,58,254,388
maximumlikelihoodestimate,296
kerneldensityestimation,369
maximumlikelihoodestimation,265,
kernelmatrix,389
293
kernelPCA,347
mean,187
kerneltrick,316,347,389
meanfunction,309
label,253 meanvector,198
Lagrangemultiplier,234 measure,180
Lagrangian,234 median,188
Lagrangiandualproblem,234 metric,76
Laplaceapproximation,170 minimal,44
Laplaceexpansion,102 minimaxinequality,234
Laplacianeigenmaps,136
misfitterm,302
LASSO,303,316
mixturemodel,349
latentvariable,275
mixtureweight,349
law,177,181
mode,188
lawoftotalvariance,203
model,251
leadingcoefficient,30
modelevidence,286
least-squaresloss,154
modelselection,258
least-squaresproblem,261
Moore-Penrosepseudo-inverse,35
least-squaressolution,88
multidimensionalscaling,136
left-singularvectors,119
multiplicationbyscalars,37
Legendretransform,242
multivariate,178
Legendre-Fencheltransform,242
multivariateGaussiandistribution,198
length,71
multivariateTaylorseries,166
likelihood,185,265,269,291
line,61,82 naturalparameters,212
linearcombination,40 negativelog-likelihood,265
linearmanifold,61 nestedcross-validation,258,284
linearmapping,48 neutralelement,36
linearprogram,239 noninvertible,24
linearsubspace,39 nonsingular,24
lineartransformation,48 norm,71
linearlydependent,40 normaldistribution,197
linearlyindependent,40 normalequation,86
linkfunction,272 normalvector,80
loading,322 nullspace,33,47,58
localminimum,225 numeratorlayout,150
log-partitionfunction,211 Occam’srazor,285
logisticregression,315 ONB,79
logisticsigmoid,315 one-hotencoding,364
lossfunction,260,381 orderedbasis,50
lossterm,382 orthogonal,77
lower-triangularmatrix,101 orthogonalbasis,79
Maclaurinseries,143 orthogonalcomplement,79
Manhattannorm,71 orthogonalmatrix,78
MAP,300 orthonormal,77
MAPestimation,269 orthonormalbasis,79
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
410 Index
outerproduct,38 regularizationparameter,263,302,380
overfitting,262,271,299 regularizedleastsquares,302
regularizer,263,302,380,382
PageRank,114
representertheorem,384
parameters,61
responsibility,352
parametricequation,61
reversemode,161
partialderivative,146
right-singularvectors,119
particularsolution,27,30
RMSE,298
PCA,317
rootmeansquareerror,298
pdf,181
rotation,91
penaltyterm,263
rotationmatrix,92
pivot,30
row,22
plane,62
rowvector,22,38
plate,281
row-echelonform,30
populationmeanandcovariance,191
positivedefinite,71,73,74,76 samplemean,192
posterior,185,269 samplespace,175
posteriorodds,287 scalar,37
poweriteration,334 scalarproduct,72
powerseriesrepresentation,145 sigmoid,213
PPCA,340 similar,56
preconditioner,230 singular,24
predictor,12,255 singularvaluedecomposition,119
primalproblem,234 singularvalueequation,124
principalcomponent,322 singularvaluematrix,119
principalcomponentanalysis,136,317 singularvalues,119
principalsubspace,327 slackvariable,379
prior,185,269 softmarginSVM,379,380
priorodds,287 solution,20
probabilisticinverse,186 span,44
probabilisticPCA,340 specialsolution,27
probabilisticprogramming,278 spectralclustering,136
probability,175 spectralnorm,131
probabilitydensityfunction,181 spectraltheorem,111
probabilitydistribution,172 spectrum,106
probabilityintegraltransform,217 squarematrix,25
probabilitymassfunction,178 standardbasis,45
productrule,184 standarddeviation,190
projection,82 standardnormaldistribution,198
projectionerror,88 standardization,336
projectionmatrix,82 statisticalindependence,194
pseudo-inverse,86 statisticallearningtheory,265
stochasticgradientdescent,231
randomvariable,172,175
strongduality,236
range,58
sufficientstatistics,210
rank,47
sumrule,184
rankdeficient,47
supportpoint,61
rank-kapproximation,130
supportvector,384
rank-nullitytheorem,60
supportinghyperplane,242
raw-scoreformulaforvariance,193
surjective,48
recognitionnetwork,344
SVD,119
reconstructionerror,88,327
SVDtheorem,119
reducedhull,388
symmetric,73,76
reducedrow-echelonform,31
symmetricmatrix,25
reducedSVD,129
symmetric,positivedefinite,74
REF,30
symmetric,positivesemidefinite,74
regression,289
systemoflinearequations,20
regular,24
regularization,262,302,382 targetspace,175
Draft(2023-12-19)of“MathematicsforMachineLearning”.Feedback:https://mml-book.com.
Index 411
Taylorpolynomial,142,166
Taylorseries,142
testerror,300
testset,262,284
Tikhonovregularization,265
trace,103
training,12
trainingerror,300
trainingset,260,292
transferfunction,315
transformationmatrix,51
translationvector,63
transpose,25,38
triangleinequality,71,76
truncatedSVD,129
Tuckerdecomposition,136
underfitting,271
undirectedgraphicalmodel,283
uniformdistribution,182
univariate,178
unscentedtransform,170
upper-triangularmatrix,101
validationset,263,284
variableselection,316
variance,190
vector,37
vectoraddition,37
vectorspace,37
vectorspacehomomorphism,48
vectorspacewithinnerproduct,73
vectorsubspace,39
weakduality,235
zero-oneloss,381
©2024M.P.Deisenroth,A.A.Faisal,C.S.Ong.PublishedbyCambridgeUniversityPress(2020).
