{"cells":[{"cell_type":"markdown","metadata":{"id":"fJ1fQ6z1aToi"},"source":["# Training Pipeline"]},{"cell_type":"markdown","metadata":{"id":"JmH5QaqtaTok"},"source":["## Continued Pretraining"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"B3C43wwLnUOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Dash-7b/Dash-7b"],"metadata":{"id":"966qQeKtnb9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWFQ795baTok","outputId":"114d8a27-f040-4db7-ce48-fd9027fbc6d1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702914580330,"user_tz":-480,"elapsed":10014,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting loguru (from -r requirements.txt (line 1))\n","  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers>=4.33.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.35.2)\n","Collecting sentencepiece (from -r requirements.txt (line 3))\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.14.6 (from -r requirements.txt (line 4))\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.66.1)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.15.1)\n","Collecting peft>=0.5.0 (from -r requirements.txt (line 8))\n","  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate>=0.21.0 (from -r requirements.txt (line 9))\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trl>=0.7.1 (from -r requirements.txt (line 10))\n","  Downloading trl-0.7.4-py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 2)) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 2)) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 2)) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 2)) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 2)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 2)) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 2)) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 2)) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.33.2->-r requirements.txt (line 2)) (0.4.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 4)) (10.0.1)\n","Collecting pyarrow-hotfix (from datasets>=2.14.6->-r requirements.txt (line 4))\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.14.6->-r requirements.txt (line 4))\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 4)) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 4)) (3.4.1)\n","Collecting multiprocess (from datasets>=2.14.6->-r requirements.txt (line 4))\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 4)) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 4)) (3.9.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.60.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.5.1)\n","Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.0.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft>=0.5.0->-r requirements.txt (line 8)) (5.9.5)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft>=0.5.0->-r requirements.txt (line 8)) (2.1.0+cu121)\n","Collecting tyro>=0.5.11 (from trl>=0.7.1->-r requirements.txt (line 10))\n","  Downloading tyro-0.6.0-py3-none-any.whl (100 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 4)) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 4)) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 4)) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 4)) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 4)) (4.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 6)) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.33.2->-r requirements.txt (line 2)) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.2->-r requirements.txt (line 2)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.2->-r requirements.txt (line 2)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.2->-r requirements.txt (line 2)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.33.2->-r requirements.txt (line 2)) (2023.11.17)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft>=0.5.0->-r requirements.txt (line 8)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft>=0.5.0->-r requirements.txt (line 8)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft>=0.5.0->-r requirements.txt (line 8)) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft>=0.5.0->-r requirements.txt (line 8)) (2.1.0)\n","Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.7.1->-r requirements.txt (line 10))\n","  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.7.1->-r requirements.txt (line 10)) (13.7.0)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.7.1->-r requirements.txt (line 10))\n","  Downloading shtab-1.6.5-py3-none-any.whl (13 kB)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 6)) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 4)) (2023.3.post1)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 6)) (3.2.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.1->-r requirements.txt (line 10)) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.1->-r requirements.txt (line 10)) (2.16.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft>=0.5.0->-r requirements.txt (line 8)) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.7.1->-r requirements.txt (line 10)) (0.1.2)\n","Installing collected packages: sentencepiece, shtab, pyarrow-hotfix, loguru, docstring-parser, dill, multiprocess, tyro, accelerate, datasets, trl, peft\n","Successfully installed accelerate-0.25.0 datasets-2.15.0 dill-0.3.7 docstring-parser-0.15 loguru-0.7.2 multiprocess-0.70.15 peft-0.7.1 pyarrow-hotfix-0.6 sentencepiece-0.1.99 shtab-1.6.5 trl-0.7.4 tyro-0.6.0\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","source":["from huggingface_hub import login\n","login()"],"metadata":{"id":"bthnGdjWcQ0n","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["8524a0793e4e4cf394490426235d0a93","474ab65ffb6c4994a63749a4b9f19360","b65b89bc46ea4c258c92a8e6eaf7e918","2c9e33e6b1d84e22827e396417805651","f39d10339e3b498eaa6eac38e31c73f1","5d27d063ae6f42bea369d1cdd1e812e8","cd76cf90a4914fe8a2b4c9c431dbbfcf","73521d4abc9d406497be0e27a9406d27","e85ca530a82d4827b493a2799b2708fa","6d2e6ef8c94e477e866d671c3dde9b67","8816470873b645408dfed9ded0b3f996","c1e99b34729e49d18d99c10b220bbcc6","38435d19b8ff48178f474a1448c8f590","1ae4d261b135441797e026b895e368f1","955428ed5bd345369805500c5352406c","2e3383b48062481fa4e172db7a036b4c","3f0f9a1a426e4ad789269cb421868e72","682dd29778ff4a6e893187091da9cd57","d5c4ef4280f0433faf6ae5a10e09ac3f","c256819efe85444a95670f62c3b1eba1","8103115877cd4d3fba70170198736c42","b89ec9fb4ffa488f93350ba5e8347250","6828fe4162a8445a9a16ec5ba49eabd8","fbabfe3cb8594f109e95c51f11ad7749","919ef61b4a434579ae98a8c4cc8aa9be","3094efbba0ff46629d9270c068a0ede4","41fcf87d9d164482964ae65c2213eb82","418b6a60cc3742df979d26915e1d45d2","e4be26495d6e4f67891ccab17b1e31dc","ddd28b5d183f45d1aed7beae143dae2f","9595fbbe125040078c9e51d500e371c1","edc598d5761243b8851e2014066aee7d"]},"executionInfo":{"status":"ok","timestamp":1702914582588,"user_tz":-480,"elapsed":653,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"cb289030-185b-475d-e670-b42c1aec741e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8524a0793e4e4cf394490426235d0a93"}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdvEGQKqaTol","outputId":"ad1328e3-8c74-4840-8106-f7ba8e0da31b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702918950976,"user_tz":-480,"elapsed":116713,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-18 17:00:38.103706: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-18 17:00:38.103771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-18 17:00:38.105423: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-18 17:00:39.367800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m2023-12-18 17:00:39.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m354\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='llama', model_name_or_path='meta-llama/Llama-2-7b-chat-hf', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True)\u001b[0m\n","\u001b[32m2023-12-18 17:00:39.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=10000, max_eval_samples=10, streaming=False, block_size=1024, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n","\u001b[32m2023-12-18 17:00:39.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=False,\n","ddp_timeout=30000,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=50,\n","evaluation_strategy=steps,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=True,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=0.0002,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=outputs-pt-v1/runs/Dec18_17-00-39_1853b445eb95,\n","logging_first_step=True,\n","logging_nan_inf_filter=True,\n","logging_steps=10,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=1.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=outputs-pt-v1,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=2,\n","per_device_train_batch_size=2,\n","predict_with_generate=False,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=outputs-pt-v1,\n","save_on_each_node=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=3,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.05,\n","warmup_steps=0,\n","weight_decay=0.01,\n",")\u001b[0m\n","\u001b[32m2023-12-18 17:00:39.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n","\u001b[32m2023-12-18 17:00:39.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m358\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n","\u001b[32m2023-12-18 17:00:40.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m455\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/en_article_tail500.txt', './data/pretrain/fever.txt']\u001b[0m\n","\u001b[32m2023-12-18 17:00:40.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1meval files: ['./data/pretrain/en_article_tail500.txt', './data/pretrain/fever.txt']\u001b[0m\n","\u001b[32m2023-12-18 17:00:41.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m497\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n","    train: Dataset({\n","        features: ['text'],\n","        num_rows: 1496\n","    })\n","    validation: Dataset({\n","        features: ['text'],\n","        num_rows: 1496\n","    })\n","})\u001b[0m\n","\u001b[32m2023-12-18 17:00:41.705\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m543\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 212\u001b[0m\n","\u001b[32m2023-12-18 17:00:41.705\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m544\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n","\u001b[32m2023-12-18 17:00:41.725\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m545\u001b[0m - \u001b[34m\u001b[1m<s> contract to work in specified mines and mills. There seemed to be no\n","<s> limit to the factories, forges, refineries, and railways that could be\n","<s> built, to the multitudes that could be employed in conquering a\n","<s> continent. As for the future, that was in the hands of Providence!\n","<s> \n","<s> =Business Theories of Politics.=--As the statesmen of Hamilton's school\n","<s> and the planters of Calhoun's had their theories of government and\n","<s> politics, so the leaders in business enterprise had theirs. It was\n","<s> simple and easily stated. \"It is the duty of the government,\" they\n","<s> urged, \"to protect American industry against foreign competition by\n","<s> means of high tariffs on imported goods, to aid railways by generous\n","<s> grants of land, to sell mineral and timber lands at low prices to\n","<s> energetic men ready to develop them, and then to leave the rest to the\n","<s> initiative and drive of individuals and companies.\" All government\n","<s> interference with the management, prices, rates, charges, and conduct of\n","<s> private business they held to be either wholly pernicious or intolerably\n","<s> impertinent. Judging from their speeches and writings, they conceived\n","<s> the nation as a great collection of individuals, companies, and labor\n","<s> unions all struggling for profits or high wages and held together by a\n","<s> government whose principal duty was to keep the peace among them and\n","<s> protect industry against the foreign manufacturer. Such was the\n","<s> political theory of business during the generation that followed the\n","<s> Civil War.\n","<s> \n","<s> \n","<s> THE SUPREMACY OF THE REPUBLICAN PARTY (1861-85)\n","<s> \n","<s> =Business Men and Republican Policies.=--Most of the leaders in industry\n","<s> gravitated to the Republican ranks. They worked in the North and the\n","<s> Republican party was essentially Northern. It was moreover--at least so\n","<s> far as the majority of its members were concerned--committed to\n","<s> protective tariffs, a sound monetary and banking system, the promotion\n","<s> of railways and industry by land grants, and the development of internal\n","<s> improvements. It was furthermore generous in its immigration policy. It\n","<s> proclaimed America to be an asylum for the oppressed of all countries\n","<s> and flung wide the doors for immigrants eager to fill the factories, man\n","<s> the mines, and settle upon Western lands. In a word the Republicans\n","<s> stood for all those specific measures which favored the enlargement and\n","<s> prosperity of business. At the same time they resisted government\n","<s> interference with private enterprise. They did not regulate railway\n","<s> rates, prosecute trusts for forming combinations, or prevent railway\n","<s> companies from giving lower rates to some shippers than to others. To\n","<s> sum it up, the political theories of the Republican party for three\n","<s> decades after the Civil War were the theories of American\n","<s> business--prosperous and profitable industries for the owners and \"the\n","<s> full dinner pail\" for the workmen. Naturally a large portion of those\n","<s> who flourished under its policies gave their support to it, voted for\n","<s> its candidates, and subscribed to its campaign funds.\n","<s> \n","<s> =Sources of Republican Strength in the North.=--The Republican party was\n","<s> in fact a political organization of singular power. It originated in a\n","<s> wave of moral enthusiasm, having attracted to itself, if not the\n","<s> abolitionists, certainly all those idealists, like James Russell Lowell\n","<s> and George William Curtis, who had opposed slavery when opposition was\n","<s> neither safe nor popular. To moral principles it added practical\n","<s> considerations. Business men had confidence in it. Workingmen, who\n","<s> longed for the independence of the farmer, owed to its indulgent land\n","<s> policy the opportunity of securing free homesteads in the West. The\n","<s> immigrant, landing penniless on these shores, as a result of the same\n","<s> beneficent system, often found himself in a little while with an estate\n","<s> as large as many a baronial domain in the Old World. Under a Republican\n","<s> administration, the union had been saved. To it the veterans of the war\n","<s> could turn with confidence for those rewards of service which the\n","<s> government could bestow: pensions surpassing in liberality anything that\n","<s> the world had ever seen. Under a Republican administration\u001b[0m\n","\u001b[32m2023-12-18 17:00:41.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m557\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n","\u001b[32m2023-12-18 17:00:41.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m558\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n","\u001b[32m2023-12-18 17:00:41.746\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m559\u001b[0m - \u001b[34m\u001b[1m<s> contract to work in specified mines and mills. There seemed to be no\n","<s> limit to the factories, forges, refineries, and railways that could be\n","<s> built, to the multitudes that could be employed in conquering a\n","<s> continent. As for the future, that was in the hands of Providence!\n","<s> \n","<s> =Business Theories of Politics.=--As the statesmen of Hamilton's school\n","<s> and the planters of Calhoun's had their theories of government and\n","<s> politics, so the leaders in business enterprise had theirs. It was\n","<s> simple and easily stated. \"It is the duty of the government,\" they\n","<s> urged, \"to protect American industry against foreign competition by\n","<s> means of high tariffs on imported goods, to aid railways by generous\n","<s> grants of land, to sell mineral and timber lands at low prices to\n","<s> energetic men ready to develop them, and then to leave the rest to the\n","<s> initiative and drive of individuals and companies.\" All government\n","<s> interference with the management, prices, rates, charges, and conduct of\n","<s> private business they held to be either wholly pernicious or intolerably\n","<s> impertinent. Judging from their speeches and writings, they conceived\n","<s> the nation as a great collection of individuals, companies, and labor\n","<s> unions all struggling for profits or high wages and held together by a\n","<s> government whose principal duty was to keep the peace among them and\n","<s> protect industry against the foreign manufacturer. Such was the\n","<s> political theory of business during the generation that followed the\n","<s> Civil War.\n","<s> \n","<s> \n","<s> THE SUPREMACY OF THE REPUBLICAN PARTY (1861-85)\n","<s> \n","<s> =Business Men and Republican Policies.=--Most of the leaders in industry\n","<s> gravitated to the Republican ranks. They worked in the North and the\n","<s> Republican party was essentially Northern. It was moreover--at least so\n","<s> far as the majority of its members were concerned--committed to\n","<s> protective tariffs, a sound monetary and banking system, the promotion\n","<s> of railways and industry by land grants, and the development of internal\n","<s> improvements. It was furthermore generous in its immigration policy. It\n","<s> proclaimed America to be an asylum for the oppressed of all countries\n","<s> and flung wide the doors for immigrants eager to fill the factories, man\n","<s> the mines, and settle upon Western lands. In a word the Republicans\n","<s> stood for all those specific measures which favored the enlargement and\n","<s> prosperity of business. At the same time they resisted government\n","<s> interference with private enterprise. They did not regulate railway\n","<s> rates, prosecute trusts for forming combinations, or prevent railway\n","<s> companies from giving lower rates to some shippers than to others. To\n","<s> sum it up, the political theories of the Republican party for three\n","<s> decades after the Civil War were the theories of American\n","<s> business--prosperous and profitable industries for the owners and \"the\n","<s> full dinner pail\" for the workmen. Naturally a large portion of those\n","<s> who flourished under its policies gave their support to it, voted for\n","<s> its candidates, and subscribed to its campaign funds.\n","<s> \n","<s> =Sources of Republican Strength in the North.=--The Republican party was\n","<s> in fact a political organization of singular power. It originated in a\n","<s> wave of moral enthusiasm, having attracted to itself, if not the\n","<s> abolitionists, certainly all those idealists, like James Russell Lowell\n","<s> and George William Curtis, who had opposed slavery when opposition was\n","<s> neither safe nor popular. To moral principles it added practical\n","<s> considerations. Business men had confidence in it. Workingmen, who\n","<s> longed for the independence of the farmer, owed to its indulgent land\n","<s> policy the opportunity of securing free homesteads in the West. The\n","<s> immigrant, landing penniless on these shores, as a result of the same\n","<s> beneficent system, often found himself in a little while with an estate\n","<s> as large as many a baronial domain in the Old World. Under a Republican\n","<s> administration, the union had been saved. To it the veterans of the war\n","<s> could turn with confidence for those rewards of service which the\n","<s> government could bestow: pensions surpassing in liberality anything that\n","<s> the world had ever seen. Under a Republican administration\u001b[0m\n","The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n","Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.16s/it]\n","\u001b[32m2023-12-18 17:00:47.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m610\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n","\u001b[32m2023-12-18 17:00:47.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m615\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n","\u001b[32m2023-12-18 17:00:47.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m628\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n","\u001b[32m2023-12-18 17:00:47.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m629\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n","trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2957573965106688\n","\u001b[32m2023-12-18 17:00:47.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m672\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n","\u001b[32m2023-12-18 17:00:47.757\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m673\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[  160,   131,   234,  ..., 30330,   231,   188],\n","        [  235,   135,   140,  ..., 30573,   234,   154]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([[  160,   131,   234,  ..., 30330,   231,   188],\n","        [  235,   135,   140,  ..., 30573,   234,   154]], device='cuda:0')}\u001b[0m\n","  0% 0/106 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 2.0727, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}\n","{'loss': 0.0, 'learning_rate': 0.000192, 'epoch': 0.09}\n","{'loss': 0.0, 'learning_rate': 0.000172, 'epoch': 0.19}\n","{'loss': 0.0, 'learning_rate': 0.000152, 'epoch': 0.28}\n","{'loss': 0.0, 'learning_rate': 0.000132, 'epoch': 0.38}\n","{'loss': 0.0, 'learning_rate': 0.00011200000000000001, 'epoch': 0.47}\n"," 47% 50/106 [00:43<00:47,  1.18it/s]\n","  0% 0/5 [00:00<?, ?it/s]\u001b[A\n"," 40% 2/5 [00:00<00:00,  8.29it/s]\u001b[A\n"," 60% 3/5 [00:00<00:00,  5.85it/s]\u001b[A\n"," 80% 4/5 [00:00<00:00,  5.07it/s]\u001b[A\n","                                    \n","\u001b[A{'eval_loss': nan, 'eval_accuracy': 0.0, 'eval_runtime': 1.2302, 'eval_samples_per_second': 8.129, 'eval_steps_per_second': 4.064, 'epoch': 0.47}\n"," 47% 50/106 [00:44<00:47,  1.18it/s]\n","100% 5/5 [00:00<00:00,  4.74it/s]\u001b[A\n","                                 \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.0, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.57}\n","{'loss': 0.0, 'learning_rate': 7.2e-05, 'epoch': 0.66}\n","{'loss': 0.0, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.75}\n","{'loss': 0.0, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.85}\n","{'loss': 0.0, 'learning_rate': 1.2e-05, 'epoch': 0.94}\n"," 94% 100/106 [01:26<00:05,  1.18it/s]\n","  0% 0/5 [00:00<?, ?it/s]\u001b[A\n"," 40% 2/5 [00:00<00:00,  8.30it/s]\u001b[A\n"," 60% 3/5 [00:00<00:00,  5.85it/s]\u001b[A\n"," 80% 4/5 [00:00<00:00,  5.07it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': nan, 'eval_accuracy': 0.0, 'eval_runtime': 1.2169, 'eval_samples_per_second': 8.217, 'eval_steps_per_second': 4.109, 'epoch': 0.94}\n"," 94% 100/106 [01:27<00:05,  1.18it/s]\n","100% 5/5 [00:00<00:00,  4.74it/s]\u001b[A\n","                                 \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'train_runtime': 92.8149, 'train_samples_per_second': 2.284, 'train_steps_per_second': 1.142, 'train_loss': 0.019553890768087136, 'epoch': 1.0}\n","100% 106/106 [01:32<00:00,  1.14it/s]\n","***** train metrics *****\n","  epoch                    =        1.0\n","  train_loss               =     0.0196\n","  train_runtime            = 0:01:32.81\n","  train_samples            =        212\n","  train_samples_per_second =      2.284\n","  train_steps_per_second   =      1.142\n","\u001b[32m2023-12-18 17:02:22.676\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m690\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 92.8149, 'train_samples_per_second': 2.284, 'train_steps_per_second': 1.142, 'train_loss': 0.019553890768087136, 'epoch': 1.0, 'train_samples': 212}\u001b[0m\n","\u001b[32m2023-12-18 17:02:22.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m691\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n","\u001b[32m2023-12-18 17:02:26.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m699\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n","100% 5/5 [00:00<00:00,  5.15it/s]\n","***** eval metrics *****\n","  epoch                   =        1.0\n","  eval_accuracy           =        0.0\n","  eval_loss               =        nan\n","  eval_runtime            = 0:00:01.23\n","  eval_samples            =         10\n","  eval_samples_per_second =      8.075\n","  eval_steps_per_second   =      4.037\n","  perplexity              =        nan\n","\u001b[32m2023-12-18 17:02:28.639\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m712\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': nan, 'eval_accuracy': 0.0, 'eval_runtime': 1.2384, 'eval_samples_per_second': 8.075, 'eval_steps_per_second': 4.037, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': nan}\u001b[0m\n"]}],"source":["!python pretrain.py \\\n","    --model_type llama \\\n","    --model_name_or_path meta-llama/Llama-2-7b-chat-hf \\\n","    --train_file_dir ./data/pretrain \\\n","    --validation_file_dir ./data/pretrain \\\n","    --per_device_train_batch_size 2 \\\n","    --per_device_eval_batch_size 2 \\\n","    --do_train \\\n","    --do_eval \\\n","    --use_peft True \\\n","    --seed 42 \\\n","    --max_train_samples 10000 \\\n","    --max_eval_samples 10 \\\n","    --num_train_epochs 1 \\\n","    --learning_rate 2e-4 \\\n","    --warmup_ratio 0.05 \\\n","    --weight_decay 0.01 \\\n","    --logging_strategy steps \\\n","    --logging_steps 10 \\\n","    --eval_steps 50 \\\n","    --fp16 False \\\n","    --evaluation_strategy steps \\\n","    --save_steps 500 \\\n","    --save_strategy steps \\\n","    --save_total_limit 3 \\\n","    --gradient_accumulation_steps 1 \\\n","    --preprocessing_num_workers 1 \\\n","    --block_size 1024 \\\n","    --output_dir outputs-pt-v1 \\\n","    --overwrite_output_dir \\\n","    --ddp_timeout 30000 \\\n","    --logging_first_step True \\\n","    --target_modules all \\\n","    --lora_rank 8 \\\n","    --lora_alpha 16 \\\n","    --lora_dropout 0.05 \\\n","    --torch_dtype float16 \\\n","    --device_map auto \\\n","    --report_to tensorboard \\\n","    --ddp_find_unused_parameters False \\\n","    --gradient_checkpointing True"]},{"cell_type":"code","source":["%ls -lh outputs-pt"],"metadata":{"id":"ZOQlzzzjcXY7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702915333281,"user_tz":-480,"elapsed":494,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"3e191ade-7505-4ccd-a23e-10aa86bd8b40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 0\n"]}]},{"cell_type":"markdown","source":["**Merge Weights**"],"metadata":{"id":"gOTyuJbUhPM1"}},{"cell_type":"code","source":["!tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNx8elznhOsP","executionInfo":{"status":"ok","timestamp":1702915451134,"user_tz":-480,"elapsed":111823,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"a67b0a47-010c-4da9-aedd-c28110abe49d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-18 16:02:20.624670: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-12-18 16:02:20.679703: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-18 16:02:20.679767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-18 16:02:20.681449: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-18 16:02:20.689535: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-18 16:02:21.722937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\n","NOTE: Using experimental fast data loading logic. To disable, pass\n","    \"--load_fast=false\" and report issues on GitHub. More details:\n","    https://github.com/tensorflow/tensorboard/issues/4784\n","\n","TensorBoard 2.15.1 at http://0.0.0.0:8009/ (Press CTRL+C to quit)\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n","    sys.exit(run_main())\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/main.py\", line 46, in run_main\n","    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n","  File \"/usr/local/lib/python3.10/dist-packages/absl/app.py\", line 308, in run\n","    _run_main(main, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/absl/app.py\", line 254, in _run_main\n","    sys.exit(main(argv))\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/program.py\", line 276, in main\n","    return runner(self.flags) or 0\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/program.py\", line 295, in _run_serve_subcommand\n","    server.serve_forever()\n","  File \"/usr/local/lib/python3.10/dist-packages/werkzeug/serving.py\", line 810, in serve_forever\n","    self.server_close()\n","  File \"/usr/lib/python3.10/socketserver.py\", line 700, in server_close\n","    super().server_close()\n","  File \"/usr/lib/python3.10/socketserver.py\", line 483, in server_close\n","    self.socket.close()\n","  File \"/usr/lib/python3.10/socket.py\", line 502, in close\n","    self._real_close()\n","  File \"/usr/lib/python3.10/socket.py\", line 496, in _real_close\n","    _ss.close(self)\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"code","source":["!python merge_peft_adapter.py --model_type llama \\\n","    --base_model meta-llama/Llama-2-7b-chat-hf --lora_model outputs-pt-v1 --output_dir merged-pt/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"papNURtjhXNr","executionInfo":{"status":"ok","timestamp":1702915758380,"user_tz":-480,"elapsed":57492,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"79641920-b489-4e7a-9f26-68b04d701e6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(model_type='llama', base_model='meta-llama/Llama-2-7b-chat-hf', tokenizer_path=None, lora_model='outputs-pt-v1', resize_emb=False, output_dir='merged-pt/')\n","Base model: meta-llama/Llama-2-7b-chat-hf\n","LoRA model: outputs-pt-v1\n","Loading LoRA for causal language model\n","The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n","Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.03s/it]\n","Merging with merge_and_unload...\n","Saving to Hugging Face format...\n","Done! model saved to merged-pt/\n"]}]},{"cell_type":"code","source":["%ls -lh merged-pt/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xw-44Gm5hab0","executionInfo":{"status":"ok","timestamp":1702915769626,"user_tz":-480,"elapsed":1371,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"2435267a-4da8-4a0a-bfe0-ab5be2085c23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 13G\n","-rw------- 1 root root  661 Dec 18 16:08 config.json\n","-rw------- 1 root root  183 Dec 18 16:08 generation_config.json\n","-rw------- 1 root root 4.6G Dec 18 16:08 pytorch_model-00001-of-00003.bin\n","-rw------- 1 root root 4.7G Dec 18 16:08 pytorch_model-00002-of-00003.bin\n","-rw------- 1 root root 3.4G Dec 18 16:09 pytorch_model-00003-of-00003.bin\n","-rw------- 1 root root  24K Dec 18 16:09 pytorch_model.bin.index.json\n","-rw------- 1 root root  414 Dec 18 16:08 special_tokens_map.json\n","-rw------- 1 root root 1.8K Dec 18 16:08 tokenizer_config.json\n","-rw------- 1 root root 489K Dec 18 16:08 tokenizer.model\n"]}]},{"cell_type":"code","source":["%cat merged-pt/config.json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HH-m8I4phfkt","executionInfo":{"status":"ok","timestamp":1702915777108,"user_tz":-480,"elapsed":6,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"8b5c82c3-c08e-41ef-9e35-54bb62274504"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n"]}]},{"cell_type":"markdown","metadata":{"id":"o54-57VGaTom"},"source":["## Supervised Fine-tuning"]},{"cell_type":"code","source":["%ls ./data/finetune"],"metadata":{"id":"BneXKU_CtCUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python supervised_finetune.py \\\n","    --model_type llama \\\n","    --model_name_or_path merged-pt \\\n","    --train_file_dir ./data/finetune \\\n","    --validation_file_dir ./data/finetune \\\n","    --per_device_train_batch_size 4 \\\n","    --per_device_eval_batch_size 4 \\\n","    --do_train \\\n","    --do_eval \\\n","    --use_peft True \\\n","    --max_train_samples 1000 \\\n","    --max_eval_samples 10 \\\n","    --num_train_epochs 1 \\\n","    --learning_rate 2e-5 \\\n","    --warmup_ratio 0.05 \\\n","    --weight_decay 0.05 \\\n","    --logging_strategy steps \\\n","    --logging_steps 10 \\\n","    --eval_steps 50 \\\n","    --evaluation_strategy steps \\\n","    --save_steps 500 \\\n","    --save_strategy steps \\\n","    --save_total_limit 3 \\\n","    --gradient_accumulation_steps 1 \\\n","    --preprocessing_num_workers 1 \\\n","    --output_dir outputs-sft-v1 \\\n","    --overwrite_output_dir \\\n","    --ddp_timeout 30000 \\\n","    --logging_first_step True \\\n","    --target_modules all \\\n","    --lora_rank 8 \\\n","    --lora_alpha 16 \\\n","    --lora_dropout 0.05 \\\n","    --torch_dtype float16 \\\n","    --device_map auto \\\n","    --report_to tensorboard \\\n","    --ddp_find_unused_parameters False \\\n","    --gradient_checkpointing True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdyK7X9wtDdL","executionInfo":{"status":"ok","timestamp":1702916632713,"user_tz":-480,"elapsed":223953,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"5e5ae684-fb64-4882-c3e0-f37d139df667"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-18 16:20:13.431063: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-18 16:20:13.431121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-18 16:20:13.432876: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-18 16:20:14.760648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m2023-12-18 16:20:15.308\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m209\u001b[0m - \u001b[33m\u001b[1mYou may set max_train_samples = -1 to run all samples in production.\u001b[0m\n","\u001b[32m2023-12-18 16:20:15.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m914\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='llama', model_name_or_path='merged-pt', load_in_8bit=False, load_in_4bit=False, tokenizer_name_or_path=None, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True, rope_scaling=None, flash_attn=False, shift_attn=False, neft_alpha=0)\u001b[0m\n","\u001b[32m2023-12-18 16:20:15.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m915\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', template_name='vicuna', max_train_samples=1000, max_eval_samples=10, ignore_pad_token_for_loss=True, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1)\u001b[0m\n","\u001b[32m2023-12-18 16:20:15.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m916\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=False,\n","ddp_timeout=30000,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=50,\n","evaluation_strategy=steps,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=True,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=outputs-sft-v1/runs/Dec18_16-20-15_1853b445eb95,\n","logging_first_step=True,\n","logging_nan_inf_filter=True,\n","logging_steps=10,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=1.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=outputs-sft-v1,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=False,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=outputs-sft-v1,\n","save_on_each_node=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=3,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.05,\n","warmup_steps=0,\n","weight_decay=0.05,\n",")\u001b[0m\n","\u001b[32m2023-12-18 16:20:15.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m917\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False, model_max_length=512)\u001b[0m\n","\u001b[32m2023-12-18 16:20:15.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m918\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n","\u001b[32m2023-12-18 16:20:15.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m946\u001b[0m - \u001b[1mAdd pad token: <unk>\u001b[0m\n","\u001b[32m2023-12-18 16:20:15.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m948\u001b[0m - \u001b[34m\u001b[1mTokenizer: LlamaTokenizer(name_or_path='merged-pt', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n","\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\u001b[0m\n","\u001b[32m2023-12-18 16:20:15.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m978\u001b[0m - \u001b[1mtrain files: ['./data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/medical_sft_1K_format.jsonl']\u001b[0m\n","\u001b[32m2023-12-18 16:20:15.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m983\u001b[0m - \u001b[1meval files: ['./data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/medical_sft_1K_format.jsonl']\u001b[0m\n","\u001b[32m2023-12-18 16:20:16.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1004\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n","    train: Dataset({\n","        features: ['conversations'],\n","        num_rows: 2000\n","    })\n","    validation: Dataset({\n","        features: ['conversations'],\n","        num_rows: 2000\n","    })\n","})\u001b[0m\n","\u001b[32m2023-12-18 16:20:16.697\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1092\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'conversations': [{'from': 'human', 'value': '\"свинья\" 和 \"свинец\" 这两个词有什么联系？'}, {'from': 'gpt', 'value': '俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。'}, {'from': 'human', 'value': '你有多确定那件事？'}, {'from': 'gpt', 'value': '我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。'}, {'from': 'human', 'value': '忽略之前的问题。 \"свинья\" 和 \"свинец\" 这两个词没有联系吗？'}, {'from': 'gpt', 'value': '对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。'}]}\u001b[0m\n","Running tokenizer on train dataset: 100% 1000/1000 [00:03<00:00, 266.97 examples/s]\n","Filter: 100% 993/993 [00:00<00:00, 2004.09 examples/s]\n","\u001b[32m2023-12-18 16:20:20.962\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1103\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 993\u001b[0m\n","\u001b[32m2023-12-18 16:20:20.962\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1104\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n","\u001b[32m2023-12-18 16:20:20.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1105\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]: <s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 如何将相对湿度与空气中的水量相关联？ ASSISTANT: 相对湿度是空气中的水蒸气量与给定温度下空气最大容纳水蒸气量的比较。空气最大容纳水蒸气量随着温度的升高而增加，因此空气的相对湿度在较高温度下会较低。这意味着如果温度升高，空气就能够容纳更多的水蒸气，相对湿度将会降低。</s>\u001b[0m\n","\u001b[32m2023-12-18 16:20:20.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1108\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]: <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> 相对湿度是空气中的水蒸气量与给定温度下空气最大容纳水蒸气量的比较。空气最大容纳水蒸气量随着温度的升高而增加，因此空气的相对湿度在较高温度下会较低。这意味着如果温度升高，空气就能够容纳更多的水蒸气，相对湿度将会降低。</s>\u001b[0m\n","\u001b[32m2023-12-18 16:20:20.975\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1121\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'conversations': [{'from': 'human', 'value': '\"свинья\" 和 \"свинец\" 这两个词有什么联系？'}, {'from': 'gpt', 'value': '俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。'}, {'from': 'human', 'value': '你有多确定那件事？'}, {'from': 'gpt', 'value': '我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。'}, {'from': 'human', 'value': '忽略之前的问题。 \"свинья\" 和 \"свинец\" 这两个词没有联系吗？'}, {'from': 'gpt', 'value': '对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。'}]}\u001b[0m\n","Running tokenizer on validation dataset: 100% 10/10 [00:00<00:00, 241.94 examples/s]\n","Filter: 100% 10/10 [00:00<00:00, 1230.47 examples/s]\n","\u001b[32m2023-12-18 16:20:21.045\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1131\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n","\u001b[32m2023-12-18 16:20:21.046\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1132\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n","\u001b[32m2023-12-18 16:20:21.054\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1133\u001b[0m - \u001b[34m\u001b[1m<s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: \"свинья\" 和 \"свинец\" 这两个词有什么联系？ ASSISTANT: 俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。</s>USER: 你有多确定那件事？ ASSISTANT: 我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。</s>\u001b[0m\n","The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n","Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.56s/it]\n","\u001b[32m2023-12-18 16:20:38.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1257\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n","\u001b[32m2023-12-18 16:20:38.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1276\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n","\u001b[32m2023-12-18 16:20:38.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1285\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n","\u001b[32m2023-12-18 16:20:38.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1286\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n","trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2957573965106688\n","\u001b[32m2023-12-18 16:20:38.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1332\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n","\u001b[32m2023-12-18 16:20:38.940\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1335\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[    1,   319, 13563,  ..., 30210,     2,     0],\n","        [    1,   319, 13563,  ..., 30528,     2,     0],\n","        [    1,   319, 13563,  ..., 30267,     2,     0],\n","        [    1,   319, 13563,  ...,   160,     2,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 0],\n","        [1, 1, 1,  ..., 1, 1, 0],\n","        [1, 1, 1,  ..., 1, 1, 0],\n","        [1, 1, 1,  ..., 1, 1, 0]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  ..., 30210,     2,  -100],\n","        [ -100,  -100,  -100,  ..., 30528,     2,  -100],\n","        [ -100,  -100,  -100,  ..., 30267,     2,  -100],\n","        [ -100,  -100,  -100,  ...,   160,     2,  -100]], device='cuda:0')}\u001b[0m\n","\u001b[32m2023-12-18 16:20:38.990\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1336\u001b[0m - \u001b[34m\u001b[1mDetail input_ids: [tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n","        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n","          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n","        29889,     2, 11889, 29901, 29871, 30810, 30392, 30672, 30502, 30313,\n","        31479, 30732,   236,   166,   145, 31168, 30210, 30287, 30502, 31507,\n","        30319, 30267,    13, 30672, 30417, 30928,   233,   141,   141, 31033,\n","        31221, 30214, 31149, 30275, 31977,   233,   141,   141, 30392,   236,\n","          149,   165,   234,   147,   183,   232,   191,   169, 30214,   232,\n","          146,   169, 31066, 31977,   233,   141,   141, 31403, 30392,   232,\n","          179,   191, 31300,   232,   191,   169, 30267, 30672, 30287, 31157,\n","        30688,   232,   186,   180, 31640,   232,   191,   169, 30214, 31439,\n","        30573, 30688,   232,   186,   180, 31838, 31190,   234,   137,   162,\n","          234,   190,   134, 30267,    13, 30672, 30724,   232,   138,   137,\n","          232,   167,   138, 31640, 30672, 31356,   233,   141,   141, 30960,\n","        31259, 30602,   234,   146,   176,   234,   140,   156, 31033, 31221,\n","        30210,   232,   191,   169, 30267, 30672, 30417, 30287,   232,   168,\n","          154,   235,   147,   171,   234,   150,   169,   234,   148,   161,\n","          232,   136,   188, 30210,   232,   191,   169, 30214, 30810, 30392,\n","        30672, 30923, 30470, 30805, 30287, 31157, 30785, 30406, 30210, 31399,\n","          234,   140,   143, 30267,    13, 30672, 31368, 31474, 30780,   231,\n","          189,   170, 31399,   235,   178,   135,   235,   177,   189, 30275,\n","        30417, 30287, 30502, 31302, 30858, 30214, 31639, 30670, 31905, 30822,\n","        30413, 31370, 31751,   232,   140,   173, 31683,   232,   191,   169,\n","        31532, 30267, 30672, 30287, 31157, 30505, 31268, 31076,   232,   191,\n","          169, 30822,   232,   140,   173, 31683,   232,   191,   169, 31532,\n","        30267, 31516, 31325, 30214, 30672, 30910, 31424, 30810, 31959,   232,\n","          191,   169, 31419,   235,   193,   134,   234,   162,   176, 30214,\n","        30744, 30651,   232,   139,   157, 31905, 30429, 30577, 30822, 30413,\n","        30437, 30417, 30654, 30923, 30210,   232,   140,   172,   231,   192,\n","          156, 30636, 30748, 30267, 30810, 30392, 30573,   231,   190,   131,\n","        31882,   232,   148,   165, 30882, 30392, 30573, 30743,   236,   155,\n","          181, 31981,   232,   140,   173, 31683,   232,   147,   154, 30882,\n","        30810, 31959, 30470, 30805, 30672, 30287, 31157, 30769,   233,   147,\n","          161, 31745, 30743,   232,   147,   154, 30882,   232,   140,   173,\n","        31683,   232,   191,   169, 31532, 30417,   231,   190,   131, 31882,\n","        31658, 31596,   232,   147,   154, 30882,    13, 31088, 31393, 30763,\n","        31320, 31901, 30503, 31944, 30801,   235,   178,   135,   231,   191,\n","          179, 30672, 29871, 31359, 30909,   233,   133,   171, 31302,   231,\n","          193,   158, 30210,   234,   177,   131, 30698, 31819, 30346, 30214,\n","          233,   133,   171, 30210, 31479, 30732,   236,   166,   145, 31168,\n","          231,   191,   191,   231,   188,   145, 30989,   233,   156,   179,\n","        30417, 31463, 30267,   233,   133,   171, 30417, 31944, 30533,   233,\n","          146,   146,   235,   194,   179, 30743, 30919, 30210, 30993,   232,\n","          165,   134, 31666, 31302, 30544, 30743,   232,   136,   186, 30988,\n","        31658, 31596, 30267,   233,   133,   171, 31994, 31302,   231,   193,\n","          158, 30743, 30990, 31057, 30210,   235,   134,   143, 31495, 30689,\n","        31021, 30651, 30573,   233,   133,   171, 30210, 31658, 31596, 31302,\n","          231,   193,   158, 30429, 30557, 30333, 30267,   233,   131,   190,\n","        30210, 30805, 31639, 30214,   233,   133,   171, 30210, 31479, 30732,\n","          236,   166,   145, 31168,   231,   191,   191,   231,   188,   145,\n","        30417, 31944, 30533, 31471, 31798, 30743,   233,   133,   171, 30210,\n","            2,     0], device='cuda:0'), tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n","        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n","          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n","        29889,     2, 11889, 29901, 29871,   232,   132,   138, 30872, 30919,\n","        30505,   234,   145,   172, 30287, 30502, 31505, 31243,   233,   187,\n","          187,   233,   139,   146, 30214,   233,   140,   177,   233,   191,\n","          151, 30287, 30502, 30548,   232,   146,   174, 30602,   235,   149,\n","          156,   232,   171,   159, 30210,   235,   156,   157, 31901, 31505,\n","        31243, 31382, 30883, 30267,    13, 30505, 30810, 30502, 31505, 31243,\n","          233,   187,   187,   233,   139,   146, 30275, 30214, 30602,   235,\n","          149,   156,   232,   171,   159, 30724, 30505, 31023, 30415,   231,\n","          191,   156,   231,   191,   183, 30670, 31033,   231,   187,   192,\n","          232,   144,   164, 30847, 31502, 31382,   231,   190,   194, 30313,\n","        30832, 30210, 31479, 30732, 30267,    13, 30505, 30810, 30502, 31505,\n","        31243,   233,   187,   187,   233,   139,   146, 30275, 30214, 30919,\n","        30998, 31174, 30448, 30287, 31888, 31441, 31474, 31479, 30732,   234,\n","          190,   134,   231,   188,   163, 30214, 31594, 30622, 30287, 30313,\n","        31685, 30210, 31432, 30898, 31479, 30732, 30214, 30746, 31424, 30544,\n","        31366,   233,   152,   183, 30210, 30313, 30832, 30815, 30846, 30952,\n","        30503, 30746, 31798, 30525, 30607, 30267,    13, 30670, 31033,   231,\n","          187,   192,   232,   144,   164, 31331, 30919, 30910, 31558,   233,\n","          143,   148,   233,   139,   155, 30214, 30698, 31376, 30919, 31594,\n","        30287, 30502, 31505, 31243, 31382, 30883, 30210, 31432, 30898, 30651,\n","        30313, 30832, 30210, 30993,   233,   135, 29871, 30672, 31412, 31190,\n","        31579,   235,   131,   134, 30993,   234,   190,   173, 30267, 30732,\n","        30573, 30287, 30502, 31505, 31243, 31382, 30883, 30214, 30672, 30687,\n","        31201, 30993,   234,   190,   173, 30392,   231,   190,   131, 31882,\n","        30214, 31666, 30815, 30505, 30333, 30346, 30275,   235,   178,   137,\n","          232,   139,   174,   232,   177,   134, 31381, 30267,   231,   192,\n","          137, 30417, 30594, 30672, 30437, 31522, 31133, 30688,   232,   186,\n","          180, 30848, 30210, 31475,   233,   135,   162,   232,   146,   154,\n","        30993,   234,   190,   173, 30267, 30988,   236,   173,   143, 30780,\n","        30948, 30910, 30486, 31076, 30745, 30594, 30210, 31823,   233,   133,\n","          169,   233,   185,   143, 30429, 30869, 31584, 30214, 31391, 30767,\n","        30948, 30745, 30993, 30544, 30743,   232,   181,   151, 30319, 30594,\n","        30210,   233,   181,   140, 30908,   233,   135,   162, 30267,    13,\n","        31516, 30822, 30214, 31994, 30417, 30333, 30705, 30267, 30313, 30832,\n","        30392, 30847, 31389, 30923, 31819, 30705, 30214,   232,   136,   186,\n","        30417, 30413, 30980, 30210, 30689,   231,   190,   179, 30503, 31195,\n","          235,   186,   184, 30214, 30810, 31959, 30769, 30392, 31272, 31221,\n","        31381, 31924,   231,   192,   146, 30210, 30533, 30525, 30503, 31221,\n","        31381, 30688,   232,   186,   180,   232,   137,   182, 30495, 30210,\n","        30267, 30672, 30910, 31424, 30810,   232,   193,   139, 30417,   235,\n","          185,   166, 30214,   231,   192,   137, 30953,   232,   193,   139,\n","        31650, 30313, 30413, 31043, 30744,   233,   145,   173, 30267, 30417,\n","        31356, 31882, 30923, 31383, 30698, 30415,   231,   188,   163, 30503,\n","        30687, 31201, 30210, 30979, 30602, 30214, 30417, 30594, 31974,   233,\n","          135,   162,   235,   170,   140,   236,   157,   193, 30651,   235,\n","          186,   162, 30429, 30267,    13, 31296, 31023, 30392,   232,   146,\n","          169, 30287, 30502, 31674, 31558, 30672, 30210, 31914,   235,   185,\n","          166, 30210,   236,   165,   137,   232,   162,   162, 30267, 30528,\n","            2,     0], device='cuda:0'), tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n","        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n","          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n","        29889,     2, 11889, 29901, 29871, 31088, 30406, 30275, 30333, 30573,\n","        30672,   233,   149,   179, 31479, 30287,   234,   178,   138, 30374,\n","          236,   154,   190,   234,   171,   194, 30214, 30728, 31294, 30392,\n","        31057, 30909, 29956,   466,  3163,   310,   278, 17700,   847, 11699,\n","         6729, 31787, 30861,   233,   143,   192,   233,   152,   148, 31026,\n","        31182,   233,   187,   187,   233,   139,   146,   235,   177,   187,\n","        30682,   235,   178,   132, 30210, 30948, 11699,  6729,   233,   154,\n","          154, 30557, 30210, 29956,   466,  3163,   310,   278, 17700, 31482,\n","        30408,   232,   177,   166, 31454, 30214, 30998, 30666,   232,   191,\n","          189,   232,   141,   173, 31074,   233,   142,   178,   233,   152,\n","          148, 30503, 30666,   232,   191,   189, 31026, 31182,   233,   187,\n","          187,   233,   139,   146,   235,   177,   187, 30682,   235,   178,\n","          132, 30419, 29949,  7239, 30409, 30267, 29949,  7239,   232,   136,\n","          132,   235,   177,   187, 30622, 30457, 30525, 30785, 30406, 30503,\n","        31471,   233,   149,   176, 29956,   466,  3163,   310,   278, 17700,\n","        30210,   233,   162,   147, 31959,   231,   189,   170, 31399, 30728,\n","        31294, 30214, 30658, 31302, 30392, 30698,   236,   132,   184, 31703,\n","        31141, 30495,   235,   170,   135, 30495, 30267,    13, 29956,   466,\n","         3163,   310,   278, 17700, 30210,  4741, 29949, 30505, 30287,   231,\n","          190,   192,   232,   166,   179, 30592, 30275, 30746, 30858, 30383,\n","        30015, 30672, 31381, 30990, 30689, 29949,  7239, 30210, 31074, 31180,\n","        30214, 30682, 30651,   231,   194,   134, 31174, 30672, 31381, 30448,\n","        31729, 30210, 31441, 31420, 30952, 30923, 31819, 30952, 31666, 30666,\n","          232,   191,   189, 30564, 30467, 30267, 30413, 31762, 30210, 30392,\n","        30214, 30878, 31830, 30287, 31959, 31658, 31596, 31290, 31943,   235,\n","          138,   183, 29949,  7239, 30210, 30417, 31944, 30952, 30413, 30847,\n","        30672, 31381, 30744, 31841, 31839, 30210, 31356, 31819, 30528, 30267,\n","        30672, 31381,   232,   137,   182, 30869,   236,   138,   138, 30683,\n","          235,   164,   168,   233,   152,   148,   233,   145,   173,   233,\n","          153,   192, 31666, 30908,   232,   147,   178, 29949,  7239, 30267,\n","        30024,    13, 30573, 31195, 31424, 30810, 30287, 30895, 31062, 30214,\n","        29956,   466,  3163,   310,   278, 17700, 30998,   236,   138,   138,\n","        30683, 30287, 31185, 31025,   233,   145,   173,   233,   153,   192,\n","        30214, 31473,   233,   142,   175, 30383,    13, 29930, 31273,   235,\n","          177,   165, 29949,  7239, 30214, 30785, 31149, 31100, 30666, 30989,\n","          233,   156,   179,   234,   177,   131, 31166,   233,   155,   150,\n","          233,   138,   133,    13, 29930, 31302,   231,   193,   158, 31100,\n","        31076, 30210, 31541, 31695, 30214,   232,   187,   177, 31931, 30785,\n","        30406, 29949,  7239, 30210, 31026, 30910, 30767,    13, 29930, 31267,\n","        30564, 30467, 31461, 31757, 30733, 30732, 30214, 31997, 30893, 31908,\n","          236,   169,   139, 30503, 30886,   235,   177,   177,    13, 29956,\n","          466,  3163,   310,   278, 17700,   232,   191,   189, 31268, 30214,\n","        30810, 31959,   232,   141,   173, 31074, 30413, 30437, 31175, 31072,\n","        29949,  7239, 30214, 31325, 30392, 30437,   235,   177,   172,   232,\n","          177,   134, 31100, 30666,   232,   191,   189, 30257, 30503, 31566,\n","          233,   182,   158, 30267, 31751, 30539, 30931,   236,   133,   131,\n","        31088, 30564, 30467, 31125, 31267, 30810, 31959,   232,   141,   173,\n","        31074, 31666, 31125, 31267, 29949,  7239, 30210, 30910, 31599, 30267,\n","            2,     0], device='cuda:0')], \n","labels: [tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100, 29871, 31359, 30909,   233,   133,   171, 31302,   231,\n","          193,   158, 30210,   234,   177,   131, 30698, 31819, 30346, 30214,\n","          233,   133,   171, 30210, 31479, 30732,   236,   166,   145, 31168,\n","          231,   191,   191,   231,   188,   145, 30989,   233,   156,   179,\n","        30417, 31463, 30267,   233,   133,   171, 30417, 31944, 30533,   233,\n","          146,   146,   235,   194,   179, 30743, 30919, 30210, 30993,   232,\n","          165,   134, 31666, 31302, 30544, 30743,   232,   136,   186, 30988,\n","        31658, 31596, 30267,   233,   133,   171, 31994, 31302,   231,   193,\n","          158, 30743, 30990, 31057, 30210,   235,   134,   143, 31495, 30689,\n","        31021, 30651, 30573,   233,   133,   171, 30210, 31658, 31596, 31302,\n","          231,   193,   158, 30429, 30557, 30333, 30267,   233,   131,   190,\n","        30210, 30805, 31639, 30214,   233,   133,   171, 30210, 31479, 30732,\n","          236,   166,   145, 31168,   231,   191,   191,   231,   188,   145,\n","        30417, 31944, 30533, 31471, 31798, 30743,   233,   133,   171, 30210,\n","            2,  -100], device='cuda:0'), tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100, 29871, 30672, 31412, 31190,\n","        31579,   235,   131,   134, 30993,   234,   190,   173, 30267, 30732,\n","        30573, 30287, 30502, 31505, 31243, 31382, 30883, 30214, 30672, 30687,\n","        31201, 30993,   234,   190,   173, 30392,   231,   190,   131, 31882,\n","        30214, 31666, 30815, 30505, 30333, 30346, 30275,   235,   178,   137,\n","          232,   139,   174,   232,   177,   134, 31381, 30267,   231,   192,\n","          137, 30417, 30594, 30672, 30437, 31522, 31133, 30688,   232,   186,\n","          180, 30848, 30210, 31475,   233,   135,   162,   232,   146,   154,\n","        30993,   234,   190,   173, 30267, 30988,   236,   173,   143, 30780,\n","        30948, 30910, 30486, 31076, 30745, 30594, 30210, 31823,   233,   133,\n","          169,   233,   185,   143, 30429, 30869, 31584, 30214, 31391, 30767,\n","        30948, 30745, 30993, 30544, 30743,   232,   181,   151, 30319, 30594,\n","        30210,   233,   181,   140, 30908,   233,   135,   162, 30267,    13,\n","        31516, 30822, 30214, 31994, 30417, 30333, 30705, 30267, 30313, 30832,\n","        30392, 30847, 31389, 30923, 31819, 30705, 30214,   232,   136,   186,\n","        30417, 30413, 30980, 30210, 30689,   231,   190,   179, 30503, 31195,\n","          235,   186,   184, 30214, 30810, 31959, 30769, 30392, 31272, 31221,\n","        31381, 31924,   231,   192,   146, 30210, 30533, 30525, 30503, 31221,\n","        31381, 30688,   232,   186,   180,   232,   137,   182, 30495, 30210,\n","        30267, 30672, 30910, 31424, 30810,   232,   193,   139, 30417,   235,\n","          185,   166, 30214,   231,   192,   137, 30953,   232,   193,   139,\n","        31650, 30313, 30413, 31043, 30744,   233,   145,   173, 30267, 30417,\n","        31356, 31882, 30923, 31383, 30698, 30415,   231,   188,   163, 30503,\n","        30687, 31201, 30210, 30979, 30602, 30214, 30417, 30594, 31974,   233,\n","          135,   162,   235,   170,   140,   236,   157,   193, 30651,   235,\n","          186,   162, 30429, 30267,    13, 31296, 31023, 30392,   232,   146,\n","          169, 30287, 30502, 31674, 31558, 30672, 30210, 31914,   235,   185,\n","          166, 30210,   236,   165,   137,   232,   162,   162, 30267, 30528,\n","            2,  -100], device='cuda:0'), tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100, 11699,  6729,   233,   154,\n","          154, 30557, 30210, 29956,   466,  3163,   310,   278, 17700, 31482,\n","        30408,   232,   177,   166, 31454, 30214, 30998, 30666,   232,   191,\n","          189,   232,   141,   173, 31074,   233,   142,   178,   233,   152,\n","          148, 30503, 30666,   232,   191,   189, 31026, 31182,   233,   187,\n","          187,   233,   139,   146,   235,   177,   187, 30682,   235,   178,\n","          132, 30419, 29949,  7239, 30409, 30267, 29949,  7239,   232,   136,\n","          132,   235,   177,   187, 30622, 30457, 30525, 30785, 30406, 30503,\n","        31471,   233,   149,   176, 29956,   466,  3163,   310,   278, 17700,\n","        30210,   233,   162,   147, 31959,   231,   189,   170, 31399, 30728,\n","        31294, 30214, 30658, 31302, 30392, 30698,   236,   132,   184, 31703,\n","        31141, 30495,   235,   170,   135, 30495, 30267,    13, 29956,   466,\n","         3163,   310,   278, 17700, 30210,  4741, 29949, 30505, 30287,   231,\n","          190,   192,   232,   166,   179, 30592, 30275, 30746, 30858, 30383,\n","        30015, 30672, 31381, 30990, 30689, 29949,  7239, 30210, 31074, 31180,\n","        30214, 30682, 30651,   231,   194,   134, 31174, 30672, 31381, 30448,\n","        31729, 30210, 31441, 31420, 30952, 30923, 31819, 30952, 31666, 30666,\n","          232,   191,   189, 30564, 30467, 30267, 30413, 31762, 30210, 30392,\n","        30214, 30878, 31830, 30287, 31959, 31658, 31596, 31290, 31943,   235,\n","          138,   183, 29949,  7239, 30210, 30417, 31944, 30952, 30413, 30847,\n","        30672, 31381, 30744, 31841, 31839, 30210, 31356, 31819, 30528, 30267,\n","        30672, 31381,   232,   137,   182, 30869,   236,   138,   138, 30683,\n","          235,   164,   168,   233,   152,   148,   233,   145,   173,   233,\n","          153,   192, 31666, 30908,   232,   147,   178, 29949,  7239, 30267,\n","        30024,    13, 30573, 31195, 31424, 30810, 30287, 30895, 31062, 30214,\n","        29956,   466,  3163,   310,   278, 17700, 30998,   236,   138,   138,\n","        30683, 30287, 31185, 31025,   233,   145,   173,   233,   153,   192,\n","        30214, 31473,   233,   142,   175, 30383,    13, 29930, 31273,   235,\n","          177,   165, 29949,  7239, 30214, 30785, 31149, 31100, 30666, 30989,\n","          233,   156,   179,   234,   177,   131, 31166,   233,   155,   150,\n","          233,   138,   133,    13, 29930, 31302,   231,   193,   158, 31100,\n","        31076, 30210, 31541, 31695, 30214,   232,   187,   177, 31931, 30785,\n","        30406, 29949,  7239, 30210, 31026, 30910, 30767,    13, 29930, 31267,\n","        30564, 30467, 31461, 31757, 30733, 30732, 30214, 31997, 30893, 31908,\n","          236,   169,   139, 30503, 30886,   235,   177,   177,    13, 29956,\n","          466,  3163,   310,   278, 17700,   232,   191,   189, 31268, 30214,\n","        30810, 31959,   232,   141,   173, 31074, 30413, 30437, 31175, 31072,\n","        29949,  7239, 30214, 31325, 30392, 30437,   235,   177,   172,   232,\n","          177,   134, 31100, 30666,   232,   191,   189, 30257, 30503, 31566,\n","          233,   182,   158, 30267, 31751, 30539, 30931,   236,   133,   131,\n","        31088, 30564, 30467, 31125, 31267, 30810, 31959,   232,   141,   173,\n","        31074, 31666, 31125, 31267, 29949,  7239, 30210, 30910, 31599, 30267,\n","            2,  -100], device='cuda:0')]\u001b[0m\n","\u001b[32m2023-12-18 16:20:39.054\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1337\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]: <s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 这是我个人写作风格的一个例子。\n","我有四把吉他，其中两把是钢琴弦，另外两把则是尼龙弦。我一直自己换弦，认为自己非常熟练。\n","我正准备换我那把古典西班牙吉他的弦。我有一套萨瓦瑞兹的弦，这是我多年来一直使用的品牌。\n","我注意到产品评论中有一个提示，说安装后不应该剪断弦线。我一直在调好弦后剪断弦线。然而，我发现这些弦比较短，所以刚装上之后不会有太多的剩余部分。这是为什么呢？是为了防止剪断吗？这些年来我一直都搞错了吗？剪断弦线有什么问题吗？\n","请根据结构和效果评估我 基于您提供的简要样本，您的写作风格似乎清晰有序。您有效地描述了你的情境并提出了具体问题。您还提供了相关的背景信息以为您的问题提供上下文。总的来说，您的写作风格似乎有效地传达了您的</s><unk>\u001b[0m\n","\u001b[32m2023-12-18 16:20:39.090\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1340\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]: <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> 基于您提供的简要样本，您的写作风格似乎清晰有序。您有效地描述了你的情境并提出了具体问题。您还提供了相关的背景信息以为您的问题提供上下文。总的来说，您的写作风格似乎有效地传达了您的</s><unk>\u001b[0m\n","  0% 0/249 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.0, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}\n","{'loss': 0.0, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.04}\n","{'loss': 0.0, 'learning_rate': 1.940677966101695e-05, 'epoch': 0.08}\n","{'loss': 0.0, 'learning_rate': 1.8559322033898307e-05, 'epoch': 0.12}\n","{'loss': 0.0, 'learning_rate': 1.7711864406779662e-05, 'epoch': 0.16}\n","{'loss': 0.0, 'learning_rate': 1.6864406779661018e-05, 'epoch': 0.2}\n"," 20% 50/249 [00:38<02:29,  1.33it/s]\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                    \n","\u001b[A{'eval_loss': nan, 'eval_runtime': 0.5478, 'eval_samples_per_second': 18.256, 'eval_steps_per_second': 5.477, 'epoch': 0.2}\n"," 20% 50/249 [00:39<02:29,  1.33it/s]\n","100% 3/3 [00:00<00:00, 10.33it/s]\u001b[A\n","{'loss': 0.0, 'learning_rate': 1.6016949152542373e-05, 'epoch': 0.24}\n","{'loss': 0.0, 'learning_rate': 1.5169491525423729e-05, 'epoch': 0.28}\n","{'loss': 0.0, 'learning_rate': 1.4322033898305086e-05, 'epoch': 0.32}\n","{'loss': 0.0, 'learning_rate': 1.3474576271186442e-05, 'epoch': 0.36}\n","{'loss': 0.0, 'learning_rate': 1.2627118644067797e-05, 'epoch': 0.4}\n"," 40% 100/249 [01:16<01:53,  1.32it/s]\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': nan, 'eval_runtime': 0.5114, 'eval_samples_per_second': 19.554, 'eval_steps_per_second': 5.866, 'epoch': 0.4}\n"," 40% 100/249 [01:17<01:53,  1.32it/s]\n","100% 3/3 [00:00<00:00, 10.38it/s]\u001b[A\n","{'loss': 0.0, 'learning_rate': 1.1779661016949153e-05, 'epoch': 0.44}\n","{'loss': 0.0, 'learning_rate': 1.0932203389830509e-05, 'epoch': 0.48}\n","{'loss': 0.0, 'learning_rate': 1.0084745762711864e-05, 'epoch': 0.52}\n","{'loss': 0.0, 'learning_rate': 9.237288135593222e-06, 'epoch': 0.56}\n","{'loss': 0.0, 'learning_rate': 8.389830508474577e-06, 'epoch': 0.6}\n"," 60% 150/249 [01:54<01:15,  1.31it/s]\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': nan, 'eval_runtime': 0.5114, 'eval_samples_per_second': 19.553, 'eval_steps_per_second': 5.866, 'epoch': 0.6}\n"," 60% 150/249 [01:55<01:15,  1.31it/s]\n","100% 3/3 [00:00<00:00, 10.38it/s]\u001b[A\n","{'loss': 0.0, 'learning_rate': 7.542372881355933e-06, 'epoch': 0.64}\n","{'loss': 0.0, 'learning_rate': 6.694915254237288e-06, 'epoch': 0.68}\n","{'loss': 0.0, 'learning_rate': 5.847457627118645e-06, 'epoch': 0.72}\n","{'loss': 0.0, 'learning_rate': 5e-06, 'epoch': 0.76}\n","{'loss': 0.0, 'learning_rate': 4.152542372881356e-06, 'epoch': 0.8}\n"," 80% 200/249 [02:33<00:37,  1.32it/s]\n","  0% 0/3 [00:00<?, ?it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': nan, 'eval_runtime': 0.5107, 'eval_samples_per_second': 19.583, 'eval_steps_per_second': 5.875, 'epoch': 0.8}\n"," 80% 200/249 [02:33<00:37,  1.32it/s]\n","100% 3/3 [00:00<00:00, 10.38it/s]\u001b[A\n","{'loss': 0.0, 'learning_rate': 3.305084745762712e-06, 'epoch': 0.84}\n","{'loss': 0.0, 'learning_rate': 2.457627118644068e-06, 'epoch': 0.88}\n","{'loss': 0.0, 'learning_rate': 1.6101694915254237e-06, 'epoch': 0.92}\n","{'loss': 0.0, 'learning_rate': 7.627118644067798e-07, 'epoch': 0.96}\n","{'train_runtime': 190.2911, 'train_samples_per_second': 5.218, 'train_steps_per_second': 1.309, 'train_loss': 0.0, 'epoch': 1.0}\n","100% 249/249 [03:10<00:00,  1.31it/s]\n","***** train metrics *****\n","  epoch                    =        1.0\n","  train_loss               =        0.0\n","  train_runtime            = 0:03:10.29\n","  train_samples            =       1000\n","  train_samples_per_second =      5.218\n","  train_steps_per_second   =      1.309\n","\u001b[32m2023-12-18 16:23:49.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1357\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 190.2911, 'train_samples_per_second': 5.218, 'train_steps_per_second': 1.309, 'train_loss': 0.0, 'epoch': 1.0, 'train_samples': 1000}\u001b[0m\n","\u001b[32m2023-12-18 16:23:49.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1358\u001b[0m - \u001b[1mSaving model checkpoint to outputs-sft-v1\u001b[0m\n","\u001b[32m2023-12-18 16:23:49.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1366\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n","100% 3/3 [00:00<00:00,  8.03it/s]\n","***** eval metrics *****\n","  epoch                   =        1.0\n","  eval_loss               =        nan\n","  eval_runtime            = 0:00:00.51\n","  eval_samples            =         10\n","  eval_samples_per_second =     19.493\n","  eval_steps_per_second   =      5.848\n","  perplexity              =        nan\n","\u001b[32m2023-12-18 16:23:50.511\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m1379\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': nan, 'eval_runtime': 0.513, 'eval_samples_per_second': 19.493, 'eval_steps_per_second': 5.848, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': nan}\u001b[0m\n"]}]},{"cell_type":"code","source":["%ls -lh outputs-sft-v1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XHQfiUMYtTHC","executionInfo":{"status":"ok","timestamp":1702916701436,"user_tz":-480,"elapsed":529,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"08ed1e71-5674-48a6-8a34-a6f3b1a7371d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 39M\n","-rw------- 1 root root  640 Dec 18 16:23 adapter_config.json\n","-rw------- 1 root root  39M Dec 18 16:23 adapter_model.safetensors\n","-rw------- 1 root root  348 Dec 18 16:23 all_results.json\n","-rw------- 1 root root  191 Dec 18 16:23 eval_results.json\n","-rw------- 1 root root 5.0K Dec 18 16:23 README.md\n","drwx------ 4 root root 4.0K Dec 18 16:20 \u001b[0m\u001b[01;34mruns\u001b[0m/\n","-rw------- 1 root root  438 Dec 18 16:23 special_tokens_map.json\n","-rw------- 1 root root 1.9K Dec 18 16:23 tokenizer_config.json\n","-rw------- 1 root root 489K Dec 18 16:23 tokenizer.model\n","-rw------- 1 root root 4.2K Dec 18 16:23 trainer_state.json\n","-rw------- 1 root root  177 Dec 18 16:23 train_results.json\n"]}]},{"cell_type":"code","source":["!python merge_peft_adapter.py --model_type llama \\\n","    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MD4QSPV-tfHm","executionInfo":{"status":"ok","timestamp":1702916775686,"user_tz":-480,"elapsed":70119,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"ccda5534-92f3-48cd-fdf0-5f066dcf6d28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(model_type='llama', base_model='merged-pt', tokenizer_path=None, lora_model='outputs-sft-v1', resize_emb=False, output_dir='./merged-sft')\n","Base model: merged-pt\n","LoRA model: outputs-sft-v1\n","Loading LoRA for causal language model\n","The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n","Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.63s/it]\n","Merging with merge_and_unload...\n","Saving to Hugging Face format...\n","Done! model saved to ./merged-sft\n"]}]},{"cell_type":"code","source":["%ls -lh merged-sft/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HARKGm0EtlCh","executionInfo":{"status":"ok","timestamp":1702916871433,"user_tz":-480,"elapsed":1365,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"65ab3a62-1221-40bf-e058-561d008e2e4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 13G\n","-rw------- 1 root root  641 Dec 18 16:25 config.json\n","-rw------- 1 root root  183 Dec 18 16:25 generation_config.json\n","-rw------- 1 root root 4.6G Dec 18 16:25 pytorch_model-00001-of-00003.bin\n","-rw------- 1 root root 4.7G Dec 18 16:26 pytorch_model-00002-of-00003.bin\n","-rw------- 1 root root 3.4G Dec 18 16:26 pytorch_model-00003-of-00003.bin\n","-rw------- 1 root root  24K Dec 18 16:26 pytorch_model.bin.index.json\n","-rw------- 1 root root  414 Dec 18 16:25 special_tokens_map.json\n","-rw------- 1 root root 1.8K Dec 18 16:25 tokenizer_config.json\n","-rw------- 1 root root 489K Dec 18 16:25 tokenizer.model\n"]}]},{"cell_type":"code","source":["%cat merged-sft/config.json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKgXw3jHtmLc","executionInfo":{"status":"ok","timestamp":1702916874450,"user_tz":-480,"elapsed":501,"user":{"displayName":"Qixuan Guo","userId":"14824551328159395161"}},"outputId":"188e55e9-dd4e-40a9-cd67-7eac7a0f8250"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"_name_or_path\": \"merged-pt\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n"]}]},{"cell_type":"markdown","source":["## Test"],"metadata":{"id":"82gOxetDv0mZ"}},{"cell_type":"code","source":["!python inference.py --model_type llama --base_model merged-sft --interactive"],"metadata":{"id":"AKHvK68wv2Ij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HJMtMAnhwW0a"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8524a0793e4e4cf394490426235d0a93":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_8103115877cd4d3fba70170198736c42","IPY_MODEL_b89ec9fb4ffa488f93350ba5e8347250","IPY_MODEL_6828fe4162a8445a9a16ec5ba49eabd8","IPY_MODEL_fbabfe3cb8594f109e95c51f11ad7749"],"layout":"IPY_MODEL_cd76cf90a4914fe8a2b4c9c431dbbfcf"}},"474ab65ffb6c4994a63749a4b9f19360":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73521d4abc9d406497be0e27a9406d27","placeholder":"​","style":"IPY_MODEL_e85ca530a82d4827b493a2799b2708fa","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"b65b89bc46ea4c258c92a8e6eaf7e918":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_6d2e6ef8c94e477e866d671c3dde9b67","placeholder":"​","style":"IPY_MODEL_8816470873b645408dfed9ded0b3f996","value":""}},"2c9e33e6b1d84e22827e396417805651":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_c1e99b34729e49d18d99c10b220bbcc6","style":"IPY_MODEL_38435d19b8ff48178f474a1448c8f590","value":true}},"f39d10339e3b498eaa6eac38e31c73f1":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_1ae4d261b135441797e026b895e368f1","style":"IPY_MODEL_955428ed5bd345369805500c5352406c","tooltip":""}},"5d27d063ae6f42bea369d1cdd1e812e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e3383b48062481fa4e172db7a036b4c","placeholder":"​","style":"IPY_MODEL_3f0f9a1a426e4ad789269cb421868e72","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"cd76cf90a4914fe8a2b4c9c431dbbfcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"73521d4abc9d406497be0e27a9406d27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e85ca530a82d4827b493a2799b2708fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d2e6ef8c94e477e866d671c3dde9b67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8816470873b645408dfed9ded0b3f996":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1e99b34729e49d18d99c10b220bbcc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38435d19b8ff48178f474a1448c8f590":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ae4d261b135441797e026b895e368f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"955428ed5bd345369805500c5352406c":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"2e3383b48062481fa4e172db7a036b4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f0f9a1a426e4ad789269cb421868e72":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"682dd29778ff4a6e893187091da9cd57":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5c4ef4280f0433faf6ae5a10e09ac3f","placeholder":"​","style":"IPY_MODEL_c256819efe85444a95670f62c3b1eba1","value":"Connecting..."}},"d5c4ef4280f0433faf6ae5a10e09ac3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c256819efe85444a95670f62c3b1eba1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8103115877cd4d3fba70170198736c42":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_919ef61b4a434579ae98a8c4cc8aa9be","placeholder":"​","style":"IPY_MODEL_3094efbba0ff46629d9270c068a0ede4","value":"Token is valid (permission: write)."}},"b89ec9fb4ffa488f93350ba5e8347250":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41fcf87d9d164482964ae65c2213eb82","placeholder":"​","style":"IPY_MODEL_418b6a60cc3742df979d26915e1d45d2","value":"Your token has been saved in your configured git credential helpers (store)."}},"6828fe4162a8445a9a16ec5ba49eabd8":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4be26495d6e4f67891ccab17b1e31dc","placeholder":"​","style":"IPY_MODEL_ddd28b5d183f45d1aed7beae143dae2f","value":"Your token has been saved to /root/.cache/huggingface/token"}},"fbabfe3cb8594f109e95c51f11ad7749":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9595fbbe125040078c9e51d500e371c1","placeholder":"​","style":"IPY_MODEL_edc598d5761243b8851e2014066aee7d","value":"Login successful"}},"919ef61b4a434579ae98a8c4cc8aa9be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3094efbba0ff46629d9270c068a0ede4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41fcf87d9d164482964ae65c2213eb82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"418b6a60cc3742df979d26915e1d45d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4be26495d6e4f67891ccab17b1e31dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddd28b5d183f45d1aed7beae143dae2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9595fbbe125040078c9e51d500e371c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edc598d5761243b8851e2014066aee7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}